review_id,review,sentence count,word count,avg. sentence length,avg. word length,vocab length,hedge words,non-hedge words,hedge ratio,sections covered (out of 14),aspects covered (out of 9),section distribution,aspect distribution
B1EA-M-0Z__R3,"This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery.  Pros: The result highlights an interesting relationship between deep nets and Gaussian processes. (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)  The paper is clear and very well written. The analysis of the phases in the hyperparameter space is interesting and insightful. On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work! Cons: Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article. I suggest using the same axis limits for all subplots in Fig 3.",9,171,24.428571428571427,5.1656441717791415,107,0,171,0.0,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}"
ByuP8yZRb__R2,"The below review addresses the first revision of the paper . The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out. The authors present a variant of the adversarial feature learning (AFL) approach by Edwards Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized.  The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model. The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. ,[none],[none]] While there is no requirement to have a distribution here—a simple loss term is sufficient—the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods. There are little theoretical contributions. Basically, instead of a single adversarial model—e.g., a single-layer NN or a multi-layer NN—the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good? In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.",20,402,17.47826086956522,5.605820105820106,191,3,399,0.007518796992481203,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}"
H1meywxRW__R1,"The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR. First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions. The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to reward the system for making partial matching predictions. Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems. An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance.  The paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers. It seems to me that this paper is a significant contribution to the field of question answering systems. ",7,182,22.75,5.398876404494382,114,1,181,0.0055248618784530384,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 1}"
NIPS_2018_184__R1,"This paper presents a new type of neural network where the transformation of each layer is conditioned on its input and its âdepthâ (computation path), generated from a very compact set of parameters. The network also can be viewed as a continuous generalization of neural decision graphs. In my opinion, this is a novel and creative work. The B-spline embedding trick is clever and efficient. The model sizes and runtime operations compared to those of CNNs of similar accuracies are truly impressive. Overall, this is an accept. My suggestions for the authors to improve their (already good) paper are as follows.  1) Test on more recent CNN architectures (e.g. VGG, if ResNet is not feasible). 2) Include SOTA results (accuracies and model sizes) under all datasets (itâs okay for a novel work to be not always better). 3) Consider using more datasets (e.g. CIFAR100 & SVHN, if ImageNet is not feasible). 4) Consider using existing methods to regularize the utilization of the splines (e.g. normalization), instead of adding an extra technique (differentiable quantization) to the already dense manuscript. (After rebuttal) I appreciate the authorsâ feedback and have updated my score accordingly.",9,190,12.666666666666666,5.227777777777778,119,0,190,0.0,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}"
NIPS_2018_184__R2,"The paper presents SplineNets that reformulate CNNs as neural decision graph using B-splines. It's comprised of four technical contributions, i.e., embedding trick, general formulation for a neural decision graph, a loss function of utilizing and specializing splines and a differentiable quantization method.  The idea is novel and sensible. It integrates the classic splines into convolutional neural networks which might be valuable for both of theoretical and practical aspects. However, the paper is difficult to read and follow. It is unclear how they implement in practice.  Some equations and symbols need to be clarified, e.g. the symbols in equation 7-10, and d indicates degree of the polynomials in line 115 but hyperparameter delta in line 235. I'm confused with several points.  a) what's the role of theta? Is the phi (i.e. D(x; theta) in line 137) analogous to attention (or gating) mechanism?  b)  Does the k/K in line 233 denote the knot number? Does the degree of the polynomial default to be k-1? Or is a fixed degree used? If larger K indicates higher-order smoothness in F-SN, does it mean linear spline works best as J-SN always works better than F-SN. b) how to demonstrate the dynamic (conditioned on the input) introduced by the method?  d) More experimental details need to be provided and clarified, e.g. optimization method, important hyperparameters for training, filter sizes.  I like the idea, but the current draft need to be improved. After reading the comments of other reviewers and rebuttal, I think it's a good work. I hope the final manuscript could include the updates present in the rebuttal.",11,263,16.4375,5.20164609053498,152,2,261,0.007662835249042145,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 4, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 2}"
NIPS_2018_184__R3,"In this paper, the authors introduce SplineNet to learn the conditional NNs. The authors propose a embedding trick to learn the embedded mainfolds and a regularized loss to encourage the maximum information gain. Through experiments on MNIST and CIFAR10, the authors demonstrate SplineNet achieve comparable performance with baselines with much less computation and parameters.  The idea is novel and the proposed method is sound. The ablative study and experimental analysis is helpful to better understand the working principles of the method. My questions on the experiments: --The authors only conduct experiments on small scale datasets (MNIST/CIFAR) using shallow networks (LeNet). How does it perform on larger datasets such as ImageNet using more complex networks such as DenseNet/ResNet. --Are the speed-up ratios presented in L249 theoretical or practical?                              Thanks for the feedback. I suggest the authors add the new experimental results in the final version. ",10,144,14.4,5.65,93,0,144,0.0,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 6, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}"
NIPS_2018_443__R1,"the paper introduces a differentiable clustering operation enabling neural methods acting on graphs to process information in a hierarchical fashion in analogy to CNN pooling layers (but preserving graph symmetry properties). The paper is very well written, the method is simple (which is not a bad thing!) and widely applicable, and the results are favourable. Building hierarchical representations is an important feature of CNNs and extending this to more general graph datatypes is clearly an important contribution. Overall I strongly support publication of this paper. I only have minor additional thoughts:  1. When the edges have labels (e.g. single/double/triple bonds in the chemistry example used in the motivation of the method), would equation (4) be extended to transform adjacency matrices for each edge type separately by S? In this case the link prediction objective will need altering. Maybe including an experiment with labelled edges would make the presentation of this otherwise simple method more complete?  2. You have to manually set the maximum number of clusters in each layer (please make this clearer near equation 6 - currently the only explicit reference to this fact is at the end of the caption of Fig2). It is nice that the GNN can learn to use less than the maximum number of clusters, but how sensitive are the results to the cluster count hyperparameters? Are the examples in Fig2 compelling because there is obvious clustering in the graphs? What does the clustering figure look like for data where the clusters are not so obvious? ",6,251,25.1,5.261410788381743,150,1,250,0.004,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 1}"
NIPS_2018_443__R2,"In this paper, the task of learning the hierarchical representation of graphs is achieved by stacking GNNs and Pooling layers.  The authors first specify the difficulty in stacking GNNs and Pooling layers then propose a differentiable pooling approach called DIFFPOOL to deal with this problem.  The paper is well-written and easily to follow.  Besides providing the theoretical formalization and analysis, the authors conducting sufficient experiments to proved that the proposed method outperformed state-of-the-art baselines in five popular tasks. While the number of classes increases to 11 in the dataset named by REDDIT-MULTI-12K, the accuracy (in percent) is quite low, at under 50%. The effects of DIFFPOOL are visualized clearly in Figure 2. A wide range of references are given. Although the intuition of using hierarchical representation in classification was appeared in previous research such as âConvolutional neural networks on graphs with fast localized spectral filteringâ [8], it is worth noting that the proposed model in this paper are able to learn the hierarchical representation end-to-end. Overall, the contribution to the literature of neural graph nets is clear and significant. ",8,179,17.9,5.682080924855492,107,0,179,0.0,8,4,"{'ABS': 0, 'INT': 3, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 1}"
NIPS_2018_443__R3,"This paper presents a hierarchical graph representation learning algorithm with the novel differentiable pooling layers.  Such differentiable pooling layers learn node assignments through standard GNNs, and then based on such node assignments pool and update node representations using such assignments. Experiments on a few benchmark graph classification tasks show consistent improvement over non-hierarchical baselines. The differentiable pooling idea proposed in this paper is a nice contribution, and I can see ideas based on this be very useful for learning hierarchical and more and more abstract representations for graphs by pooling on them.  The empirical results presented in the paper also seems convincing. One question I had about the proposed approach is that, the current approach would assign nodes with similar representations to similar higher level nodes.  This is quite different from the pooling in convnets, which groups pixels that are nearby, rather than grouping pixels that are similar.  Do you ever see non-local nodes be grouped together in DiffPool?  Is this type of pooling by similarity better or worse than pooling by locality?  Overall I quite liked this paper, the presentation is clear and the contribution is sound.  I would recommend to accept this paper.",9,195,21.666666666666668,5.670212765957447,111,1,194,0.005154639175257732,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}"
NIPS_2018_456__R1,- The problem is interesting and worth studying. - The author(s) propose a solution for general activation functions. This problem is still unexplored before.,3,22,7.333333333333333,6.0476190476190474,18,0,22,0.0,2,0,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 0, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}"
NIPS_2018_456__R2,"Summary: This paper proposes a general framework CROWN to efficiently certify robustness of neural networks with general activation functions. CROWN adaptively bounds a given activation function with linear and quadratic functions, so it can tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid, and arctan. Experimental results demonstrate the effectiveness, efficiency, and flexibility of the proposed framework. Quality: We are glad to find a work which conducts the efficiently certifying of the non-trivial robustness for general activation functions in neural networks. It is also interesting that the proposed framework can flexibly select upper bounds and lower bounds which can reduce the approximation error. This paper is of high quality.  The problem is well-motivated, and the experimental comparisons and analysis are sufficient and complete. The proofs of the theorems and bounds are reasonable.  Clarity: This paper is very well written and enjoyable to read. The theory analysis and proof are clear and easy to follow. Originality: This is an original research work especially of the robustness certification with the general activation functions, which may provide a good direction for the future research on this problem.  Overall impression, this is a high-quality paper with solid analysis, proof, and experimental results.   ",11,205,15.76923076923077,5.795,107,1,204,0.004901960784313725,9,3,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}"
NIPS_2018_456__R3,"The paper proposes a framework (CROWN) to efficiently compute for the minimum distortion for a given neural network using linear (and quadratic) upper bounds for the activation functions (that might not be piece-wise linear), generalizing some earlier approaches such as Fast-Lin. The experimental results demonstrate quality comparable with the earlier algorithms. The paper is well-written, interesting and about a very relevant topic, but the results might be to incremental for NIPS.  Minor comments: 97: It would be helpful to elaborate/highlight further what fast means (such as computational complexity). 97: What does certified mean in this context? Does the process result in some verifiable certificate?",4,104,20.8,5.838383838383838,73,2,102,0.0196078431372549,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}"
NIPS_2018_582__R1,"Thanks for the feedback.  To clarify, parallel NMT in the worked I pointed to get rids of auto-regressivity and not just recurrence. I agree it's orthogonal but could be the fastest NMT system --------- The paper develops a method for fast computation of top-k hypotheses (words in this paper) during decoding for natural language generation. In an offline pre-processing step, a graph is constructed with words as nodes in the graph and edges using parameters of a model trained in the usual way. The paper provides thorough experiments comparing to previous methods in the literature.  The speedup offered by these methods is impressive. Other Comments:    1) There has been lot of recent work on parallel neural machine translation (https://arxiv.org/abs/1711.02281 ; https://arxiv.org/abs/1805.11063).  Importantly, beam search is not required by these methods but computing the softmax is still necessary. It would be interesting to see how the proposed speedups compare to them and see if both these directions can be combined. 2) It would be useful to include decode time numbers in Table 1 3) Even though attention methods generally do not require k-best lists, there are some methods that do top-k attention instead of full attention (https://arxiv.org/abs/1607.01426). Having experiments on these models would make the paper even stronger. ",11,205,17.083333333333332,5.221105527638191,129,0,205,0.0,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}"
NIPS_2018_582__R2,"Update after author response: Thanks for the detailed response! It's a strong submission and I vote for an accept.              This paper aims to speed up the computation of the softmax over a large vocabulary, which is quite common in some NLP tasks like e.g., language modeling. Specifically, the proposed method formulates the problem into a nearest neighbor search in a small world graph, and applies a log time algorithm to find the approximate top K predictions. The resulting time complexity reduces to logarithmic in the vocabulary size in expectation, in contrast to the linear one in a standard softmax. The proposed method is empirically compared to both full softmax and a state-of-the-art baseline on language modeling and neural machine translation. Experimental results show that the method achieves significant speed-up, without sacrificing too much accuracy, and that it scales to larger-size models.  The paper is overall well-motivated and clearly-written, and I find it interesting to read. My major concern is perhaps how the method generalizes to training,  detailed below.  Concerns:  - Small world graph construction: It would be nice to see some discussions on the complexity of the graph construction step, and whether it could become a potential problem if the algorithm is applied to training. - Probabilities: Are there any good reasons to not report perplexities in the language modeling task? It appears to me that in a FGD, it is less straightforward to assign probabilities to the corpus. One thing on top of my head is to attribute most of the mass to the retrieved top K, with some additional tricks to avoid numerical issues. Have the authors tried this?  - The title is a bit confusing, since ``language models'' usually mean the language modeling task, but here the algorithm applies to many other tasks involving a large softmax.  - There are several statements that look like ``finding top K in sublinear time complexity,'' especially around lines 30--40. I find such statements a bit misleading, since essentially one would need approximations to achieve this.",14,331,23.642857142857142,5.336538461538462,179,4,327,0.012232415902140673,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 8, 'EXP': 7, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}"
NIPS_2018_582__R3,"This paper proposes to use small world graphs as data structures to embed words vectors into for a fast lookup of potentially relevant words that are then put into a softmax. It's a useful datastructure. Not quite machine learning... but useful for softmax classifications in MT and language modeling.  Wish it was trained on a fast and competitive language model and actually mention perplexity numbers. Ideally you can use: https://github.com/salesforce/awd-lstm-lm  On NMT, it would have been better to use a parallelizable transformer network also. In spite of the large number of words in a vocabulary, human brain is capable --> In spite of the large number of words in a vocabulary, THE human brain is capable  speedup while attaining the accuracy --> speedup while maintaining the accuracy",6,125,17.857142857142858,5.3277310924369745,79,0,125,0.0,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 5, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}"
