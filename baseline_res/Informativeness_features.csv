review_id,meta_decision,review,sentence count,word count,avg. sentence length,avg. word length,vocab length,hedge words,non-hedge words,hedge ratio,hedge_score,vader compound sentiment,noun_count,adj_count,verb_count,adverb_count,sections covered (out of 14),aspects covered (out of 9),section distribution,aspect distribution,ABS,INT,RWK,PDI,DAT,MET,EXP,RES,TNF,ANA,FWK,OAL,BIB,EXT,APR,NOV,IMP,CMP,PNF,REC,EMP,SUB,CLA,section score,aspect score,new info score
ICLR2018-B11bwYgfM-R1,,"The idea of using cross-task transfer performance to do task clustering is not new. Please refer to the paper ""Discovering structure in multiple learning tasks: The TC algorithm"" published in ICML 1996. One issue of the use of cross-task transfer performance to measure task relations is that it ignores the negative correlations between tasks, which is useful for learning from multiple tasks. For example, in binary classification tasks, a very small S_{ij} indicates that by changing the sign of the classification function these two tasks are useful to each other. So the use of cross-task transfer performance and the task clustering approach can only capture positive correlations between tasks but ignore the negative task relations which are also important to the sharing among tasks in multi-task learning. Problem (2) is identical to robust PCA and Theorem 3.1 is common in matrix completion literature. I don't see much novelty. Appendix A seems obvious but it cannot prove the validity of the assumption made in problem (2). Based on previous works such as ""Multi-task Sparse Structure Learning with Gaussian Copula Models"" and ""Learning Sparse Task Relations in Multi-Task Learning"", when the number of tasks is large, the task relation exhibits the sparse structure. I don't know whether the low-rank structure does exist in the cross-task transfer performance or not. The two parts in this paper are not new. The combination of the two parts seems a bit incremental and does not bring much novelty.",12,242,20.166666666666668,5.223175965665236,121,2,240,0.0083333333333333,0.0537190082644628,-0.7295,77,34,38,11,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,2,1,0,5,0,0,0,0,0,3,0,0,0,3,0,0,0,0,3,0,0,0.2868863796879095,0.2235563522609444,0.12507625505983952
ICLR2018-B11bwYgfM-R2,,"This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j). The matrix completion approach is based on robust PCA. When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds). This can be computationally demanding. The key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering. However, in MTL, we usually assume that there are not enough samples to learn each task, and so this performance matrix may not be reliable. There have been a number of MTL methods based on task clustering. For example, [1] A convex formulation for learning task relationships in multi-task learning (UAI) [2] A dirty model for multi-task learning (NIPS) [3] Clustered multi-task learning: A convex formulation (NIPS) [4] Convex multitask learning with flexible task clusters (ICML) [5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD) [6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD) In particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model. This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model).  However, a disadvantage of the proposed method is that it is a two-step approach (first perform task clustering, then re-learn the cluster weights), while [5] is not. For few-shot learning, the authors mentioned that the alpha's are adaptable parameters but did not mention how they are adapted. Experimental results are not convincing. - Comparison with existing clustered MTL methods mentioned above are missing. - As mentioned above, the proposed method can be computationally expensive (when used for MTL), but no timing results are reported. - As the authors mentioned in section 4.2, most of the tasks have a significant amount of training data (and single-task baselines achieve good results), and so this is not a good benchmark dataset for MTL.",19,402,25.125,5.311653116531165,158,2,400,0.005,0.0073891625615763,-0.2271,133,44,76,27,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 17, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,0,0,1,1,17,3,1,0,0,0,0,0,0,0,0,0,0,0,0,7,1,0,0.3613183225468291,0.22595396683128,0.1653929493767955
ICLR2018-B11bwYgfM-R3,,"The authors propose techniques for multitask and few shot learning, where the number of tasks is potentially very large, and the different tasks might have different output spaces. Prior techniques which can address some of these aspects do not necessarily work with deep learning, which is a key focus of the paper. The authors suggest computing a similarity matrix amongst the tasks. Given such a matrix, they propose to do multitask learning by clustering the similarity matrix, and learning a single model for each cluster. If the tasks in a cluster have different output spaces, then a separate output layer is learned for each task in the cluster following a common encoding module. To deal with the large number of tasks, the authors further propose computing a few randomly sampled entries of the similarity matrix, and then using ideas from robust matrix completion to induce the full matrix. The resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning, as well as two datasets from intent classification in dialog systems. I think there are some interesting ideas in this paper, and the use of matrix completion techniques to deal with a large number of tasks is nice. But I believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form. 1. The prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used (see e.g. Kang et al. which is cited, as well as Kshirsagar et al. in ECML 2017 as two examples). In this paper, on the other hand, the clusters are obtained in a manner which only accounts for pairwise similarities of tasks, using a pairwise similarity metric which is quite different from how the cluster is eventually used. This seems quite suboptimal. 2. The pairwise similarity measure appears to be one that might have a high false negative rate. That is, it might rate many tasks as dissimilar even when they are not. This is because you train individual model on i and apply it to j. It is possible that this model does not do well, but there is an equally good model for i which also does well on j. Such a model would indeed be found if i and j are put in the same cluster, but the method would fail to do so, leading to high fragmentation. 3. I do not see how you apply the model from task i to task j when the two have different output spaces. Since this is a major motivation of the paper, I actually do not see how the setup makes sense! 4. It seems odd to put absolute errors on task j instead of regret to the model trained on j in the similarity matrix. 5. The inducing of edges in the Y matrix by comparing to a mean and standard deviation is completely baseless.  Without good reasoning from the authors, I see no reason why the entries in the row of a matrix should have a normal-like distribution. Furthermore, in the matrix completion scenario, you have O(log^2n) entries per row on average, which means with high probability few rows should have a constant number of entries. In this case, the means are standard deviations do not even make sense to me. At the very least, I would consider using regret to the model of the task, and compute some quantiles on that which is still suspect in the matrix completion setting. 6 .In the evaluation, why are just 12 tasks used in the Amazon dataset? Why don't you present evaluation results on all tasks in the multitask setting? 7. Why is average accuracy the right thing? If the error rates are different for different tasks, it is not sensible to measure raw accuracies. The authors also seem to miss a potentially relevant baseline in Cross-Stitch Networks (https://arxiv.org/abs/1604.03539) Besides these major issues, there are also a few minor issues I have with the paper.  I do not see why there's need for a proof for the matrix completion result.  This appears to be a direct application of Chandrasekaran et al, and in fact matrix completion has been used for clustering before (https://arxiv.org/abs/1104.4803). Given this, the presentation in the paper makes the idea look more novel than it is. I also think that the authors might benefit from dropping the whole few-shot learning angle here, and instead do a more thorough job of evaluating their multitask learning method.",35,763,19.56410256410257,4.860955056179775,278,11,752,0.014627659574468,0.0378590078328981,-0.7209,216,72,136,49,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 5, 'DAT': 3, 'MET': 23, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 24, 'SUB': 0, 'CLA': 0}",0,1,3,5,3,23,1,2,0,0,0,3,0,0,0,1,0,4,1,0,24,0,0,0.5775496941726226,0.4591009531174028,0.3237709866343145
ICLR2018-B12Js_yRb-R1,Accept,"Summary:  - This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. Contribution: - This paper proposes a new object counting module which operates on a graph of object proposals. Clarity: - The paper is well written and clarity is good. Figure 2 & 3 helps the readers understand the core algorithm. Pros: - De-duplication modules of inter and intra object edges are interesting. - The proposed method improves the baseline by 5% on counting questions. Cons: - The proposed model is pretty hand-crafted. I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016). - One major bottleneck of the model is that the proposals are not jointly finetuned. So if the proposals are missing a single object, this cannot really be counted. In short, if the proposals don't have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals. The paper didn't study what is the recall of the proposals and how sensitive the threshold is. - The paper doesn't study a simple baseline that just does NMS on the proposal domain. - The paper doesn't compare experiment numbers with (Chattopadhyay et al., 2017). - The proposed algorithm doesn't handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges). This is similar to a density map approach and the problem is that the model doesn't develop a notion of instance. - Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions.  - Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL   Conclusion: - I feel that the motivation is good, but the proposed model is too hand-crafted. Also, key experiments are missing: 1) NMS baseline 2) Comparison with VQA counting work  (Chattopadhyay et al., 2017). Therefore I recommend reject. References: - Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017. - Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016. Update: Thank you for the rebuttal. The paper is revised and I saw NMS baseline is added. I understood the reason not to compare with certain related work. The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy. However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6.",30,447,14.9,5.240196078431373,210,1,446,0.0022421524663677,0.0149253731343283,0.9741,129,51,99,32,11,7,"{'ABS': 0, 'INT': 2, 'RWK': 10, 'PDI': 3, 'DAT': 0, 'MET': 12, 'EXP': 4, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 4, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 5, 'PNF': 1, 'REC': 3, 'EMP': 11, 'SUB': 10, 'CLA': 1}",0,2,10,3,0,12,4,3,1,0,1,4,2,1,0,0,1,5,1,3,11,10,1,0.7898535493299337,0.7854656918741908,0.6279043628531911
ICLR2018-B12Js_yRb-R2,Accept,"Summary  - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism. The authors propose a differentiable counting component, which explicitly counts the number of objects. Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals. In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset. Strengths  - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly. - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model. - The proposed model can be easily applied to any VQA model using soft attention. - The paper is well written and the contribution is clear. Weaknesses  - Although the proposed model is helpful to model counting information in VQA, it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. - Qualitative examples of intermediate values in counting component--adjacency matrix (A), distance matrix (D) and count matrix (C)--need to be presented to show the contribution of each part, especially in the real examples that are not compatible with the strong assumptions in modeling counting component. Comments  - It is not clear if the value of count c is same with the final answer in counting questions.   ",12,243,20.25,5.757446808510639,125,0,243,0.0,0.0232558139534883,0.9816,83,24,46,11,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 3, 'DAT': 1, 'MET': 4, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,2,1,3,1,4,3,1,0,0,0,3,0,0,0,0,0,1,0,0,6,0,1,0.5725864956224708,0.3364431205075482,0.2881125247100452
ICLR2018-B12Js_yRb-R3,Accept,"This paper tackles the object counting problem in visual question answering. It is based on the two-stage method that object proposals are generated from the first stage with attention. It proposes many heuristics to use the object feature and attention weights to find the correct count. In general, it treats all object proposals as nodes on the graph. With various agreement measures, it removes or merges edges and count the final nodes. The method is evaluated on one synthetic toy dataset and one VQA v2 benchmark dataset. The experimental results on counting are promising. Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems. Additionally, this method is built on a series of heuristics without sound theoretically justification, and these heuristics cannot be easily adapted to other machine learning applications. I thus believe the overall contribution is not sufficient for ICLR. Pros: 1. Well written paper with clear presentation of the method.  2. Useful for object counting problem. 3. Experimental performance is convincing. Cons: 1. The application range of the method is very limited. 2. The technique is built on a lot of heuristics without theoretical consideration. Other comments and questions:  1. The determinantal point processes [1] should be able to help with the correct counting the objects with proper construction of the similarity kernel. It may also lead to simpler solutions. For example, it can be used for deduplication using A (eq 1) as the similarity matrix. 2. Can the author provide analysis on scalability the proposed method? When the number of objects is very large, the graph could be huge. What are the memory requirements and computational complexity of the proposed method? In the end of section 3, it mentioned that without normalization, the method will not scale to an arbitrary number of objects. I think that it will only be a problem for extremely large numbers. I wonder whether the proposed method scales. 3. Could the authors provide more insights on why the structured attention (etc) did not significantly improve the result?  Theoritically, it solves the soft attention problems. 4. The definition of output confidence (section 4.3.1) needs more motivation and theoretical justification. [1] Kulesza, Alex, and Ben Taskar. Determinantal point processes for machine learning. Foundations and Trendsu00ae in Machine Learning 5.2u20133 (2012): 123-286.",28,390,10.833333333333334,5.5,190,2,388,0.0051546391752577,0.0203562340966921,0.8915,121,42,65,18,10,5,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 20, 'EXP': 2, 'RES': 5, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 16, 'SUB': 3, 'CLA': 1}",0,2,0,2,1,20,2,5,1,1,0,2,1,0,1,0,0,0,2,0,16,3,1,0.7194786054093912,0.5651598390186464,0.4563820991840862
ICLR2018-B13EC5u6W-R1,Reject,"* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label. As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc). * This is quite an interesting paper with a sensible goal. It seems like the method could be more informative than the other methods. However, there are quite a number of problems, as explained below.  * The explanation of eqs 1 and 2 is quite poor. alpha in (1) seems to be gamma in Alg 1 (line 5). L_target is a target objective which can be a negative class probability .. this assumes that the example is a positive class. Could we not also apply this to negative examples? or in the case of heart failure, predicted BNP level -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level? Also specific details should be reserved until a general explanation of the problem has been made. * The trade-off parameter gamma is a fiddle factor -- how was this set for the lung image and MNIST examples? Were these values different? * In typical ICLR style the authors use a deep network to learn the encoder and decoder networks. It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings. * The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s? Why have you not reported results for these kinds of tasks? * Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side). The reconstructions show poor detail relative to the originals.  This loss of detail could be a limitation. * A serious problem with the method is that we are asked to evaluate it in terms of images like Fig 4 or Fig 8. A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for. * The references section is highly inadequate -- no venues of publication are given. If these are arXiv give the proper ref. Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014. * Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold. * Text:  sec 2 para 4. reconstruction loss on the validation set was similar to the reconstruction loss on the validation set. ?? * p 3 bottom -- give size of dataset  * p 5 AUC curve -> ROC curve  * p 6 Fig 4 use text over each image to better specify the details given in the caption.    ",26,466,15.533333333333331,4.847775175644028,229,3,463,0.0064794816414686,0.0241448692152917,-0.5452,139,49,87,18,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 10, 'EXP': 2, 'RES': 5, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 3, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 3, 'CLA': 0}",0,1,0,1,3,10,2,5,2,0,0,2,3,0,1,0,0,0,0,0,14,3,0,0.64558355950197,0.3416321330029349,0.32624280833088154
ICLR2018-B13EC5u6W-R2,Reject,"The main contribution of the paper is a method that provides visual explanations of classification decisions. The proposed method uses   - a generator trained in a GAN setup  - an autoencoder to obtain a latent space representation  - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image (or rather the reconstruction of it).  The method is evaluated on a medical images dataset and some additional demonstration on MNIST is provided. - The paper proposes a (I believe) novel method to obtain visual explanations. The results are visually compelling although most results are shown on a medical dataset - which I feel is very hard for most readers to follow. The MNIST explanations help a lot. It would be great if the authors could come up with an additional way to demonstrate their method to the non-medical reader. - The paper shows that the results are plausible using a neat trick. The authors train their system with the testdata included which leads to very different visualizations. It would be great if this analysis could be performed for MNIST as well. From the related work, it would be nice to mention that generative models (p(x|c)) also often allow for explaining their decisions, e.g. the work by Lake and Tenenbaum on probabilistic program induction. Also, there is the work by Hendricks et al on Generating Visual Explanations. This should probably also be referenced. minor comments:  - some figures with just two parts are labeled from left to right - it would be better to just write left: ... right: ... n- figure 2: do these images correspond to each other? If yes, it would be good to show them pairwise. - figure 5: please explain why the saliency map is relevant. This looks very noisy and non-interesting.  ",17,298,14.9,5.095406360424028,163,2,296,0.0067567567567567,0.0157232704402515,0.9864,73,35,62,16,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 5, 'MET': 6, 'EXP': 2, 'RES': 2, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,2,0,5,6,2,2,4,1,0,1,0,0,0,1,0,0,4,0,7,0,0,0.5731161744259907,0.3372497847138012,0.29079395504844535
ICLR2018-B13EC5u6W-R3,Reject,"The authors address two important issues: semi-supervised learning from relatively few labelled training examples in the presence of many unlabelled examples, and visual rationale generation: explaining the outputs of the classifiier by overlaing a visual rationale on the original image. This focus is mainly on medical image classification but the approach could potentially be useful in many more areas. The main idea is to train a GAN on the unlabeled examples to create a mapping from a lower-dimensional space in which the input features are approximately Gaussian, to the space of images, and then to train an encoder to map the original images into this space minimizing reconstruction error with the GAN weights fixed. The encoder is then used as a feature extractor for classification and regression of targets (e.g. heard disease). The visual rationales are generated by optimizing the encoded representation to simultaneously reconstruct an image close to the original and to minimize the probability of the target class. This gives an image that is similar to the original but with features that caused the classification of the disease removed. The resulting image can be subtracted from the original encoding to highlight problematic areas. The approach is evaluated on an in-house dataset and a public NIH dataset, demonstrating good performance, and illustrative visual rationales are also given for MNIST. The idea in the paper is, to my knowledge, novel, and represents a good step toward the important task of generating interpretable visual rationales. There are a few limitations, e.g. the difficulty of evaluating the rationales, and the fact that the resolution is fixed to 128x128 (which means discarding many pixels collected via ionizing radiation), but these are readily acknowledged by the authors in the conclusion. Comments: 1) There are a few details missing, like the batch sizes used for training (it is difficult to relate epochs to iterations without this). Also, the number of hidden units in the 2 layer MLP from para 5 in Sec 2. 2) It would be good to include PSNR/MSE figures for the reconstruction task (fig 2) to have an objective measure of error. 3) Sec 2 para 4: the reconstruction loss on the validation set was similar to the reconstruction loss on the validation set -- perhaps you could be a little more precise here. E.g. learning curves would be useful. 4) Sec 2 para 5: paired with a BNP blood test that is correlated with heart failure I suspect many readers of ICLR, like myself, will not be well versed in this test, correlation with HF, diagnostic capacity, etc., so a little further explanation would be helpful here. The term correlated is a bit too broad, and it is difficult for a non-expert to know exactly how correlated this is. It is also a little confusing that you begin this paragraph saying that you are doing a classification task, but then it seems like a regression task which may be postprocessed to give a classification. Anyway, a clearer explanation would be helpful. Also, if this test is diagnostic, why use X-rays for diagnosis in the first place? 5) I would have liked to have seen some indicative times on how long the optimization takes to generate a visual rationale, as this would have practical implications.. 6) Sec 2 para 7: L_target is a target objective which can be a negative class probability or in the case of heart failure, predicted BNP level -- for predicted BNP level, are you treating this as a probability and using cross entropy here, or  mean squared error?. 7) As always, it would be illustrative if you could include some examples of failure cases, which would be helpful both in suggesting ways of improving the proposed technique, and in providing insight into where it may fail in practical situations.",22,630,25.2,5.158432708688245,268,4,626,0.0063897763578274,0.0205371248025276,0.982,165,72,108,25,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 1, 'MET': 15, 'EXP': 8, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 18, 'SUB': 4, 'CLA': 0}",0,0,0,4,1,15,8,1,0,0,0,0,0,0,0,0,0,0,0,0,18,4,0,0.3614268168633704,0.2331155281395172,0.1644551652629094
ICLR2018-B13njo1R--R1,Accept,"This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially. The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task. The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously. Questions: - When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network? - What data do you use for the distillation? Section 4.1 statesWe use a method similar to the DAGGER algorithm, but what is your method. If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory? - I do not understand the purpose of input injection nor where it is used in the paper.  Strengths: - The method is simple but novel. The results support the method's utility. - The testbed is nice; the tasks seem significantly different from each other. It seems that no reward shaping is used. - Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker. Weaknesses: - Figure 2: the plots are too small. - Distilling may hurt performance ( Figure 2.d) - The method lacks details (see Questions above) - No comparisons with prior work are provided. The paper cites many previous approaches to this but does not compare against any of them. - A second testbed (such as navigation or manipulation) would bring the paper up a notch. In conclusion, the paper's approach to multitask learning is a clever combination of prior work. The method is clear but not precisely described. The results are promising. I think that this is a good approach to the problem that could be used in real-world scenarios. With some filling out, this could be a great paper.",25,318,16.736842105263158,5.03030303030303,162,3,315,0.0095238095238095,0.0181268882175226,0.968,88,30,65,12,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 16, 'EXP': 0, 'RES': 5, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 3, 'REC': 0, 'EMP': 17, 'SUB': 3, 'CLA': 1}",0,0,3,1,0,16,0,5,3,0,0,1,0,0,0,1,0,2,3,0,17,3,1,0.4327546895752175,0.6770716390455938,0.3076970766122433
ICLR2018-B13njo1R--R2,Accept,"This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum. Pros: - PLAID masters several distinct tasks in sequence, building up ""skills"" by learning ""related"" tasks of increasing difficulty. - Although the main focus of this paper is on continual learning of ""related"" tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain. Cons: - PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case. - Are the experiments single runs? Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour. Clarifications: - What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks? - How were the network architecture and network size chosen, especially for the multitasker? Would policies generalize to later tasks better with larger, or smaller networks? - Was any kind of regularization used, how does it influence task performance vs. transfer? - I find figure 1 (c) somewhat confusing. Is performance maintained only on the last 2 tasks, or all previously seen tasks? That's what the figure suggests at first glance, but that's a different goal compared to the learning strategies described in figures 1 (a) and (b). ",13,259,28.77777777777778,5.457831325301205,153,2,257,0.0077821011673151,0.0261194029850746,0.3296,84,30,45,13,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,8,2,0,2,0,0,0,0,0,0,0,0,0,1,0,11,1,0,0.3590036956917651,0.339552907681763,0.18068698825062107
ICLR2018-B13njo1R--R3,Accept,"Hi,   This was a nice read. I think overall it is a good idea. But I find the paper lacking a lot of details and to some extend confusing. Here are a few comments that I have:  Figure 2 is very confusing for me.  Please first of all make the figures much larger. ICLR does not have a strict page limit, and the figures you have are hard to impossible to read. So you train in (a) on the steps task until 350k steps? Is (b), (d),(c) in a sequence or is testing moving from plain to different things? The plot does not explicitly account for the distillation phase. Or at least not in an intuitive way. But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted. Going then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines. Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all. So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough). Where exactly is input injection used? Is it experiments from figure 3. What input is injecting? What do you do when you go back to the task that doesn't have the input, feed 0? What happens if 0 has semantics ?  Please say in the main text that details in terms of architecture and so on are given in the appendix. And do try to copy a bit more of them in the main text where reasonable. What is the role of PLAID? Is it to learn a continual learning solution? So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills? Will this be feasible? Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning? Or is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)? And even then maximize transfer with respect to what? Frames required from the environment?  If that are you reusing the frames you used during training to distill? Can we afford to keep all of those frames around? If not we have to count the distillation frames as well. Also more baselines are needed. A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way. Or at least have a good argument of why this is suboptimal compared to PLAID. I think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score. But I want to make sure the authors put a bit more effort into cleaning up the paper, making it more clear and easy to read. Providing at least one more baseline (if not more considering the other things cited by them).   ",38,568,24.695652173913043,4.456603773584906,242,4,564,0.0070921985815602,0.0086505190311418,0.9943,110,52,126,38,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 20, 'EXP': 6, 'RES': 0, 'TNF': 5, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 1, 'EMP': 27, 'SUB': 5, 'CLA': 0}",0,0,4,2,0,20,6,0,5,0,0,4,0,0,0,0,0,1,4,1,27,5,0,0.4342862695278665,0.5723378616661687,0.2807294141654612
ICLR2018-B14TlG-RW-R1,Accept,"Summary:  This paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention. The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference. Authors also propose a paraphrasing based data augmentation method which helps in improving the performance. Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference. My Comments:  The proposed model is convincing and the paper is well written. 1. Why don't you report your model performance without data augmentation in Table 1? Is it because it does not achieve SOTA? The proposed data augmentation is a general one and it can be used to improve the performance of other models as well. So it does not make sense to compare your model + data augmentation against other models without data augmentation. I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models. 2. Can you mention your leaderboard test accuracy in the rebuttal?   3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset. That will show the generality of the proposed architecture. Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision. 4. Are you willing to release your code to reproduce the results? n  Minor comments:  1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent. n2. In the first contribution bullet point, ""that exclusive built upon"" should be ""that is exclusively built upon"". ",17,280,13.333333333333334,5.245210727969349,140,1,279,0.003584229390681,0.0138888888888888,0.9796,76,26,56,11,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 3, 'DAT': 3, 'MET': 6, 'EXP': 2, 'RES': 5, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 5, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 1}",0,2,1,3,3,6,2,5,1,0,0,1,0,0,0,0,1,3,5,0,6,2,1,0.6446334069214267,0.6703637311587962,0.4572163242536493
ICLR2018-B14TlG-RW-R2,Accept,"This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally. Firstly, I suggest the authors rewrite the end of the introduction. The current version tends to mix everything together and makes the misleading claim. When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1. But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method. Secondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows: (1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC. The novelty is limited but it is a good idea to speed up the RC models. However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs. Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct. (2) I feel that the model design is the main reason for the good overall RC performance. However, in the paper there is no motivation about why the architecture was designed like this. Moreover, the whole model architecture is only evaluated on the SQuAD dataset. As a result, it is not convincing that the system design has good generalization. If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself. (3) I like the idea of data augmentation with paraphrasing. Currently, the improvement is only marginal, but there seems many other things to play with. For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation. n I am looking forward to the test performance of this work on SQuAD.",21,417,23.166666666666668,5.103274559193955,186,1,416,0.0024038461538461,0.0191846522781774,0.9927,122,42,67,24,10,5,"{'ABS': 0, 'INT': 4, 'RWK': 1, 'PDI': 2, 'DAT': 7, 'MET': 9, 'EXP': 1, 'RES': 8, 'TNF': 0, 'ANA': 2, 'FWK': 3, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 3, 'CLA': 1}",0,4,1,2,7,9,1,8,0,2,3,2,0,0,0,3,3,0,0,0,12,3,1,0.7171079657260124,0.5627967288214033,0.45433666932967437
ICLR2018-B14TlG-RW-R3,Accept,"This paper presents a reading comprehension model using convolutions and attention. This model does not use any recurrent operation but it is not per se simpler than a recurrent model. Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation. On SQuAD dataset, their results show some small improvements using the proposed augmentation technique. Their best results, however, do not outperform the best results reported on the leader board. Overall, this is an interesting study on SQuAD dataset. I would like to see results on more datasets and more discussion on the data augmentation technique. At the moment, the description in section 3 is fuzzy in my opinion.  Interesting information could be: - how is the performance of the NMT system? - how many new data points are finally added into the training data set? - what do 'data aug' x 2 or x 3 exactly mean? ",11,154,17.11111111111111,5.287671232876712,97,0,154,0.0,0.0062893081761006,0.962,47,20,27,7,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 7, 'MET': 2, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,1,1,7,2,0,4,0,1,0,1,0,0,0,0,1,1,3,0,3,1,0,0.5720624136598452,0.5569226082728481,0.36499417710341103
ICLR2018-B14uJzW0b-R1,Reject,"Summary:  The paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper - as I describe below, it was not clear at all the setting of the problem - if I'm mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem). Given this architecture, the authors focus on characterizing the objective landscape of such a problem. The techniques used depend on previous work. According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit. Originality:  The paper heavily depends on the approach followed by Brutzkus and Globerson, 2017. To this end, slighly novel. Importance:  Understanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work. I would say that the topic is an important one. Presentation/Clarity:  To the best of my understanding, the paper has some misconceptions. The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units. In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer? in total?). Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units. These are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K   2 RELU activations (however, that complicated my understanding on how it compares with state of the art). Another issue is the fact that, on my humble opinion, the main text looks like a long proof. It would be great to have more intuitions. Comments: 1. The paper mainly focuses on a specific problem instance, where the weight vectors are unit-normed and orthogonal to each other. While the authors already identify that this might be a restriction, it still does not lessen the fact that the configuration considered is a really specific one. 2. The paper reads like a collection of lemmas, with no verbose connection. It was hard to read and understand their value, just because mostly the text was structured as one lemma after the other. 3. It is not clear from the text whether the setting is already considered in Brutzkus and Globerson, 2017. Please clarify how your work is different/new from previous works. ",22,427,17.08,4.924242424242424,193,2,425,0.0047058823529411,0.0344036697247706,0.2933,117,45,61,18,8,5,"{'ABS': 1, 'INT': 2, 'RWK': 6, 'PDI': 3, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 3}",1,2,6,3,0,8,0,3,0,0,0,3,0,1,0,2,0,3,0,0,6,2,3,0.573928715791531,0.5592137220786981,0.3568499025899154
ICLR2018-B14uJzW0b-R2,Reject,"In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network. Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods. In general I found this paper clearly written and technically sound. I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures. Contribution:  As discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units. The analysis becomes considerably more complicated, and the contribution seems to be novel and significant. I am not sure why did the authors mentioned the work on over-parameterization though. It doesn't seem to be relevant to the results of this paper (because the NN architecture proposed in this paper is rather small). Comments on the Assumptions: - Please explain the motivation behind the standard Gaussian assumption of the input vector x. - Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^top w_2 0 (or the acute angle assumption in Section 6). Without extra justifications, it seems that the theoretical result only holds for an artificial problem setting. While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results. General Comment:   The technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs. From the analysis in the main paper, I believe the theoretical contribution is correct and sound. While I appreciate the technical contributions, in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples). Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector, and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems. ",20,445,24.72222222222222,5.392941176470588,195,2,443,0.0045146726862302,0.024390243902439,0.5701,124,72,57,26,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 8, 'TNF': 0, 'ANA': 6, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 4, 'CLA': 1}",0,1,2,1,0,6,1,8,0,6,0,2,0,0,0,1,0,2,0,0,14,4,1,0.5733189488599768,0.5640781942906686,0.3618139545528315
ICLR2018-B14uJzW0b-R3,Reject,"This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer. As a result, a gradient descent algorithm converges to the unique solution. This works address a conjecture proposed by Tian (2017). While it is clearly written, my main concern is whether this model is significant enough. The assumptions K 2 and v1 v2 1 reduces the difficulty of the analysis, but it makes the model considerably simpler than any practical setting.  ",6,81,13.5,5.347222222222222,58,0,81,0.0,0.0240963855421686,0.4678,20,10,12,6,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,0,0,1,0,1,0,1,0,2,0,0,0,0,1,0,0,0,3,0,1,0.4286168473794792,0.3345772482030192,0.21508735605921384
ICLR2018-B16_iGWCW-R1,Reject,"This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. While the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:  - AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners. It's not clear what kind of loss function is really being optimised here. It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end. - While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup. - Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners. - Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method) 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST) 3) comparison to simply ensembling with random initialisations. Other comments: - Paper would benefit from writing improvements to make it read better. - simply use the weighted error function: I don't think this is correct, AdaBoost loss function is an exponential loss. When you train the base learners, their loss functions will become weighted. -  to replace the softmax error function (used in deep learning): I don't think we have softmax error function",14,375,28.846153846153847,5.077562326869806,187,5,370,0.0135135135135135,0.0546875,0.9428,102,42,74,28,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 9, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,1,4,1,1,9,1,0,0,0,0,1,0,0,0,0,0,3,0,0,6,3,0,0.5022430006174293,0.3368907986385711,0.24500714323565717
ICLR2018-B16_iGWCW-R2,Reject,"In conventional boosting methods, one puts a weight on each sample. The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right.  Thus the learned weak learner at this round will make different mistakes. This idea however is difficult to be applied to deep learning with a large amount of data. This paper instead designed a new boosting method which puts large weights on the category with large error in this round. In other words samples in the same category will have the same weight   Error bound is derived. Experiments show its usefulness  though experiments are limited ",8,108,15.428571428571429,4.9523809523809526,69,1,107,0.0093457943925233,0.0176991150442477,-0.802,30,20,16,5,3,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 0, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,0,0,3,0,0,2,0,0,0,0,0,0,3,0,0,0,0,0,0,2,1,0,0.2145091659374486,0.2228441796570651,0.09688236724000636
ICLR2018-B16_iGWCW-R3,Reject,"This paper applies the boosting trick to deep learning. The idea is quite straightforward, and the paper is relatively easy to follow. The proposed algorithm is validated on several image classification datasets. The paper is its current form has the following issues: 1. There is hardly any baseline compared in the paper. The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against. 2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise. 3. In Eq. (3), tilde{D} is not defined. 4. Under the assumption $epsilon_t(l) > frac{1}{2lambda}$, the definition of $beta_t$ in Eq.8 does not satisfy $0 < beta_t < 1$. 5. How many layers is the DenseNet-BC used in this paper? Why the error rate reported here is higher than that in the original paper? Typo:  In Session 3 Line 7, there is a missing reference. In Session 3 Line 10, ""1,00 object classes"" should be ""100 object classes"". In Line 3 of the paragraph below Equation 5, ""classe"" should be ""class"". ",13,194,10.77777777777778,5.017045454545454,106,1,193,0.005181347150259,0.0251256281407035,-0.5291,54,22,39,10,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 2}",0,1,3,1,1,6,0,1,0,0,0,0,0,0,0,0,0,3,0,0,5,3,2,0.4299833649451198,0.4474610873856933,0.24130989552478543
ICLR2018-B16yEqkCZ-R1,Reject,"The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states. The authors propose to train a predictive 'fear model' that penalizes states that lead to catastrophes.  The proposed technique is validated both empirically and theoretically. Experiments show a clear advantage during learning when compared with a vanilla DQN.  Nonetheless, there are some criticisms than can be made of both the method and the evaluations: The fear radius threshold k_r seems to add yet another hyperparameter that needs tuning. Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally. There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable. No empirical results on the effect of the parameter are given. The experimental results support the claim that this technique helps to avoid catastrophic states during initial learning. The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies. This problem does not seem to be really solved by this method. Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier. While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties. As such the method wouldn't prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state. It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited. Overall, the current evaluations focus on performance and give little insight into the behaviour of the method. The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]). In general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  'if the fear model reaches arbitrarily high accuracy' or 'if the probability is negligible'. It is wasn't clear to me that the properties described in Theorem 1 actually hold. The motivation in the appendix is very informal and no clear derivation is provided. The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states. However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states. It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty. It is therefore not clear to me that any claims can be made about its performance without additional assumptions. It seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state. Any optimal policy would therefore need to spend some time e in the danger state, on average. A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards. E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax. By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax  0 < n*_M - e (Rmax - Rmin)   e*Rmin + (1-e)*Rmax - e (Rmax - Rmin). This seems to contradict the theorem. It wasn't clear what assumptions the authors make to exclude situations like this. [1] T. de Bruin, J. Kober, K. Tuyls and R. Babuu0161ka, Improved deep reinforcement learning for robotics through distribution-based experience retention, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952. [2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017).  Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835.",35,717,16.295454545454547,5.282317979197623,322,9,708,0.0127118644067796,0.0421768707482993,-0.9974,215,76,132,43,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 21, 'EXP': 9, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 17, 'SUB': 3, 'CLA': 3}",0,1,0,2,0,21,9,4,0,1,1,0,3,0,0,0,1,1,0,0,17,3,3,0.5773731715875928,0.5658824976713942,0.35859529079899966
ICLR2018-B16yEqkCZ-R3,Reject,"The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL). The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting. The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that ""DQNs  are susceptible to periodically repeating mistakes"". I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues. The paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions. In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel. Still, many of the design choices appear quite arbitrary and can most likely be improved upon. In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal. Instead I view the proposed techniques mostly as useful inspiration for future papers to build on.  As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest. The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong.  To conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning. A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems.  ",14,310,22.142857142857142,5.308724832214765,164,4,306,0.0130718954248366,0.0317460317460317,0.9949,69,48,59,22,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 4, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,0,2,0,5,0,2,0,0,4,3,0,0,0,2,4,0,1,0,4,0,1,0.4297904294020708,0.5576106453899463,0.26751143953617806
ICLR2018-B17JTOe0--R1,Accept,"The authors train an RNN to perform deduced reckoning (ded reckoning) for spatial navigation, and then study the responses of the model neurons in the RNN. They find many properties reminiscent of neurons in the mammalian entorhinal cortex (EC): grid cells, border cells, etc. When regularization of the network is not used during training, the trained RNNs no longer resemble the EC. This suggests that those constraints (lower overall connectivity strengths, and lower metabolic costs) might play a role in the EC's navigation function. The paper is overall quite interesting and the study is pretty thorough: no major cons come to mind. Some suggestions / criticisms are given below.  1) The findings seem conceptually similar to the older sparse coding ideas from the visual cortex. That connection might be worth discussing because removing the regularizing (i.e., metabolic cost) constraint from your RNNS makes them learn representations that differ from the ones seen in EC. The sparse coding models see something similar: without sparsity constraints, the image representations do not resemble those seen in V1, but with sparsity, the learned representations match V1 quite well. That the same observation is made in such disparate brain areas (V1, EC) suggests that sparsity / efficiency might be quite universal constraints on the neural code. 2) The finding that regularizing the RNN makes it more closely match the neural code is also foreshadowed somewhat by the 2015 Nature Neuro paper by Susillo et al. That could be worthy of some (brief) discussion. Sussillo, D., Churchland, M. M., Kaufman, M. T., & Shenoy, K. V. (2015). A neural network that finds a naturalistic solution for the production of muscle activity. Nature neuroscience, 18(7), 1025-1033. 3) Why the different initializations for the recurrent weights for the hexagonal vs other environments? I'm guessing it's because the RNNs don't work in all environments with the same initialization (i.e., they either don't look like EC, or they don't obtain small errors in the navigation task). That seems important to explain more thoroughly than is done in the current text. 4) What happens with ongoing training? Animals presumably continue to learn throughout their lives. With on-going (continous) training, do the RNN neurons' spatial tuning remain stable, or do they continue to drift (so that border cells turn into grid cells turn into irregular cells, or some such)?  That result could make some predictions for experiment, that would be testable with chronic methods (like Ca2+ imaging) that can record from the same neurons over multiple experimental sessions. 5) It would be nice to more quantitatively map out the relation between speed tuning, direction tuning, and spatial tuning (illustrated in Fig. 3). Specifically, I would quantify the cells' direction tuning using the circular variance methods that people use for studying retinal direction selective neurons. And I would quantify speed tuning via something like the slope of the firing rate vs speed curves. And quantify spatial tuning somehow (a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific locations). Then make scatter plots of these quantities against each other. Basically, I'd love to see the trends for how these types of tuning relate to each other over the whole populations: those trends could then be tested against experimental data (possibly in a future study).",27,557,18.566666666666663,5.296786389413989,279,9,548,0.0164233576642335,0.0195729537366548,0.9891,164,73,96,29,9,2,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 2, 'DAT': 1, 'MET': 11, 'EXP': 8, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 0, 'CLA': 0}",0,2,5,2,1,11,8,2,1,0,0,1,0,0,0,0,0,2,0,0,15,0,0,0.6464249068303669,0.2310467888672138,0.2938092870066339
ICLR2018-B17JTOe0--R2,Accept,"Congratulations on a very interesting and clear paper. While ICLR is not focused on neuroscientific studies, this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navigation. Interestingly, these include representations that have been observed in mammals and that have attracted considerable attention, even honored with a Nobel prize.  I found it is very interesting that the emergence of these representations was contingent on some regularization constraint. This seems similar to the visual domain where edge detectors emerge easily when trained on natural images with sparseness constraints as in Olshausen&Field and later reproduced with many other models that incorporate sparseness constraints. I do have some questions about the training itself. The paper mentions a metabolic cost that is not specified in the paper. This should be added. My biggest concern is about Figure 6a. I am puzzled why is the error is coming down before the boundary interaction? Even more puzzling, why does this error go up again for the blue curve (no interaction)? Shouldn't at least this curve be smooth? ",11,180,18.0,5.526011560693641,117,1,179,0.0055865921787709,0.0164835164835164,0.9751,44,20,37,17,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 1, 'EXP': 1, 'RES': 2, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 1}",0,0,0,0,1,1,1,2,3,2,0,3,0,0,0,0,0,0,0,0,5,2,1,0.5002178548108401,0.3359278395810267,0.2532780852395158
ICLR2018-B17JTOe0--R3,Accept,"This paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an RNN to perform a navigation task. On the positive side:   This is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand. I enjoyed reading the paper which is in general clearly written. I have a few, mostly cosmetic, complaints but this can easily be addressed in a revision. On the negative side:   The manuscript is not written in a way that is suitable for the target ICLR audience which will include, for the most part, readers that are not expert on the entorhinal cortex and/or spatial navigation. First, the contributions need to be more clearly spelled out. In particular, the authors tend to take shortcuts for some of their statements. For instance, in the introduction, it is stated that previous attractor network type of models (which are also recurrent networks) ""[...] require hand-crafted and fined tuned connectivity patterns, and the evidence of such specific 2D connectivity patterns has been largely absent. "" This statement is problematic for two reasons:   (i) It is rather standard in the field of computational neuroscience to start from reasonable assumptions regarding patterns of neural connectivity then proceed to show that the resulting network behaves in a sensible way and reproduces neuroscience data. This is not to say that demonstrating that these patterns can arise as a byproduct is not important, on the contrary. These are just two complementary lines of work. In the same vein, it would be silly to dismiss the present work simply because it lacks spikes. (ii) the authors do not seem to address one of the main criticisms they make about previous work and in particular [a lack of evidence] of such specific 2D connectivity patterns.  My understanding is that one of the main assumptions made in previous work is that of a center-surround pattern of lateral connectivity. I would argue that there is a lot of evidence for local inhibitory connection in the cortex. Somewhat related to this point, it would be insightful to show the pattern of local connections learned in the RNN to see how it differs from the aforementioned pattern of connectivity .  Second, the navigation task used needs to be better justified. Why training a network to predict 2D spatial location from velocity inputs? Why is this a reasonable starting point to study the emergence of grid cells? It might be obvious to the authors but it will not be to the ICLR audience. Dead-reckoning (i.e., spatial localization from velocity inputs) is of critical ecological relevance for many animals. This needs to be spelled out and a reference needs to be added. As a side note, I would have expected the authors to use actual behavioral data but instead, the network is trained using artificial trajectories based on modified Brownian motion"". This seems like an important assumption of the manuscript but the issue is brushed off and not discussed. Why is this a reasonable assumption to make? Is there any reference demonstrating that rodent locomotory behavior in a 2D arena is random? Figure 4 seems kind of strange. I do not understand how the ""representative units"" are selected and where the ""late"" selectivity on the far right side in panel a arises if not from ""early"" units that would have to travel ""far"" from the left side...  Apologies if I am missing something obvious. I found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated. It would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cells. ",31,622,22.214285714285715,5.047377326565144,272,3,619,0.0048465266558966,0.0220472440944881,0.5606,149,83,114,37,10,6,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 10, 'EXP': 4, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 7, 'BIB': 1, 'EXT': 2}","{'APR': 2, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 15, 'SUB': 2, 'CLA': 4}",0,2,2,0,1,10,4,4,1,0,0,7,1,2,2,0,0,4,1,0,15,2,4,0.7173509841111906,0.6760853060578901,0.5075540970346255
ICLR2018-B1CEaMbR--R1,Reject,"This paper presents some reviews on clustering methods with deep learning. Based on the review taxonomy, the authors presents a mixed objective which aims for bretter clustering performance. The proposed method is then tested on two image data sets.   The claimed main contribution of the paper is the taxonomy. There are no new things in such kind of reviews. The taxonomy gives no scientific axioms. Therefore the impact or actual contribution to the ICLR community is very limited.   The proposed clustering method is problematic. It is hard to set the paramter alpha. The experimental results are also disappointing. For example, the COIL20 accuracy is only 0.762, much worse than the state of the art. Moreover, results on only two image data sets are not sufficient for convincing.",12,127,10.583333333333334,5.064,79,0,127,0.0,0.0076335877862595,-0.9473,40,16,22,7,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,1,1,2,2,7,2,4,0,0,0,1,0,0,1,0,1,1,0,0,8,1,0,0.5732784161828656,0.5599092575994563,0.3663712489276827
ICLR2018-B1CEaMbR--R2,Reject,"In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods. Finally, the authors propose a new method by using one unexplored combination of taxonomy features. The paper is well-written and easy to follow . The proposed combination is straightforward, but lack of novelty. From table 1, it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining. I am not convinced that these changes will lead to a significant difference. The performance of the proposed method and DEPICK are also similar in table 1. In addition, the experiments section is not comprehensive enough as well the author only tested on two datasets. More datasets should be tested for evaluation. In addition, It seems that nearly all the experiments results from comparison methods are borrowed from the original publications. The authors should finish the experiments on comparison methods and fill the entries in Table 1.   In summary, the proposed method is lack of novelty compare to existing methods. The survey part is nice, however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios.  Therefore, I think the paper cannot be accepted at this stage. ",16,220,14.666666666666666,5.393364928909953,113,3,217,0.0138248847926267,0.0488888888888888,0.3031,63,22,40,12,9,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 3, 'MET': 10, 'EXP': 5, 'RES': 0, 'TNF': 3, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 5, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 1, 'EMP': 10, 'SUB': 4, 'CLA': 1}",0,1,3,1,3,10,5,0,3,3,0,2,0,0,0,5,0,3,2,1,10,4,1,0.6458282658232909,0.7841717485925848,0.4956234602818865
ICLR2018-B1CEaMbR--R3,Reject,"The paper is mostly a survey about clustering methods with neural networks. Section 2 presents a taxonomy for the different neural network clustering methods. A rich lists of the possible components of the neural network-based clustering methods are given, that include the different neural network architectures, feature to use for clustering, loss functions used and more. In Section 3, a few methods from the literature are classified according to the proposed taxonomy. Furthermore, in Section 4 a new method is proposed, that is to combine the best parts of the already existing models in the literature. Unfortunately, the experiments is Section 5 reveal that the proposed method yields results that are at most comparable with the existing methods. The paper is written well and provides good insights (mostly taxonomy) on the existing methods for neural network-based clustering. However, the paper lacks novel content. The novel content of the paper sums up to the proposed method, that is composed of building blocks of existing models, and fails to impress in experimental results. It could be that this paper belongs to another venue that is more appropriate for survey papers. Also, it overall rather appears short. ",11,194,16.166666666666668,5.405405405405405,92,1,193,0.005181347150259,0.0102564102564102,0.8979,52,23,39,10,7,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,1,3,1,0,6,2,3,0,0,0,4,0,0,2,1,0,3,0,1,2,1,1,0.5017457923749705,0.7786437262379688,0.3980115073170718
ICLR2018-B1CNpYg0--R1,Reject,"This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions. The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings. The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings. The paper is clearly written, and has useful ablation experiments. However, I have a couple of questions/concerns: - Most of the gains seem to come from using the spelling of the word. As the authors note, this kind of character level modelling has been used in many previous works. - I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss, but I don't know the area well enough to make specific suggestions  - I'm a little skeptical about how often this method would really be useful in practice. It seems to assume that you don't have much unlabelled text (or you'd use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well. All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task. - The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81. It seems like the proposed method is quite generic, so why not apply it to a stronger baseline? ",14,251,20.916666666666668,5.138075313807532,143,4,247,0.0161943319838056,0.0579150579150579,0.9856,61,34,52,20,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",0,1,6,1,2,4,2,2,0,0,0,1,0,1,0,0,0,5,0,0,7,1,1,0.6441881471347284,0.4486448392822622,0.3502757210353585
ICLR2018-B1CNpYg0--R2,Reject,"This paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas: * Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older). * Using character-level models a la Ling et al. * Using dictionary embeddings a la Hill et al. None of these ideas are new before but I haven't seen them combined in this way before. This is a very practical idea, well-explained with a thorough set of experiments across three different tasks. The paper is not surprising but this seems like an effective technique for people who want to build effective systems with whatever data they've got.  ",8,122,17.428571428571427,5.034782608695652,86,2,120,0.0166666666666666,0.0551181102362204,0.8963,36,16,24,5,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,5,2,0,1,1,0,0,0,0,1,3,0,0,2,0,0,0,0,2,0,0,0.5004060374810663,0.2228892872415833,0.21771590413078176
ICLR2018-B1CNpYg0--R3,Reject,"This paper examines ways of producing word embeddings for rare words on demand. The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets. The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task. The contribution is clear but not huge. In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category. The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible. More things could have been considered. Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet. The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3). However, it is not made very clear why this matrix is needed or what the qualitative effect of its addition is. The paper is clearly written. A paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings for Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf . While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK. However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words. That method could also be considered as a possible approach to compare against here. Other comments:  - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.). While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics. - p.2: You use pretrained GloVe vectors that you do not update. My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?  - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on only 6 billion words u2013 2 orders of magnitude less data. - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector? - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1) but well below current best single models of around 76-78 F1. The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective. This model shows the rather strong performance of spelling models u2013 at least on this task u2013 which he again benefit from training in the context of the end objective. - Fig 2: It's weird that only the +dict (left) model learns to connect In and where. The point made in the text between Where and overseas is perfectly reasonable, but it is a mystery why the base model on the right doesn't learn to associate the common words where and in both commonly expressing a location. - Table 2: These results are interestingly different. Dict is much more useful than spelling here. I guess that is because of the nature of NLI, but it isn't 100% clear why NLI benefits so much more than QA from definitional knowledge. - p.7: I was slightly surprised by how small vocabs (3k and 5k words) are said to be optimal for NLI (and similar remarks hold for SQuAD). My impression is that most papers on NLI use much larger vocabs, no? - Fig 3: This could really be drawn considerably better: make the dots bigger and their colors more distinct. - Table 3: The differences here are quite small and perhaps the least compelling, but the same trends hold. ",34,779,22.257142857142856,4.846675712347354,342,14,765,0.0183006535947712,0.0365699873896595,0.9978,196,91,137,62,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 3, 'DAT': 3, 'MET': 12, 'EXP': 5, 'RES': 5, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 2, 'REC': 0, 'EMP': 18, 'SUB': 4, 'CLA': 2}",0,1,7,3,3,12,5,5,3,0,0,4,1,0,0,0,0,5,2,0,18,4,2,0.7184619792383854,0.5670602156962679,0.44926055296379536
ICLR2018-B1CQGfZ0b-R1,Reject,"The paper proposes a method for identifying representative examples for program synthesis to increase the scalability of existing constraint programming solutions. The authors present their approach and evaluate it empirically. The proposed approach is interesting, but I feel that the experimental section does not serve to show its merits for several reasons. First, it does not demonstrate increased scalability.  Only 1024 examples are considered, which is by no means large. Even then, the authors approach selects the highest number of examples (figure 4).  CEGIS both selects fewer examples and has a shorter median time for complete synthesis. Intuitively, the authors' method should scale better, but they fail to show this -- a missed opportunity to make the paper much more compelling. This is especially true as a more challenging benchmark could be created very easily by simply scaling up the image. Second, there is no analysis of the representativeness of the found sets of constraints. Given that the results are very close to other approaches, it remains unclear whether they are simply due to random variations, or whether the proposed approach actually achieves a non-random improvement. In addition to my concerns about the experimental evaluation, I have concerns about the general approach. It is unclear to me that machine learning is the best approach for modeling and solving this problem. In particular, the selection probability of any particular example could be estimated through a heuristic, for example by simply counting the number of neighbouring examples that have a different color, weighted by whether they are in the set of examples already, to assess its borderness, with high values being more important to achieve a good program. The border pixels are probably sufficient to learn the program perfectly, and in fact this may be exactly what the neural net is learning. The above heuristic is obviously specific to the domain, but similar heuristics could be easily constructed for other domains. I feel that this is something the authors should at least compare to in the empirical evaluation. Another concern is that the authors' approach assumes that all parameters have the same effect. Even for the example the authors give in section 2, it is unclear that this would be true. The text says that rand+cegis selects 70% of examples of the proposed approach, but figure 4 seems to suggest that the numbers are very close -- is this initial examples only? Overall the paper appears rushed -- the acknowledgements section is left over from the template and there is a reference to figure blah. There are typos and grammatical mistakes throughout the paper. The reference to Model counting is incomplete. In summary, I feel that the paper cannot be accepted in its current form.",25,450,19.565217391304348,5.289351851851852,218,4,446,0.0089686098654708,0.0373626373626373,0.9887,104,49,92,33,11,7,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 17, 'EXP': 6, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 12, 'SUB': 5, 'CLA': 1}",0,1,0,1,3,17,6,2,1,1,1,3,1,0,0,0,1,1,1,1,12,5,1,0.7903670594819813,0.7850460155943364,0.6141141654268794
ICLR2018-B1CQGfZ0b-R2,Reject,"This paper presents a method for choosing a subset of examples on which to run a constraint solver in order to solve program synthesis problems. This problem is basically active learning for programming by example, but the considerations are slightly different than in standard active learning. The assumption here is that labels (aka outputs) are easily available for all possible inputs, but we don't want to give a constraint solver all the input-output examples, because it will slow down the solver's execution. The main baseline technique CEGIS (counterexample-guided inductive synthesis) addresses this problem by starting with a small set of examples, solving a constraint problem to get a hypothesis program, then looking for counterexamples where the hypothesis program is incorrect. This paper instead proposes to learn a surrogate function for choosing which examples to select. The paper isn't presented in exactly these terms, but the idea is to consider a uniform distribution over programs and a zero-one likelihood for input-output examples (so observations of I/O examples just eliminate inconsistent programs). We can then compute a posterior distribution over programs and form a predictive distribution over the output for all the remaining possible inputs. The paper suggests always adding the I/O example that is least likely under this predictive distribution (i.e., the one that is most surprising). Forming the predictive distribution explicitly is intractable, so the paper suggests training a neural net to map from a subset of inputs to the predictive distribution over outputs. Results show that the approach is a bit faster than CEGIS in a synthetic drawing domain. The paper starts off strong. There is a start at an interesting idea here, and I appreciate the thorough treatment of the background, including CEGIS and submodularity as a motivation for doing greedy active learning, although I'd also appreciate a discussion of relationships between this approach  and what is done in the active learning literature. Once getting into the details of the proposed approach,  the quality takes a downturn, unfortunately. Main issues: - It's not generally scalable to build a neural network whose size scales with the number of possible inputs. I can't see how this approach would be tractable in more standard program synthesis domains where inputs might be lists of arrays or strings, for example. It seems that this approach only works due to the peculiarities of the formulation of the only task that is considered, in which the program maps a pixel location in 32x32 images to a binary value. - It's odd to write we do not suggest a specific neural network architecture for the middle layers, one should seelect whichever architecture that is appropriate for the domain at hand.  Not only is it impossible to reproduce a paper without any architectural details, but the result is then that Fig 3 essentially says inputs -> magic -> outputs. Given that I don't even think the representation of inputs and outputs is practical in general, I don't see what the  contribution is here. - This paper is poor in the reproducibility category. The architecture is never described, it is light on details of the training objective, it's not entirely clear what the DSL used in the experiments is (is Figure 1 the DSL used in experiments), and it's not totally clear how the random images were generated (I assume values for the holes in Figure 1 were sampled from some distribution, and then the program was executed to generate the data?). - Experiments are only presented in one domain, and it has some peculiarities relative to  more standard program synthesis tasks (e.g., it's tractable to enumerate all possible inputs). It'd be stronger if the approach could also be demonstrated in another domain. - Technical point: it's not clear to me that the training procedure as described is consistent with the desired objective in sec 3.3. Question for the authors: in the limit of infinite training data and model capacity, will the neural network training lead to a model that will reproduce the probabilities in 3.3? Typos: - The paper needs a cleanup pass for grammar, typos, and remnants like Figure blah shows our  neural network architecture on page 5. Overall: There's the start of an interesting idea here, but I don't think the quality is high enough to warrant publication at this time. ",29,708,26.22222222222222,5.297904191616767,312,7,701,0.0099857346647646,0.0442600276625172,0.9856,198,74,123,49,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 10, 'DAT': 0, 'MET': 12, 'EXP': 5, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 2, 'CLA': 1}",0,1,1,10,0,12,5,3,2,0,0,4,0,0,0,0,1,0,0,1,12,2,1,0.5753301247740163,0.5625037638471497,0.35538003651786443
ICLR2018-B1CQGfZ0b-R3,Reject,"General-purpose program synthesizers are powerful but often slow, so work that investigates means to speed them up is very much welcomeu2014this paper included. The idea proposed (learning a selection strategy for choosing a subset of synthesis examples) is good. For the most paper, the paper is clearly-written, with each design decision justified and rigorously specified. The experiments show that the proposed algorithm allows a synthesizer to do a better job of reliably finding a solution in a short amount of time (though the effect is somewhat small). I do have some serious questions/concerns about this method: Part of the motivation for this paper is the goal of scaling to very large sets of examples. The proposed neural net setup is an autoencoder whose input/output size is proportional to the size of the program input domain. How large can this be expected to scale (a few thousand)? The paper did not specify how often the neural net must be trained. Must it be trained for each new synthesis problem? If so, the training time becomes extremely important (and should be included in the ""NN Phase"" time measurements in Figure 4). If this takes longer than synthesis, it defeats the purpose of using this method in the first place. Alternatively, can the network be trained once for a domain, and then used for every synthesis problem in that domain (i.e. in your experiments, training one net for all possible binary-image-drawing problems)? If so, the training time amortizes to some extentu2014can you quantify this? These are all points that require discussion which is currently missing from the paper. I also think that this method really ought to be evaluated on some other domain(s) in addition to binary image drawing. The paper is not an application paper about inferring drawing programs from images; rather, it proposes a general-purpose method for program synthesis example selection. As such, it ought to be evaluated on other types of problems to demonstrate this generality. Nothing about the proposed method (e.g. the neural net setup) is specific to images, so this seems quite readily doable. Overall: I like the idea this paper proposes, but I have some misgivings about accepting it in its current state. What follows are comments on specific parts of the paper: In a couple of places early in the paper, you mention that the neural net computes ""the probability"" of examples. The probability of what? This was totally unclear until fairly deep into Section 3. - Page 2: ""the neural network computes the probability for other examples not in the subset"" - Page 3: ""the probability of all the examples conditioned on ...""  On a related note, I don't like the term ""Selection Probability"" for the quantity it describes. This quantity is 'the probability of an input being assigned the correct output. ' That happens to be (as you've proven) a good measure by which to select examples for the synthesizer. The first property (correctness) is a more essential property of this quantity, rather than the second (appropriateness as an example selection measure). Page 5: ""Figure blah shows our neural network architecture"" - missing reference to Figure 3. Page 5: ""note that we do not suggest a specific neural network architecture for the middle layers, one should select whichever architecture that is appropriate for the domain at hand"" - such as? What are some architectures that might be appropriate for different domains? What architecture did you use in your experiments? The description of the neural net in Section 3.3 (bottom of page 5) is hard to follow on first read-through. It would be better to lead with some high-level intuition about what the network is supposed to do before diving into the details of how it's set up. The first sentence on page 6 gives this intuition; this should come much earlier. Page 5: ""a feed-forward auto-encoder with N input neurons..."" Previously, N was defined as the size of the input domain. Does this mean that the network can only be trained when a complete set of input-output examples is available (i.e. outputs for all possible inputs in the domain)? Or is it fine to have an incomplete example set?  ",41,689,22.966666666666665,5.0473282442748095,277,6,683,0.0087847730600292,0.0229226361031518,0.9417,184,75,116,37,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 21, 'EXP': 14, 'RES': 2, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 24, 'SUB': 3, 'CLA': 2}",0,0,0,3,1,21,14,2,2,1,0,4,0,0,2,0,0,0,2,1,24,3,2,0.5778868149977568,0.6813374105903235,0.4106711621588281
ICLR2018-B1D6ty-A--R1,Reject,"After reading the rebuttal:  The authors addressed some of my theoretical questions. I think the paper is borderline, leaning towards accept. I do want to note my other concerns:  I suspect the theoretical results obtained here are somewhat restricted to the least-squares, autoencoder loss. And note that the authors show that the proposed algorithm performs comparably to SGD, but not significantly better. The classification result (Table 1) was obtained on the autoencoder features instead of training a classifier on the original inputs. So it is not clear if the proposed algorithm is better for training the classifier, which may be of more interest.                                                                This paper presents an algorithm for training deep neural networks. Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers. The theoretical justification is obtained for single-hidden-layer auto-encoders. Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation. As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015. This itself can be a small, nice contribution. What concerns me is the extension to multiple layers. Some questions are not clear from section 3.4: 1. Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it? A negative answer to this question will somewhat undermine the significance of the single-hidden-layer result. 2. Practically, even if the authors can perform efficient optimization of weights in individual layers, when there are many layers, the alternating optimization nature of the algorithm can possibly result in overall slower convergence. Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time. As a result, I am not sure if the proposed algorithm is on par with / faster than the regular SGD algorithm in actual runtime. In the experiments, the authors plotted the training progress w.r.t. the minibatch iterations, I do not know if the minibatch iteration is a proxy for actual runtime (or number of floating point operations). 3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD. Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind? ",22,437,16.807692307692307,5.448687350835322,199,3,434,0.0069124423963133,0.0396825396825396,0.9923,128,47,74,24,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 1, 'MET': 12, 'EXP': 2, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 15, 'SUB': 0, 'CLA': 0}",0,0,3,0,1,12,2,4,1,0,0,3,0,1,0,0,1,4,0,1,15,0,0,0.5746857995709088,0.4535033362038159,0.31978351538714683
ICLR2018-B1D6ty-A--R2,Reject,"The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks. The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem. Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable. This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established. They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer. The ideas are interesting, but I have some concerns regarding this work. Major comments:  1. When dealing with a 2 layer network where there are 2 matrices W_1, W_2 to optimize over, It is not clear to me why optimizing over W_1 is a quasi-convex optimization problem? The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem. However, optimizing w.r.t. W_1 is definitely not a GLM problem, since W_1 undergoes two non-linear transformations one via phi_1 and another via phi_2. Could the authors justify why minimizing w.r.t. W_1 is still a quasi-convex optimization problem? 2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations. This is an interesting result, and useful in its own right. However, it is not clear to me why this result is even relevant here. The main application of this paper is autoencoders, which are functions from R^d -> R^d. However, GLMs are functions from R^d ---> R. So, it is not at all clear to me how Theorem 3.4, 3.5 and eventually 3.6 are useful for the autoencoder problem that the authors care about. Yes they are useful if one was doing 2-layer neural networks for binary classification, but it is not clear to me how they are useful for autoencoder problems. 3. Experimental results for classification are not convincing enough. If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS. 4. The results on reconstruction do not show any benefits for DANTE over SGD (Figure 3). I would recommend the authors to rerun these experiments but truncate the iterations early enough. If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result.",20,417,13.9,5.393782383419689,186,0,417,0.0,0.0593824228028503,0.9906,116,50,72,26,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 7, 'DAT': 1, 'MET': 8, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,1,0,7,1,8,2,5,0,0,0,1,0,0,0,0,0,2,0,0,11,0,0,0.5024161473301737,0.2285589591278419,0.2188978737952601
ICLR2018-B1D6ty-A--R3,Reject,"In this paper an alternating optimization approach is explored for training Auto Encoders (AEs). The authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase. Then they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD. The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs. Some comments on the theoretical part: -The theoretical part is partly misleading. While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer. Regarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply. The authors should mention this point. -Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting. -It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives. Concretely: Is there any known theory for such objectives? What guarantees can we hope to achieve? The extension to muti-layer AEs makes sense and seems to works quite well in practice .  The experimental part is satisfactory, and seems to be done in a decent manner. It will be useful if the authors could relate to the issue of parameter tuning for their algorithm. Concretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification. ",16,261,17.4,5.40625,144,4,257,0.0155642023346303,0.0378787878787878,0.9859,69,34,50,11,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,1,2,1,0,10,3,3,0,2,0,0,0,0,0,0,0,2,0,0,9,2,0,0.5026407316758985,0.3385328318775886,0.24976730583441112
ICLR2018-B1DmUzWAW-R1,Accept,"This work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples (for few-shot classification) or action-reward pairs (for reinforcement learning) in order to take the appropriate action. The resulting model is general-purpose and experiments demonstrate efficacy on few-shot image classification and a range of reinforcement learning tasks. Strengths  - The proposed model is a generic meta-learning useful for both classification and reinforcement learning. - A wide range of experiments are conducted to demonstrate performance of the proposed method. Weaknesses  - Design choices made for the reinforcement learning setup (e.g. temporal convolutions) are not necessarily applicable to few-shot classification. - Discussion of results relative to baselines is somewhat lacking. The proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining efficient. The depth of the TC block is determined by the sequence length. In few-shot classification, the sequence length can be known a prior. How is the sequence length determined for reinforcement learning tasks? In addition, what is done at test-time if the sequence length differs from the sequence length at training time? The causality assumption does not seem to apply to the few-shot classification case. Have the authors considered lifting this restriction for classification and if so does performance improve? The Prototypical Networks results in Tables 1 and 2 do not appear to match the performance reported in Snell et al. (2017). The paper is well-written overall. Some additional discussion of the results would be appreciated (for example, explaining why the proposed method achieves similar performance to the LSTM/OPSRL baselines). I am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient-based methods. I am wondering if the authors can clarify this point. Overall, the proposed approach is novel and achieves good results on a range of tasks. EDIT: I have read the author's comments and am satisfied with their response. I believe the paper is suitable for publication in ICLR.",20,332,16.6,5.699059561128527,163,0,332,0.0,0.0266272189349112,0.9671,109,34,67,8,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 3, 'RES': 5, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 2}",0,1,4,1,0,14,3,5,1,0,0,2,0,0,1,1,0,2,0,0,11,2,2,0.5753631505772298,0.6731912151514619,0.404060817048839
ICLR2018-B1DmUzWAW-R2,Accept,"The authors propose a model for sequence classification and sequential decision making.  The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution. The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making. Unfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control). The experiment section definitely demonstrate the effort put into this work. However, my primary concern is that the model seems somewhat lacking in novelty. Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO. The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding. I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making. I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information). My second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning. I think this information would be very useful to the community in terms of what to take away from this paper. In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains. Overall, I am inclined to accept this paper on the basis of its experimental results. However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain). Some minor feedback/questions for the authors: - I would prefer mathematical equations as opposed to pseudocode formulation - In the experiment section for Omniglot, when the authors say 1200 classes for training and 432 for testing, it sounds like the authors are performing zero-shot learning. How does this particular model generalize to classes not seen during training?",18,353,17.65,5.529585798816568,185,3,350,0.0085714285714285,0.0337078651685393,0.979,103,44,58,20,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 8, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 4, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 3, 'CLA': 0}",0,1,1,0,1,8,3,2,0,2,1,1,0,2,0,4,1,2,0,1,5,3,0,0.7162992000071071,0.6696203347334259,0.4977064405183687
ICLR2018-B1DmUzWAW-R3,Accept,"The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning. Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods. The experiment results seem solid and the proposed structure is with simple design and highly generalizable. The concern is that the contribution is quite incremental from the theoretical side though it involves large amount of experimental efforts, which could be impactful. Please see the major comment below.  One major comment: - Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side. Specifically, for two folds:  (1) in general, some more work in investigating the task space would be nice. The paper assumes the tasks are ""related"" or ""similar"" and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution. But what exactly should the distribution be like to be learnable and how to quantify such ""related"" or ""similar"" relationship across tasks?  (2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods? The result from the paper seems to answer with ""in all cases"" but then that always brings the issue of ""overfitting"" or parameter tuning issue. I believe the paper would have been much stronger if either of the two above are further investigated. More detailed comments: - On Page 1, ""the optimal strategy for an arbitrary range of tasks"" lacks definition of ""range""; also, in the setting in this paper, these tasks should share ""similarity"" or follow the same ""distribution"" and thus such ""arbitrariness"" is actually constrained. - On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined. It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the ""task distribution"" is too large to be meaningful. One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable. - For results in Table 1 and Table 2, how are the confidence intervals computed? Is it over multiple runs or within the same run? It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests. The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming ""not statistically-significantly different"" because no significance test is reported. - At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?",21,534,31.41176470588235,5.423371647509579,248,2,532,0.0037593984962406,0.0330882352941176,0.9914,133,72,87,32,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 12, 'EXP': 4, 'RES': 6, 'TNF': 2, 'ANA': 6, 'FWK': 2, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 7, 'CLA': 0}",0,1,0,1,1,12,4,6,2,6,2,0,0,0,0,0,2,0,0,0,12,7,0,0.6462973788128478,0.3408629608149827,0.3234467395141873
ICLR2018-B1EA-M-0Z-R1,Accept,"Neal (1994) showed that a one hidden layer Bayesian neural network, under certain conditions, converges to a Gaussian process as the number of hidden units approaches infinity. Neal (1994) and Williams (1997) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions. Similarly, the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer, and show the form of the resulting kernel functions. For certain transfer functions, the authors perform a numerical integration to compute the resulting kernels. They perform experiments on MNIST and CIFAR-10, doing classification by scaled regression. Overall, the work is an interesting read, and a nice follow-up to Neal's earlier observations about 1 hidden layer neural networks. It combines several insights into a nice narrative about infinite Bayesian deep networks. However, the practical utility, significance, and novelty of this work -- in its current form -- are questionable, and the related work sections, analysis, and experiments should be significantly extended. In detail:  (1) This paper misses some obvious connections and references, such as  * Krauth et. al (2017): ""Exploring the capabilities and limitations of Gaussian process models"" for recursive kernels with GPs. * Hazzan & Jakkola (2015): ""Steps Toward Deep Kernel Methods from Infinite Neural Networks"" for GPs corresponding to NNs with more than one hidden layer. * The growing body of work on deep kernel learning, which ""combines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes"". E.g.: (i) ""Deep Kernel Learning"" (AISTATS 2016); (ii) ""Stochastic Variational Deep Kernel Learning"" (NIPS 2016); (iii) ""Learning Scalable Deep Kernels with Recurrent Structure"" (JMLR 2017). These works should be discussed in the text. (2) Moreover, as the authors rightly point out, covariance functions of the form used in (4) have already been proposed. It seems the novelty here is mainly the empirical exploration (will return to this later), and numerical integration for various activation functions. That is perfectly fine -- and this work is still valuable. However, the statement ""recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework"" is incorrect. For example, Hazzan & Jakkola (2015) in ""Steps Toward Deep Kernel Methods from Infinite Neural Networks"" consider GP constructions with more than one hidden layer. Thus the novelty of this aspect of the paper is overstated. See also comment [*] later on the presentation. In any case, the derivation for computing the covariance function (4) of a multi-layer network is a very simple reapplication of the procedure in Neal (1994). What is less trivial is estimating (4) for various activations, and that seems to the major methodological contribution. Also note that multidimensional CLT here is glossed over. It's actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions. This derivation should be treated more thoroughly and carefully. (3) Most importantly, in this derivation, we see that the kernels lose the interesting representations that come from depth in deep neural networks. Indeed, Neal himself says that in the multi-output settings, all the outputs become uncorrelated. Multi-layer representations are mostly interesting because each layer shares hidden basis functions. Here, the sharing is essentially meaningless, because the variance of the weights in this derivation shrinks to zero. In Neal's case, the method was explored for single output regression, where the fact that we lose this sharing of basis functions may not be so restrictive. However, these assumptions are very constraining for multi-output classification and also interesting multi-output regressions. [*]: Generally, in reading the abstract and introduction, we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes, and even without the pain of training these networks. ""Deep neural networks without training deep networks"". This is not an accurate portrayal. The very title ""Deep neural networks as Gaussian processes"" is misleading, since it's not really the deep neural networks that we know and love. In fact, you lose valuable structure when you take these limits, and what you get is very different than a standard deep neural network. In this sense, the presentation should be re-worked. (4) Moreover, neural networks are mostly interesting because they learn the representation. To do something similar with GPs, we would need to learn the kernel. But here, essentially no kernel learning is happening. The kernel is fixed. (5) Given the above considerations, there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation. In other words, how structured is this prior and does it really give us some of the interesting properties of deep neural networks, or is it mostly a cute mathematical trick? Unfortunately, the empirical evaluation is very preliminary, and provides no reassurance that this approach will have any practical relevance: (i) Directly performing regression on classification problems is very heuristic and unnecessary. (ii) Given the loss of dependence between neurons in this approach, it makes sense to first explore this method on single output regression, where we will likely get the best idea of its useful properties and advantages. (iii) The results on CIFAR10 are very poor. We don't need to see SOTA performance to get some useful insights in comparing for example parametric vs non-parametric, but 40% more error than SOTA makes it very hard to say whether any of the observed patterns hold weight for more competitive architectural choices. A few more minor comments: (i) How are you training a GP exactly on 50k training points? Even storing a 50k x 50k matrix requires about 20GB of RAM. Even with the best hardware, computing the marginal likelihood dozens of times to learn hyperparameters would be near impossible. What are the runtimes? (ii) One benefit in using the GP is due to its Bayesian nature, so that predictions have uncertainty estimates (Equation (9)). ""  The main benefit of the GP is not the uncertainty in the predictions, but the marginal likelihood which is useful for kernel learning.",55,1008,19.764705882352946,5.476732161323682,400,4,1004,0.0039840637450199,0.0166340508806262,0.9968,278,170,140,62,10,6,"{'ABS': 1, 'INT': 4, 'RWK': 13, 'PDI': 0, 'DAT': 2, 'MET': 24, 'EXP': 16, 'RES': 6, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 4, 'IMP': 2, 'CMP': 11, 'PNF': 3, 'REC': 0, 'EMP': 28, 'SUB': 3, 'CLA': 0}",1,4,13,0,2,24,16,6,0,1,0,4,0,2,0,4,2,11,3,0,28,3,0,0.7228356660906955,0.6851509932455786,0.5188935956897669
ICLR2018-B1EA-M-0Z-R2,Accept,"This paper leverages how deep Bayesian NNs, in the limit of infinite width, are Gaussian processes (GPs). After characterizing the kernel function, this allows us to use the GP framework for prediction, model selection, uncertainty estimation, etc. - Pros of this work  The paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite-width non-Bayesian NN. The provided phase analysis and its relation to the depth of the network is also very interesting. Both are useful contributions as long as deep wide Bayesian NNs are concerned. A different question is whether that regime is actually useful. - Cons of this work  Although this work introduces a new GP covariance function inspired by deep wide NNs, I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful. For instance, looking at the experiments, we can see that on MNIST-50k (the one with most data, and therefore, the one that best informs about the true underlying NN structure) the inferred depth is 1 for the GP and 2 for the NN, i.e., not deep. Similarly for CIFAR, where only up to depth 3 is used. None of these results beat state-of-the-art deep NNs. Also, the results about the phase structure show how increased depth makes the parameter regime in which these networks work more and more constrained.  In [1], it is argued that kernel machines with fixed kernels do not learn a hierarchical representation. And such representation is generally regarded as essential for the success of deep learning. My impression is that the present line of work will not be relevant for deep learning and will not beat state-of-the-art results because of the lack of a structured prior. In that sense, to me this work is more of a negative result informing that to be successful, deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime. - Other comments:  In Fig. 5, use a consistent naming for the axes (bias and variances). In Fig. 1, I didn't find the meaning of the acronym NN with no specified width. Does the unit norm normalization used to construct the covariance disallow ARD input selection? [1] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The Curse of Dimensionality for Local Kernel Machines. 2005.",20,403,18.318181818181817,4.9246753246753245,198,0,403,0.0,0.0121951219512195,0.9546,119,58,55,20,12,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 3, 'MET': 6, 'EXP': 2, 'RES': 3, 'TNF': 2, 'ANA': 3, 'FWK': 1, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,1,2,1,3,6,2,3,2,3,1,1,2,0,0,0,1,2,0,0,11,0,0,0.8588292163978554,0.339670070238953,0.43836199153710537
ICLR2018-B1EA-M-0Z-R3,Accept,"This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery. Pros:  The result highlights an interesting relationship between deep nets and Gaussian processes. (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.) The paper is clear and very well written. The analysis of the phases in the hyperparameter space is interesting and insightful. On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work! Cons:  Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article. I suggest using the same axis limits for all subplots in Figure 3.",9,171,24.428571428571427,5.184049079754601,107,0,171,0.0,0.0231213872832369,0.8243,51,29,21,3,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,2,1,2,1,1,1,1,0,0,0,1,1,0,0,0,3,0,1,0.5717344654350656,0.4456883593141303,0.3209359704913877
ICLR2018-B1EGg7ZCb-R1,Reject,"This paper proposes to use deep reinforcement learning to solve a multiagent coordination task. In particular, the paper introduces a benchmark domain to model fleet coordination problems as might be encountered in taxi companies. The paper does not really introduce new methods, and as such, this paper should be seen more as an application paper. I think that such a paper could have merits if it would really push the boundary of the feasible, but I do not think that is really the case with this paper: the task still seems quite simplistic, and the empirical evaluation is not convincing (limited analysis, weak baselines). As such, I do not really see any real grounds for acceptance. Finally, there are also many other weaknesses. The paper is quite poorly written in places, has poor formatting (citations are incorrect and half a bibtex entry is inlined), and is highly inadequate in its treatment of related work. For instance, there are many related papers on:  -taxi fleet management (e.g., work by Pradeep Varakantham)   -coordination in multi-robot systems for spatially distributed tasks (e.g., Gerkey and much work since)  -scaling up multiagent reinforcement learning and multiagent MDPs (Guestrin et al 2002, Kok & Vlassis 2006, etc.) -dealing with partial observability (work on decentralized POMDPs by Peshkin et al, 2000, Bernstein, Amato, etc.) -multiagent deep RL has been very active last 1-2 years. E.g., see other papers by Foerster, Sukhbataar, Omidshafiei Overall, I see this as a paper which with improvements could make a nice workshop contribution, but not as a paper to be published at a top-tier venue.  ",10,262,26.2,5.186991869918699,152,4,258,0.0155038759689922,0.0297397769516728,-0.2705,75,44,42,19,6,6,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 3, 'EXT': 0}","{'APR': 2, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,2,5,0,0,2,1,0,0,0,0,4,3,0,2,1,0,0,1,1,1,0,1,0.4293301336578908,0.6666763325776349,0.2986172702779843
ICLR2018-B1EGg7ZCb-R2,Reject,"In this paper, the authors define a simulated, multi-agent ""taxi pickup"" task in a GridWorld environment. In the task, there are multiple taxi agents that a model must learn to control. ""Customers"" randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer. Since there are multiple customer and taxi agents, there is a multi-agent coordination problem. Further, the taxi agents have ""batteries"", which starts at a positive number, ticks down by one on each time step and a large negative reward is given if this number reaches zero. The battery can be ""recharged"" by moving to a ""charge"" tile. Cooperative multi-agent problem solving is an important problem in machine learning, artificial intelligence, and cognitive science. This paper defines and examines an interesting cooperative problem: Assignment and control of agents to move to certain squares under ""physical"" constraints. The authors propose a centralized solution to the problem by adapting the Deep Q-learning Network model. I do not know whether using a centralized network where each agent has a window of observations is a novel algorithm. The manuscript itself makes it difficult to assess (more on this later). If it were novel, it would be an incremental development. They assess their solution quantitatively, demonstrating their model performs better than first, a simple heuristic model (I believe de-centralized Dijkstra's for each agent, but there is not enough description in the manuscript to know for sure), and then, two other baselines that I could not figure out from the manuscript (I believe it was Dijkstra's with two added rules for when to recharge). Although the manuscript has many positive aspects to it, I do not believe it should be accepted for the following reasons. First, the manuscript is poorly written, to the point where it has inhibited my ability to assess it. Second, given its contribution, the manuscript is better suited for a conference specific to multi-agent decision-making. There are a few reasons for this. 1) I was not convinced that deep Q-learning was necessary to solve this problem. The manuscript would be much stronger if the authors compared their method to a more sophisticated baseline, for example having each agent be a simple Q-learner with no centralization or ""deepness"". This would solve another issue, which is the weakness of their baseline measure. There are many multi-agent techniques that can be applied to the problem that would have served as a better baseline. 2) Although the problem itself is interesting, it is a bit too applied and specific to the particular task they studied than is appropriate for a conference with as broad interests as ICLR. It also is a bit simplistic (I had expected the agents to at least need to learn to move the customer to some square rather than get reward and move to the next job from just getting to the customer's square). Can you apply this method to other multi-agent problems? How would it compare to other methods on those problems? I encourage the authors to develop the problem and method further, as well as the analysis and evaluation.  ",27,520,20.8,5.186094069529652,228,0,520,0.0,0.0383141762452107,0.9795,127,63,89,28,7,7,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 8, 'DAT': 0, 'MET': 14, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 2, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,1,4,8,0,14,0,1,0,1,0,4,0,0,2,2,0,5,0,1,8,1,1,0.5040453166434945,0.7826549035459247,0.3903356091299271
ICLR2018-B1EGg7ZCb-R3,Reject,"The main contribution of the paper seems to be the application to this problem, plus minor algorithmic/problem-setting contributions that consist in considering partial observability and to balance multiple objectives. On one hand, fleet management is an interesting and important problem. On the other hand, although the experiments are well designed and illustrative, the approach is only tested in a small 7x7 grid and 2 agents and in a 10x10 grid with 4 agents. In spirit, these simulations are similar to those in the original paper by M. Egorov. Since the main contribution is to use an existing algorithm to tackle a practical application, it would be more interesting to tweak the approach until it is able to tackle a more realistic scenario (mainly larger scale, but also more realistic dynamics with traffic models, real data, etc.). Simulation results compare MADQN with Dijkstra's algorithm as a baseline, which offers a myopic solution where each agent picks up the closest customer. Again, since the main contribution is to solve a specific problem, it would be worthy to compare with a more extensive benchmark, including state of the art algorithms used for this problem (e.g., heuristics and metaheuristics). The paper is clear and well written. There are several minor typos and formatting errors (e.g., at the end of Sec. 3.3, the authors mention Figure 3, which seems to be missing, also references [Egorov, Maxim] and [Palmer, Gregory] are bad formatted). -- Comments and questions to the authors:  1. In the introduction, please, could you add references to what is called traditional solutions? n 2. Regarding the partial observability, each agent knows the location of all agents, including itself, and the location of all obstacles and charging locations; but it only knows the location of customers that are in its vision range. This assumption seems reasonable if a central station broadcasts all agents' positions and customers are only allowed to stop vehicles in the street, without ever contacting the central station; otherwise if agents order vehicles in advance (e.g., by calling or using an app) the central station should be able to communicate customers locations too. On the other hand, if no communication with the central station is allowed, then positions of other agents may be also partial observable. In other words, the proposed partial observability assumption requires some further motivation. Moreover, in Sec. 4.3, it is said that agents can see around them +10 spaces away; however, experiments are run in 7x7 and 10x10 grid worlds, meaning that the agents are able to observe the grid completely. 3. The fact that partial observability helped to alleviate the credit-assignment noise caused by the missing customer penalty might be an artefact of the setting. For instance, since the reward has been designed arbitrarily, it could have been defined as giving a penalty for those missing customers that are at some distance of an agent. 4. Please, could you explain the last sentence of Sec. 4.3 that says The drawback here is that the agents will not be able to generalize to other unseen maps that may have very different geographies. In particular, how is this sentence related to partial observability?",19,524,20.15384615384616,5.196787148594377,243,7,517,0.0135396518375241,0.0190114068441064,0.817,140,60,93,27,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 6, 'EXP': 7, 'RES': 2, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 1}",0,1,1,2,1,6,7,2,1,2,0,2,0,0,0,0,0,3,1,0,11,2,1,0.7162389895481217,0.5621161315266867,0.4535559429834025
ICLR2018-B1EPYJ-C--R1,Reject,"This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance. The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server. Experiments are based on the federated averaging algorithm.  The work is valuable, but has room for improvement. The paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms.  This is not a criticism, however, it is difficult to see the reason for including the structured low-rank experiments in the paper (itAs a reader, I found it difficult to understand the actual procedures used.  For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)? How is the structured update learned? It would be very helpful to include algorithms. It seems like a good strategy is to subsample, perform Hadamard rotation, then quantise. For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data.  It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo. If convenient, could the authors comment on a similarly motivated paper under review at iclr 2018: VARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING pros:  - good use of intuition to guide algorithm choices - good compression with little loss of accuracy on best strategy n- good problem for FA algorithm / well motivated -   cons:  - some experiment choices do not appear well motivated / inclusion is not best choice - explanations of algos / lack of 'algorithms' adds to confusion a useful reference:  Strom, Nikko. Scalable distributed dnn training using commodity gpu cloud computing. Sixteenth Annual Conference of the International Speech Communication Association. 2015.  ",20,321,21.4,5.639871382636656,197,3,318,0.0094339622641509,0.0175953079178885,0.9943,97,46,57,22,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 9, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 0}",0,1,1,1,1,9,4,2,0,0,0,3,1,0,0,0,0,1,0,0,10,2,0,0.6452638066513569,0.3390376267552415,0.32737344554428116
ICLR2018-B1EPYJ-C--R2,Reject,"The authors examine several techniques that lead to low communication updates during distributed training in the context of Federated learning (FL). Under the setup of FL, it is assumed that training takes place over edge-device like compute nodes that have access to subsets of data (potentially of different size), and each node can potentially be of different computational power. Most importantly, in the FL setup, communication is the bottleneck. Eg a global model is to be trained by local updates that occur on mobile phones, and communication cost is high due to slow up-link. The authors present techniques that are of similar flavor to quantized+sparsified updates. They distinguish theirs approaches into 1) structured updates and 2) sketched updates. For 1) they examine a low-rank version of distributed SGD where instead of communicating full-rank model updates, the updates are factored into two low rank components, and only one of them is optimized at each iteration, while the other can be randomly sampled. They also examine random masking, eg a sparsification of the updates, that retains a random subset of the entries of the gradient update (eg by zero-ing out a random subset of elements). This latter technique is similar to randomized coordinate descent. Under the theme of sketched updates, they examine quantized and sparsified updates with the property that in expectation they are identical to the true updates. The authors specifically examine random subsampling (which is the same as random masking, with different weights) and probabilistic quantization, where each element of a gradient update is randomly quantized to b bits. The major contribution of this paper is their experimental section, where the authors show the effects of training with structured, or sketched updates, in terms of reduced communication cost, and the effect on the training accuracy. They present experiments on several data sets, and observe that among all the techniques, random quantization can have a significant reduction of up to 32x in communication with minimal loss in accuracy. My main concern about this paper is that although the presented techniques work well in practice, some of the algorithms tested are similar algorithms that have already been proven to work well in practice. For example, it is unclear how the performance of the presented quantization algorithms compares to say  QSGD [1] and Terngrad [2]. Although the authors cite QSGD, they do not directly compare against it in experiments. As a matter of fact, one of the issues of the presented quantized techniques (the fact that random rotations might be needed when the dynamic range of elements is large, or when the updates are nearly sparse) is easily resolved by algorithms like QSGD and Terngrad that respect (and promote) sparsity in the updates. A more minor comment is that it is unclear that averaging is the right way to combine locally trained models for nonconvex problems. Recently, it has been shown that averaging can be suboptimal for nonconvex problems, eg a better averaging scheme can be used in place [3]. However, I would not worry too much about that issue, as the same techniques presented in this paper apply to any weighted linear averaging algorithm. Another minor comment: The legends in the figures are tiny, and really hard to read. Overall this paper examines interesting structured and randomized low communication updates for distributed FL, but lacks some important experimental comparisons. [1] QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks https://arxiv.org/abs/1610.02132 [2] TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning https://arxiv.org/abs/1705.07878 [3] Parallel SGD: When does averaging help?  https://arxiv.org/abs/1606.07365  ",26,590,25.65217391304348,5.418563922942207,260,1,589,0.0016977928692699,0.0117845117845117,0.9267,162,84,105,25,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 15, 'EXP': 10, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,0,0,0,1,15,10,2,1,1,0,0,2,0,0,0,0,4,1,0,6,0,0,0.5043465466159102,0.3367946081791181,0.2567938750999424
ICLR2018-B1EPYJ-C--R3,Reject,"This paper proposes a new learning method, called federated learning, to train a centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. Experiments on both convolutional and recurrent networks are used for evaluation. The studied problem in this paper seems to be interesting, and with potential application in real settings like mobile phone-based learning. Furthermore, the paper is easy to read with good organization. However, there exist several major issues which are listed as follows: Firstly, in federated learning, each client independently computes an update to the current model based on its local data, and then communicates this update to a central server where the client-side updates are aggregated to compute a new global model. This learning procedure is heuristic, and there is no theoretical guarantee about the correctness (convergence) of this learning procedure. The authors do not provide any analysis about what can be learned from this learning procedure. Secondly, both structured update and sketched update methods adopted by this paper are some standard techniques which have been widely used in existing works. Hence, the novelty of this paper is limited. Thirdly, experiments on larger datasets, such as ImageNet, will improve the convincingness.  ",11,206,18.727272727272727,5.641791044776119,133,1,205,0.0048780487804878,0.0288461538461538,0.8105,52,35,36,10,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,0,0,1,1,5,3,0,0,1,0,3,0,0,0,2,0,0,1,0,6,1,0,0.4298354262260857,0.4475993392031774,0.24036427643000347
ICLR2018-B1EVwkqTW-R1,Reject,"After reading the rebuttal:  This paper does have encouraging results. But as mentioned earlier, it still lacks systematic comparisons with existing (and strongest) baselines, and perhaps a better understanding the differences between approaches and the pros and cons. The writing also needs to be improved. So I think the paper is not ready for publication and my opinion remains.                                                              This paper presents an algorithm for few shot learning. The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels). I think the idea of representation learning using a somewhat artificial task makes sense in this setting.   I have several concerns for this submission. 1. I am not very familiar with the literature of few shot learning. I think a very related approach that learns the representation using pretty much the same information is the contrastive loss: -- Hermann and Blunsom. Multilingual Distributed Representations without Word Alignment. ICLR 2014. The intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin. This approach is useful even when there is only weak supervision to provide the similarity/dissimilarity information. I wonder how does this approach compare with the proposed method. 2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT. I do not understand why the compared methods are not consistently used in both experiments. Also, the experiment of speaker classification on TIMIT (where the inputs are audio segments with different durations and sampling frequency) is a quite nonstandard task; I do not have a sense of how challenging it is. It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy. It would be nice to understand/visualize what information have been extracted in the representation learning phase. 3. Relatively minor: The writing of this paper is readable, but could be improved. It sometimes uses vague/nonstandard terminology (parameterless) and statement. The term siamese kernel is not very informative: yes, you are learning new representations of data using DNNs, but this feature mapping does not have the properties of RKHS; also you are not solving the SVM dual problem as one typically does for kernel SVMs. In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning.",23,440,16.296296296296298,5.351674641148326,231,6,434,0.0138248847926267,0.0336633663366336,0.6496,119,55,88,35,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 0, 'DAT': 1, 'MET': 14, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 2, 'CLA': 4}",0,0,6,0,1,14,2,2,0,0,0,5,0,0,0,0,0,4,0,1,9,2,4,0.4325499384791695,0.5612327844267528,0.26968839569044223
ICLR2018-B1EVwkqTW-R2,Reject,"Make SVM great again with Siamese kernel for few-shot learning  ** PAPER SUMMARY **  The author proposes to combine siamase networks with an SVM for pair classification. The proposed approach is evaluated on few shot learning tasks, on omniglot and timit. ** REVIEW SUMMARY **  The paper is readable but it could be more fluent. It lacks a few references and important technical aspects are not discussed. It contains a few errors. Empirical contribution seems inflated on omniglot as the authors omit other papers reporting better results. Overall, the contribution is modest at best.v  ** DETAILED REVIEW **  On mistakes, it is wrong to say that an SVM is a parameterless classifier. It is wrong to cite (Boser et al 92) for the soft-margin SVM. I think slack variables come from (Cortes et al 95).  consistent has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2. You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel . On incompleteness, it is not obvious how the classifier is used at test time. Could you explain how classes are predicted given a test problem? The setup of the experiments on TIMIT is extremely unclear. What are the class you are interested in? How many classes and examples does the testing problems have? On clarity, I do not understand why you talk again about non-linear SVM in the last paragraph of 3.2. since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons. You need to mention explicitely somewhere that (w,theta) are optimized jointly. The sentence this paper investigates only the one versus rest approach is confusing, as you have only two classes from the SVM perspective i.e. pairs (x1,x2) where both examples come from the same class and pairs (x1,x2) where they come from different class. So you use a binary SVM, not one versus rest. You need to find a better justification for using L2-SVM than L2-SVM loss variant is considered to be the best by the author of the paper, did you try classical SVM and found them performing worse? Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3? On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification. So I will focus on the omniglot experiments. Few-Shot Learning Through an Information Retrieval Lens, Eleni Triantafillou, Richard Zemel, Raquel Urtasun, NIPS 2017 [arxiv July'17]  and the reference therein give a few more recent baselines than your table. Some of the results are better than your approach. I am not sure why you do not evaluate on mini-imagenet as well as most work on few shot learning generally do. This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results. Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set. ",33,508,18.142857142857142,5.112266112266112,250,3,505,0.0059405940594059,0.0267175572519083,0.863,147,67,99,30,11,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 3, 'MET': 18, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 3, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 22, 'SUB': 3, 'CLA': 3}",0,1,4,1,3,18,4,2,1,0,0,2,3,1,0,0,0,2,0,0,22,3,3,0.7906656909211245,0.4579983362916879,0.44771461816391306
ICLR2018-B1EVwkqTW-R3,Reject,"Summary:  The paper proposes to pre-train a deep neural network to learn a similarity function and use the features obtained by this pre-trained network as input to an SVM model. The SVM is trained for the final classification task at hand using the last layer features of the deep network. The motivation behind all this is to learn the input features to the SVM as opposed to hand-crafting them, and use the generalization ability of the SVM to do well on tasks which have only a handful of training examples. The authors apply their technique to two datasets, namely, the Omniglot dataset and the TIMIT dataset and show that their model does a reasonable job in these two tasks. While the paper is reasonably clearly written and easy to read I have a number of objections to it. First, I did not see any novel idea presented in this paper. Lots of people have tried pre-training a neural network on auxiliary task(s) and using the features from it as input to the final SVM classifier. People have also specifically tried to train a siamese network and use its features as input to the SVM. These works go way back to the years 2005 - 2007, when deep learning was not called deep learning.  Unless I have missed something completely, I did not see any novel idea proposed in this paper. Second, the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach. For example, the authors compare their model against rather weak baselines. While the approach (as has been shown in the past) is very reasonable, I would have liked the experiments to be more thorough, with comparison to the state of the art models for the two datasets.  ,",15,294,21.0,4.771428571428571,145,0,294,0.0,0.0066889632107023,0.871,84,23,53,19,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 4, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 2}",0,1,3,4,1,5,2,0,0,0,0,4,0,3,0,2,0,1,0,0,3,0,2,0.5730093236262199,0.4458146019695025,0.3270220588700197
ICLR2018-B1G6uM0WG-R1,,"This paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting. The authors propose a new learning strategy called Q-masking which couples well a defined low level controller with a high level tactical decision making policy. The authors rightly say that one of the skills an autonomous car must have is the ability to change lanes, however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicles. Real vehicles also decouple wayfinding with local vehicle control, similar to the strategy employed here.  To make a stronger case for this research being relevant to the real autonomous driving problem, the authors would need to compare their algorithm to a real algorithm and prove that it is more ""data efficient. ""  This is a difficult comparison since the sensing strategies employed by real vehicles u2013 LIDAR, computer vision, recorded, labeled real maps are vastly different from the slot car model proposed by the authors. In term of impact, this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving case. In this paper the authors investigate ""the use and place"" of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses Q-learning to learn ""high level tactical decisions"" and introduce ""Q-masking"" a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the Q-values. The authors claim that ""By relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing, which makes it a possibility to perform training directly on real systems. ""  I am not sure what is meant by this since in this paper the authors never test their algorithm on real systems and in real systems it is not possible to completely eliminate collisions. If it were, this would be a much sought breakthrough. Additionally for their experiment authors use the SUMO top view driving simulator.  This choice makes their algorithm not currently relevant to most autonomous vehicles that use ego-centric sensing.  This paper presents a learning algorithm that can ""outperform a greedy baseline in terms of efficiency"" and ""humans driving the simulator in terms of safety and success"" within their top view driving game. The game can be programmed to have an ""n"" lane highway, where n could reasonable go up to five to represent larger highways. The authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target (random) speed within these limits. Cars follow a fixed model of behavior, do not collide with each other and cannot switch lanes. It is unclear if the simulator extends beyond a single straight section of highway, as shown in Figure 1.  The agent is tasked with driving the ego-car down the n-lane highway and stopping at ""the exit"" in the right hand lane D km from the start position. The authors use deep Q learning from Mnih et al 2015 to learn their optimal policy. They use a sparse reward function of +10 for reaching the goal and -10x(lane difference from desired lane) as a penalty for failure.  This simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collisions. The authors limit what the car is able to do u2013 for example it is not allowed to take actions that would get it off the highway. This makes the high level learning strategy more efficient because it does not have to explore these possibilities (Q-masking). The authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these actions. In the real world, however, it is unlikely that any low level controller would be able to do this perfectly. In terms of evaluation, the authors do not compare their result against any other method. Instead, using only one set of test parameters, the authors compare their algorithm to a ""greedy baseline"" policy that is specified a ""always try to change lanes to the right until the lane is correct"" then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front. It seems that baseline is additionally constrained vs the ego car due to the speed limit and the collision avoidance criteria and is not a fair comparison. So given a fixed policy and these constraints it is not surprising that it underperforms the Q-masked Q-learning algorithm. With respect to the comparison vs. human operators of the car simulation, the human operators were not experts. They were only given ""a few trials"" to learn how to operate the controls before the test. It was reported that the human participants ""did not feel comfortable"" with the low level controller on, possibly indicating that the user experience of controlling the car was less than ideal. With the low level controller off, collisions became possible. It is possibly not a fair claim to say that human drivers were ""less safe"" but rather that it was difficult to play the game or control the car with the safety module on. This could be seen as a game design issue. It was not clear from this presentation how the human participants were rewarded for their performance. In more typical HCI experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated, including compensation (reward) are specified. Overall, this paper presents an overly simplified game simulation with a weak experimental result. ",38,972,23.70731707317073,5.005364806866953,371,4,968,0.0041322314049586,0.0336048879837067,0.9232,263,108,178,58,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 6, 'DAT': 0, 'MET': 23, 'EXP': 7, 'RES': 10, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 8, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 0}",0,1,6,6,0,23,7,10,1,0,1,0,0,1,0,0,1,8,0,0,12,0,0,0.6500787999662413,0.3409950030169358,0.3252140891459958
ICLR2018-B1G6uM0WG-R2,,"The paper describes a deep Q-learning approach to the problem of lane changing, whereby the action space is abstracted to high-level maneuvers that are then associated with low-level controllers. The paper proposes a Q-masking strategy that reduces the action space according to constraints or prior knowledge. The method is trained and evaluated in a multi-lane simulator and compared against a baseline approach and human drivers. Planning lane-change maneuvers is an interesting, important problem for self-driving vehicles. What makes this problem particularly challenging is the need to predict/respond to the actions of other drivers. However, these issues are ignored here,  and it is is unclear why existing optimization/planning approaches are poorly suited to this problem, which is a fundamental assumption being made here.  Indeed, there is a long history of motion planning research that specifically addresses the problem of planning in the face of dynamic obstacles, as well as work that plans using predictive models of vehicle behavior (e.g., see the work by Jon How's group). However, the related work discussion is significantly lacking. The paper does an insufficient job describing why deep RL is the right way to formulate this problem. There are vague references to the policy being difficult to define, but that motivates the importance of learning in general, not deep RL. Why is it reasonable given: (i) the challenge in defining appropriate rewards (i.e., it's not clear to me what would constitute the right reward for this problem); (ii) the large amount of data required to learn the policy;  and (iii) the significant risks associated with training with a physical vehicle; One can see the merits in employing a hierarchical action space, whereby decision making operates over high-level actions, each associated with low-level controllers, but that the adopted formulation is not fundamental to this abstraction. Indeed, this largely regulates the hard problems (i.e., controlling the low-level actions of the vehicle while avoiding collisions) to a separate controller. Further, Q-masking largely amounts to simply removing actions that are infeasible (e.g., changing lanes to the left when in the left-most lane), but is seems to be no more than a heuristic, the advantages of which are not evaluated. The method is evaluated in simulation with comparisons to a simple baseline that tries to get over to the right lane as well as human performance. In the runs that reach the goal, the proposed method is about 20% faster than the simple baseline, though it does not reach the goal every time. Given the claim that not reaching the goal is considered a failure, it isn't clear which performance is preferred. Meanwhile, the evaluation could be improved with the use of a better baseline (e.g., using an existing planning framework such as a predictive RRT that plans to the goal). Additional comments/questions:  * The description of the Q-learning implementation is unclear. How is the terminal time known a priori? Why are two buffers necessary? * The paper claims that the method permits training without any collisions, even for real training runs (strong claim), however it isn't clear how this is guaranteed beyond the assumption that you have a low-level controller that can ensure collisions are avoided. This is secondary to the proposed framework. * The paper overstates the contributions of Q-masking, emphasizing improvements to data efficiency among others. The authors should validate these claims with an ablation study that compares performance with and without masking. This would help to address the contribution of Q-masking vs. simply abstracting the action space. * The network takes as input a 2.5m (this is large) occupancy grid representation of the local environment. How sensitive is the network to errors in this model? Does the occupancy grid account for sensing limitations (e.g., occlusions)?  ",31,614,23.615384615384617,5.351851851851852,282,1,613,0.0016313213703099,0.0256410256410256,0.744,162,74,120,30,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 8, 'DAT': 1, 'MET': 18, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 3, 'CLA': 0}",0,1,5,8,1,18,1,1,0,0,0,0,0,1,0,0,0,2,0,0,15,3,0,0.5764188880054368,0.3423712529949678,0.2910703119661206
ICLR2018-B1G6uM0WG-R3,,"Summary:  Authors propose a method which uses a Q-learning-based high-level policy which is combined with a contextual mask derived from safety-contraints and low-level controllers which disable certain actions from being selectable at certain states. The high-level policy is learnt via fairly standard Q-learning (epsilon-greedy exploration policy and a NN function approximator.) Experiments in a simple car simulator on a task which requires the car to take a certain exit while navigating through traffic are presented with two baselines: 1. a greedy policy which navigates to the right-most lane asap and then follows traffic till the exit is reached. 2. human subjects driving cars in the simulator. Comments:  - Why use a model-free technique like Q-learning especially when one knows the model of the car in autonomous driving setting and can simply run model-predictive control (MPC) (convolve forward the model to get candidate trajectories of certain reasonable horizon, evaluate and pick the best trajectory, execute selected trajectory for a few time-steps and then rinse-and-repeat. This is a very well-accepted method actually used in real-world autonomous cars. See the Urmson et al. 2008 paper in the bibliography.) At the very least this technique should be a baseline. This method is not learning-based, doesn't need training data in a simulator, generalizes to **any** exit and lane configuration and variants of this basic technique continue to be used on real-world autonomous cars. - What kind of safety constraints cannot be expressed by masking actions? It seems that most safety constraints can be expressed via masking. But certain kinds of safety constraints like 'do not drive in the blindspot of other vehicles' sometimes require the ego car to speed up for a bit beyond the speed limit to pass the blindspot area and then slow down. This is an example of a constraint which cannot be expressed by masking actions and in fact requires breaking the top speed limit for a bit in order to be safer in the longer term. - This work also assumes that other cars in the vicinity can be simply observed without any perception uncertainty or even through occlusions. Figure 1c is pretty unrealistic to obtain for a real vehicle, especially for the four cars near the top where the topmost vehicles would be occluded at least partially from the vantage point of the ego-car. ",14,382,23.875,5.32409972299169,203,3,379,0.0079155672823219,0.0103092783505154,0.986,114,40,65,24,7,1,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 4, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,2,3,4,0,8,1,3,1,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0.5022271429015622,0.1148428557201689,0.2051181327790377
ICLR2018-B1Gi6LeRZ-R1,Accept,"Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes. Previous approaches to training data mixing are (1) from random classes, or (2) from the same class. The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network. Results look like significant improvements over standard learning setups. Detailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks. In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power. The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters. Minor Asks: (1) Clarification on how the error rates are defined. Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label). (2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes. Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales. I.e. are we talking about a space of 10 total classes or 10000 total classes? How does num required epochs get impacted as we increase this class space? (3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels. (4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation... what happens if you just randomly pick one of the 10 crops for prediction?",16,323,21.53333333333333,5.233333333333333,171,0,323,0.0,0.0060975609756097,0.985,97,40,62,11,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 1, 'CLA': 0}",0,0,1,1,0,10,0,4,0,2,0,1,0,0,0,1,0,1,0,0,12,1,0,0.4310149826542351,0.4512859762277171,0.24748037038569853
ICLR2018-B1Gi6LeRZ-R2,Accept,"This manuscript proposes a method to improve the performance of a generic learning method by generating in between class (BC) training samples. The manuscript motivates the necessity of such technique and presents the basic intuition. The authors show how the so-called BC learning helps training different deep architectures for the sound recognition task. My first remark regards the presentation of the technique. The authors argue that it is not a data augmentation technique, but rather a learning method. I strongly disagree with this statement, not only because the technique deals exactly with augmenting data, but also because it can be used in combination to any learning method (including non-deep learning methodologies). Naturally, the literature review deals with data augmentation technique, which supports my point of view. In this regard, I would have expected comparison with other state-of-the-art data augmentation techniques. The usefulness of the BC technique is proven to a certain extent (see paragraph below) but there is not comparison with state-of-the-art. In other words, the authors do not compare the proposed method with other methods doing data augmentation. This is crucial to understand the advantages of the BC technique. There is a more fundamental question for which I was not able to find an explicit answer in the manuscript. Intuitively, the diagram shown in Figure 4 works well for 3 classes in dimension 2. If we add another class, no matter how do we define the borders, there will be one pair of classes for which the transition from one to another will pass through the region of a third class. The situation worsens with more classes. However, this can be solved by adding one dimension, 4 classes and 3 dimensions seems something feasible. One can easily understand that if there is one more class than the number of dimensions, the assumption should be feasible, but beyond it starts to get problematic. This discussion does not appear at all in the manuscript and it would be an important limitation of the method, specially when dealing with large-scale data sets. Overall I believe the paper is not mature enough for publication. Some minor comments: - 2.1: We introduce --> We discussion - Pieczak 2015a did not propose the extraction of MFCC. - the x_i and t_i of section 3.2.2 should not be denoted with the same letters as in 3.2.1. - The correspondence with a semantic feature space is too pretentious, specially since no experiment in this direction is shown. - I understand that there is no mixing in the test phase, perhaps it would be useful to recall it.",23,424,18.43478260869565,5.1840796019900495,204,2,422,0.0047393364928909,0.0209302325581395,0.7042,117,35,68,26,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 1, 'MET': 15, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 3, 'REC': 1, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,4,0,1,15,2,0,1,0,0,2,0,0,0,0,0,5,3,1,6,2,0,0.5038757678998965,0.5593638073144584,0.3175906597780883
ICLR2018-B1Gi6LeRZ-R3,Accept,"The propose data augmentation and BC learning is relevant, much robust than frequency jitter or simple data augmentation. In equation 2, please check the measure of the mixture. Why not simply use a dB criteria ? The comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform. The authors might discuss more how to extend their model to image recognition, or at least of other modalities as suggested. Section 3.2.2 shall be placed later on, and clarified. Discussion on mixing more than two sounds leads could be completed by associative properties, we think... ? ",7,111,15.857142857142858,5.235849056603773,85,2,109,0.018348623853211,0.0350877192982456,0.8633,35,12,19,4,2,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,0,1,0,0,6,0,0,0,0,0,0,0,0,0,1,0,2,0,0,5,0,1,0.1441081034260662,0.4470494367410063,0.08003991546914545
ICLR2018-B1IDRdeCW-R1,Accept,"This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that:  (1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet. (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. (3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer. The first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge. The second observation is much less clear to me. Specifically, a.tThe author claim that ""A sufficient condition for delta u to be the same in both cases is L'(x   f(u)) ~ L'(x   g(u))"". However, I'm not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. b.tRelated to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified. c.tFor BNNs, where both the weights and activations are binarized, shouldn't we compare weights*activations to (binarized weights)*(binarized activations)? d.tTo make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3. e.tIt is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?),perhaps this should be further clarified. The third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized. To summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. %%% After Author's response %%% a. My mistake. Perhaps it should be clarified in the text that u are the weights. I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation). Following the author's response and revisions, I have raised my grade. ",24,528,22.95652173913044,5.255533199195171,227,5,523,0.0095602294455066,0.068901303538175,0.9886,134,85,93,37,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 19, 'EXP': 0, 'RES': 8, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 16, 'SUB': 2, 'CLA': 0}",0,0,0,1,1,19,0,8,3,0,0,1,0,1,0,1,0,0,0,1,16,2,0,0.5049399672185033,0.4538804824754104,0.2729855120468162
ICLR2018-B1IDRdeCW-R2,Accept,"This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). My main concerns are on the usage of the given observations. 1. Can the observations be used to explain more recent works? Indeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network. However, as the authors mentioned, there are more recent works which give better performance than this one. For example, we can use +1, 0, -1 to approximate the weights. Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance. So, how can the given observations be used to explain more recent works? 2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)? The authors call their findings theory. From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations. Besides, giving interesting observations is not good enough. I wish to see how they can be used to improve binary networks. Reference [a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017",14,186,10.333333333333334,5.298245614035087,94,0,186,0.0,0.0053763440860215,0.971,55,23,39,13,10,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 3, 'CLA': 0}",0,1,4,1,0,2,1,2,0,1,0,2,1,1,0,0,0,3,0,0,3,3,0,0.714878490807411,0.3350249263340422,0.36568092646970335
ICLR2018-B1IDRdeCW-R3,Accept,"This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector. It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with Dot Product Proportionality Property.  It also proposes Generalized Binarization Transformation for the first layer of a neural network. In general, I think the paper is written clearly and in detail. Some typos and minor issues are listed in the Cons part below. Pros: The authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. Cons: * it seems that there are quite some typos in the paper, for example:     1. Section 1, in the second contribution, there are two thens. 2. Section 1, the citation format of Bengio et al. (2013) should be (Bengio et al. 2013). * Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD.  * Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear. * In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(row) to make it clearer? I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it. ",13,290,16.11111111111111,5.147058823529412,154,4,286,0.0139860139860139,0.0299003322259136,0.8518,87,34,42,14,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 7}",0,0,2,1,0,5,1,0,1,1,0,4,0,0,0,0,0,0,1,0,2,0,7,0.5012175127816744,0.3344421011933006,0.2500302556356262
ICLR2018-B1J_rgWRW-R1,Accept,"This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks. I summarize the main results as below:  (1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN. Any smooth function can be approximated by such networks. (2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN. (3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces (4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm. Among these results (1), (2), (4) are sort of known in the literature. This paper extends the existing results in some subtle ways. For (1), the authors show that the DNN has a tighter bound on the depth. For (2), the hard functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger.  For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum. The stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU. Other than that, I don't get much more insight from the theoretical result. When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient. Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough. Result (3) is more interesting as it is a new result. The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces. However, the construction seems artificial and these functions don't seem to be visually very complex. Overall, this is an incremental work in the direction of studying the representation power of neural networks. The results might be of theoretical interest, but I doubt if a pragmatic ReLU network user will learn anything by reading this paper.",21,327,17.210526315789473,5.385906040268456,148,3,324,0.0092592592592592,0.0243161094224924,0.9504,76,58,51,13,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 11, 'EXP': 0, 'RES': 17, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,0,0,0,0,11,0,17,0,0,1,1,0,0,0,2,0,0,0,0,10,1,0,0.2891074371354835,0.3389760578314382,0.14568668494420373
ICLR2018-B1J_rgWRW-R2,Accept,"The paper presents a series of definitions and results elucidating details about the functions representable by ReLU networks, their parametrisation, and gaps between deep and shallower nets. The paper is easy to read, although it does not seem to have a main focus (exponential gaps vs. optimisation vs. universal approximation). The paper makes a nice contribution to the details of deep neural networks with ReLUs, although I find the contributed results slightly overstated. The 1d results are not difficult to derive from previous results. The advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network. The optimisation method appears close to brute force and is limited to 2 layers. Theorem 3.1 appears to be easily deduced from the results from Montufar, Pascanu, Cho, Bengio, 2014. For 1d inputs, each layer will multiply the number of regions at most by the number of units in the layer, leading to the condition w' geq w^{k/k'}. Theorem 3.2 is simply giving a parametrization of the functions, removing symmetries of the units in the layers. In the list at the top of page 5. Note that, the function classes might be characterized in terms of countable properties, such as the number of linear regions as discussed in MPCB, but still they build a continuum of functions. Similarly, in page 5 ``Moreover, for fixed n,k,s, our functions are smoothly parameterized''. This should not be a surprise. In the last paragraph of Section 3 ``m   w^k-1'' This is a very big first layer. This also seems to subsume the first condition, sgeq  w^k-1 +w(k-1) for the network discussed in Theorem 3.9. In the last paragraph of Section 3 ``To the best of our knowledge''. . In the construction presented here, the network's size is essentially in the layer of size m.. Under such conditions, Corollary 6 of MPCB also reads as s^n.. Here it is irrelevant whether one artificially increases the depth of the network by additional, very narrow, layers, which do not contribute to the asymptotic number of units.. The function class Zonotope is a composition of two parts. It would be interesting to consider also a single construction, instead of the composition of two constructions.. Theorem 3.9 (ii) it would be nice to have a construction where the size becomes 2m + wk when k' k.. Section 4, while interesting, appears to be somewhat disconnected from the rest of the paper.. In Theorem 2.3. explain why the two layer case is limited to n 1.. At some point in the first 4 pages it would be good to explain what is meant by ``hard'' functions (e.g. functions that are hard to represent, as opposed to step functions, etc.)  .",26,450,15.0,4.946987951807229,200,5,445,0.0112359550561797,0.0218818380743982,0.978,131,47,70,23,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 19, 'EXP': 0, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 0, 'CLA': 1}",0,1,1,0,0,19,0,6,0,0,0,1,0,0,0,0,0,1,0,0,19,0,1,0.3619248246545283,0.3445285671605068,0.1838432059301619
ICLR2018-B1J_rgWRW-R3,Accept,"The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of ""hard"" functions that are not easily representable by shallower networks. It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss. I think these contributions warrant publishing the paper at ICLR 2018. The paper is also well written, a bit dense in places, but overall well organized and easy to follow. A key limitation of the paper in my opinion is that typically DNNs do not contain a linear final layer. It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer. I also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like D^nw, would like hearing authors' comments in case I'm missing some simplification. One minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL. For example, adding PWL to Theorems and Corollaries in Section 3.1 will help.  Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN.",10,265,24.09090909090909,5.297619047619048,153,2,263,0.0076045627376425,0.018796992481203,0.9809,76,35,43,17,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 1}",0,1,1,1,0,6,0,0,0,1,0,2,0,0,1,0,0,0,1,1,6,3,1,0.4298678079484024,0.6699898068575245,0.3032842705811919
ICLR2018-B1KFAGWAZ-R1,Reject,"This paper investigates multiagent reinforcement learning  making used of a master slave architecture (MSA). On the positive side, the paper is mostly well-written, seems technically correct, and there are some results that indicate that the MSA is working quite well on relatively complex tasks. On the negative side, there seems to be relatively limited novelty: we can think of MSA as one particular communication (i.e, star) configuration one could use is a multiagent system. One aspect does does strike me as novel is the gated composition module, which allows differentiation of messages to other agents based on the receivers internal state. (So, the *interpretation* of the message is learned). I like this idea, however, the results are mixed, and the explanation given is plausible, but far from a clearly demonstrated answer. There are some important issues that need clarification:  * Sukhbaatar et al. (2016) proposed the ""CommNet"", where broadcasting communication channel among all agents were set up to share a global information which is the summation of all individual agents. [...] however the summed global signal is hand crafted information and does not facilitate an independently reasoning master agent.  -Please explain what is meant here by 'hand crafted information', my understanding is that the f^i in figure 1 of that paper are learned modules? -Please explain what would be the differences with CommNet with 1 extra agent that takes in the same information as your 'master'. *This relates also to this:   Later we empirically verify that, even when the overall in- formation revealed does not increase per se, an independent master agent tend to absorb the same information within a big picture and effectively helps to make decision in a global manner. Therefore compared with pure in-between-agent communications, MS-MARL is more efficient in reasoning and planning once trained. [...]  Specifically, we compare the performance among the CommNet model, our MS-MARL model without explicit master state (e.g. the occupancy map of controlled agents in this case), and our full model with an explicit occupancy map as a state to the master agent. As shown in Figure 7 (a)(b), by only allowed an independently thinking master agent and communication among agents, our model already outperforms the plain CommNet model which only supports broadcast- ing communication of the sum of the signals.   -Minor: I think that the statement which only supports broadcast-ing communication of the sum of the signals is not quite fair: surely they have used a 1-channel communication structure, but it would be easy to generalize that. -Major: When I look at figure 4D, I see that the proposed approach *also* only provides the master with the sum (or really mean) with of the individual messages...? So it is not quite clear to me what explains the difference.   *In 4.4, it is not quite clear exactly how the figure of master and slave actions is created. This seems to suggest that the only thing that the master can communicate is action information? It this the case? * In table 2, it is not clear how significant these differences are. What are the standard errors? * The section 3.2 explains standard things (policy gradient), but the details are a bit unclear. In particular, I do not see how the Gaussian/softmax layers are integrated; they do not seem to appear in figure 4? * I cannot understand figure 7 without more explanation. (The background is all black - did something go wrong with the pdf?) Details: * references are wrongly formatted throughout. * In this regard, we are among the first to combine both the centralized perspective and the decentralized perspective This is a weak statement (E.g., I suppose that in the greater scheme of things all of us will be amongst the first people that have walked this earth...) * Therefore they tend to work more like a commentator analyzing and criticizing the play, rather than a coach coaching the game.  -This sounds somewhat vague. Can it be made crisper? * Note here that, although we explicitly input an occupancy map to the master agent, the actual infor- mation of the whole system remains the same.  This is a somewhat peculiar statement. Clearly, the distribution of information over the agents is crucial. For more insights on this one could refer to the literature on decentralized POMDPs.     ",32,703,24.24137931034483,5.129080118694362,313,5,698,0.0071633237822349,0.0341997264021887,0.9807,190,72,137,54,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 15, 'EXP': 0, 'RES': 4, 'TNF': 7, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 8, 'REC': 0, 'EMP': 13, 'SUB': 3, 'CLA': 1}",0,1,2,2,0,15,0,4,7,0,0,6,1,0,0,2,0,4,8,0,13,3,1,0.5756050362684173,0.6751710866241366,0.40197628116232076
ICLR2018-B1KFAGWAZ-R2,Reject,"The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients. Authors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines. Overall, I had a very confusing feeling when reading the paper. u2028First, authors do not formulate what exactly is the problem statement for MARL. Is it an MDP or poMDP?  How do different agents perceive their time, is it synchronized or not? Do they (partially) share the incentive or may have completely arbitrary rewards? What is exactly the communication protocol? I find this question especially important for MARL, because the assumption on synchronous and noise-free communication, including gradients is too strong to be useful in many practical tasks. Second, even though the proposed architecture proved to perform empirically better that the considered baselines, the extent to which it advances RL research is unclear to me. Currently, it looks   Based on that, I can't recommend acceptance of the paper. To make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients). It would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives.",13,248,27.55555555555556,5.55,149,1,247,0.0040485829959514,0.0199203187250996,0.9653,65,29,52,16,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 9, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,1,2,1,1,9,1,1,0,0,0,2,0,0,0,0,0,0,0,1,10,0,1,0.5735560150492833,0.33893095024692,0.290285881476636
ICLR2018-B1KFAGWAZ-R3,Reject,"The paper presents results across a range of cooperative multi-agent tasks, including a simple traffic simulation and StarCraft micro-management. The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS-MARL. A gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each agent. This contribution is added to the logits coming from each agent. Pros:  -The results on StarCraft are encouraging and present state of the art performance if reproducible. Cons: -The experimental evaluation is not very thorough: No uncertainty of the mean is stated for any of the results. 100 evaluation runs is very low. It is furthermore not clear whether training was carried out on multiple seeds or whether these are individual runs. -BiCNet and CommNet are both aiming to learn communication protocols which allow decentralized execution. Thus they represent weak baselines for a fully centralized method such as MS-MARL. The only fully centralized baseline in the paper is GMEZO, however results stated are much lower than what is reported in the original paper (eg. 63% vs 79% for M15v16).  The paper is missing further centralized baselines. -It is unclear to what extends the novelty of the paper (specific architecture choices) are required. For example, the gating mechanism for producing the action logits is rather complex and seems to only help in a subset of settings (if at all). Detailed comments: For all tasks, the number of batch per training epoch is set to 100.  What does this mean? Figure 1:  This figure is very helpful, however the colour for M->S is wrong in the legend. Table 2: GMEZO win rates are low compared to the original publication. What many independent seeds where used for training? What are the confidence intervals?  How many runs for evaluation? Figure 4: B) What does it mean to feed two vectors into a Tanh? This figure currently very unclear. What was the rational for choosing a vanilla RNN for the slave modules? Figure 5: a) What was the rational for stopping training of CommNet after 100 epochs? The plot looks like CommNet is still improving. c) This plot is disconcerting. Training in this plot is very unstable. The final performance of the method ('ours') does not match what is stated in 'Table 2'. I wonder if this is due to the very small batch size used (a small batch size of 4 ).   ",31,413,16.52,5.005141388174807,191,1,412,0.0024271844660194,0.0308056872037914,0.8475,120,45,78,22,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 0, 'DAT': 4, 'MET': 10, 'EXP': 12, 'RES': 5, 'TNF': 5, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 17, 'SUB': 2, 'CLA': 1}",0,0,4,0,4,10,12,5,5,1,0,1,0,0,0,1,0,3,2,0,17,2,1,0.5752941895019256,0.677020556170659,0.40387136057597156
ICLR2018-B1KJJf-R--R1,Reject,"This paper presents a seq2Tree model to translate a problem statement in natural  language to the corresponding functional program in a DSL. The model uses an RNN encoder to encode the problem statement and uses an attention-based doubly recurrent network for generating tree-structured output. The learnt model is  then used to perform Tree-beam search using a search algorithm that searches  for different completion of trees based on node types. The evaluation is performed on a synthetic dataset and shows improvements over seq2seq baseline approach. Overall, this paper tackles an important problem of learning programs from  natural language and input-output example specifications. Unlike previous neural program synthesis approaches that consider only one of the specification  mechanisms (examples or natural language), this paper considers both of them  simultaneously. However, there are several issues both in the approach and the  current preliminary evaluation, which unfortunately leads me to a reject score, but the general idea of combining different specifications is quite promising. First, the paper does not compare against a very similar approach of Parisotto et al. Neuro-symbolic Program Synthesis (ICLR 2017) that uses a similar R3NN network for generating the program tree incrementally by decoding one node at a time. Can the authors comment on the similarity/differences between the approaches? Would it be possible to empirically evaluate how the R3NN performs on this dataset? Second, it seems that the current model does not use the input-output examples at  all for training the model. The examples are only used during the search algorithm. Several previous neural program synthesis approaches (DeepCoder (ICLR 2017),  RobustFill (ICML 2017)) have shown that encoding the examples can help guide  the decoder to perform efficient search. It would be good to possibly add another  encoder network to see if encoding the examples as well help improve the accuracy. Similar to the previous point, it would also be good to evaluate the usefulness of encoding the problem statement by comparing the final model against a model in which the encoder only encodes the input-output examples. Finally, there is also an issue with the synthetic evaluation dataset. Since the  problem descriptions are generated syntactically using a template based approach,  the improvements in accuracy might come directly from learning the training templates instead of learning the desired semantics. The paper mentions that it is prohibitively  expensive to obtain human-annotated set, but can it be possible to at least obtain a  handful of real tasks to evaluate the learnt model? There are also some recent  datasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors might consider in future. Questions for the authors:  Why was MAX_VISITED only limited to 100? What happens when it is set to 10^4 or 10^6? The Search algorithm only shows an accuracy of 0.6% with MAX_VISITED 100. What would the performance be for a simple brute-force algorithm with a timeout of say 10 mins?  Table 3 reports an accuracy of 85.8% whereas the text mentions that the best result is 90.1% (page 8)?  What all function names are allowed in the DSL (Figure 1)? Can you clarify the contributions of the paper in comparison to the R3NN? Minor typos:  page 2: allows to add constrains --> allows to add constraints page 5: over MAX_VISITED programs has been --> over MAX_VISITED programs have been  ",26,541,28.473684210526315,5.462890625,236,4,537,0.0074487895716946,0.015929203539823,0.9881,144,69,90,27,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 3, 'DAT': 7, 'MET': 15, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 17, 'SUB': 0, 'CLA': 2}",0,1,3,3,7,15,3,1,1,0,0,1,0,0,0,0,0,4,0,1,17,0,2,0.6470283440013859,0.454828386144356,0.3691818291119275
ICLR2018-B1KJJf-R--R2,Reject,"This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples. The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs. In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs. Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results, and that search without any training performs worse, although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a technique.  Strengths:  - Reasonable approach, quality is good - The DSL is richer than that of previous related work like Balog et al. (2016). - Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting. Weaknesses:  - There are now several papers on using a trained neural network to guide search, and this approach doesn't add too much on top of previous work. Using beam search on tree outputs is a bit of a minor contribution. - The baselines are just minor variants of the proposed method. It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset. - Data is synthetic, and it's hard to get a sense for how difficult the presented problem is, as there are just four example problems given. Questions:  - Why not compare against Seq2Seq + Search? - How about comparing wall time against a traditional program synthesis technique (i.e., no machine learning), ignoring the descriptions. I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do. - Are there plans to release the dataset? Could you provide a large sample of the data at an anonymized link? I'd re-evaluate my rating after looking at the data in more detail. ",19,339,21.1875,5.337579617834395,181,1,338,0.0029585798816568,0.0112994350282485,0.1877,97,48,56,11,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 3, 'DAT': 7, 'MET': 10, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 1, 'EMP': 9, 'SUB': 2, 'CLA': 1}",0,0,3,3,7,10,1,2,1,0,0,1,0,0,0,0,0,5,1,1,9,2,1,0.5742321223131832,0.6722176528824918,0.4090968506649889
ICLR2018-B1KJJf-R--R3,Reject,This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network. This work to my knowledge is the first to use a DSL closer to a full language. The paper is very clear and easy to follow. One way it could be improved is if it were compared with another system. The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work.,5,84,16.8,5.177215189873418,61,0,84,0.0,0.0119047619047619,0.7548,21,10,19,2,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,1,2,1,0,1,0,1,0,0,0,2,0,0,0,0,0,1,1,0,2,0,1,0.4286973352818631,0.4450664018792874,0.24320696720978868
ICLR2018-B1Lc-Gb0Z-R1,Accept,"The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x). Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches. Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation. More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z). The paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen. Deciding on these targets is formulated as a combinatorial optimization problem. Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization. The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation. At each layer, the targets can be chosen using a variety of search algorithms such as beam search. Experiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks. This paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions. The experimental results are encouraging, though I have a few questions below that prevent me for now from rating the paper higher. Comments and questions:  1) How computationally expensive is FTP? The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details. 2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks? I do not see a comparison. If that is so, should this paper also compare with STE? How do we know if generalizing saturated STE is more worthwhile than generalizing STE? 3) It took me a while to understand the authors' subtle comparison with target propagation, where they say Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets, whereas previous approaches employed continuous optimization.  It seems that the difference is greater than explicitly stated, that prior target propagation used continuous optimization to set *continuous targets*. (One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem.) Focusing on discrete targets gains the benefits of quantized networks. If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear. 4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation? 5) In Table 1, the significance of the last two columns is unclear. It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good). I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?",27,535,23.26086956521739,5.618110236220472,244,3,532,0.0056390977443609,0.0167597765363128,0.9954,163,63,102,26,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 7, 'DAT': 2, 'MET': 16, 'EXP': 3, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 7, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,1,2,7,2,16,3,2,1,0,0,1,0,0,0,1,0,7,1,0,8,1,0,0.6473104958534516,0.5606122329425962,0.41021382028458864
ICLR2018-B1Lc-Gb0Z-R2,Accept,"This paper examines the problem of optimizing deep networks of hard-threshold units. This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks. While none of the contributions are especially novel, the analysis is clear and well-organized, and the authors do a nice job in connecting their analysis to other work. ",4,65,16.25,5.555555555555555,46,0,65,0.0,0.0151515151515151,0.9134,18,10,9,3,3,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,2,0,0,0,1,0,0,0,0,0,0,0,0,0,0,3,0,0,0.2145952062613547,0.112355025980797,0.08700425302098934
ICLR2018-B1Lc-Gb0Z-R3,Accept,"The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it. The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy. The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems. However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem. These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such. Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently. They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet. They show superiority of their algorithm over SSTE. I thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds. The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting. The results are moderately convincing in favor of the proposed approach. Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem. Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community. There are a few flaws/weaknesses in the paper though, making it somewhat lose. - The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1. Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier. - The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees. - There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition - The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach. The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit.  ",20,419,20.95,5.388888888888889,199,2,417,0.0047961630695443,0.0352941176470588,0.938,109,46,72,19,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 3, 'MET': 11, 'EXP': 5, 'RES': 7, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,0,1,4,3,11,5,7,0,0,1,3,0,0,0,1,2,1,0,0,11,1,1,0.5749504057709648,0.6729342776635443,0.40018380187715363
ICLR2018-B1NGT8xCZ-R1,Reject,"This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al. The construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem. The only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model. The authors perform numerous empirical experiments on several types of problems. They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks.  On the downside, all these experiments concern predictive (discriminative) problems. Given the paper title, I would have expected some experiments in a generative context. Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus. I would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016). Minor comments: - Paragraph below Equation 6:  The meaning of $phi(psi)$ is unclear - Equation (7): phi and psi seems inverted - Section 4: The acronym MLP is used but never defined.     update     I lowered my score and confidence, see my new post below.",14,215,16.53846153846154,5.505,126,1,214,0.0046728971962616,0.0131578947368421,0.4588,69,25,38,8,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 3, 'REC': 1, 'EMP': 5, 'SUB': 3, 'CLA': 1}",0,0,3,1,0,5,3,1,0,2,0,1,0,0,0,0,0,2,3,1,5,3,1,0.5013588552239402,0.6696081498274782,0.3555878650097348
ICLR2018-B1NGT8xCZ-R2,Reject,"This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances. However, I have the following concerns on novelty. 1. Although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model, it does not give new formulation or algorithm to handle domain adaptation. At this point, the novelty is weaken. 2. In the introduction, the authors mentioned ""limitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings. "" However, mSDA cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input features. It is not clear why mSDA cannot handle time-series data but DAuto can. DAuto does not consider the sequence/ordering of data either. 3. If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e. prediction loss, reconstruction loss, domain difference loss). As far as I know, this kind of loss is commonly used in most existing methods. ",10,194,12.933333333333334,5.605405405405405,119,0,194,0.0,0.0151515151515151,-0.9118,62,19,32,16,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 7, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,2,1,1,2,7,1,0,0,0,0,2,0,0,0,3,0,1,0,0,5,1,0,0.5016081811599674,0.4470224893528526,0.28420342686207545
ICLR2018-B1NGT8xCZ-R3,Reject,"The authors propose a probabilistic framework for semi-supervised learning and domain adaptation. By varying the prior distribution, the framework can incorporate both generative and discriminative modeling. The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation. A domain confusion loss is added to learn domain-invariant feature representations. The authors compared Dauto with several baseline methods on several datasets and showed improvement. The paper is well-organized and easy to follow. The probabilistic framework itself is quite straight-forward. The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme. The proposed DAuto is essentially DANN+autoencoder. The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator. It would be interesting to see if the additional auto-encoder part help address the issue. The experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017). It could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block. DAuto does seem to offer more boost in domain pairs that are less similar. ",14,229,14.3125,5.484162895927602,125,0,229,0.0,0.017391304347826,0.9243,68,42,37,8,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 9, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 3, 'CLA': 1}",0,1,2,2,1,9,2,0,0,0,0,1,0,0,0,0,0,1,1,0,3,3,1,0.5022275204662258,0.5570128234418844,0.3169280460773395
ICLR2018-B1NOXfWR--R1,Reject,"In the context of multitask reinforcement learning, this paper considers the problem of learning behaviours when given specifications of subtasks and the relationship between them, in the form of a task graph. The paper presents a neural task graph solver (NTS), which encodes this as a recursive-reverse-recursive neural network. A method for learning this is presented, and fine tuned with an actor-critic method. The approach is evaluated in a multitask grid world domain. This paper addresses an important issue in scaling up reinforcement learning to large domains with complex interdependencies in subtasks. The method is novel, and the paper is generally well written. I unfortunately have several issues with the paper in its current form, most importantly around the experimental comparisons. The paper is severely weakened by not comparing experimentally to other learning (hierarchical) schemes, such as options or HAMs. None of the comparisons in the paper feature any learning. Ideally, one should see the effect of learning with options (and not primitive actions) to fairly compare against the proposed framework. At some level, I question whether the proposed framework is doing any more than just value function propagation at a task level, and these experiments would help resolve this. Additionally, the example domain makes no sense. Rather use something more standard, with well-known baselines, such as the taxi domain. I would have liked to see a discussion in the related work comparing the proposed approach to the long history of reasoning with subtasks from the classical planning literature, notably HTNs. I found the description of the training of the method to be rather superficial, and I don't think it could be replicated from the paper in its current level of detail. The approach raises the natural questions of where the tasks and the task graphs come from. Some acknowledgement and discussion of this would be useful. The legend in the middle of Fig 4 obscures the plot (admittedly not substantially). There are also a number of grammatical errors in the paper, including the following non-exhaustive list: 2: as well as how to do -> as well as how to do it Fig 2 caption: through bottom-up -> through a bottom-up 3: Let S be a set of state -> Let S be a set of states 3: form of task graph -> form of a task graph 3: In addtion -> In addition 4: which is propagates -> which propagates 5: investigated following -> investigated the following",21,401,21.105263157894736,5.275401069518717,191,4,397,0.0100755667506297,0.0269607843137254,0.9438,119,38,62,26,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 8, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 6, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 3}",0,0,2,2,0,6,8,0,2,0,0,2,0,0,0,1,0,6,1,0,5,1,3,0.4306342642153096,0.6699025793336966,0.3013284753517305
ICLR2018-B1NOXfWR--R2,Reject,"Summary: the paper proposes an idea for multi-task learning where tasks have shared dependencies between subtasks as task graph. The proposed framework, task graph solver (NTS), consists of many approximation steps and representations: CNN to capture environment states, task graph parameterization, logical operator approximation; the idea of reward-propagation policy helps pre-training. The framework is evaluated on a relevant multi-task problem. In general, the paper proposes an idea to tackle an interesting problem. It is well written, the idea is well articulated and presented. The idea to represent task graphs are quite interesting. However it looks like the task graph itself is still simple and has limited representation power. Specifically, it poses just little constraints and presents no stochasticity (options result in stochastic outcomes). The method is evaluated in one experiment with many different settings. The task itself is not too complex which involves 10 objects, and a small set of deterministic options . It might be only complex when the number of dependency layer is large. However, it's still more convinced if the paper method is demonstrated in more domains. About the description of problem statement in Section 3: - How the MDP M and options are defined, e.g. transition functions, are tochastic? - What is the objective of the problem in section 3 Related work: many related work in robotics community on the topic of task and motion planning (checkout papers in RSS, ICRA, IJRR, etc.) should also be discussed.",16,238,17.0,5.4094827586206895,140,1,237,0.0042194092827004,0.0373443983402489,0.699,86,21,40,15,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 7, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 0}",0,0,1,7,0,7,1,1,0,0,0,1,0,0,0,0,0,1,0,0,12,0,0,0.430428380425271,0.2290637540054949,0.1918090415767541
ICLR2018-B1NOXfWR--R3,Reject,"This paper proposes to train recursive neural network on subtask graphs in order to execute a series of tasks in the right order, as is described by the subtask graph's dependencies. Each subtask execution is represented by a (non-learned) option. Reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs. While this paper is as far as I can tell novel in how it does what it does, the authors have failed to convey to me why this direction of research is relevant. - We know finding options is the hard part about options n- We already have good algorithms that take subtask graphs and execute them in the right order from the planning litterature An interesting avenue would be if the subtask graphs were instead containing some level of uncertainty, or representing stochasticity, or anything that more traditional methods are unable to deal with efficiently, then I would see a justification for the use of neural networks. Alternatively, if the subtask graphs were learned instead of given, that would open the door to scaling an general learning. Yet, this is not discussed in the paper. Another interesting avenue would be to learn the options associated with each task, possibly using the information from the recursive neural networks to help learn these options.   The proposed algorithm relies on fairly involved reward shaping, in that it is a very strong signal of supervision on what the next action should be. Additionaly, it's not clear why learning seems to completely fail without the pre-trained policy. The justification given is that it is to address the difficulty of training due to the complex nature of the problem but this is not really satisfying as the problems are not that hard. This also makes me question the generality of the approach since the pre-trained policy is rather simple while still providing an apparently strong score. In your experiments, you do not compare with any state-of-the-art RL or hierarchical RL algorithm on your domain, and use a new domain which has no previous point of reference. It it thus hard to properly evaluate your method against other proposed methods. What the authors propose is a simple idea, everything is very clearly explained, the experiments are somewhat lacking but at least show an improvement over more a naive approach, however, due to its simplicity, I do not think that this paper is relevant for the ICLR conference. Comments: - It is weird to use both a discount factor gamma *and* a per-step penalty. While not disallowed by theory, doing both is redundant because they enforce the same mechanism. - It seems weird that the smoothed logical AND/OR functions do not depend on the number of inputs; that is unless there are always 3 inputs (but it is not explained why; logical functions are usually formalised as functions of 2 inputs) as suggested by Fig 3. - It does not seem clear how the whole training is actually performed (beyond the pre-training policy). The part about the actor-critic learning seems to lack many elements (whole architecture training? why is the policy a sum of p^{cost} and p^{reward}?  is there a replay memory? How are the samples gathered?).  (On the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.) - The experiments cover different settings with different task difficulties. However, only one type of tasks is used. It would be good to motivate (in addition to the paragraph in the intro) the cases where using the algorithm described in the paper may be (or not?) the only viable option and/or compare it to other algorithms. Even tough not mandatory, it would also be a clear good addition to also demonstrate more convincing experiments in a different setting. - The episode length (time budget) was randomly set for each episode in a range such that 60% u2212 80% of subtasks are executed on average for both training and testing.  --> this does not seem very precise: under what policy is the 60-80% defined? Is the time budget different for each new generated environment? - why wait until exactly 120 epochs for NTS-RProp before fine-tuning with actor-critic?  It seems that much less would be sufficient from figure 4? - In the table 1 caption, it is written same graph structure with training set --> do you mean same graph structure than the training set?",39,730,28.07692307692308,5.018518518518518,322,9,721,0.0124826629680998,0.032171581769437,0.9673,175,85,145,53,9,7,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 5, 'DAT': 0, 'MET': 15, 'EXP': 17, 'RES': 0, 'TNF': 3, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 23, 'SUB': 3, 'CLA': 1}",0,2,1,5,0,15,17,0,3,0,1,2,0,1,0,2,1,3,0,1,23,3,1,0.6480936982236694,0.7919536270598643,0.5096974565360095
ICLR2018-B1QRgziT--R1,Accept,"This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator. This is demonstrated in comparison to weight normalization in Figure 4. The experimental results are very good and give strong support for the proposed normalization. While the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art. More details in the comments below. Comments: 1. One concern about this paper is that it doesn't fully answer the reasons why this normalization works better. I found the discussion about rank to be very intuitive, however this intuition is not fully tested. Figure 4 reports layer spectra for SN and WN. The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. 2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win? 3. Section 4 needs some careful editing for language and grammar. ",21,363,16.5,5.051873198847262,200,1,362,0.0027624309392265,0.021917808219178,0.9835,94,41,69,26,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 12, 'EXP': 3, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 2}",0,0,6,2,0,12,3,1,2,0,0,3,0,0,0,1,0,2,0,1,6,3,2,0.5035003797856321,0.6701881044855685,0.3541975338440532
ICLR2018-B1QRgziT--R2,Accept,"This paper proposes spectral normalization -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function. The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient. Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods. Overall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach. The experimental results seem solid and seem to support the authors' claims. I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer. Like the anonymous commenter, I also initially thought that the proposed spectral normalization  is basically the same as spectral norm regularization, but given the authors' feedback on this I think the differences should be made more explicit in the paper. Overall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication. Small Nits:   Section 4: In order to evaluate the efficacy of our experiment: I think you mean approach. There are a few colloquial English usages which made me smile, e.g.   * Sec 4.1.1. As we prophesied ..., and in the paragraph below   * ... is a tad slower ....",10,233,17.923076923076923,5.413636363636364,134,3,230,0.0130434782608695,0.0404858299595141,0.975,69,37,35,9,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 5, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 0, 'CLA': 3}",0,1,2,2,1,5,3,2,0,1,0,2,0,0,0,0,0,2,0,1,5,0,3,0.6442712113606986,0.4472117068827144,0.3559697125151349
ICLR2018-B1QRgziT--R3,Accept,"The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator. The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a spectrally normalized objective. I think the methodology presented in this paper is neat and the experimental results are encouraging. However, I do have some comments on the presentation of the paper:  1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato). For example,  Matrix Analysis, book by Bhatia Matrix computation, book by Golub and Van Loan.  Some recent work in theory of (noisy) power method might also be helpful and should be cited, for example, https://arxiv.org/abs/1311.2495  2. I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients. Please clarify this. 3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator. Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?",9,246,20.5,5.262711864406779,135,4,242,0.0165289256198347,0.04,0.9853,70,30,43,15,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,0,3,3,0,3,2,3,0,0,0,0,1,0,0,1,0,2,0,0,5,0,1,0.4295489879046049,0.4470494367410063,0.2374091988277686
ICLR2018-B1QgVti6Z-R1,Accept,"This paper studies empirical risk in deep neural networks. Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks. Results for deep linear neural networks are puzzling. Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression. So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play? Also this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d. from a given distribution D. In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.  ",7,136,17.0,5.048780487804878,70,1,135,0.0074074074074074,0.0289855072463768,0.2732,42,16,25,8,4,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,3,0,4,0,0,0,0,0,0,0,0,0,1,1,0,3,0,0,0.2863817756199834,0.3345772482030192,0.14301042667409752
ICLR2018-B1QgVti6Z-R2,Accept,"This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs). Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks. The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts. 2) Uniform convergence of the empirical risk to population risk. 3) Generalization bound based on stability. The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations. Here are two detailed comments:  1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points. Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y XW. Should the risk bound only depends on the dimensions of the matrix W? 2) The comparison with Bartlett & Maass's (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations. Thus, the authors need to refine BM's result for comparison.",11,169,16.9,5.81875,95,0,169,0.0,0.0116959064327485,-0.9169,55,32,21,7,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 6, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,4,2,0,3,0,6,0,3,0,0,0,0,0,0,0,3,0,0,5,0,0,0.4297005245927854,0.224944377075974,0.19565188482330537
ICLR2018-B1QgVti6Z-R3,Accept,"Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape. The analysis answers:  1) When empirical gradients are close to true gradients n2) When empirical isolated saddle points are close to true isolated saddle points 3) When the empirical risk is close to the true risk. The answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities. I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct. Another reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data. Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not. An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern. However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one. Overall, I believe this paper is a nice contribution to the deep learning theory literature. However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem. Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters. I suspect their upper bounds will be loose, but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds.",15,431,30.785714285714285,5.183132530120482,203,1,430,0.0023255813953488,0.0344036697247706,0.9793,117,64,64,23,10,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 5, 'MET': 5, 'EXP': 1, 'RES': 7, 'TNF': 0, 'ANA': 5, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 3}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 0}",0,0,2,1,5,5,1,7,0,5,1,1,0,3,1,0,0,2,0,0,8,3,0,0.7159630786178468,0.4491286620621781,0.3987575514766775
ICLR2018-B1X0mzZCW-R1,Accept,"This paper suggests a simple yet effective approach for learning with weak supervision. This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision. The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the fidelity of the weak label when training the student at the final step. The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students. The suggested method seems to work well on several document classification tasks. Overall, I liked the paper. I would like the authors to consider the following questions -   - Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few). First, I'd suggest acknowledging these works and discussing the differences to your work. Second - Is your approach applicable to these frameworks? It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve. - Can this approach be applied to semi-supervised learning? Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework? - The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data.  Is it clear that this step in needed?  Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?using different architecture than the student? - I went over the authors comments and I appreciate their efforts to help clarify the issues raised.",16,307,27.90909090909091,5.418367346938775,158,2,305,0.0065573770491803,0.0282131661442006,0.9513,84,40,59,6,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 3, 'MET': 12, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 0}",0,0,3,0,3,12,1,2,0,1,0,1,0,1,0,0,0,3,0,0,9,1,0,0.5744695438574672,0.338543317926457,0.28823532130411106
ICLR2018-B1X0mzZCW-R2,Accept,"The problem of interest is to train deep neural network models with few labelled training samples. The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data. The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching. The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model. The teacher also supplies an uncertainty estimate to each predicted label. How about the heuristic function? This is used for learning initial feature representation of the student model. Crucially, the teacher model will also rely on these learned features. Labelled data and unlabelled data are therefore lie in the same dimensional space. Specific questions to be addressed: 1)tClustering of strongly-labelled data points. Thinking about the statement ""each an expert on this specific region of data space"", if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points. Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data. On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited. As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations). It will be informative to provide results with a single GP model. 2)tFrom modifying learning rates to weighting samples. Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more ""intuitive"" to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty. Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression).  ",18,359,19.944444444444443,5.564841498559078,178,1,358,0.0027932960893854,0.0083102493074792,0.6486,121,49,49,16,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 8, 'MET': 11, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,0,0,2,8,11,1,1,0,0,0,0,0,1,0,0,0,0,0,0,9,2,0,0.4313852847503853,0.2273045582092874,0.19755083266794204
ICLR2018-B1X0mzZCW-R3,Accept,"The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data. This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable. The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework. The authors evaluate their proposed methods on one toy problem and two real-world problems. The paper is well written, easy to follow, and have good experimental study. My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model? To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data? (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)? (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly? Why not learning the representation using an unsupervised learning method (unsupervised pre training)? This should be at least one of the baselines. (4) the idea of using surrogate labels to learn representation is also not new. One example work is Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. The authors didn't compare their method with this one.",14,308,30.8,5.050675675675675,142,2,306,0.0065359477124183,0.0354838709677419,-0.6497,81,39,55,16,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 5, 'MET': 10, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",0,1,2,1,5,10,4,0,0,0,1,1,0,0,0,1,1,1,0,0,7,1,1,0.5741637386897173,0.6703984112757245,0.3985484831679424
ICLR2018-B1X4DWWRb-R1,Reject,"The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y   f(T,X) is a function of a treatment T and covariates X. The goal is to infer the treatment effect E(Y|T 1,X x) - E(Y|T 0,X x) for binary treatments at every location x. If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still work and the distribution of X can also change without affecting the accuracy of the predictions. What is proposed seems to be twofold: - instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound. The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size. - as an additional ingredient the authors also propose representation learning by mapping x to some representation Phi(x). The goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance. Pros:  - The problem is relevant and also appears in similar form in domain adaptation and transfer learning. - The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al. Cons: - I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others. - The manuscript is written in a very compact style and I wish some passages would have been explained in more depth and detail. Especially the second half of page 5 is at times very hard to understand as it is so dense. - The implications of the assumptions in Theorem 1 are not easy to understand, especially relating to the quantities B_Phi, C^mathcal{F}_{n,delta} and D^{Phi,mathcal{H}}_delta. Why would we expect these quantities to be small or bounded? How does that compare to the assumptions needed for standard inverse probability weighting? - I appreciate that it is difficult to find good test datasets for evaluating causal estimator. The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?). The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.          ",18,429,25.23529411764705,5.130864197530864,218,3,426,0.0070422535211267,0.0266075388026607,0.8768,140,40,72,27,7,4,"{'ABS': 0, 'INT': 5, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 6, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 0}",0,5,1,2,2,6,2,0,0,0,0,2,0,0,1,0,0,0,1,0,7,2,0,0.5015800414876936,0.4482828655618237,0.2813047287188641
ICLR2018-B1X4DWWRb-R2,Reject,"This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function. Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs. Overall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs. The main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift. However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section.",5,123,24.6,5.741379310344827,79,0,123,0.0,0.016260162601626,0.7178,38,20,17,4,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,1,0,2,0,1,1,0,0,1,0,1,0,0,0,0,0,0,0,0,2,1,1,0.4286307284332843,0.3339552907681763,0.21665168020269324
ICLR2018-B1X4DWWRb-R3,Reject,"Summary: This paper proposes a new approach to tackle the problem of prediction under the shift in design, which consists of the shift in policy (conditional distribution of treatment given features) and the shift in domain (marginal  distribution of features). Given labeled samples from a source domain and unlabeled samples from a target domain, this paper proposes to minimize the risk on the target domain by  jointly learning the shift-invariant representation and the re-weighting  function for the induced representations. According to Lemma 1 and its finite sample version in Theorem 1, the risk on the target domain can be upper bounded by the combination of 1) the re-weighted empirical risk on the source domain; and 2) the distributional discrepancy between the re-weighted source domain and the target domain. These theoretical results justify the objective function shown in Equation 8. Experiments on the IHDP dataset demonstrates the advantage of the proposed approach compared to its competing alternatives. Comments: 1) This paper is well motivated. For the task of prediction under the shift in design, shift-invariant representation learning (Shalit 2017) is biased even in the inifite data limit. On the other hand, although re-weighting methods are unbiased, they suffer from the drawbacks of high variance and unknown optimal weights. The proposed approach aims to overcome these drawbacks. 2) The theoretical results justify the optimization procedures presented in section 5. Experimental results on the IHDP dataset confirm the advantage of the proposed approach. 3) I have some questions on the details. In order to make sure the second  equality in Equation 2 holds, p_mu (y|x,t)   p_pi (y|x,t) should hold as well. Is this a standard assumption in the literature? 4) Two drawbacks of previous methods motivate this work, including the bias of representation learning and the high variance of re-weighting. According to Lemma 1, the proposed method is unbiased for the optimal weights in the large data limit. However, is there any theoretical guarantee or empirical evidence to show the proposed method does not suffer from the drawback of high variance? 5) Experiments on synthetic datasets, where both the shift in policy and the shift in domain are simulated and therefore can be controlled, would better  demonstrate how the performance of the proposed approach (and thsoe baseline  methods) changes as the degree of design shift varies. 6) Besides IHDP, did the authors run experiments on other real-world datasets,  such as Jobs, Twins, etc?",20,400,23.529411764705884,5.457894736842105,175,0,400,0.0,0.0073349633251833,0.9516,127,41,60,9,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 13, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 0}",0,1,1,2,2,13,3,3,0,0,0,2,0,0,0,1,0,1,0,0,13,1,0,0.5748554815726236,0.4519079336625601,0.32986769279731354
ICLR2018-B1Yy1BxCZ-R1,Accept,"The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant. This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time. The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent. In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size. It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance. COMMENTS:  The paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales. The observation is explained well and substantiated by clear experimental evidence. The main issue I have is with the part about momentum. The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased. It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$. The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value.  This effect is well known, but it can easily be remedied. For example, the update equations in Adam were specifically designed to correct for this effect. The mechanism is called bias-corrected moment estimate in the Adam paper, arXiv:1412.6980. The correction requires only two extra multiplications per model parameter and update step. Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size?. It would be great to see the equivalent of Figure 7 with correctly rescaled $A$. Minor issues: * The last paragraph of Section 5 refers to a figure 8, which appears to be missing.  * In Eqs. 4 & 5, the momentum parameter $m$ is not yet defined (it will be defined in Eqs. 6 & 7 below). * It appears that a minus sign is missing in Eq. 7. The update steps describe gradient ascent. * Figure 3 suggests that most of the time between the first and second change of the noise scale (approx. epochs 60 to 120) are spent on overfitting. This suggests that the number of updates in this segment was chosen unnecessarily large to begin with. It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy. * It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs. While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs. * It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?",27,554,16.78787878787879,5.132183908045977,229,6,548,0.010948905109489,0.0141592920353982,0.9233,158,57,104,25,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 9, 'RES': 8, 'TNF': 6, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 6, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 0}",0,1,2,1,0,6,9,8,6,1,1,0,0,0,0,0,1,1,6,0,8,2,0,0.6453845607146188,0.5603237787267945,0.4099400149447539
ICLR2018-B1Yy1BxCZ-R2,Accept,"The paper represents an empirical validation of the well-known idea (it was published several times before)  to increase the batch size over time. Inspired by recent works on large-batch studies, the paper suggests to adapt the learning rate as a function of the batch size. I am interested in the following experiment to see how useful it is to increase the batch size compared to fixed batch size settings. 1) The total budget / number of training samples is fixed. 2) Batch size is scheduled to change between B_min and B_max 3) Different setting of B_min and B_max> B_min are considered, e.g., among [64, 128, 256, 512, ...] or [64, 256, 1024, ...] if it is too expensive. 4) Drops of the learning rates are scheduled to happen at certain times represented in terms of the number of training samples passed so far (not parameter updates). 5) Learning rates and their drops should be rescaled taking into account the schedule of the batch size and the rules to adapt learning rates in large-scale settings as by Goyal. ",7,174,21.75,4.8,91,1,173,0.0057803468208092,0.0167597765363128,0.9367,54,14,34,6,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,1,1,0,2,5,0,0,0,0,0,0,0,0,0,0,1,0,0,3,0,0,0.3577378324234765,0.2234661370919081,0.1618235860743647
ICLR2018-B1Yy1BxCZ-R3,Accept,"## Review Summary  Overall, the paper's paper core claim, that increasing batch sizes at a linear rate during training is as effective as decaying learning rates, is interesting but doesn't seem to be too surprising given other recent work in this space. The most useful part of the paper is the empirical evidence to backup this claim, which I can't easily find in previous literature. I wish the paper had explored a wider variety of dataset tasks and models to better show how well this claim generalizes, better situated the practical benefits of the approach (how much wallclock time is actually saved? how well can it be integrated into a distributed workflow?), and included some comparisons with other recent recommended ways to increase batch size over time. ## Pros / Strengths  + effort to assess momentum / Adam / other modern methods + effort to compare to previous experimental setups ## Cons / Limitations  - lack of wallclock measurements in experiments - only ~2 models / datasets examined, so difficult to assess generalization - lack of discussion about distributed/asynchronous SGD ## Significance  Many recent previous efforts have looked at the importance of batch sizes during training, so topic is relevant to the community. Smith and Le (2017) present a differential equation model for the scale of gradients in SGD, finding a linear scaling rule proportional to eps N/B, where eps   learning rate, N   training set size, and B   batch size. Goyal et al (2017) show how to train deep models on ImageNet effectively with large (but fixed) batch sizes by using a linear scaling rule. A few recent works have directly tested increasing batch sizes during training. De et al (AISTATS 2017) have a method for gradually increasing batch sizes, as do Friedlander and Schmidt (2012). Thus, it is already reasonable to practitioners that the proposed linear scaling of batch sizes during training would be effective. While increasing batch size at the proposed linear scale is simple and seems to be effective, a careful reader will be curious how much more could be gained from the backtracking line search method proposed in De et al. ## Quality  Overall, only single training runs from a random initialization are used. It would be better to take the best of many runs or to somehow show error bars, to avoid the reader wondering whether gains are due to changes in algorithm or to poor exploration due to bad initialization. This happens a lot in Sec. 5.2. Some of the experimental setting seem a bit haphazard and not very systematic. In Sec. 5.2, only two learning rate scales are tested (0.1 and 0.5). Why not examine a more thorough range of values? Why not report actual wallclock times? Of course having reduced number of parameter updates is useful, but it's difficult to tell how big of a win this could be. What about distributed SGD or asyncronous SGD (hogwild)? Small batch sizes sometimes make it easier for many machines to be working simultaneously. If we scale up to batch sizes of ~ N/10, we can only get 10x speedups in parallelization (in terms of number of parameter updates). I think there is some subtle but important discussion needed on how this framework fits into modern distributed systems for SGD. ## Clarity  Overall the paper reads reasonably well. Offering a related work feature matrix that helps readers keep track of how previous efforts scale learning rates or minibatch sizes for specific experiments could be valueable. Right now, lots of this information is just provided in text, so it's not easy to make head-to-head comparisons. Several figure captions should be updated to clarify which model and dataset are studied. For example, when skimming Fig. 3's caption there is no such information. ## Paper Summary  The paper examines the influence of batch size on the behavior of stochastic gradient descent to minimize cost functions. The central thesis is that instead of the conventional wisdom to fix the batch size during training and decay the learning rate, it is equally effective (in terms of training/test error reached) to gradually increase batch size during training while fixing the learning rate. These two strategies are thus equivalent. Furthermore, using larger batches means fewer parameter updates per epoch, so training is potentially much faster. Section 2 motivates the suggested linear scaling using previous SGD analysis from Smith and Le (2017). Section 3 makes connections to previous work on finding optimal batch sizes to close the generaization gap.  Section 4 extends analysis to include SGD methods with momentum. In Section 5.1, experiments training a 16-4 ResNet on CIFAR-10 compare three possible SGD schedules: * increasing batch size * decaying learning rate * hybrid (increasing batch size and decaying learning rate) Fig. 2, 3 and 4 show that across a range of SGD variants (+/- momentum, etc) these three schedules have similar error vs. epoch curves. This is the core claimed contribution: empirical evidence that these strategies are equivalent. In Section 5.3, experiments look at Inception-ResNet-V2 on ImageNet, showing the proposed approach can reach comparable accuracies to previous work at even fewer parameter updates (2500 here, vs. u223c14000 for Goyal et al 2007) ",43,838,20.95,5.238636363636363,365,3,835,0.0035928143712574,0.0171428571428571,0.9974,267,105,159,46,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 14, 'PDI': 5, 'DAT': 5, 'MET': 12, 'EXP': 15, 'RES': 0, 'TNF': 3, 'ANA': 2, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 6, 'PNF': 3, 'REC': 0, 'EMP': 11, 'SUB': 6, 'CLA': 1}",0,0,14,5,5,12,15,0,3,2,1,1,0,0,0,0,1,6,3,0,11,6,1,0.6483152176327139,0.6741285741902603,0.4600626804677626
ICLR2018-B1Z3W-b0W-R1,Reject,"This paper proposes an iterative inference scheme for latent variable models that use inference networks. Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al. The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network). My main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach. It appears that the iterative method should result in direct improvement with additional samples and inference iterations. I am supposing this is at the test time.  It is not clear exactly when this will be useful.  I believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al. 2014 (they used this method to perform data imputation). The paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE. In summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method. ",11,213,14.2,5.004901960784314,106,1,212,0.0047169811320754,0.0416666666666666,0.8811,62,22,41,8,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 8, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,1,3,1,1,8,1,0,0,1,0,0,0,0,0,0,0,0,0,0,5,2,0,0.5019123206012607,0.2248167284699155,0.2217555182938114
ICLR2018-B1Z3W-b0W-R2,Reject,"This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution. The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm. This approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family). This source of error is often ignored in the literature, although there are some exceptions that may be worth mentioning: * Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class). * The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. * The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network. * A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs. They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables. I think this is good and potentially important work, although I do have some questions/concerns about the results in Table 1 (see below). Some more specific comments:  Figure 2: I think this might be clearer if you unrolled a couple of iterations in (a) and (c). (Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset. A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm. Section 4: The statement p(z) N(z;mu_p,Sigma_p) doesn't quite match the formulation of Rezende&Mohamed (2014). First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance. Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1. An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter. Why not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}? Table 1: These results are strange in a few ways: * The gap between the standard and iterative inference network seems very small (0.3 nats at most). This is much smaller than the gap in Figure 5(a). * The MNIST results are suspiciously good overall, given that it's ultimately a Gaussian approximation and simple fully connected architecture. I've read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don't think I've ever seen a number better than ~87 nats.",22,585,21.666666666666668,5.217391304347826,281,11,574,0.019163763066202,0.0287162162162162,0.9894,185,74,85,35,10,3,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 2, 'MET': 11, 'EXP': 4, 'RES': 4, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,1,6,1,2,11,4,4,2,0,0,1,0,1,0,0,0,4,1,0,13,0,0,0.7176751566893353,0.3411483102230189,0.3608452271231146
ICLR2018-B1Z3W-b0W-R3,Reject,"Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two. In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients. The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate. Although probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context. From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016). Indeed the paper perhaps anticipates this perspective and preemptively offers that variational inference is a qualitatively different optimization problem than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work. But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient. That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. Beyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE.  While these results are enlightening, most of the conclusions are not entirely unexpected. For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result. It would certainly seem strange if this were not the case. And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance. Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016). In terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison?  For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration. In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained. Other minor comment: * In Fig. 5(a), it seems like the performance of the standard inference model is still improving but the iterative inference model has mostly saturated. * A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not.",21,588,22.615384615384617,5.719424460431655,281,8,580,0.0137931034482758,0.0303541315345699,0.9963,162,84,92,39,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 3, 'RES': 8, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 0}",0,1,5,1,0,8,3,8,1,0,0,1,0,0,0,3,0,4,0,0,11,2,0,0.5740641726669835,0.4512123981418018,0.3216935729820919
ICLR2018-B1ZZTfZAW-R1,Reject,"The authors propose to use synthetic data generated by GANs as a replacement for personally identifiable data in training ML models for privacy-sensitive applications such as medicine. In particular it demonstrates adversarial training of a recurrent generator for an ICU monitoring multidimensional time series, proposes to evaluate such models by the performance (on real data) of supervised classifiers trained on the synthetic data (TSTR), and empirically analyzes the privacy implications of training and using such a model. This paper touches on many interesting issues -- deep/recurrent models of time series, privacy-respecting ML, adaptation from simulated to real-world domains. But it is somewhat unfocused and does not seem make a clear contribution to any of these. The recurrent GAN architecture does not appear particularly novel --- the authors note that similar architectures have been used for discrete tasks such language modeling (and fail to note work that uses convolutional or recurrent generators for video prediction, a more relevant continuous task, see e.g.  http://carlvondrick.com/tinyvideo/, or autoregressive approaches to deep models of time series, e.g. WaveNet https://arxiv.org/abs/1609.03499) and there is no obvious new architectural innovation.  I also find it difficult to assess whether the proposed model is actually generating reasonable time series. It may be true that one plot showing synthetic ICU data would not provide enough information to evaluate its actual similarity to the real data because it could not rule out that case that the model has captured the marginal distribution in each dimension but not joint structure. However producing marginal distributions that look reasonable is at least a *necessary* condition and without seeing those plots it is hard to rule out that the model may be producing highly unrealistic samples. The basic privacy paradigm proposed seems to be: 1. train a GAN using private data 2. generate new synthetic data, assume this data does not leak private information 3. train a supervised classifier on the private data so that the GAN training-sampling loop basically functions as an anonymization procedure. For this to pan out, we'd need to see that the GAN samples are a) useful for a range of supervised tasks, and b) do not leak private information. But the results  in Table 2 show that the TSTR results are quite a lot worse than real data in most cases, and it's not obvious that the small set of tasks evaluated are representative of all tasks people might care about. The attempts to demonstrate empirically that the GAN does not memorize training data aren't particularly convincing; this is an adversarial setting so the fact that a *particular* test doesn't reveal private data doesn't imply that a determined attacker wouldn't succeed. In this vein, the experiments with DP-u000fSGD are more interesting, although a more direct comparison would be helpful (it is frustrating to flip back and forth between Tables 2 and 3 in an attempt to tease out relative performance) and and it is not clear how the settings (u03b5 u000fu000fu000f  0.5 and u03b4 u2264 9.8 u00d7 10u22123) were selected or whether they provide a useful level of privacy. That said I agree this is an interesting avenue for future work. Finally it's worth noting that discarding patients with missing data is unlikely to be innocuous for ICU applications; data are quite often not missing at random (e.g., a patient going into a seizure may dislocate a sensor). It appears that the analysis in this paper threw out more than 90% of the patients in their original dataset, which would present serious concerns in using the resulting synthetic data to represent the population at large. One could imagine coding missing data in various ways (e.g. asking the generator to produce a missingness pattern as well as a time series and allowing the discriminator to access only the masked time series, or explicitly building a latent variable model) and some sort of principled approach to missing data seems crucial for meaningful results on this application. ",19,651,27.125,5.305825242718447,313,8,643,0.0124416796267496,0.0410334346504559,0.1706,170,103,113,44,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 6, 'MET': 8, 'EXP': 7, 'RES': 1, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 3, 'CLA': 0}",0,0,0,3,6,8,7,1,2,3,0,1,0,0,0,1,1,0,0,0,11,3,0,0.5740688811204342,0.4508773718095171,0.31817238536085624
ICLR2018-B1ZZTfZAW-R2,Reject,"In this paper, the authors propose a recurrent GAN architecture that generates continuous domain sequences. To accomplish this, they use a generator LSTM that takes in a sequence of random noise as well as a sequence of conditonal information and outputs a sequence. The discriminator LSTM takes a sequence (and conditional information) as input and classifies each element of the sequence as real or synthetic -- the entire sequence is then classified by vote. The authors evaluate on several synthetic tasks, as well as an ICU timeseries data task. Overall, I thought the paper was clearly written and extremely easy to follow. To the best of my knowledge, the method proposed by the authors is novel, and differs from traditional sentence generation (as an example) models because it is intended to produce continuous domain outputs. Furthermore, the story of generating medical training data for public release is an interesting use case for a model like this, particularly since training on synthetic data appears to achieve not competitive but quite reasonable accuracy, even when the model is trained in a differentially private fashion. My most important piece of feedback is that I think it would be useful to include a few examples of the eICU time series data, both real and synthetic. This might give a better sense of: (1) how difficult the task is, (2) how much variation there is in the real data from patient to patient, and (3) how much variation we see in the synthetic time series. Are the synthetic time series clearly multimodal, or do they display some of the mode collapse behavior occasionally seen in GANs? I would additionally like to see a few examples of the time series data at both the 5 minute granularity and the 15 minute granularity. You claim that downsampling the data to 15 minute time steps still captures the relevant dynamics of the data -- is it obvious from the data that variations in the measured variables are not significant over a 5 minute interval? As it stands, this is somewhat an unknown, and should be easy enough to demonstrate.",13,349,31.727272727272727,5.08868501529052,171,3,346,0.0086705202312138,0.0227920227920227,0.9782,91,45,51,21,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 4, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,0,2,4,6,1,0,0,1,0,1,0,0,0,1,0,1,0,0,4,0,1,0.5014185326504595,0.4463103167489733,0.2817328948811422
ICLR2018-B1ZZTfZAW-R3,Reject,"This paper proposes to use RGANs and RCGANS to generate synthetic sequences of actual data. They demonstrate the quality of the sequences on sine waves, MNIST, and ICU telemetry data. The authors demonstrate novel approaches for generating real-valued sequences using adversarial training, a train on synthetic, test of real and vice versa method for evaluating GANS, generating synthetic medical time series data, and an empirical privacy analysis. Major - the medical use case is not motivating. de-identifying the 4 telemetry measures is extremely easy and there is little evidence to show that it is even possible to reidentify individuals using these 4 measures. our institutional review board would certainly allow self-certification of the data (i.e. removing the patient identifiers and publishing the first 4 hours of sequences). - the labels selected by the authors for the icu example are to forecast the next 15 minutes and whether a critical value is reached. Please add information about how this critical value was generated. Also it would be very useful to say that a physician was consulted and that the critical values were clinically useful. - the changes in performance of TSTR are large enough that I would have difficulty trusting any experiments using the synthetic data. If I optimized a method using this synthetic data, I would still need to assess the result on real data. - In addition it is unclear whether this synthetic process would actually generate results that are clinically useful. The authors certainly make a convincing statement about the internal validity of the method. An externally valid measure would strengthen the results. I'm not quite sure how the authors could externally validate the synthetic data as this would also require generating synthetic outcome measures. I think it would be possible for the synthetic sequence to also generate an outcome measure (i.e. death) based on the first 4 hours of stay. Minor - write in the description for table 1 what task the accuracies correspond. Summary The authors present methods for generating synthetic sequences. The MNIST example is compelling. However the ICU example has some pitfalls which need to be addressed.",20,348,15.818181818181818,5.402402402402402,174,1,347,0.0028818443804034,0.0481586402266288,0.9668,97,48,64,18,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 9, 'MET': 7, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,1,0,0,9,7,2,3,1,3,0,0,0,0,0,1,0,0,0,0,7,1,0,0.5020369169402159,0.3370650779423911,0.2465537677454814
ICLR2018-B1ZvaaeAZ-R1,Accept,"The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision. The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks. Positives - Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision. Negatives - While the exhaustive analysis is extremely useful the overall technical contribution of the paper that of widening the networks is fairly small. - The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches. However, the results are more focused on compute cost. Also large batches are used mainly during training where memory is generally not a huge issue. Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes. It might help to emphasize the speed-up in compute more in the contributions.  ",10,199,19.9,5.324873096446701,109,4,195,0.0205128205128205,0.0196078431372549,0.5609,64,24,35,9,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 2, 'EXP': 3, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,0,1,2,2,3,4,0,1,0,0,0,0,0,0,0,0,0,0,5,1,0,0.5006257801152239,0.2247100519615941,0.22609631587621848
ICLR2018-B1ZvaaeAZ-R2,Accept,"This is a well-written paper with good comparisons to a number of earlier approaches. It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs. Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper. Originality This seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction. Significance Going down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design. It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today. Pros: - Positive results with low precision (4-bit, 2-bit and even 1-bit) - Moving the state of the art in low precision forward - Strong potential impact, especially on constrained power environments (but not limited to them) - Uses same hyperparameters as original training, making the process of using this much simpler. Cons/Questions - They mention not quantizing the first and last layer of every network. How much does that impact the overall compute? - Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model?  This could be interesting for low power case, even if the effective compute is larger than the baseline. ",14,234,23.4,5.214912280701754,130,1,233,0.0042918454935622,0.024793388429752,0.943,65,42,31,12,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 2, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 3, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,0,5,0,0,4,2,5,0,0,2,2,0,0,0,2,3,4,0,0,7,0,1,0.4300025096945279,0.5597799687175972,0.27009604702325596
ICLR2018-B1ZvaaeAZ-R3,Accept,"This paper presents an simple and interesting idea to improve the performance for neural nets. The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced). The paper is aiming to solve a practical problem, and has done some solid research work to validate that. In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach. In addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet. The results are convincing to me. On the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study. So the novelty is limited. So overall given limited novelty  but the paper presents useful results, I would recommend borderline leaning towards reject.",11,172,19.11111111111111,5.035714285714286,105,0,172,0.0,0.0115606936416184,0.94,45,20,33,10,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 6, 'DAT': 1, 'MET': 1, 'EXP': 1, 'RES': 5, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,0,0,6,1,1,1,5,0,1,0,1,0,0,0,2,0,1,0,1,8,0,0,0.5005193068801166,0.4488432540728634,0.28512530024334465
ICLR2018-B1ae1lZRb-R1,Accept,"The authors investigate knowledge distillation as a way to learn low precision networks. They propose three training schemes to train a low precision student network from a teacher network. They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works   Pros: (+) The paper is well written, the schemes are well explained (+) Ablations are thorough and comparisons are fair  Cons: (-) The gap with full precision models is still large  (-) Transferability of the learned low precision models to other tasks is not discussed   The authors tackle a very important problem, the one of learning low precision models without comprosiming performance. For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks. One observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks (see Table 1, 2 & 3). As a matter of fact, under some scenarios performance even decreases. The authors do not discuss the gains of their best low-precision regime in terms of computation and memory. Finally, the true applications for models with a low memory footprint are not necessarily related to image classification models (e.g. ImageNet-1k). How good are the low-precision models trained by the authors at transferring to other tasks? Is it possible to transfer student-teacher training practices to other tasks?",14,236,23.6,5.546255506607929,120,0,236,0.0,0.020242914979757,-0.4565,83,32,34,12,9,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 11, 'EXP': 4, 'RES': 1, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 5, 'CLA': 1}",0,2,2,2,0,11,4,1,1,3,0,1,0,0,0,1,0,2,0,0,7,5,1,0.6458323968249032,0.5598311687550893,0.4075355940613223
ICLR2018-B1ae1lZRb-R2,Accept,"The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network. Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly. The paper shows an interesting result that the distilled low precision network actually performs better than high precision network. I found the paper interesting but the contribution seems quite limited. Pros: 1. The paper is well written and easy to read. 2. The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally. Cons: 1. The name Apprentice seems a bit confusing with apprenticeship learning. 2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights). 3. It is unclear how the proposed method outperforms other methods based on fine-tuning. It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model.",11,190,12.666666666666666,5.770949720670391,103,4,186,0.021505376344086,0.0368421052631578,0.9447,55,21,33,15,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 3, 'CLA': 1}",0,1,4,2,0,4,1,2,0,1,1,4,0,0,0,0,0,3,1,0,3,3,1,0.6441004410843657,0.5572471485562643,0.39872012180656974
ICLR2018-B1ae1lZRb-R3,Accept,Summary: The paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation. Scheme A consists of training a high precision teacher jointly with a low precision student. Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode. Review: The paper is well written.  The experiments are clear and the three different schemes provide good analytical insights. Using scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.   Comments: Tensorflow citation is missing.  Conclusion is short and a few directions for future research would have been useful.,8,121,15.125,5.990909090909091,68,0,121,0.0,0.0079365079365079,0.2263,43,18,23,5,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,0,1,0,4,2,1,0,1,1,1,1,0,0,0,1,0,1,0,4,0,1,0.6436939149902055,0.4463103167489733,0.36708636033973946
ICLR2018-B1al7jg0b-R1,Accept,"The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report. Figure (1) in the paper is identical to the one in the (short version of) the Jaeger report but is missing an explicit reference. Figure 2 is almost identical, again a reference to the original would be better. Conceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression. What I am missing here is a clear indication what is an original contribution of the paper, and what is already possible using the original approach. The fact that additional conceptors can be trained does not appear new for the approach described here. If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version. The evaluation also leaves me a little confused in an additional dimension: the paper title and abstract suggested that the contribution is about overcoming catastrophic forgetting. The evaluation shows that the approach performs better classifying MNIST digits than another approach. This is nice but doesn't really tell me much about overcoming catastrophic forgetting.  ",10,207,18.818181818181817,5.274111675126903,94,0,207,0.0,0.0478468899521531,0.9538,48,27,39,11,11,5,"{'ABS': 1, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 0, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",1,1,4,1,1,6,1,2,2,0,0,2,1,0,0,4,0,4,2,0,4,1,0,0.7873311286518276,0.5579698072090121,0.482399121907841
ICLR2018-B1al7jg0b-R2,Accept,"[Reviewed on January 12th]  This article applies the notion of ""conceptors"" -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation. After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation. Follows a section of experiments on variants of MNIST commonly used for continual learning.   Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea. The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic. The numeric examples, although quite toy, provide a clear illustration. A few things are still missing to back the strong claims of this paper: * Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer. This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures. * It could also be welcome to use a more grounded vocabulary, e.g. on p.2 ""Figure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources"" could be much more simply said as ""Figure 1 shows the ellipses corresponding to three sets of R^3 points"". Being less grandiose would make the value of this article nicely on its own. * Some examples beyond the contrived MNIST toy examples would be welcome. For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation. I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning. Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST. The authors transparently explain in their answer that they do not (yet!) belong to the deep learning community and hope finding some collaborations to pursue this further. If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication. The later point would normally make me attribute a score of 6: Marginally above acceptance threshold by current DL community standards, but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept. ",19,537,26.85,5.279527559055119,285,4,533,0.0075046904315197,0.0127737226277372,0.9929,150,66,79,33,12,7,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 3, 'FWK': 3, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 5, 'SUB': 6, 'CLA': 0}",0,1,1,1,1,10,2,1,1,3,3,4,0,1,0,2,4,1,2,1,5,6,0,0.8596747391472369,0.7810497765124222,0.6818639569270368
ICLR2018-B1al7jg0b-R3,Accept,"This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors. This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is unused until the present task in hand.   The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem. In Section 2 the authors review conceptors. This method is algebraic method closely related to spanning sub spaces and SVD. The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally. In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation. The authors provide a version with batch SGD as well.   In Section 4, the authors show their method on permuted MNIST. They compare the method to EWC with the same architecture. They show that their method more efficiently suffers on permuted MNIST from less degradation. Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance. In general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life. Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.   I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones. What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods. Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.   ",17,356,19.77777777777778,5.014577259475218,167,5,351,0.0142450142450142,0.0301369863013698,0.9511,94,42,65,16,6,4,"{'ABS': 0, 'INT': 3, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 16, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,3,2,1,2,16,0,0,0,0,1,0,0,0,0,0,1,2,0,0,7,1,0,0.4324919267791069,0.4482933516106923,0.24170549568986166
ICLR2018-B1bgpzZAZ-R1,Reject,"This paper gives an elaboration on the Gated Attention Reader (GAR) adding gates based on answer elimination in multiple choice reading comprehension. I found the formal presentation of the model reasonably clear the the empirical evaluation reasonably compelling. In my opinion the main weakness of the paper is the focus on the RACE dataset. This dataset has not attracted much attention and most work in reading comprehension has now moved to the SQUAD dataset for which there is an active leader board. I realize that SQUAD is not explicitly multiple choice and that this is a challenge for an answer elimination architecture. However, it seems that answer elimination might be applied to each choice of the initial position of a possible answer span.  In any case, competing with an active leader board would be much more compelling.",7,137,19.571428571428573,5.157894736842105,78,2,135,0.0148148148148148,0.0289855072463768,0.74,41,16,22,9,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 1, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,0,1,3,1,1,2,0,0,0,0,0,0,0,0,1,0,1,0,4,1,0,0.4286993119439249,0.4463103167489733,0.23938678189360793
ICLR2018-B1bgpzZAZ-R2,Reject,"In this paper, a model is built for reading comprehension with multiple choices. The model consists of three modules: encoder, interaction module and elimination module. The major contributions are two folds: firstly, proposing the interesting option elimination problem for multi-step reading comprehension;  and secondly, proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document representations. Intuitively, one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignored. The elimination module is interesting, but the usefulness of ""elimination"" is not well justified for two reasons. First, the improvement of the proposed model over the previous state of the art is limited. Second, the model is built upon GAR until the elimination module, then according to Table 1 it seems to indicate that the elimination module does not help significantly (0.4% improvement). In order to show the usefulness of the elimination module, the model should be exactly built on the GAR with an additional elimination module (i.e. after removing the elimination module, the performance should be similar to GAR but not something significantly worse with a 42.58% accuracy). Then we can explicitly compare the performance between GAR and the GAR w/ elimination module to tell how much the new module helps. Other issues:  1) Is there any difference to directly use $x$ and $h^z$ instead of $x^e$ and $x^r$ to compute $tilde{x}_i$? Even though the authors find the orthogonal vectors, they're gated summed together very soon. It would be better to show how much ""elimination"" and ""subtraction"" effect the final performance, besides the effect of subtraction gate.  2) A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model.   3) $c_i$ in page 5 is not defined. What's the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function? 4) It would be better to have the experiments trained with different $n$ to show how multi-hop effects the final performance, besides the case study in Figure 3 .  Minor issues:  1) In Eqn. (4), it would be better to use a vector as the input of softmax.   2) It would be easier for discussion if the authors could assign numbers to every equation.",19,383,21.27777777777778,5.236914600550964,184,1,382,0.0026178010471204,0.0178117048346055,0.9884,112,38,64,23,8,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 16, 'EXP': 3, 'RES': 3, 'TNF': 3, 'ANA': 5, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 2, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,2,2,1,0,16,3,3,3,5,0,0,0,0,0,0,0,2,2,0,9,2,0,0.5757167287797554,0.449705511912503,0.3261996301890417
ICLR2018-B1bgpzZAZ-R3,Reject,"This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations. It is a well-written paper, however, I am not very convinced by its motivation, the proposed model and the experimental results. First of all, the improvement is rather limited. It is only 0.4 improvement overall on the RACE dataset; although it outperforms GAR on 7 out of 13 categories; but why is it worse on the other 6 categories? I don't see any convincing explanations here. Secondly, in terms of the development of reading comprehension models, I don't see why we need to care about eliminating the irrelevant options. It is hard to generalize to any other RC/QA tasks. If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to.  The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option. Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined. Some visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that. ",14,229,20.818181818181817,5.354838709677419,117,1,228,0.0043859649122807,0.0389610389610389,0.9533,56,22,43,13,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 2, 'MET': 5, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 1}",0,1,3,1,2,5,2,3,0,2,0,1,0,0,0,0,0,2,0,0,6,3,1,0.6442772968146869,0.4478847471924922,0.3567563039779126
ICLR2018-B1e5ef-C--R1,Accept,"The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams. This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition. Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings. I didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.",6,110,18.33333333333333,5.375,67,1,109,0.0091743119266055,0.0363636363636363,0.4767,32,13,22,2,3,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,4,0,4,0,1,0,0,0,0,0,0,0,0,0,0,4,0,0,0.2152033963051966,0.11297698341564,0.0854726940675008
ICLR2018-B1e5ef-C--R2,Accept,"The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized. -In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.",4,60,15.0,5.839285714285714,43,1,59,0.0169491525423728,0.0333333333333333,0.9062,16,9,14,3,3,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,2,0,1,0,0,0,2,0,0,0,0,0,0,1,0,3,0,0,0.2145813252075495,0.2234661370919081,0.0954709114333942
ICLR2018-B1e5ef-C--R3,Accept,"My review reflects more from the compressive sensing perspective, instead that of deep learners. In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective. The paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow. The second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory. Partial explanations are provided, again using results in compressive sensing theory. In my personal opinion, the explanations are opaque and unsatisfactory. An alternative route is suggested in my detailed review. Finally, extensive experiments are conducted and they are in accordance with the theory. My most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5. Specifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not. This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery. In particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]. There, a simple deterministic condition (the null space property) for successful recovery is proved. It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs. Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all. While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC. Exactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise. Several minor comments:  1. Please avoid the use of ""information theory"", especially ""classical information theory"", in the current context. These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon. I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there's no need to mention information theory here. 2. In Theorem 4.1, please be specific about how the l2-regularization is chosen. 3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case. I understood the necessity only through reading proofs. 4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical? 5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017]. 6. Page 2, first paragraph of related work, the sentence ""Our method also closely related to ..."" is incomplete. 7. Page 2, second paragraph of related work, ""Pagliardini also introduceD a linear ...""  8. Page 9, conclusion, the beginning sentence of the second paragraph is erroneous. [1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, ""The Convex Geometry of Linear Inverse Problems"", Foundations of Computational Mathematics, 2012.",33,650,15.853658536585366,5.4819078947368425,303,3,647,0.0046367851622874,0.018348623853211,0.9826,196,98,97,35,10,6,"{'ABS': 0, 'INT': 0, 'RWK': 14, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 3, 'FWK': 3, 'OAL': 1, 'BIB': 1, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 2}",0,0,14,1,0,9,1,3,0,3,3,1,1,2,0,0,3,3,2,0,8,3,2,0.7175120265450169,0.6717068241331435,0.507323014153947
ICLR2018-B1gJ1L2aW-R1,Accept,"The paper considers a problem of adversarial examples applied to the deep neural networks. The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples. More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4). Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time. In other words, the paper proposes a particular approach for the adversarial defence. It turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) . Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours. The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples. For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN. These values are finally used as features in classifying adversarial examples from normal and noisy ones. The authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature. The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1). Interestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design. Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time. In other words, the proposed technique is rather stable and can not be easily exploited. I really enjoyed reading this paper. All the statements are very clear, the structure is transparent and easy to follow. The writing is excellent. I found only one typo (page 8, We also NOTE that...), otherwise I don't actually have any comments on the text. Unfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method. However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper.  ",22,406,18.454545454545453,5.658031088082901,194,1,405,0.0024691358024691,0.0195599022004889,-0.2809,99,61,71,32,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 5, 'DAT': 2, 'MET': 10, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 4}",0,0,1,5,2,10,2,2,1,0,0,6,0,1,0,1,0,0,1,1,4,0,4,0.6457511537931923,0.5576648330726467,0.40688034500254444
ICLR2018-B1gJ1L2aW-R2,Accept,"This paper tried to analyze the subspaces of the adversarial examples neighborhood. More specifically, the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces. The characteristics and theoretical analysis of the proposed method are discussed and explained. This paper helps others to better understand the vulnerabilities of DNNs.",4,53,13.25,6.188679245283019,35,0,53,0.0,0.0188679245283018,0.34,14,6,10,4,4,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,2,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0.2860237776899261,0.1111111111111111,0.11539005048500703
ICLR2018-B1gJ1L2aW-R3,Accept,"The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly. The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility.  My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector. Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique. However, this does not invalidate the contributions of this manuscript. Specific comments/questions: - (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting. - Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described. - Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique? - Alg 1, L12: Is LID_norm computed using a leave-one-out estimate? Otherwise, r_1(.) for each point is 0, leading to a somewhat under-estimate of the true LID of the normal points in the training set. I understand that it is not an issue in the test set. - Section 4 and Alg 1: S we do not really care about the labels/targets of the examples. All examples in the dataset are considered  ormal to start with. Is this assuming that the initial training set which is used to obtain the pre-trained DNN free of adversarial examples? - Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the leave-one-out version of the estimation. - Section 5: The authors have done a great job at evaluating every aspect of the proposed method. ",16,305,21.785714285714285,5.278169014084507,153,3,302,0.009933774834437,0.0095238095238095,0.777,83,35,52,19,6,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 6, 'MET': 8, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 0, 'CLA': 0}",0,0,0,1,6,8,3,3,0,0,0,1,0,0,0,0,0,0,0,0,15,0,0,0.4307870224362249,0.1198185151989127,0.17695731161473
ICLR2018-B1hYRMbCW-R1,Accept,"This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1. The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss. The reasons for this choice are discussed and linked to theoretical properties of OT. Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset. The paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea. Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport. Discussion:  + The paper spends a lot of time justifying the proposed method by discussing the limits of the Improved training of Wasserstein GAN from Gulrajani et al. (2017). The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix. The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next). + The proposed approach is a relaxation of the constraints on the dual variable for the OT problem. As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss. This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture. + The paper is rather vague on the reason to go from Eq. (6) to Eq. (7). (gradient approximation between samples to gradient on samples). Does it lead to better stability to choose one or the other? How is it implemented in practice?  recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix. Numerical experiments comparing the two implementation or at least a discussion is necessary. + The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization. I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve. A discussion of the links is necessary and will clearly bring more theoretical ground to the method. Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization. In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that. + The numerical experiments are encouraging but a bit short. The 2D example seem to work very well and the convergence curves are far better with the proposed regularization. But the real data CIFAR experiments are much less detailed with only a final inception score (very similar to the competing method) and no images even in appendix. The authors should also define (maybe in appendix) the conditional and unconditional inception scores and why they are important (and why only some of them are computed in Table 1). + This is more of a suggestion. The comparison of the dual critic to the true Wasserstein distance is very interesting. It would be nice to see the behavior for different values of lambda. [1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447. Review update after reply:  The authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT. I change the rating to Accept.   001 101",32,697,17.871794871794872,5.275650842266463,289,4,693,0.0057720057720057,0.0253878702397743,0.998,197,77,121,42,11,7,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 2, 'MET': 18, 'EXP': 8, 'RES': 2, 'TNF': 2, 'ANA': 3, 'FWK': 1, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 9, 'CLA': 1}",0,0,4,2,2,18,8,2,2,3,1,2,1,0,0,2,1,5,0,1,12,9,1,0.7910816339221861,0.7859864794409003,0.6226594717727612
ICLR2018-B1hYRMbCW-R2,Accept,"This paper is proposing a new formulation for regularization of Wasserstein Generative Adversarial models (WGAN). The original min/max formulation of the WGAN aim at minimizing over all measures, the maximal dispersion of expectation for 1-Lipschitz with the one provided by the empirical measure. This problem is often regularized by adding a gradient penalty, ie a  penalty of the form lambda E_{z~tau}}(||grad f (z)||-1)^2 where tau is the distribution of (tx+(1-x)y) where x is drawn according to the empirical measure and y is drawn according to the target measure. In this work the authors consider substituting the previous penalty by lambda E_{z~tau}}(max( ||grad f (z)||-1,0)^2. Overall the paper is too vague on the mathematical part, and the experiments provided are not particularly convincing in assessing the benefit of the new penalty. The authors have tried to use mathematical formulations to motivate their choice, but they lack rigorous definitions/developments to make their point convincing. They should also present early their model and their mathematical motivation: in what sense is their new penalty preferable? Presentation issues: - in printed black and white versions most figures are meaningless. - red and green should be avoided on the same plots, as colorblind people will not perceived any difference... - format for images should be vectorial (eps or pdf), not jpg or png... - legend/sizes are not readable (especially in printed version). References issues: - harmonize citations: if you add first name for some authors add them for all of them: why writing Harold W. Kuhn and C. Vilani for instance? - cramer->Cramer - wasserstein->Wasserstein (2x) - gans-> GANs - Salimans et al. is provided twice, and the second is wrong anyway. Specific comments:  page 1: - different more recent contributions -> more recent contributions - avoid double brackets )) page 2: - Please rewrite the first sentence below Definition 1 in a meaningful way. - Section 3: if mu is an empirical distribution, it is customary to write it mu_n or hat mu_n (in a way that emphasizes the number of observations available). - d is used as a discriminator and then as a distance. This is confusing... page 3: - f that plays the role of an appraiser (or critic)...: this paragraph could be extended and possibly elements of the appendix could be added here. - Section 4: the way clipping is presented is totally unclear and vague. This should be improved. - Eq (5): as written the distribution of tilde{x} tx+(1-t)y is meaningless: What is x and y in this context? please can you describe the distributions in a more precise way? - Proof of Proposition 5 (cf. page 13): this is a sketch of proof to me. Please state precise results using mathematical formulation. - Observation 1: real and generated data points are not introduced at this stage... data points are not even introduced neither! page 5: - the examples are hard to understand. It would be helpful to add the value of pi^* and f^* for both models, and explaining in details how they fit the authors model. - in Figure 2 the left example is useless to me. It could be removed to focus more extensively on the continuous case (right example). - the the -> the  page 6: - deterministic coupling could be discussed/motivated when introduced. Observation 3 states some property of non non-deterministic coupling but the concept itself seems somehow to appear out of the blue. page 10: - Figure 6: this example should be more carefully described in terms of distribution, f*, etc. page 14: - Proposition 1: the proof could be shorten by simply stating in the proposition that f and g are distribution... page 15: - we wish to compute-> we aim at showing? - f_1 is not defined sot the paragraph the latter equation... showing that almost surely x leq y is unclear to me, so is the result then. It could be also interesting to (geometrically) interpret the coupling proposed. The would help understanding the proof, and possibly reuse the same idea in different context. page 16: - proof of Proposition 2 : key idea here is using the positive and negative part of (f-g). This could simplify the proof.",36,665,17.05128205128205,5.153970826580227,305,4,661,0.0060514372163388,0.0157142857142857,0.8004,203,85,115,35,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 18, 'EXP': 2, 'RES': 4, 'TNF': 7, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 15, 'REC': 0, 'EMP': 18, 'SUB': 1, 'CLA': 1}",0,1,0,1,1,18,2,4,7,1,0,1,2,0,0,1,0,0,15,0,18,1,1,0.7189444624589676,0.5669907968811327,0.4590047796326045
ICLR2018-B1hYRMbCW-R3,Accept,"The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric. Basics on mass transportation are briefly recalled in section 2, while section 3 formulate the GANs approach in the Wasserstein context. Taking into account the Lipschitz constraint and (non-) differentiability of optimal critic functions f are discussed in section 4 and Section 5 proposes a way to penalize candidate functions f that do not satisfy the Lipschitz condition using a tuning parameter lambda, ruling a trade-off between marginal fitting and gradient control. The approach is illustrated by numerical experiments. Such results are hardly convincing, since the tuning of the parameter lambda plays a crucial role in the performance of the method. More importantly, The heuristic proposed in the paper is interesting and promising in some respects but there is a real lack of theoretical guarantees motivating the penalty form chosen, such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular.",7,168,28.0,5.7290322580645165,105,1,167,0.0059880239520958,0.0119047619047619,0.9218,55,18,26,4,4,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,6,3,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0.2871376378666264,0.111733068545954,0.11672140971478408
ICLR2018-B1hcZZ-AW-R1,Accept,"This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process. The draft is well-written, and the method is clearly explained. However, I have the following concerns for this draft:  1. The technical contribution is not enough. First, the use of reinforcement learning is quite straightforward. Second, the proposed method seems not significantly different from the architecture search method in [1][2] u2013 their major difference seems to be the use of ""remove"" instead of ""add"" when manipulating the parameters. It is unclear whether this difference is substantial, and whether the proposed method is better than the architecture search method. 2. I also have concern with the time efficiency of the proposed method. Reinforcement learning involves multiple rounds of knowledge distillation, and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations. Therefore, the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets (such as ImageNet). It would be necessary for the authors to make direct discussions on this issue, in order to convince others that their proposed method has practical value. [1] Zoph, Barret, and Quoc V. Le. Neural architecture search with reinforcement learning. ICLR (2017). [2] Baker, Bowen, et al. Designing Neural Network Architectures using Reinforcement Learning. ICLR (2017). ",11,229,11.45,5.784403669724771,121,3,226,0.0132743362831858,0.0346320346320346,0.7216,79,27,38,15,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 9, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 1}",0,1,3,1,1,9,1,1,0,0,0,1,1,1,0,0,0,3,0,0,4,3,1,0.7164482270007595,0.4467579948799962,0.39799201676621343
ICLR2018-B1hcZZ-AW-R2,Accept,"Summary: The manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model. The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model. After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding. This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection. The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy. As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models. Review: The manuscript describes the proposed algorithm in great detail and the description is easy to follow. The experimental analysis of the approach is very convincing and confirms the author's claims. Using the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation. I am looking forward to seeing work on the research goals outlined in the Future Directions section. A few questions/comments: 1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to? Are these multiple rollouts of the policies? If so, shouldn't the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)? I think I might have missed something here. 2) Minor: some of the citations are a bit awkward, e.g. on page 7: ""algorithm from Williams Williams (1992). I would use the citet command from natbib for such citations and citep for parenthesized citations, e.g. ""... incorporate dark knowledge (Hinton et al., 2015)"" or ""The MNIST (LeCun et al., 1998) dataset..."" 3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations. 4) Appendix: Section 8 states ""Below are the results"", but the figure landed on the next page. I would either try to force the figures to be output at that position (not in or after Section 9) or write Figures X-Y show the results. Also in Section 11, Figure 13 should be referenced with the ref command 5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)? 6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments? 7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution? Overall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score.",24,524,27.57894736842105,5.430641821946169,261,2,522,0.003831417624521,0.0095238095238095,0.9965,180,55,73,15,12,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 11, 'EXP': 7, 'RES': 3, 'TNF': 3, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 4, 'REC': 1, 'EMP': 12, 'SUB': 0, 'CLA': 0}",0,1,2,1,1,11,7,3,3,1,1,1,2,0,1,0,1,2,4,1,12,0,0,0.8604128336450092,0.6738100677785392,0.6150469285264947
ICLR2018-B1hcZZ-AW-R3,Accept,"On the positive side the paper is well written and the problem is interesting. On the negative side there is very limited innovation in the techniques proposed, that are indeed small variations of existing methods.  ",2,35,11.666666666666666,5.057142857142857,27,0,35,0.0,0.027027027027027,0.399,8,4,8,3,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 1, 'CLA': 1}",0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,1,0.2857142857142857,0.3333333333333333,0.14277909088247898
ICLR2018-B1i7ezW0--R1,Reject,"After reading the revision:  The authors addressed my detailed questions on experiments. It appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution. On the other hand, the theoretical part of the paper is not really improved in my opinion, I still can not see how previous work by Balestriero and Baraniuk 2017 motivates and backups the proposed method. My rating of this paper would remain the same.                                                                                               This paper propose to use the reconstruction loss, defined in a somewhat unusual way, as a regularizar for semi-supervised learning. Pros:  The intuition is that the ReLU network output is locally linear for each input, and one can use the conjugate mapping (which is also linear) for reconstructing the inputs, as in PCA. Realizing that the linear mapping is the derivative of network output w.r.t. the input (the Jacobian), the authors proposed to use the reconstruction loss defined in (8). Different from typical auto-encoders, this work does not require another reconstruction network, but instead uses the derivative. This observation is neat in my opinion, and does suggest a different use of the Jacobian in deep learning. The related work include auto-encoders where the weights of symmetric layers are tied. Cons:  The motivation (Section 2) needs to be improved. In particular, the introduction/review of the work of Balestriero and Baraniuk 2017 not very useful to the readers. Notations in eqns (2) and (3) are not fully explained (e.g., boldface c). Intuition and implications of Theorem 1 is not sufficiently discussed: what do you mean by optimal DNN, what is the criteria for optimality? is there a generative assumption of the data underlying the theorem? and the assumption of all samples being norm 1 seems too strong and perhaps limits its application? As far as I see, section 2 is somewhat detached from the rest of the paper. The main contribution of this paper is supposed to be the reconstruction mapping (6) and its effect in semi-supervised learning. The introduction of entropy regularization in sec 2.3 seems somewhat odd and obscures the contribution. It also bears the questions that how important is the entropy regularization vs. the reconstruction loss. In experiments, results with beta 1.0 need to be presented to assess the importance of network inversion and the reconstruction loss. Also, a comparison against typical auto-encoders (which uses another decoder networks, with weights possibly tied with the encoder networks) is missing.  ",22,411,18.681818181818183,5.258312020460358,181,6,405,0.0148148148148148,0.0411764705882352,0.9422,120,36,65,27,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 16, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 5, 'CLA': 0}",0,1,2,0,0,16,2,1,0,0,0,2,0,0,0,0,0,2,0,1,9,5,0,0.4325364127803418,0.4499639725136641,0.23947619471601073
ICLR2018-B1i7ezW0--R2,Reject,"In summary, the paper is based on a recent work Balestriero & Baraniuk 2017 to do semi-supervised learning. In Balestriero & Baraniuk, it is shown that any DNN can be approximated via a linear spline and hence can be inverted to produce the reconstruction of the input, which can be naturally used to do unsupervised or semi-supervised learning.  This paper proposes to use automatic differentiation to compute the inverse function efficiently. The idea seems interesting. However, I think there are several main drawbacks, detailed as follows:  1. The paper lacks a coherent and complete review of the semi-supervised deep learning. Herewith some important missing papers, which are the previous or current state-of-the-art. [1] Laine S, Aila T. Temporal Ensembling for Semi-Supervised Learning[J]. arXiv preprint arXiv:1610.02242, ICLR 2016. [2] Li C, Xu K, Zhu J, et al. Triple Generative Adversarial Nets[J]. arXiv preprint arXiv:1703.02291, NIPS 2017. [3] Dai Z, Yang Z, Yang F, et al. Good Semi-supervised Learning that Requires a Bad GAN[J]. arXiv preprint arXiv:1705.09783, NIPS 2017. Besides, some papers should be mentioned in the related work such as Kingma et. al. 2014. I'm not an expert of the network inversion and not sure whether the related work of this part is sufficient or not. 2. The motivation is not sufficient and not well supported. As stated in the introduction, the authors think there are several drawbacks of existing methods including training instability, lack of topology generalization and computational complexity.  Based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not.  The generative approaches based on VAEs and GANs are time consuming, but according to my experience, the training of VAE-based methods are stable and the topology generalization ability of such methods are good. Besides, the feed-forward approaches including [1] mentioned above are efficient and not too sensitive with respect to the network architectures. Overall, I think the drawbacks mentioned in the paper are not common in existing methods and I do not see clear benefits of the proposed method. Again, I strongly suggest the authors to provide a complete review of the literature. Further, please explicitly support your claim via experiments. For instance, the proposed method should be compared  with the discriminative approaches including VAT and [1] in terms of the training efficiency. It's not fair to say GAN-based methods require more training time because these methods can do generation and style-class disentanglement while the proposed method cannot. 3. The experimental results are not so convincing. First, please systematically compare your methods with existing methods on the widely adopted benchmarks including MNIST with 20, 100 labels and SVHN with 500, 1000 labels and CIFAR10 with 4000 labels. It is not safe to say the proposed method is the state-of-the-art by only showing the results in one setting. Second, please report the results of the proposed method with comparable architectures used in previous methods and state clearly the number of parameters in each model. Resnet is powerful but previous methods did not use that. Last, show the sensitive results of the proposed method by tuning alpha and beta. For instance, please show what is the actual contribution of the proposed reconstruction loss to the classification accuracy with the other losses existing or not? I think the quality of the paper should be further improved by addressing these problems and currently it should be rejected.",29,564,14.461538461538462,5.414772727272728,245,5,559,0.0089445438282647,0.0402802101576182,0.9612,173,79,105,31,10,6,"{'ABS': 0, 'INT': 2, 'RWK': 11, 'PDI': 4, 'DAT': 1, 'MET': 12, 'EXP': 5, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 14, 'PNF': 1, 'REC': 1, 'EMP': 6, 'SUB': 10, 'CLA': 1}",0,2,11,4,1,12,5,4,0,1,0,1,0,1,0,0,0,14,1,1,6,10,1,0.7185580139210312,0.6722596556592444,0.49731612528633157
ICLR2018-B1i7ezW0--R3,Reject,This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks. The numerical experiments show a significant improvement in accuracy of the approach.,2,31,15.5,6.482758620689655,27,0,31,0.0,0.032258064516129,0.7506,11,7,3,0,3,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0.2142857142857142,0.1111111111111111,0.08529904721560959
ICLR2018-B1jscMbAW-R1,Accept,"This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics. Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients. The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent). Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP. However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story. I will be happy to revisit the rating if the experimental section is enriched. Pros: - very easy to follow idea and model - simple merge or RL and SL in an end-to-end trainable model - improvements over previous solutions Cons: - K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. - TSP experiments show that in distribution DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). - in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments Side notes: - DCN is already quite commonly used abbreviation for Deep Classifier Network as well as Dynamic Capacity Network, thus might be a good idea to find different name. - please fix cite calls to citep, when authors name is not used as part of the sentence, for example: Graph Neural Network Nowak et al. (2017)  should be Graph Neural Network (Nowak et al. (2017)) # After the update  Evaluation section has been updated threefold: - TSP experiments are now in the appendix rather than main part of the paper - k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering - Knapsack problem has been added Paper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method. I can see the benefit of trainable approach here, the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that. I increased rating for the paper, however in order to put the clear accept mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).  ",27,680,32.38095238095238,5.004538577912254,320,3,677,0.0044313146233382,0.028530670470756,0.9689,195,87,128,55,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 1, 'MET': 12, 'EXP': 10, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 3, 'EMP': 19, 'SUB': 0, 'CLA': 0}",0,1,1,3,1,12,10,4,0,0,0,2,0,0,0,0,0,0,1,3,19,0,0,0.5752875710154713,0.3445681681048371,0.2902978709144949
ICLR2018-B1jscMbAW-R2,Accept,"This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems. The network has two components: one component learns to split the problem and the other learns to combine solutions to sub-problems. Using this setup, the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach. In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem. In all three cases, the proposed solution outperforms the baselines on larger problem instances. ",5,111,18.5,5.546296296296297,71,0,111,0.0,0.0089285714285714,-0.8555,40,10,22,0,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,2,2,0,3,0,0,0,0,0,0,0,0,0,0,0,2,0,0,1,0,0,0.2863544577060947,0.2223393847794121,0.13040450799431422
ICLR2018-B1jscMbAW-R3,Accept,"Summary of paper:  The paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks. Review:  The paper is clearly written. It is sometimes difficult to communicate ideas in this area, so I appreciate the author's effort in choosing good notation. Using an architecture to learn how to split the input, find solutions, then merge these is novel. Previous work in using recursion to solve problems (Cai 2017) used explicit supervision to learn how to split and recurse. The ideas and formalism of the merge and partition operations are valuable contributions. The experimental side of the paper is less strong. There are good results on the convex hull problem, which is promising. There should also be a comparison to a k-means solver in the k-means section as an additional baseline. I'm also not sure TSP is an appropriate problem to demonstrate the method's effectiveness. Perhaps another problem that has an explicit divide and conquer strategy could be used instead. It would also be nice to observe failure cases of the model. This could be done by visually showing the partition constructed or seeing how the model learned to merge solutions.. This is a relatively new area to tackle, so while the experiments section could be strengthened, I think the ideas present in the paper are important and worth publishing. Questions:  1. What is rho on page 4?. I assume it is some nonlinearity, but this was not specified. 2. On page 5, it says the merge block takes as input two sequences. I thought the merge block was defined on sets? Typos: 1. Author's names should be enclosed in parentheses unless part of the sentence. 2. I believe then should be removed in the sentence ...scale invariance, then exploiting... on page 2.",20,296,12.333333333333334,5.060931899641577,156,3,293,0.0102389078498293,0.0367892976588628,-0.5185,80,24,69,17,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 3}",0,1,1,3,0,9,1,1,0,0,0,2,0,0,0,1,0,1,1,0,7,3,3,0.5021655554420393,0.6707740344340756,0.34824790394350247
ICLR2018-B1kIr-WRb-R1,Reject,"In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S. Additionally, they propose an extension JCP-S, for n-order tensor decompositions. This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods. The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis. Additionally, it is evaluated over the MEN and Mturk datasets. For the JCP-S model, the loss function is unclear to me. L is defined for 3rd order tensors only;  how is the extended to n > 3? Intuitively it seems that L is redefined, and for, say, n   4, the model is M(i,j,k,n)   sum_1^R u_ir u_jr u_kr u_nr. However, the statement since we are using at most third order tensors in this work I am further confused. Is it just that JCP-S also incorporates 2nd order embeddings? I believe this requires clarification in the manuscript itself. For the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:  Pengfei Liu, Xipeng Qiuu2217 and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015  Jingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo. Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014  Mo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors  to name a few via quick googling. Additionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated. There are many such methods, see for example   Jiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding? and citations within, plus quick googling for more recent works. I am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3-way outlier detection). Observe also that in table 2, NNSE gets the highest performance in both MEN and MTurk. Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well-suited for matrix factorization. So, statements like as we can see, our embeddings very clearly outperform the random embedding at this task is  an unnecessary inflation of a result that 1) is not good and 2) is reasonable to not be good. Overall, I think for a more sincere evaluation, the authors need to better pick tasks that clearly exploit 3-way information and compare against other methods proposed to do the same. The multiplicative relation analysis is interesting, but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model. In conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. ",25,535,24.318181818181817,5.344422700587084,269,3,532,0.0056390977443609,0.0345454545454545,0.9899,153,84,94,34,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 1, 'DAT': 1, 'MET': 18, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 0}",0,0,6,1,1,18,0,1,1,2,0,0,2,0,0,1,0,1,0,0,16,0,0,0.5761213892602842,0.3426626948559779,0.2884251549514555
ICLR2018-B1kIr-WRb-R2,Reject,"The paper proposes to extend the usual PPMI matrix factorization (Levy and Goldberg, 2014) to a (3rd-order) PPMI tensor factorization. The paper chooses symmetric CP decomposition so that word representations are tied across all three views. The MSE objective (optionally interpolated with a 2nd-order tensor) is optimized incrementally by SGD. The paper's most clear contribution is the observation that the objective results in multiplicative compositionality of vectors, which indeed does not seem to hold in CBOW. While the paper reports superior performance, the empirical claims are not well substantiated. It is *not* true that given CBOW, it's not important to compare with SGNS and GloVe. In fact, in certain cases such as unsupervised word analogy, SGNS is clearly and vastly superior to other techniques (Stratos et al., 2015). The word similarity scores are also generally low: it's easy to achieve >0.76 on MEN using the plain PPMI matrix factorization on Wikipedia. So it's hard to tell if it's real improvement. Quality: Borderline. The proposed approach is simple and has an appealing compositional feature, but the work is not adequately validated and the novelty is somewhat limited. Clarity: Clear. Originality: Low-rank tensors have been used to derive features in many prior works in NLP (e.g., Lei et al., 2014). The paper's particular application to learning word embeddings (PPMI factorization), however, is new although perhaps not particularly original. The observation on multiplicative compositionality is the main strength of the paper. Significance: Moderate. For those interested in word embeddings, this work suggests an alternative training technique, but it has some issues (described above).  ",17,260,14.444444444444445,5.433070866141732,162,2,258,0.0077519379844961,0.0381679389312977,0.9662,82,39,42,20,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,2,2,0,8,0,4,0,0,0,4,0,0,0,3,1,2,0,0,6,0,1,0.430765923234441,0.5588727204559967,0.26673872534778903
ICLR2018-B1kIr-WRb-R3,Reject,"The paper presents the word embedding technique which consists of: (a) construction of a positive (i.e. with truncated negative values) pointwise mutual information order-3 tensor for triples of words in a sentence and (b) symmetric tensor CP factorization of this tensor. The authors propose the CP-S (stands for symmetric CP decomposition) approach which tackles such factorization in a batch manner by considering small random subsets of the original tensor. They also consider the JCP-S approach, where the ALS (alternating least squares) objective is represented as the joint objective of the matrix and order-3 tensor ALS objectives. The approach is evaluated experimentally on several tasks such as outlier detection, supervised analogy recovery, and sentiment analysis tasks. CLARITY: The paper is very well written and is easy to follow. However, some implementation details are missing, which makes it difficult to assess the quality of the experimental results. QUALITY: I understand that the main emphasis of this work is on developing faster computational algorithms, which would handle large scale problems, for factorizing this tensor. However, I have several concerns about the algorithms proposed in this paper: - First of all, I do not see why using small random subsets of the original tensor would give a desirable factorization. Indeed, a CP decomposition of a tensor can not be reconstructed from CP decompositions of its subtensors. Note that there is a difference between batch methods in stochastic optimization where batches are composed of a subset of observations (which then leads to an approximation of desirable quantities, e.g. the gradient, in expectation) and the current approach where subtensors are considered as batches. I would expect some further elaboration of this question in the paper. Although similar methods appeared in the tensor literature before, I don't see any theoretical ground for their correctness.    - Second, there is a significant difference between the symmetric CP tensor decomposition and the non-negative symmetric CP tensor decomposition. In particular, the latter problem is well posed and has good properties (see, e.g., Lim, Comon. Nonengative approximations of nonnegative tensors (2009)). However, this is not the case for the former (see, e.g., Comon et al., 2008 as cited in this paper). Therefore, (a) computing the symmetric and not non-negative symmetric decomposition does not give any good theoretical guarantees (while achieving such guarantees seems to be one of the motivations of this paper)  and (b) although the tensor is non-negative, its symmetric factorization is not guaranteed to be non-negative and further elaboration of this issue seem to be important to me. - Third, the authors claim that one of their goals is an experimental exploration of tensor factorization approaches with provable guarantees applied to the word embedding problem. This is an important question that has not been addressed in the literature and is clearly a pro of the paper. However, it seems to me that this goal is not fully implemented. Indeed, (a) I mentioned in the previous paragraph the issues with the symmetric CP decomposition and (b) although the paper is motivated by the recent algorithm proposed by Sharan&Valiant (2017), the algorithms proposed in this paper are not based on this or other known algorithms with theoretical guarantees. This is therefore confusing and I would be interested in the author's point of view to this issue. - Further, the proposed joint approach, where the second and third order information are combined requires further analysis. Indeed, in the current formulation the objective is completely dominated by the order-3 tensor factor, because it contributes O(d^3) terms to the objective vs O(d^2) terms contributed by the matrix part. It would be interesting to see further elaboration of the pros and cons of such problem formulation. - Minor comment. In the shifted PMI section, the authors mention the parameter alpha and set specific values of this parameter based on experiments. However, I don't think that enough information is provided, because, given the author's approach, the value of this parameter most probably depends on other parameters, such as the bach size. - Finally, although the empirical evaluation is quite extensive and outperforms the state-of the art, I think it would be important to compare the proposed algorithm to other tensor factorization approaches mentioned above. ORIGINALITY: The idea of using a pointwise mutual information tensor for word embeddings is not new, but the authors fairly cite all the relevant literature. My understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evaluation. However, such batch approaches for tensor factorization are not new and I am quite skeptical about their correctness (see above). The experimental evaluation presents indeed interesting results. However, I think it would also be important to compare to other tensor factorization approaches. I would also be quite interested to see the performance of the proposed algorithm for different values of parameters (such as the butch size). SIGNIFICANCE: I think the paper addresses very interesting problem and significant amount of work is done towards the evaluation, but there are some further important questions that should be answered before the paper can be published. To summarize, the following are the pros of the paper:    - clarity and good presentation; - good overview of the related literature; - extensive experimental comparison and good experimental results. While the following are the cons:    - the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees; - lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm; - lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem); - the novelty of the approach is somewhat limited, although the idea of the extensive experimental comparison is good.   ",44,948,22.571428571428573,5.502183406113537,319,8,940,0.0085106382978723,0.0143737166324435,0.9927,282,131,153,48,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 6, 'PDI': 8, 'DAT': 2, 'MET': 27, 'EXP': 7, 'RES': 6, 'TNF': 0, 'ANA': 7, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 6, 'PNF': 1, 'REC': 0, 'EMP': 20, 'SUB': 6, 'CLA': 2}",0,2,6,8,2,27,7,6,0,7,0,3,0,0,0,3,0,6,1,0,20,6,2,0.6512761241432613,0.6797744034961307,0.466069326779851
ICLR2018-B1l8BtlCb-R1,Accept,"This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words. The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs. To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations. For training, apply knowledge distilation for better training followed by fine tuning by reinforce. Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding. The motivation is clear and proposed methods are very sound. Experiments are carried out very carefully. I have only minor concerns to this paper:  - The experiments are designed to achieve comparable BLEU with improved latency. I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough. - It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment. - What is the impact of the external word aligner quality? For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner. - The positional attention is rather unclear and it would be better to revise it. Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.",14,265,20.384615384615383,5.473076923076923,151,2,263,0.0076045627376425,0.0330882352941176,0.9643,83,36,44,16,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 6, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,0,2,0,3,6,3,0,0,0,0,0,0,0,0,0,0,0,0,6,2,0,0.3582449239762445,0.2254386859047585,0.15974317163508248
ICLR2018-B1l8BtlCb-R2,Accept,"This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models). The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described). The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search. Overall, this is an interesting paper. It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores. Table 1 gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as NAT -- losing 2-5 BLEU points on WMT14 is significant. While the Ro->En results are goodG this particular language pair has not been used much by others; it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why WMT14 en->de and de->en are not improving more. Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.  ",8,268,29.77777777777778,4.813953488372093,138,2,266,0.0075187969924812,0.0256410256410256,0.9243,70,48,46,14,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 2, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,0,0,0,0,7,3,2,2,2,0,1,0,0,0,0,0,0,0,0,6,1,0,0.4303486920715862,0.225332009396437,0.19330838735525888
ICLR2018-B1l8BtlCb-R3,Accept,"This paper can be seen as an extension of the paper attention is all you need that will be published at nips in a few weeks (at the time I write this review). The goal here is to make the target sentence generation non auto regressive. The authors propose to introduce a set of latent variables to represent the fertility of each source words. The number of target words can be then derived and they're all predicted in parallel. The idea is interesting and trendy. However, the paper is not really stand alone. A lot of tricks are stacked to reduce the performance degradation. However, they're sometimes to briefly described to be understood by most readers. The training process looks highly elaborate with a lot of hyper parameters. Maybe you could comment on this. For instance, the use fertility supervision during training could be better motivated and explained. Your choice of IBM 2 is wired since it doesn't include fertility. Why not IBM 4, for instance ? How you use IBM model for supervision. This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated. ",15,195,13.0,4.82258064516129,115,2,193,0.0103626943005181,0.015228426395939,0.4807,52,12,40,16,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 4, 'CLA': 0}",0,0,1,4,0,6,3,2,0,0,1,2,0,0,0,1,1,1,1,0,7,4,0,0.501702372438668,0.6707184408006889,0.3554899840849817
ICLR2018-B1lMMx1CW-R1,Reject,"The paper proposes a new neural network based method for recommendation., The main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed. This contribution is not bad for an empirical paper. There's certainly not that much here that's groundbreaking methodologically, though it's certainly nice to know that a simple and scalable method works. There's not much detail about the data (it is after all an industrial paper). It would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets (compared to the same baselines), in order to get a sense as to whether the improvement is actually due to having a better model, versus being due to some unique attributes of this particular industrial dataset under consideration. As it is, I am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewhere. Other than that, it's nice to see an evaluation on real production data, and it's nice that the authors have provided enough info that the method should be (more or less) reproducible. There's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution. Again, this could be somewhat alleviated by evaluating on some standard and reproducible benchmarks.",10,257,28.55555555555556,5.153846153846154,133,3,254,0.0118110236220472,0.0544747081712062,0.9788,52,38,47,23,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 3, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,1,1,0,3,4,0,0,0,0,0,3,0,0,1,0,0,1,0,0,6,1,0,0.3580564525800992,0.4475542316186593,0.19515033030359558
ICLR2018-B1lMMx1CW-R2,Reject,"Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning. They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.) Most of the originality comes from integrating time decay of purchases into the learning framework. Rest of presented work is more or less standard. Paper may be useful to practitioners who are looking to implement something like this in production.",5,84,21.0,5.795180722891566,70,1,83,0.0120481927710843,0.0238095238095238,0.5106,32,9,14,2,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,2,0,0,0,0,1,1,0,0,0,2,1,0,0,0,0,0,0,0.2859644778280704,0.2222673298067403,0.12829411553782913
ICLR2018-B1lMMx1CW-R3,Reject,"This paper presents a practical methodology to use neural network for recommending products to users based on their past purchase history. The model contains three components: a predictor model which is essentially a RNN-style model to capture near-term user interests, a time-decay function which serves as a way to decay the input based on when the purchase happened, and an auto-encoder component which makes sure the user's past purchase history get fully utilized, with the consideration of time decay. And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability. It also showed good online A/B test performance, which indicates that this approach has been tested in real world. Two small concerns: 1. In Section 3.3. I am not fully sure why the proposed predictor model is able to win over LSTM. As LSTM tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model. Some insights might be useful there. 2. The title of this paper is weird. Suggest to rephrase unreasonable to something more positive. ",9,184,14.153846153846152,5.142857142857143,114,2,182,0.0109890109890109,0.054054054054054,0.9314,57,25,28,10,2,3,"{'ABS': 0, 'INT': 3, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,3,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,1,0,0.144159141284697,0.3340168596919796,0.07016717736564196
ICLR2018-B1mAkPxCZ-R1,Reject,"This paper proposes a feature augmentation method for one-shot learning. The proposed approach is very interesting. However, the method needs to be further clarified and the experiments need to be improved. Details: 1. The citation format used in the paper is not appropriate, which makes the paper, especially the related work section, very inconvenient to read. 2. The approach: (1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3. However, under one-shot learning, won't this  make each class still have only one instance for training? (2) Moreover, the augmenting features x_i^A (regardless A F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation? (3) It is not clear how will the vocabulary-information be exploited? In particular, how to ensure the semantic space u to be same as the vocabulary semantic space? How to generate the neighborhood in Neigh(hat{u}_i) on page 5? 3.  In the experiments:  (1) The authors didn't compare the proposed method with existing state-of-the-art one-shot learning approaches, which makes the results not very convincing. (2) The results are reported for different numbers of augmented instances. Clarification is needed.  ",13,232,16.571428571428573,5.373271889400922,112,1,231,0.0043290043290043,0.0210970464135021,0.6706,67,25,45,15,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 1}",0,0,2,1,0,8,3,2,0,0,0,1,0,0,0,0,0,1,0,0,11,0,1,0.430631354746432,0.339552907681763,0.21782854444257066
ICLR2018-B1mAkPxCZ-R2,Reject,"This paper proposes a (new) semantic way for data augmentation problem, specifically targeted for one-shot learning setting, i.e. synthesizing training samples based on semantic similarity with a given sample . Specifically, the authors propose to learn an autoencoder model, where the encoder translates image data into the lower dimensional subspace of semantic representation (word-to-vec representation of image classes), and the decoder translates semantic representation back to the original input space. For one-shot learning, in addition to a given input image, the following data augmentation is proposed: a) perturbed input image (Gaussian noise added to input image features); b) perturbed decoded image; c) perturbed decoded neighbour image, where neighbourhood is searched in the semantic space. The idea is nice and simple, however the current framework has several weaknesses: 1. The whole pipeline has three (neural network) components: a) input image features are extracted from VGG net pre-trained on auxiliary data; 2) auto-encoder that is trained on data for one-shot learning; 3) final classifier for one-shot learning is learned on augmented image space with two (if I am not mistaken) fully connected layers. This three networks need to be clearly described; ideally combined into one end-to-end training pipeline. 2. The empirical performance is very poor. If you look into literature for zero shot learning, work by Z. Akata in CVPR 2015, CVPR2016, the performance on AwA and on CUB-bird goes way above 50%, where in the current paper it is 30.57% and 8.21% at most (for the most recent survey on zero shot learning papers using attribute embeddings, please, refer to Zero-Shot Learning - The Good, the Bad and the Ugly by Xian et al, CVPR 2017). It is important to understand, why there is such a big drop in performance in one-shot learning comparing to zero-shot learning? One possible explanation is as follows: in the zero-shot learning, one has access to large training data to learn the semantic embedding (training classes). In contrary, in the proposed approach, the auto-encoder model (with 10 hidden layers) is learned using 50 training samples in AwA, and 200 images of birds (or am I missing something?). I am not sure, how can the auto-encoder model not overfit completely to the training data instances. Perhaps, one could try to explore the zero-shot learning setting, where there is a split between train and test classes: training the autoencoder model using large training dataset, and adapting the weights using single data points from test classes in one-shot learning setting. Overall, I like the idea, so I am leaning towards accepting the paper, but the empirical evaluations are not convincing.          ",19,429,25.23529411764705,5.370646766169155,196,1,428,0.0023364485981308,0.0249433106575963,-0.7483,136,61,77,16,8,2,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 4, 'DAT': 3, 'MET': 12, 'EXP': 7, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,2,1,4,3,12,7,2,0,0,0,1,0,0,0,0,0,1,0,0,10,0,0,0.5750291613178339,0.2278198391358089,0.259121556917318
ICLR2018-B1mAkPxCZ-R3,Reject,"Summary: This paper proposes a data augmentation method for one-shot learning of image classes. This is the problem where given just one labeled image of a class, the aim is to correctly identify other images as belonging to that class as well. The idea presented in this paper is that instead of performing data augmentation in the image space, it may be useful to perform data augmentation in a latent space whose features are more discriminative for classification. One candidate for this is the image feature space learned by a deep network. However they advocate that a better candidate is what they refer to as semantic space formed by embedding the (word) labels of the images according to pre-trained language models like word2vec. The reasoning here is that the image feature space may not be semantically organized so that we are not guaranteed that a small perturbation of an image vector will yield image vectors that correspond to semantically similar images (belonging to the same class). On the other hand, in this semantic space, by construction, we are guaranteed that similar concepts lie near by each other. Thus this space may constitute a better candidate for performing data augmentation by small perturbations or by nearest neighbour search around the given vector since 1) the augmented data is more likely to correspond to features of similar images as the original provided image and 2) it is more likely to thoroughly capture the intra-class variability in the augmented data. The authors propose to first embed each image into a feature space, and then feed this learned representation into a auto-encoder that handles the projection to and from the semantic space with its encoder and decoder, respectively. Specifically, they propose to perform the augmentation on the semantic space representation, obtained from the encoder of this autoencoder. This involves producing some additional data points, either by adding noise to the projected semantic vector, or by choosing a number of that vector's nearest neighbours. The decoder then maps these new data points into feature space, obtaining in this way the image feature representations that, along with the feature representation of the original (real) image will form the batch that will be used to train the one-shot classifier. They conduct experiments in 3 datasets where they experiment with augmentation in the image feature space by random noise, as well as the two aforementioned types of augmentation in the semantic space. They claim that these augmentation types provide orthogonal benefits and can be combined to yield superior results. Overall I think this paper addresses an important problem in an interesting way, but there is a number of ways in which it can be improved, detailed in the comments below. Comments: -- Since the authors are using a pre-trained VGG for to embed each image, I'm wondering to what extent they are actually doing one-shot learning here. In other words, the test set of a dataset that is used for evaluation might contain some classes that were also present in the training set that VGG was originally trained on. It would be useful to clarify whether this is happening. Can the VGG be instead trained from scratch in an end-to-end way in this model? -- A number of things were unclear to me with respect to the details of the training process: the feature extractor (VGG) is pre-trained. Is this finetuned during training? If so, is this done jointly with the training of the auto-encoder? Further, is the auto-encoder trained separately or jointly with the training of the one-shot learning classifier? -- While the authors have convinced me that data augmentation indeed significantly improves the performance in the domains considered (based on the results in Table 1 and Figure 5a), I am not convinced that augmentation in the proposed manner leads to a greater improvement than just augmenting in the image feature domain. In particular, in Table 2, where the different types of augmentation are compared against each other, we observe similar results between augmenting only in the image feature space versus augmenting only in the semantic feature space (ie we observe that FeatG performs similarly as SemG and as SemN). When combining multiple types of augmentation the results are better, but I'm wondering if this is because more augmented data is used overall. Specifically, the authors say that for each image they produce 5 additional virtual data points, but when multiple methods are combined, does this mean 5 from each method? Or 5 overall? If it's the former, the increased performance may merely be attributed to using more data. It is important to clarify this point. -- Comparison with existing work: There has been a lot of work recently on one-shot and few-shot learning that would be interesting to compare against. In particular, mini-ImageNet is a commonly-used benchmark for this task that this approach can be applied to for comparison with recent methods that do not use data augmentation. Some examples are: - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. (Finn et al.) - Prototypical Networks for Few-shot Learning (Snell et al.) - Matching Networks for One-shot Learning (Vinyals et al.) - Few-Shot Learning Through an Information Retrieval Lens (Triantafillou et al.) -- A suggestion: As future work I would be very interested to see if this method can be incorporated into common few-shot learning models to on-the-fly generate additional training examples from the support set of each episode that these approaches use for training.",37,900,32.142857142857146,5.208955223880597,330,9,891,0.0101010101010101,0.0242024202420242,0.9952,235,103,155,49,10,3,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 5, 'DAT': 6, 'MET': 25, 'EXP': 5, 'RES': 5, 'TNF': 2, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 17, 'SUB': 0, 'CLA': 0}",0,2,4,5,6,25,5,5,2,0,1,0,3,0,0,0,1,2,0,0,17,0,0,0.7215906355079577,0.3434018148480108,0.36527751723257845
ICLR2018-B1mSWUxR--R1,Reject,"This paper dives deeper into understand reward augmented maximum likelihood training. Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the Bayes decision rule. Please elaborate on this. Did you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates? Section 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution. However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10), Please explain the difference.",7,114,14.25,5.242990654205608,75,0,114,0.0,0.0175438596491228,0.796,34,13,25,2,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 1}",0,0,1,1,0,4,2,0,0,1,0,2,0,0,0,0,0,1,1,0,2,2,1,0.4294536195125418,0.55628418949872,0.270765756049806
ICLR2018-B1mSWUxR--R2,Reject,"This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule. I have a few questions on the motivation and the results. - In the section Open Problems in RAML, both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q. This is not true. The globally optimal solution is related to both the underlying data distribution P and q, and not the same as q. It is given by q'(y | x, tau)   sum_{y'} P(y' | x) q(y | y', tau). - Both Theorem 1 and Theorem 2 do not directly justify that RAML has similar reward as the Bayes decision rule, Can anything be said about this? Are the KL divergence small enough to guarantee similar predictive rewards? - In Theorem 2, when does the exponential tail bound assumption hold? - In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML. Are the differences actually significant? Are the differences between SQDML/RAML and ML significant? In addition, how should tau be chosen in these experiments? ",14,194,24.25,4.938888888888889,103,1,193,0.005181347150259,0.0147058823529411,0.9708,67,21,32,11,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 12, 'EXP': 4, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,12,4,1,1,0,0,0,0,0,0,0,2,0,1,0,7,0,0,0.4315821291981859,0.3371131145908391,0.21917380798808286
ICLR2018-B1mSWUxR--R3,Reject,"The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML. Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary. The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule. (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically. (3) Through one experiment using synthetic data on multi-class classiufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speciufb01c metrics that is desired to optimize. I found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric. Given a reward function, one can define the Bayes decision rule. The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule. The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17). Of course, the moving-out is biased but the replacing is unbiased. The second contribution is partially valid, although I doubt how much improvement one can get from SQDML. The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15). In fact, this step can result in biased estimation because the replacement is inside the nonlinear function. When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example. However, when x is not repeated frequently, both RAML and SQDML are biased. Experiment in section 4.1.2 do not validate significant improvement, either. The numerical results are relatively weak. The synthetic experiment verifies the reward-maximizing property of RAML and SQDML. However, from Figure 2, we can see that the result is quite sensitive to the temperature tau. Is there any guidelines to choose tau? For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper. These experiment results show very small improvement compared to the ML baselines (see Table 2,3 and 5). These results are also lower than the state of the art performance. A few questions: (1). The author may want to check whether (8) can be called a Bayes decision rule. This is a direct result from definition of conditional probability. No Bayesian elements, like prior or likelihood appears here. (2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator. Compared with the n-gram replacement used in the paper, which one is better? (3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14. This will make the comparison much more clear. (4). What is Theorem 2 trying to convey? Although tau goes to 0, there is still a gap between Q and Q'. This seems to suggest that for small tau, Q' is not a good approximation of Q. Are the assumptions in Theorem 2 reasonable? There are several typos in the proof of Theorem 2. (5). In section 4.2.2, the authors write the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important. Could you explain it in more details?  ",38,610,12.97872340425532,5.306194690265487,250,4,606,0.0066006600660066,0.011437908496732,0.9856,161,70,97,34,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 30, 'EXP': 11, 'RES': 7, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 23, 'SUB': 0, 'CLA': 1}",0,2,1,1,1,30,11,7,2,0,0,1,0,0,0,0,0,2,1,0,23,0,1,0.6513577003202637,0.4582446705681798,0.3746025945740443
ICLR2018-B1mvVm-C--R1,Accept,"The authors propose to decompose reinforcement learning into a PATH function that can learn how to solve reusable sub-goals an agent might have in a specific environment and a GOAL function that chooses subgoals in order to solve a specific task in the environment using path segments. So I guess it can be thought of as a kind of hierarchical RL. The exposition of the model architecture could use some additional detail to clarify some steps and possibly fix some minor errors (see below). I would prefer less material but better explained. I had to read a lot of sections more than once and use details across sections to fill in gaps. The paper could be more focused around a single scientific question: does the PATH function as formulated help? The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature. I also like that they try to design experiments to understand the role of specific parts of the proposed architecture. The graphs are WAY TOO SMALL to read. Figure #s are missing off several figures. MODEL & ARCHITECTURE  The PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A). ( If the goal state s' was just the next state, then this would just be a dynamics model and this would be model-based learning? So I assume there are multiple steps between s and s'?). At the beginning of section 2.1, I think the authors suggest the PATH function could be pre-trained independently by sampling a random state in the state space to be the initial state and a second random state to be the goal state and then using an RL algorithm to find a path. Presumably, once one had found a path ( (s, a0), (s1, a1), (s2, a2), ..., (sn-1,an-1),  s' ) one could then train the PATH policy on the triple (s, s', a0) ? This seems like a pretty intense process: solving some representative subset of all possible RL problems for a particular environment ... Maybe one choses s and s' so they are not too far away from each other (the experimental section later confirms this distance is >  7. Maybe bring this detail forward)? The expression Trans'( (s,s'), a)   (Trans(s,a), s') was confusing. I think the idea here is that the expression  Trans'(  (s,s') , a ) represents the n-step transition function and 'a' represents the first action? The second step is to train the goal function for a specific task. So I gather our policy takes the form of a composed function and the chain rule gives close to their expression in 2.2 PATH(  s,  Tau( s, th^g ),  a ; th^p )      d / { d th^g }  PATH(  s,  Tau( s, th^g ),  a ; th^p )    {d / d {s' }  PATH } ( s,  Tau( s, th^g ),  a )    d / {d th^g}  Tau( s, th^g) What is confusing is that they define      A( s, a, th^p, th^g, th^v )   sum_i   gamma^i  r_{t+1}  +  gamma^k  V(  s_{t+k}  ;  th^v  )   -   V( s_t ;  th^v ) The left side contains th^p and th^g, but the right side does not. Should these parameters be take out of the n-step advantage function A? The second alternative for training the goal function tau seems confusing. I get that tau is going to be constrained by whatever representation PATH function was trained on and that this representation might affect the overall performance - performance. I didn't get the contrast with method one. How do we treat the output of Tau as an action? Are you thinking of the gradient coming back through PATH as a reward signal? More detail here would be helpful. EXPERIMENTS:  Lavaworld: authors show that pretraining the PATH function on longer 7-11 step policies leads to better performance when given a specific Lava world problem to solve. So the PATH function helps and longer paths are better. This seems reasonable. What is the upper bound on the size of PATH lengths you can train? Reachability: authors show that different ways of abstracting the state s into a vector encoding affect the performance of the system. From a scientific point of view, this seems orthogonal to the point of the paper, though is relevant if you were trying to build a system. Taxi: the authors train the PATH problem on reachability and then show that it works for TAXI. This isn't too surprising. Both picking up the passenger (reachability) and dropping them off somewhere are essentially the same task: moving to a point. It is interesting that the Task function is able to encode the higher level structure of the TAXI problem's two phases. Another task you could try is to learn to perform the same task in two different environments. Perhaps the TAXI problem, but you have two different taxis that require different actions in order to execute the same path in state space. This would require a phi(s) function that is trained in a way that doesn't depend on the action a. ATARI 2600 games: I am not sure what state restoration is. Is this where you artificially return an agent to a state that would normally be hard to reach? The authors show that UA results in gains on several of the games. The authors also demonstrate that using multiple agents with different policies can be used to collect training examples for the PATH function that improve its utility over training examples collected by a single agent policy. RELATED WORK:  Good contrast to hierarchical learning: we don't have switching regimes here between high-level options I don't understand why the authors say the PATH function can be viewed as an inverse? Oh - now I get it. Because it takes an extended n-step transition and generates an action.        ",52,955,24.487179487179485,4.825433526011561,361,15,940,0.0159574468085106,0.0228353948620361,0.9948,303,98,171,53,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 37, 'EXP': 16, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 40, 'SUB': 2, 'CLA': 0}",0,1,1,2,0,37,16,2,2,0,0,0,0,0,0,1,0,1,2,0,40,2,0,0.5104381526871499,0.5799801409465563,0.3278034614467261
ICLR2018-B1mvVm-C--R2,Accept,"In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task-specific knowledge into different modules. Several complex but discrete control tasks, with relatively small action spaces, are cast as continuous control problems, and the task specific module is trained to produce non-linear representations of goals in the domain of transformed high-dimensional inputs. Pros - ""Monolithic"" policy representations can make it difficult to reuse or jointly represent policies for related tasks in the same environment; a modular architecture is hence desirable. - An extensive study of methods for dimensionality reduction is performed for a task with sparse rewards. - Despite all the suggestions and questions below, the method is clearly on par with standard A3C across a wide range of tasks, which makes it an attractive architecture to explore further. Cons - In general, learning a Path function could very well turn out to be no simpler than learning a good policy for the task at hand. I have 2 main concerns: The data required for learning a good Path function may include similar states to those visited by some optimal policy. However, there is no such guarantee for random walks; indeed, for most Atari games which have several levels, random policies don't reach beyond the first level, so I don't see how a Path function would be informative beyond the 'portions' of the state space which were visited by policies used to collect data. Hence, several policies which are better than random are likely to be required for sampling this data, in general.  In my mind this creates a chicken-and-egg issue: how to get the data, to learn the right Path function which does not make it impossible to still reach optimal performance on the task at hand?  How can we ensure that some optimal policy can still be represented using appropriate Goal function outputs?  I don't see this as a given in the current formulation. - Although the method is atypical compared to standard HRL approaches, the same pitfalls may apply, especially that of 'option collapse': given a fixed Path function, the Goal function need only figure out which goal state outputs almost always lead to the same output action in the original action space, irrespective of the current state input phi(s), and hence bypass the Path function altogether; then, the role of phi(s) could be taken by tau(s), and we would end up with the original RL problem but in an arguably noisier (and continuous) action space.  I recommend comparing the Jacobian w.r.t the phi(s) and tau(s) inputs to the Path function using saliency maps [1, 2]; alternatively, evaluating final policies with out of date input states s to phi, and the correct tau(s) inputs to Path function should degrade performance severely if it playing the role assumed. Same goes for using a running average of phi(s) and the correct tau(s) in final policies. - The ability to use state restoration for Path function learning is actually introducing a strong extra assumption compared to standard A3C, which does not technically require it. For cheap emulators and fully deterministic games (Atari) this assumption holds, but in general restoring expensive, stochastic environments to some state is hard (e.g. robot arms playing ping-pong, ball at given x, y, z above the table, with given velocity vector). - If reported results are single runs, please replace with averages over several runs, e.g. a few random seeds. Given the variance in deep RL training curves, it is hard to make definitive claims from single runs. If curves are already averages over several experiment repeats, some form of error bars or variance plot would also be informative. - How much data was actually used to learn the Path function in each case? If the amount is significant compared to task-specific training, then UA/A3C-L curves should start later than standard A3C curves, by that amount of data. References [1] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. [2] Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N De Freitas, Dueling network architectures for deep reinforcement learning arXiv preprint arXiv:1511.06581 ",23,691,27.64,5.219847328244275,322,3,688,0.004360465116279,0.0113636363636363,0.993,231,96,113,36,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 7, 'MET': 15, 'EXP': 1, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 2, 'CLA': 0}",0,1,1,2,7,15,1,4,0,0,1,0,1,0,0,0,1,2,0,0,14,2,0,0.6468027824294732,0.4527537301629145,0.3699735524919519
ICLR2018-B1mvVm-C--R3,Accept,"Thank you for the submission.  It was an interesting read. Here are a few comments:   I think when talking about modelling the dynamics of the world, it is natural to discuss world models and model based RL, which also tries to explicitly take advantage of the separation between the dynamics of the world and the reward scheme. Granted, most world model also try to predict the reward. I'm not sure there is something specific I'm proposing here, I do understand the value of the formulation given in the work, I just find it strange that model based RL is not mention at all in the paper. I think reading the paper, it should be much clearer how the embedding is computed for Atari, and how this choice was made. Going through the paper I'm not sure I know how this latent space is constructed. This however should be quite important. The goal function tries to predict states in this latent space. So the simpler the structure of this latent space, the easier it should be to train a goal function, and hence quickly adapt to the current reward scheme.  In complex environments learning the PATH network is far from easy. I.e. random walks will not expose the model to most states of the environment (and dynamics). Curiosity-driven RL can be quite inefficient at exploring the space.  If the focus is transfer, one could argue that another way of training the PATH net could be by training jointly the PATH net and goal net, with the intend of then transferring to another reward scheme. A3C is known to be quite high variance. I think there are a lot of little details that don't seem that explicit to me. How many seeds are run for each curve (are the results an average over multiple seeds). What hyper-parameters are used. What is the variance between the seeds. I feel that while the proposed solution is very intuitive, and probably works as described, the paper does not do a great job at properly comparing with baseline and make sure the results are solid.  In particular looking at Riverraid-new is the advantage you have there significant? How does the game do on the original task? The plots could also use a bit of help. Lines should be thicker. Even when zooming, distinguishing between colors is not easy. Because there are more than two lines in some plots, it can also hurt people that can't distinguish colors easily.   ",27,412,15.846153846153848,4.63,204,4,408,0.0098039215686274,0.0522565320665083,0.9878,106,39,87,30,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 6, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,0,1,1,0,10,6,3,2,0,1,1,0,1,0,0,1,1,3,0,11,0,0,0.6456745525858737,0.4507871566404808,0.35383341130777657
ICLR2018-B1n8LexRZ-R1,Accept,"In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly). n I think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance). In the literature, specially in machine learning, there is ``fever'' about HMC, in my opinion, partially unjustified. If I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point. However, I have other additional comments:  - Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader. - I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes L. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013,   adaptive techniques,  H. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223u2013242, April 2001, and component-wise strategies as Gibbs Sampling,   W. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337u2013348, 199.u2028 At least, add a brief paragraph in the introduction citing and discussing this possible alternatives.",10,261,12.428571428571429,5.441048034934497,142,3,258,0.0116279069767441,0.025830258302583,0.9136,94,32,34,9,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 5, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,5,2,0,4,0,0,0,0,0,0,5,1,0,0,0,5,1,0,2,0,0,0.4297528284035233,0.3344239409969362,0.2152508104472402
ICLR2018-B1n8LexRZ-R2,Accept,"The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. The derivations look correct to me. In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well. ",7,101,14.428571428571429,5.224489795918367,64,0,101,0.0,0.0098039215686274,0.2058,25,8,20,5,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,0,0,0,5,1,0,0,1,0,0,0,1,0,0,0,1,0,0,2,1,0,0.3581436255979958,0.3339552907681763,0.18219644333418508
ICLR2018-B1n8LexRZ-R3,Accept,"The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions. These functions are trained to maximize expected squared jump distance. This works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016. Overall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases. A few downsides are commented on below.   The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2.  Fig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper.  I can't see where the number 124x in sec 5.1 stems from. As a user, I would be interested in the typical computational cost of both MCMC sampler training and MCMC sampler usage (inference?), compared to competing methods. This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation. Would it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC? I am missing an intuition for several things: eq7, the time encoding defined in Appendix C  Appendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC). The number 124x ESS in sec5.1 seems at odds with the number in the abstract, 50x. # Minor errors - sec1: The sampler is trained to minimize a variation: should be maximize as well as on a the real-world - sec3.2 and 1/2 v^T v the kinetic: energy missing  - sec4: the acronym L2HMC is not expanded anywhere in the paper  The sentence We will denote the complete augmented...p(d) might be moved to after from a uniform distribution in the same paragraph. In paragraph starting We now update x:     - specify for clarity: the first update, which yields x' / the second update, which yields x''     - only affects $x_{bar{m}^t}$: should be $x'_{bar{m}^t}$  (prime missing)   - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg mask(x',m^t)?   - inside zeta_2 and zeta_3, do you not mean $m^t and $bar{m}^t$ ?  - sec5: add reference for first mention of A NICE MC  - Appendix A:      - Let's -> Let  - eq12 should be x'' ... - Appendix C: space missing after Section 5.1 - Appendix D1: In this section is presented : sounds odd n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify.",27,446,29.73333333333333,4.946078431372549,228,5,441,0.0113378684807256,0.0384615384615384,0.9295,146,43,87,23,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 23, 'EXP': 1, 'RES': 1, 'TNF': 3, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 14, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 3}",0,1,1,1,0,23,1,1,3,3,0,2,0,0,0,0,0,2,14,0,6,3,3,0.6485028228511018,0.5599585244547547,0.40188394087032137
ICLR2018-B1nLkl-0Z-R1,Reject,"I think I should understand the gist of the paper, which is very interesting, where the action of tilde Q(s,a) is drawn from a distribution. The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper expected policy gradient by Ciosek & Whiteson. All these seems very sound and interesting. Weakness: 1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical). Such representation style is highly discouraging and brings about un-necessary readability difficulties. 2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is). So I suggest moving them to the Appendix and make the major focus more narrowed down.",7,169,16.9,5.166666666666667,94,6,163,0.0368098159509202,0.0529411764705882,-0.5536,42,27,25,14,3,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,2,0,0,4,0,0,0,0,0,1,0,0,0,1,0,1,0,0,3,0,0,0.2151167785294522,0.3345772482030192,0.10488002820221362
ICLR2018-B1nLkl-0Z-R2,Reject,"The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter. It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques. Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG. The paper is very clearly written and easy to read, and its contributions are easy to extract. The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper. The inclusion of proof summaries in the main text would strengthen this aspect of the paper. On the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques. In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact. Overall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance.",9,231,25.666666666666668,5.308035714285714,139,0,231,0.0,0.0086580086580086,0.9861,60,26,38,16,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 2}",0,1,1,1,0,6,2,1,0,0,0,3,0,0,0,0,2,0,1,1,3,1,2,0.5014279939767331,0.6680397532556546,0.3566809769912124
ICLR2018-B1nLkl-0Z-R3,Reject,"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems. The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies. To be honest, I didn't really get this paper. * As far I understand, all of the original work policy gradients involved stochastic policies. Many are/were Gaussian. * All Q-value estimators are designed to marginalize out the randomness in these stochastic policies. * As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (mu,Sigma) and the environment samples an action from that distribution. In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value. Ultimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas. I thought the little 2-mode MOG was a nice example of the premise of the model. While I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out. Figures 2&3 are unconvincing - the differences do not appear to be statistically significant. Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention. They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, Very well, run your algorithm 10x longer and see where you end up!  I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published. The idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that? ",16,336,21.0,4.980582524271845,187,6,330,0.0181818181818181,0.0406976744186046,0.9698,72,38,75,29,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,1,3,0,10,1,0,1,0,0,2,1,0,1,0,0,0,2,1,6,2,0,0.5738443189843955,0.5588335881618952,0.35442404092387253
ICLR2018-B1nZ1weCZ-R1,Accept,"The paper present online algorithms for learning multiple sequential problems. The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm. Experimental results are given on different multi-task instances. The contributions are interesting and experimental results seem promising. But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix (ten sections in the Appendix and 28 pages). Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known. This is a very strong assumption. In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption. Therefore, I am not convinced that the paper is ready for publication at ICLR'18. * Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks. The gap is so large that I am not convinced on the fairness of the comparison . For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge. * I do not see how the single output layer is defined. * As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm. * Section 7.1. It is not clear why degradation does not happen.  It seems to be only an experimental fact.",16,251,14.764705882352942,5.045643153526971,130,1,250,0.004,0.0544747081712062,0.8574,70,36,45,12,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,1,0,1,0,10,4,2,0,0,0,1,0,0,0,0,0,2,1,1,6,3,0,0.431137446863326,0.5589958583036033,0.2626830287992814
ICLR2018-B1nZ1weCZ-R2,Accept,"The authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful. They design and explore several approaches  to the active learning (or active sampling) problem, from a basic  change to the distribution to UCB to feature-based neural-network based RL. The domain is video games. All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks). Pros:  - very promising results with an interesting active learning approach to multitask RL - a number of approaches developed for the basic idea - a variety of experiments, on challenging multiple task problems (up to 21 tasks/games) - paper is overall well written/clear Cons:  - Comparison only to a very basic baseline (i.e. uniform sampling) Couldn't comparisons be made, in some way, to other multitask work? Additional  comments:  - The assumption of the availability of a target score goes against the motivation that one need not learn individual networks ..  authors say instead one can use 'published' scores, but that only assumes someone else has done the work (and furthermore, published it!). The authors do have a section on eliminating the need by doubling an estimate for each task) which makes this work more acceptable (shown for 6 tasks or MT1, compared to baseline uniform sampling). Clearly there is more to be done here for a future direction (could be mentioned in future work section). - The averaging metrics (geometric, harmonic vs arithmetic, whether   or not to clip max score achieved) are somewhat interesting, but in   the main paper, I think they are only used in section 6 (seems like   a waste of space). Consider moving some of the results, on showing   drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to   the main paper. - The can be several benefits to multitask learning, in particular   time and/or space savings in learning new tasks via learning more   general features. Sections 7.2 and 7.3 on specificity/generality of   features were interesting. --> Can the authors show that a trained network (via their multitask     approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from     scratch? --> How does the performance improve/degrade (or the variance), on the     same set of tasks, if the different multitask instances (MT_i)     formed a supersets hierarchy, ie if MT_2 contained all the     tasks/games in MT_1, could training on MT_2 help average     performance on the games in MT_1 ?  Could go either way since the network    has to allocate resources to learn other games too. But is there a pattern? - 'Figure 7.2' in section 7.2 refers to Figure 5. - Can you motivate/discuss better why not providing the identity of a   game as an input is an advantage? Why not explore both   possibilities? what are the pros/cons? (section 3)     ",26,476,29.75,5.191111111111111,230,2,474,0.0042194092827004,0.0108303249097472,0.9895,132,56,81,30,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 11, 'EXP': 3, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 15, 'SUB': 0, 'CLA': 1}",0,0,3,3,0,11,3,2,1,0,1,1,0,1,0,0,1,5,1,0,15,0,1,0.6458667329995958,0.564731609872117,0.4134325873529499
ICLR2018-B1nZ1weCZ-R3,Accept,"In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games. A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works. Although the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper. Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper.  The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs. Although performance is still reduced compared to single task learning in some cases, this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale. I wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct. ",8,205,25.625,5.675,122,2,203,0.0098522167487684,0.0241545893719806,0.3693,66,31,33,14,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 2, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,0,3,0,3,0,1,0,0,2,0,0,0,0,0,2,0,0,0,3,0,0,0.3577659720957504,0.223514173740356,0.1606516968290288
ICLR2018-B1nxTzbRZ-R1,Reject,"The paper considers a problem of predicting hidden information in a poMDP with an application to Starcraft. Authors propose a number of baseline models as well as metrics to assess the quality of ""defogging"". I find the problem of defogging quite interesting, even though it is a bit too Starcraft-specific some findings could perhaps be translated to other partially observed environments. Authors use the dataset provided for Starcraft: Brood war by Lin et al, 2017. My impression about the paper is that even though it touches a very interesting problem, it neither is written well nor it contains much of a novelty in terms of algorithms, methods or network architectures. Detailed comments: * Authors should at very least cite (Vinyals et al, 2017) and explain why the environment and the dataset released for Starcraft 2 is less suited than the one provided by Lin et al. * Problem statement in section 3.1 should certainly be improved. Authors introduce rather heavy notation which is then used in a confusing way. For example, what is the top index in $s_t^{3-p}$ supposed to mean? The notation is not much used after sec. 3.1, for example, figure 1 does not use it. * A related issue, is that the definition of metrics is very informal and, again, does not use the already defined notation. Including explicit formulas would be very helpful, because, for example, it looks like when reported in table 1 the metrics are spatially averaged, yet I could not find an explicit notion of that. * Authors seem to only consider deterministic defogging models. However, to me it seems that even in 15 game steps the uncertainty over the hidden state is quite high and thus any deterministic model has a very limited potential in prediction it. At least the concept of stochastic predictions should be discussed * The rule-based baselines are not described in detail. What does ""using game rules to infer the existence of unit types"" mean? * Another detail which I found missing is whether authors use just a screen, a mini-map or both. In the game of Starcraft, only screen contains information about unit-types, but it's field of view is limited. Hence, it's unclear to me whether a model should infer hidden information based on just a single screen + minimap observation (or a history of them) or due to how the dataset is constructed, all units are observed without spatial limitations of the screen.  ",20,400,21.05263157894737,5.04,201,2,398,0.0050251256281407,0.0293398533007334,-0.7062,104,40,77,38,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 3, 'MET': 9, 'EXP': 0, 'RES': 0, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 8, 'SUB': 4, 'CLA': 1}",0,1,2,2,3,9,0,0,3,1,0,1,0,0,0,1,0,0,3,0,8,4,1,0.5736886735042886,0.5603524249720274,0.35891997048416474
ICLR2018-B1nxTzbRZ-R2,Reject,"The authors introduce the task of defogging, by which they mean attempting to infer the contents of areas in the game StarCraft hidden by the fog of war. The authors train a neural network to solve the defogging task, define several evaluation metrics, and argue that the neural network beats several naive baseline models. On the positive side, the task is a nice example of reasoning about a complex hidden state space, which is an important problem moving forwards in deep learning. On the negative side, from what I can tell, the authors don't seem to have introduced any fundamentally new architectural choices in their neural network, so the contribution seems fairly specific to mastering StarCraft, but at the same time, the authors don't evaluate how much their defogger actually contributes to being able to win StarCraft games. All of their evaluation is based on the accuracy of defogging. Granted, being able to infer hidden states is of course an important problem, but the authors appear to mainly have applied existing techniques to a benchmark that has minimal practical significance outside of being able to win StarCraft competitions, meaning that, at least as the paper is currently framed, the critical evaluation metric would be showing that a defogger helps to win games. Two ways I could image the contribution being improved are either highlighting and generalizing novel insights gleaned from the process of building the neural network that could help people build defoggers for other domains (and spelling out more explicitly what domains the authors expect their insights to generalize to), or doubling down on the StarCraft application specifically and showing that the defogger helps to win games. A minimal version of the second modification would be having a bot that has access to a defogger play against a bot that does not have access to one. All that said, as a paper on an application of deep learning, the paper appears to be solid, and if the area chairs are looking for that sort of contribution, then the work seems acceptable. Minor points: - Is there a benefit to having a model that jointly predicts unit presence and count, rather than having two separate models (e.g., one that feeds into the next)?  Could predicting presence or absence separately be a way to encourage sparsity, since absence of a unit is already representable as a count of zero? The choice to have one model seems especially peculiar given the authors say they couldn't get one set of weights that works for both their classification and regression tasks - Notation: I believe the space U is never described in the main text. What components precisely does an element of U have? - The authors say they use gameplay from no later than 11 minutes in the game to avoid the difficulties of increasing variance. How long is a typical game?  Is this a substantial fraction of the time of the games studied?  If it is not, then perhaps the defogger would not help so much at winning. - The F1 performance increases are somewhat small. The L1 performance gains are bigger, but the authors only compare L1 on true positives. This means they might have very bad error on false positives. (The authors state they are favoring the baseline in this comparison, but it would be nice to have those numbers.) - I don't understand when the authors say the deep model has better memory than baselines (which includes a perfect memory baseline)",23,577,36.0625,5.021857923497268,268,7,570,0.0122807017543859,0.0256410256410256,0.9975,150,58,119,31,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 18, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 3, 'CLA': 0}",0,1,3,3,0,18,1,1,0,0,0,0,0,0,0,0,0,2,0,0,15,3,0,0.4331042700342473,0.3423712529949678,0.21870170741444936
ICLR2018-B1nxTzbRZ-R3,Reject,"# Summary This paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state. This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map. The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft. [Pros] - Interesting problem [Cons] - The proposed method is not much novel. - The evaluation is a bit limited to two specific downstream prediction tasks. # Novelty and Significance - The problem considered in this paper is interesting. - The proposed method is not much novel. - Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks. It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks. Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible. # Quality - The experimental result is not much comprehensive. The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks. It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks). # Clarity - I did not fully understand the learning objective. Does the model try to reconstruct the state of the current time-step or the future? The learning objective is not clearly defined. In Section 4.1, the target x and y have time steps from t1 to t2. What is the range of t1 and t2? If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results.",20,293,17.235294117647058,5.333333333333333,132,1,292,0.0034246575342465,0.022875816993464,0.8965,97,33,51,20,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 4, 'FWK': 2, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 4, 'CLA': 0}",0,2,1,4,0,10,1,3,0,4,2,1,0,0,0,2,2,0,0,0,9,4,0,0.6455019833249677,0.4498332776811187,0.3639388079620296
ICLR2018-B1p461b0W-R1,Reject,"The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise. It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named structured noise, and (3) Samples from out-of-dataset classes. The experiments show robustness to these types of noise. Review:  The claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise. This is supported by the literature on data cleaning (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition. While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy. It would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality). The paper did not get a chance to test these differences, and I can only raise a few hypotheses. First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle. This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition.  Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise. Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general. Without such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers. Other specific comments:  -- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search. For example, for Conv4, how many channels at each layer? how was the net initialized?  which hyper parameters were tuned and with which values? were hyper parameters tuned on a separate validation set? How was the train/val/test split done, etc. These details are useful for judging technical correctness. -- Section 4, importance of large datasets. The recent paper by Chen et al (2017) would be relevant here. -- Figure 8 failed to show for me. -- Figure 9,10, need to specify which noise model was used.      ",24,422,19.181818181818183,5.153465346534653,220,2,420,0.0047619047619047,0.0321100917431192,0.9727,124,46,72,20,11,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 4, 'EXP': 6, 'RES': 3, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 15, 'SUB': 3, 'CLA': 0}",0,1,1,1,3,4,6,3,2,2,0,1,0,2,0,0,0,1,1,1,15,3,0,0.7871367716888689,0.5644763126600001,0.49312773749819594
ICLR2018-B1p461b0W-R2,Reject,"The authors study the effect of label noise on classification tasks. They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size.  Although, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental. Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015). Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains. The main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules. The authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips. However, the previous settings can be reinterpreted in the authors setting. I found the formulation of the alpha to be non-intuitive and confusing at times. The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels. In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels). This can be improved to help readers understand better. There are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels. Overall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting. Missing citation: Training Deep Neural Networks on Noisy Labels with Bootstrapping, Reed et al. ",15,333,17.526315789473685,5.160377358490566,170,0,333,0.0,0.0238805970149253,0.9742,100,43,51,16,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 3, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 0}",0,0,4,2,3,5,0,2,1,0,0,1,1,0,1,0,0,1,0,0,7,2,0,0.5728579868252143,0.4482828655618237,0.3221573571741438
ICLR2018-B1p461b0W-R3,Reject,"The authors study the effect of label noise on classification tasks. They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size.  Although, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental. Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015). Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains. The main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules. The authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips. However, the previous settings can be reinterpreted in the authors setting. I found the formulation of the alpha to be non-intuitive and confusing at times. The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels. In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels). This can be improved to help readers understand better. There are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels. Overall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting. Missing citation: Training Deep Neural Networks on Noisy Labels with Bootstrapping, Reed et al. ",15,333,17.526315789473685,5.160377358490566,170,0,333,0.0,0.0238805970149253,0.9742,100,43,51,16,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 3, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 0}",0,0,4,2,3,5,0,2,1,0,0,1,1,0,1,0,0,1,0,0,7,2,0,0.5728579868252143,0.4482828655618237,0.3221573571741438
ICLR2018-B1spAqUp--R1,Reject,"This paper is well written and easy to follow. The authors propose pixel deconvolutional layers for convolutional neural networks. The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers. The method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer.  PixelDCL is applied sequentially, therefore it is slower than the original deconvolutional layer. The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset). The authors justify the proposed method as a way to alleviate the checkerboard effect (while introducing more complexity to the model and making it slower). In the experimental section, however, they do not compare with other approaches to do so For example, the upsampling+conv approach, which has been shown to remove the checkerboard effect while being more efficient than the proposed method (as it does not require any sequential computation). Moreover, the PixelDCL does not seem to bring substantial improvements on DeepLab (a state-of-the-art semantic segmentation algorithm).  More comments and further exploration on this results should be done. Why no performance boost? Is it because of the residual connection? Or other component of DeepLab? Is the proposed layer really useful once a powerful model is used? I also think the experiments on VAE are not conclusive. The authors simply show set of generated images. First, it is difficult to see the different of the image generated using deconv and PixelDCL. Second, a set of 20 qualitative images does not (and cannot) validate any research idea.",18,266,19.0,5.465384615384616,147,1,265,0.0037735849056603,0.0186567164179104,0.9424,70,38,50,16,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 7, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,1,0,2,0,8,1,7,1,0,0,1,0,0,1,0,0,2,1,0,10,0,1,0.5021448560146051,0.5612703350263323,0.31782542513482387
ICLR2018-B1spAqUp--R2,Reject,"Paper summary: This paper proposes a technique to generalize deconvolution operations used in standard CNN architectures. Traditional deconvolution operation uses independent filter weights to compute output features at adjacent pixels. This work proposes to do sequential prediction of adjacent pixel features (via intermediate feature maps) resulting in more spatially smooth outputs for deconvolution layer. This new layer is referred to as 'pixel deconvolution layer' and it is demonstrated on two tasks of semantic segmentation and face generation. Paper Strengths: - Despite being simple technique, the proposed pixel deconvolution layer is novel and interesting.",5,92,18.4,6.186813186813187,65,0,92,0.0,0.032258064516129,0.7717,36,14,15,3,3,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,0,3,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0.2144043140094258,0.2222222222222222,0.09537606996169405
ICLR2018-B1spAqUp--R3,Reject,"This paper proposed the new approach for feature upsampling called pixel deconvolution, which aims to resolve checkboard artifact of conventional deconvolution. By sequentially applying a series of decomposed convolutions, the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some extent. Overall, the paper is clearly written and easy to understand the main motivation and methods. However, the checkboard artifact is a well-known problem of deconvolution network, and has been addressed by several approaches which are simpler than the proposed pixel deconvolution. For example, it is well known that simple bilinear interpolation optionally followed by convolutions effectively removes checkboard artifact to some extent, and bilinear additive upsampling proposed in Wonja et al., 2017 also demonstrated its effectiveness as an alternative for deconvolution. Comparisons against these approaches would make the paper stronger. Besides, comparisons/discussions based on extensive analysis on various deconvolution architectures presented in Wonja et al., 2017 would also be interesting. Wonja et al, The Devil is in the Decoder, In BMVC, 2017 ",8,178,22.25,6.109826589595376,108,0,178,0.0,0.0055865921787709,0.9565,58,19,31,11,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,4,2,0,7,0,0,0,0,0,1,1,0,0,1,0,4,0,0,1,0,1,0.4303733448231442,0.4447959321160143,0.2456388617640655
ICLR2018-B1suU-bAW-R1,Reject,"This paper produces word embedding tensors where the third order gives covariate information, via venue or author. The model is simple: tensor factorization, where the covariate can be viewed as warping the cosine distance to favor that covariate's more commonly cooccuring vocabulary (e.g. trump on hillary and crooked) There is a nice variety of authors and words, though I question if even with all those books, the corpus is big enough to produce meaningful vectors. From my own experience, even if I spend several hours copy-pasting from project gutenberg, it is not enough for even good matrix factorization embeddings, much less tensor embeddings. It is hard to believe that meaningful results are achieved using such a small dataset with random initialization. I think table 5 is also a bit strange. If the rank is > 1000 I wonder how meaningful it actually is. For the usual analogies task, you can usually find what you are looking for in the top 5 or less. It seems that table 1 is the only evaluation of the proposed method against any other type of method (glove, which is not a tensor-based method). I think this is not sufficient. Overall, I believe the idea is nice, and the initial analysis is good, but I think the evaluation, especially against other methods, needs to be stronger. Methods like neelakantan et al's multisense embedding, for example, which the work cites, can be used in some of these evaluations, specifically on those where covariate information clearly contributes (like contextual tasks). The addition of one or two tables with either a standard task against reported results or created tasks against downloadable contextual / tensor embeddings would be enough for me to change my vote. ",14,284,20.285714285714285,5.130597014925373,163,5,279,0.017921146953405,0.0383275261324041,0.9757,68,50,46,18,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 2, 'MET': 4, 'EXP': 0, 'RES': 1, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,1,3,2,4,0,1,3,1,0,0,0,1,0,0,0,1,0,1,6,0,0,0.6438090055835151,0.3364431205075482,0.31910420194517913
ICLR2018-B1suU-bAW-R2,Reject,"The authors present a method for learning word embeddings from related groups of data. The model is based on tensor factorization which extends GloVe to higher order co-ocurrence tensors, where the co-ocurrence is of words within subgroups of the text data. These two papers need to be cited:  Rudolph et al., NIPS 2017, Sturctured Embedding Models for Grouped Data: This paper also presents a method for learning embeddings specific for subgroups of the data, but based on hierarchical modeling. An experimental comparison is needed. Cotterell et al., EACL 2017 Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis: This paper also derives a tensor factorization based approach for learning word embeddings for different covariates. Here the covariates are morphological tags such as part-of-speech tags of the words. Due to these two citations, the novelty of both the problem set-up of learning different embeddings for each covariate and the novelty of the tensor factorization based model are limited. The writing is ok. I appreciated the set-up of the introduction with the two questions. However, the questions themselves could have been formulated differently:  Q1: the way Q1 is formulated makes it sound like the covariates could be both discrete and continuous while the method presented later in the paper is only for discrete covariates (i.e. group structure of the data). Q2: The authors mention topic alignment without specifying what the topics are aligned to. It would be clearer if they stated explicitly that the alignment is between covariate-specific embeddings. It is also distracting that they call the embedding dimensions topics. Also, why highlight the problem of authorship attribution of Shakespear's work in the introduction, if that problem is not addressed later on? In the model section, the paragraphs  otation and objective function and discussion are clear. I also liked the idea of having the section A geometric view of embeddings and tensor decomposition, but that section needs to be improved. For example, the authors describe RandWalk (Arora et al. 2016) but how their work falls into that framework is unclear. In the third paragraph, starting with Therefore we consider a natural extension of this model, ... it is unclear which model the authors are referring to. (RandWalk or their tensor factorization?). What are the context vectors in Figure 1?  I am guessing the random walk transitions are the ellipsoids? How are they to be interpreted? In the last paragraph, beginning with Note that this is essentially saying..., I don't agree with the argument that the base embeddings decompose into independent topics. The dimensions of the base embeddings are some kind of latent attributes and each individual dimension could be used by the model to capture a variety of attributes. There is nothing that prevents the model from using multiple dimensions to capture related structure of the data. Also, the qualitative results in Table 3 do not convince me that the embedding dimensions represent topics. For example horses has highest value in embedding dimension 99. It's nearest neighbours in the embedding space (i.e. semantically similar words) will also have high values in coordinate 99. Hence, the apparent semantic coherence in what the authors call topics. The authors present multiple qualitative and quantitative evaluations. The clustering by weight (4.1.) is nice and convincing that the model learns something useful. 4.2, the only quantitative analysis was missing some details. Please give references for the evaluation metrics used, for proper credit and so people can look up these tasks. Also, comparison needed to fitting GloVe on the entire corpus (without covariates) and existing methods Rudolph et al. 2017 and Cotterell et al. 2017. Section 5.2 was nice and so was 5.3. However, for the covariate specific analogies (5.3.) the authors could also analyze word similarities without the analogy component and probably see similar qualitative results. Specifically, they could analyze for a set of query words, what the most similar words are in the embeddings obtained from different subsections of the data. PROS: + nice tensor factorization model for learning word embeddings specific to discrete covariates. + the tensor factorization set-up ensures that the embedding dimensions are aligned  + clustering by weights (4.1) is useful and seems coherent + covariate-specific analogies are a creative analysis CONS: - problem set-up not novel and existing approach not cited (experimental comparison needed) - interpretation of embedding dimensions as topics not convincing - connection to Rand-Walk (Aurora 2016) not stated precisely enough - quantitative results (Table 1) too little detail:         * why is this metric appropriate ?         * comparison to GloVe on the entire corpus (not covariate specific)         * no reference for the metrics used (AP, BLESS, etc.?) - covariate specific analogies presented confusingly and similar but simpler analysis might be possible by looking at variance in neighbours v_b and v_d without involving v_a and v_c (i.e. don't talk about analogies but about similarities)",45,790,18.80952380952381,5.492084432717678,321,3,787,0.0038119440914866,0.0192076830732292,0.9831,242,89,143,42,11,5,"{'ABS': 0, 'INT': 3, 'RWK': 5, 'PDI': 2, 'DAT': 2, 'MET': 22, 'EXP': 1, 'RES': 5, 'TNF': 3, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 23, 'SUB': 1, 'CLA': 3}",0,3,5,2,2,22,1,5,3,5,0,1,2,0,0,2,0,5,0,0,23,1,3,0.7918173741932331,0.5699146470770872,0.5052421104844071
ICLR2018-B1suU-bAW-R3,Reject,"This paper presents an embedding algorithm for text corpora that allows known covariates, e.g. author information, to modify a shared embedding to take context into account. The method is an extension of the GloVe method and in the case of a single covariate value the proposed method reduces to GloVe. The covariate-dependent embeddings are diagonal scalings of the shared embedding. The authors demonstrate the method on a corpus of books by various authors and on a corpus of subreddits. Though not technically difficult, the extension of GloVe to covariate-dependent embeddings is very interesting and well motivated. Some of the experimental results do a good job of demonstrating the advantages of the models. However, some of the experiments are not obvious that the model is really doing a good job. I have some small qualms with the presentation of the method. First, using the term size m for the number of values that the covariate can take is a bit misleading. Usually the size of a covariate would be the dimensionality. These would be the same if the covariate is one hot coded, however, this isn't obvious in the paper right now. Additionally, v_i and c_k live in R^d, however, it's not really explained what 'd' is, is it the number of 'topics', or something else? Additionally, the functional form chosen for f() in the objective was chosen to match previous work but with no explanation as to why that's a reasonable form to choose. Finally, the authors say toward the end of Section 2 that A careful comparision shows that this approximation is precisely that which is implied by equation 4, as desired.  This is cryptic, just show us that this is the case. Regarding the experiments there needs to be more discussion about how the different model parameters were determined. The authors say ... and after tuning our algorithm to emged this dataset, ..., but this isn't enough. What type of tuning did you do to choose in particular the latent dimensionality and the learning rate? I will detail concerns for the specific experiments below.  Section 4.1: - How does held-out data fit into the plot? Section 4.2: - For the second embedding, what exactly was the algorithm trained on? Just the   book, or the whole corpus? - What is the reader supposed to take away from Table 1? Are higher or lower   values better? Maybe highlight the best scores for each column. Section 4.3: - Many of these distributions don't look sparse. - There is a terminology problem in this section. Coordinates in a vector are   not sparse, the vector itself is sparse if there are many zeros, but   coordinates are either zero or not zero. The authors' use of 'sparse' when   they mean 'zero' is really confusing. - Due to the weird sparsity terminology Table 1 is very confusing. Based on how   the authors use 'sparse' I think that Table 1 shows the fraction of zeros in   the learned embedding vectors. But if so, then these vectors aren't sparse at all   as most values are non-zero. Section 5.1: - I don't agree with the authors that the topics in Table 3 are interpretable. As such, I think it's a reach to claim the model is learning interpretable topics. This isn't necessarily a problem, it's fine for models to not do everything well,   but it's a stretch for the authors to claim that these results are a positive   aspect of the model. The results in Section 5.2 seem to make a lot of sense and   show the big contribution of the model. Section 5.3: - What is the a : b :: c : d notation? ",35,595,18.59375,4.8610108303249095,251,4,591,0.0067681895093062,0.0205371248025276,0.9882,154,50,107,44,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 19, 'EXP': 4, 'RES': 5, 'TNF': 5, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 22, 'SUB': 0, 'CLA': 0}",0,1,0,1,1,19,4,5,5,0,0,0,0,0,0,0,0,1,0,0,22,0,0,0.5050780448369142,0.2352833283539246,0.2303175859858039
ICLR2018-B1tC-LT6W-R1,Reject,"The problem considered in the paper is of compressing large networks (GRUs) for faster inference at test time. The proposed algorithm uses a two step approach: 1)  use trace norm regularization (expressed in variational form) on dense parameter matrices at training time without constraining the number of parameters, b) initializing from the SVD of parameters trained in stage 1, learn a new network with reduced number of parameters. The experiments on WSJ dataset are promising towards achieving a trade-off between number of parameters and accuracy. I have the following questions regarding the experiments: 1. Could the authors confirm that the reported CERS are on validation/test dataset and not on train/dev data? It is not explicitly stated. I hope it is indeed the former, else I have a major concern with the efficacy of the algorithm as ultimately, we care about the test performance of the compressed models in comparison to uncompressed model. 2. In B.1 the authors use an increasing number units in the hidden layers of the GRUs as opposed to a fixed size like in Deep Speech 2, an obvious baseline that is missing from the  experiments is the comparison with *exact* same GRU (with  768, 1024, 1280, 1536 hidden units) *without any compression*. 3.  What do different points in Fig 3 and 4 represent. What are the values of lamdas that were used to train (the l2 and trace norm regularization) the Stage 1 of models shown in Fig 4. I want to understand what is the difference in the  two types of  behavior of orange points (some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stage 1. It is interesting that although L2 regularization does not lead to low  u parameters in Stage 1, the compression stage does have comparable performance to that of trace norm minimization. The authors point it out, but a further investigation might be interesting. Writing: 1. The GRU model for which the algorithm is proposed is not introduced until the appendix. While it is a standard network, I think the details should still be included in the main text to understand some of the notation referenced in the text like ""lambda_rec"" and ""lambda_norec""",15,377,22.176470588235293,5.122448979591836,175,2,375,0.0053333333333333,0.0259740259740259,0.9387,116,35,61,11,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 8, 'EXP': 7, 'RES': 1, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 6, 'SUB': 4, 'CLA': 0}",0,0,1,1,2,8,7,1,2,1,0,0,0,0,0,0,0,1,4,0,6,4,0,0.5737564796759163,0.4480589679150337,0.32191579231852824
ICLR2018-B1tC-LT6W-R2,Reject,"The authors propose a strategy for compressing RNN acoustic models in order to deploy them for embedded applications. The technique consists of first training a model by constraining its trace norm, which allows it to be well-approximated by a truncated SVD in a second fine-tuning stage. Overall, I think this is interesting work, but I have a few concerns which I've listed below:  1. Section 4, which describes the experiments of compressing server sized acoustic models for embedded recognition seems a bit ""disjoint"" from the rest of the paper. I had a number of clarification questions spefically on this section: - Am I correct that the results in this section do not use the trace-norm regularization at all? It would strengthen the paper significantly if the experiments presented on WSJ in the first section were also conducted on the ""internal"" task with more data. - How large are the training/test sets used in these experiments (for test sets, number of words, for training sets, amount of data in hours (is this ~10,000hrs), whether any data augmentation such as multi-style training was done, etc.) - What are the ""tier-1"" and ""tier-2"" models in this section? It would also aid readability if the various models were described more clearly in this section, with an emphasis on structure, output targets, what LMs are used, how are the LMs pruned for the embedded-size models, etc. Also, particularly given that the focus is on embedded speech recognition, of which the acoustic model is one part, I would like a few more details on how decoding was done, etc. - The details in appendix B are interesting, and I think they should really be a part of the main paper. That being said, the results in Section B.5, as the authors mention, are somewhat preliminary, and I think the paper would be much stronger if the authors can re-run these experiments were models are trained to convergence. - The paper focuses fairly heavily on speech recognition tasks, and I wonder if it would be more suited to a conference on speech recognition. 2. Could the authors comment on the relative training time of the models with the trace-norm regularizer, L2-regularizer and the unconstrained model in terms of convergence time. 3. Clarification question: For the WSJ experiments was the model decoded without an LM? If no LM was used, then the choice of reporting results in terms of only CER is reasonable, but I think it would be good to also report WERs on the WSJ set in either case. 4. Could the authors indicate the range of values of lambda_{rec} and lambda_{nonrec} that were examined in the work? Also, on a related note, in Figure 2, does each point correspond to a specific choice of these regularization parameters? 5. Figure 4: For the models in Figure 4, it would be useful to indicate the starting CER of the stage-1 model before stage-2 training to get a sense of how stage-2 training impacts performance. 6. Although the results on the WSJ set are interesting, I would be curious if the same trends and conclusions can be drawn from a larger dataset -- e.g., the internal dataset that results are reported on later in the paper, or on a set like Switchboard. I think these experiments would strengthen the paper. 7. The experiments in Section 3.2.3 were interesting, since they demonstrate that the model can be warm-started from a model that hasn't fully converged. Could the authors also indicate the CER of the model used for initialization in addition to the final CER after stage-2 training in Figure 5. 8. In Section 4, the authors mention that quantization could be used to compress models further although this is usually degrades WER by 2--4% relative. I think the authors should consider citing previous works which have examined quantization for embedded speech recognition [1], [2]. In particular, note that [2] describes a technique for training with quantized forward passes which results in models that have smaller performance degradation relative to quantization after training. References: [1] Vincent Vanhoucke, Andrew Senior, and Mark Mao, ""Improving the speed of neural networks on cpus,"" in Deep Learning and Unsupervised Feature Learning Workshop, NIPS, 2011. [2] Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin, ""On the efficient representation and execution of deep acoustic models,"" Proc. of Interspeech, pp. 2746 -- 2750, 2016.  9. Minor comment: The authors use the term ""warmstarting"" to refer to the process of training NNs by initializing from a previous model. It would be good to clarify this in the text.",29,751,22.08823529411765,5.112554112554113,287,9,742,0.0121293800539083,0.025,0.9936,221,78,129,26,10,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 3, 'MET': 9, 'EXP': 12, 'RES': 6, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 19, 'SUB': 7, 'CLA': 0}",0,1,0,2,3,9,12,6,3,1,0,1,2,0,0,0,0,0,1,0,19,7,0,0.7177043846362275,0.3451686262104356,0.36366255462041963
ICLR2018-B1tC-LT6W-R3,Reject,"Paper is well written and clearly explained. The paper is a experimental paper as it has more content on the experimentation and less content on problem definition and formulation. The experimental section is strong and it has evaluated across different datasets and various scenarios. However, I feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venue. It only considers a slight modification into the loss function by adding a trace norm regularization.",6,83,16.6,5.443037974683544,56,0,83,0.0,0.0240963855421686,0.5956,21,11,14,7,3,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 1, 'SUB': 4, 'CLA': 1}",0,0,0,0,0,3,2,0,0,0,0,4,0,0,0,0,0,0,0,1,1,4,1,0.215008550729144,0.4447644739694089,0.12046338966942838
ICLR2018-B1tExikAW-R1,Reject,"The idea is clearly stated (but lacks some details) and I enjoyed reading the paper. I understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better. From the adversary's standpoint, it would be easier to manipulate inputs than latent variables. On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation. In Section 3.1, the attack methods #2 and #3 should be detailed more. I could not imagine how VAE and T are trained simultaneously. In Section 3.2, the authors listed a couple of loss functions. How were these loss functions are combined? The final optimization problem that is used for training of the propose VAE should be formally defined. Also, the detailed specification of the VAE should be detailed. From figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner, however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines. For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks? Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?   ",14,205,18.63636363636364,5.478947368421053,105,0,205,0.0,0.048076923076923,0.717,57,19,38,15,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 0}",0,0,2,1,0,12,2,0,1,2,0,0,0,0,0,0,0,3,0,0,8,2,0,0.4315149448977688,0.3380280369999355,0.21214144776003158
ICLR2018-B1tExikAW-R2,Reject,"This paper misses the point of what VAEs (or GANs, in general) are used for. The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples. The use of these techniques for compressing is still unclear and their quality today is too low. So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense. But let's assume that at some point they can be used as the authors propose. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless? I think this will make the first attack useless. The other two attacks require that the foe is inserted in the middle of the training of the VAE. This is even less doable, because the encoder and decoder are not train remotely. They are train of the same machine or cluster in a controlled manner by the person that would use the system. Once it is train it will give away the decoder and keep the encoder for sending information.  ",12,250,20.83333333333333,4.36734693877551,138,1,249,0.0040160642570281,0.0277777777777777,-0.98,62,16,45,13,3,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 0}",0,0,0,0,0,7,3,5,0,0,0,0,0,0,0,0,0,0,0,0,11,1,0,0.2161820661226774,0.2284417965706519,0.09720138595426865
ICLR2018-B1tExikAW-R3,Reject,"This paper is concerned with both security and machine learning. Assuming that data is encoded, transmited, and decoded using a VAE, the paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified. The objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input; 2) make minimal change in the middle so that the attack is not detectable. This paper is concerned with both security and machine learning, but there is no clear contributions to either field. From the machine learning perspective, the proposed attacking method is standard without any technical novelty. From the security perspective, the scenarios are too simplistic. The encoding-decoding mechanism being attacked is too simple without any security enhancement. This is an unrealistic scenario. For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them. Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community. I am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas. For example, what are the necessary and sufficient conditions for an attacking method to be undetectable? Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient? Is there any reference for them? Why do we require the correspondence between the classification confidence of tranformed and original data? Would it be enough to match the DISTRIBUTION of the confidence? ",17,274,21.07692307692308,5.458646616541353,135,0,274,0.0,0.0218181818181818,-0.6458,75,25,54,7,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 14, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 0}",0,1,1,2,3,14,2,3,0,0,0,1,0,0,0,1,0,0,0,0,11,1,0,0.5750101498265423,0.339552907681763,0.2906507627334582
ICLR2018-B1twdMCab-R1,Reject,"This paper proposes a model for adding background knowledge to natural language understanding tasks. The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction. The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion). I think the paper does a fairly good job at doing what it does, it is just hard to get excited by it. Here are my major comments:  * The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic. But then they just concept net to augment text. This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation. As is, I don't really see how this motivation has anything to do with getting things out of a KB. A KB is usually a pretty static entity, and things are added to it at a slow pace. * The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI. Specifically they take text and add common sense knowledge from concept net. The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis. However, is this statement enough to cross the acceptance threshold of ICLR? Seems a bit marginal to me. * The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them. To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way. For example, have another RNN read the assertions and somehow integrate that. The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated. There are no comparisons to other possibilities. As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful. As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution. So to conclude, the paper is well-written, clear, and has nice results and analysis. The conclusion is that reading background knowledge from concept net boost performance using some architecture. This is nice to know but I think does not cross the acceptance threshold.  ",25,479,19.95833333333333,5.030634573304158,227,8,471,0.0169851380042462,0.0494845360824742,0.9974,120,55,96,32,6,6,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 13, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 14, 'SUB': 0, 'CLA': 1}",0,2,0,4,0,13,0,3,0,2,0,6,0,0,2,0,2,1,0,1,14,0,1,0.4321403696580153,0.6748098158790413,0.29710206858312266
ICLR2018-B1twdMCab-R2,Reject,"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems . They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format. However, in this paper, the way it is added is simply by updating word representations based on this extra text. This seems too simple to really be the right way to add background knowledge. In practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing; the paper never says exactly how, not even if you read the supplementary material). This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching! As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning). pp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as reading is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%). A note on the QA results: The QA results are certainly good enough to be in the range of good systems, but none of the results really push the SOTA. The best SQuAD (devset) results are shown as several percent below the SOTA. In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission, but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD. Similar remarks perhaps apply to the NLI results. p.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge. Biggest question:  - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge? Minor notes:  - The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used u2794 and can be used; that rely on u2794 that relies on. - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems? - On p.3 above sec 3.1: What is u? Was that meant to be z? - On p.8, I'm a bit suspicious of the Is additional knowledge used?  experiment which trains with knowledge and then tests without knowledge. It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone. - In the supplementary material the paper notes that the numbers are from the best result from 3 runs. This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.   ",28,636,27.65217391304348,4.671052631578948,273,10,626,0.0159744408945686,0.0338983050847457,0.9905,166,65,107,54,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 2, 'DAT': 4, 'MET': 12, 'EXP': 2, 'RES': 13, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 4, 'REC': 0, 'EMP': 16, 'SUB': 2, 'CLA': 1}",0,0,5,2,4,12,2,13,1,0,0,2,0,0,0,0,0,3,4,0,16,2,1,0.5754702456835475,0.5654106254723116,0.36021455698172805
ICLR2018-B1twdMCab-R3,Reject,"The quality of this paper is good. The presentation is clear  but I find lack of description of a key topic. The proposed model is not very innovative but works fine for the DQA task. For the TE task, the proposed method does not perform better than the state-of-the-art systems. - As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM. - The reference of ESIM is not correct. - Figure 1 is hard to understand. What do you indicate with the box and arrow? Arrows seem to have some different meanings. - What corpus did you use to pre-train word vectors? - As the proposed method was successful for the QA task, you need to explain QA data sets and how the questions are solved. - I also expect performance and  error analysis of the task results. - To claim task-agnostic, you need to try to apply your method to other NLP tasks as well. - Page 3. Sigma is not defined.",16,172,13.23076923076923,4.574850299401198,101,0,172,0.0,0.0329670329670329,0.0943,45,18,37,8,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 6, 'SUB': 6, 'CLA': 1}",0,0,1,0,2,7,1,1,3,2,0,3,0,0,0,0,0,1,4,0,6,6,1,0.5731279677693035,0.5593834320427877,0.3569249088489419
ICLR2018-B1uvH_gC--R1,Reject,"The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks. The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension. The paper has several major shortcomings: * Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000. Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example. In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE. Please add the Swiss roll example. * Distance preservation appears more and more like a dated DR paradigm. Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation. Computation accuracy of the geodesic distances in high-dimensional spaces can be poor. This could be discussed and some experiments on very HD data should be reported. * Some key historical references are overlooked, like the SAMMANN. There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago. Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity). Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too. * As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach. The motivation in the end of page 3 seems to be computational only. * Section 3 should be further detailed (step 2 in particular). * The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress. The running times are not in favor of the proposed method. The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold. * The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...'). What is the take-home message of the paper?",22,385,17.5,5.35792349726776,208,6,379,0.0158311345646438,0.0229591836734693,0.2588,101,60,59,28,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 3, 'MET': 12, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 7, 'CLA': 0}",0,1,0,2,3,12,2,3,0,0,0,1,1,0,0,0,1,1,0,0,9,7,0,0.5745097655989729,0.4500601629731171,0.32395935969201795
ICLR2018-B1uvH_gC--R2,Reject,"The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension. They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure. The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework. The paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional). The abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc. In the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities. This is in general not true. Even if the number of parameters is small, learning them might require complex computations on the whole data set. On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N). In section 2.1, the authors claim Spectral techniques are non-parametric in nature; this is wrong again. E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words. In section 2.2, it says observation that the double centering.... Can you provide a citation for this? In section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity. It is not quite clear from the text what the resulting complexity would be. With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2). Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is. Figure 3, contrary to text, does not provide a visualisation to the sampling mechanism. In the experiments section, can you provide a citation for ADAM and explain how the parameters were selected? Also, it is not meaningful to measure the quality of a visualisation via the MDS fit. There are more useful approaches to this task, such as the quality framework [*]. In figure 4a, x-axis should be  umber of landmarks. It is not clear why the equation 6 holds. Citation? It is also not clear how exactly the equation 7 is evaluated. It says By varying the number of layers and the number of nodes..., but the nodes and layer are not a part of the equation. The notation for equation 8 is not explained. Figure 6a shows visualisations by different techniques and is evaluated by looking at it. Again, use [*].  [*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013. ",29,528,14.27027027027027,4.945783132530121,213,2,526,0.0038022813688212,0.0335195530726257,-0.6797,157,57,81,30,10,5,"{'ABS': 0, 'INT': 3, 'RWK': 1, 'PDI': 3, 'DAT': 4, 'MET': 13, 'EXP': 0, 'RES': 4, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 4, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 2}",0,3,1,3,4,13,0,4,3,1,0,2,4,0,0,0,0,1,1,0,14,1,2,0.7178622288752571,0.5637221372793683,0.4487564231175679
ICLR2018-B1uvH_gC--R3,Reject,"The key contribution of the paper is a new method for nonlinear dimensionality reduction. The proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling. While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry. The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets). The paper revisits mostly familiar ideas. The importance of preserving both local and global information in manifold learning is well known, so unclear what the main conceptual novelty is. This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution. Moreover, in this reviewer's experience, the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold, and such an estimation is simply too difficult for any reasonable dataset encountered in practice. However, the authors do not address this, and instead simply use the Isomap approach for approximating geodesics by graph distances, which opens up a completely different set of challenges (how to construct the graph, how to deal with holes in the manifold, how to avoid short circuiting in the all-pairs shortest path computations etc etc). Finally, the experimental results are somewhat uninspiring. It seems that the proposed method does roughly as well as Landmark Isomap (with slightly better generalization properties) but is slower by a factor of 1000x. The horizon articulation data, as well as the pose articulation data, are both far too synthetic to draw any practical conclusions.  ",12,301,23.15384615384616,5.468965517241379,167,2,299,0.0066889632107023,0.0132013201320132,0.7463,81,42,46,28,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 8, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,1,1,2,8,1,1,0,0,0,0,0,0,0,1,0,1,0,0,5,1,0,0.5017874355363862,0.4469322741838163,0.2848342832528082
ICLR2018-B1wN2f2-G-R1,,"This paper surveys models for collaborative filtering with user/item covariate. Overall the authors seems to have captured the essence of a large number of popular CF models and I found that the proposed model classification is reasonable. The notation also make it easy to understand the differences between different models. In that sense this paper could be useful to researchers wanting to better understand this field. It may also be useful to develop further insights into current models (although the authors do not go that route). The impact of this paper may be limited in this community since it is a survey about a fairly niche topic (a subset of recommender systems) that may not be of central interest at ICLR.  Overall, I think this paper would be a better fit in a recsys, applied ML or information retrieval journal. A few comments:   I find that there are several ways the paper could make a stronger contribution: 1) Use the unifying notation to discuss strengths and weaknesses of current approaches (ideally with insights about possible future approaches). n2) Report the results of a large study of many of the surveyed models on a large number of datasets. Ideally further insights could be derived from these results. n3) Provide a common code framework with all methods n4) Add a discussion on more structured sources of covariates (e.g., social networks). This could probably more or less easily be added as a subsection using the current classification. - A similar classification of collaborative filtering models with covariates is proposed in this thesis (p.41): https://tspace.library.utoronto.ca/bitstream/1807/68831/1/Charlin_Laurent_201406_PhD_thesis.pdf - The paper is well written overall but the current version of the paper contains several typos.",17,276,21.23076923076923,5.445736434108527,149,7,269,0.0260223048327137,0.0320284697508896,0.945,80,37,43,13,11,7,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 4, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 5, 'OAL': 4, 'BIB': 1, 'EXT': 1}","{'APR': 2, 'NOV': 0, 'IMP': 5, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 2}",0,1,1,2,1,4,0,2,0,2,5,4,1,1,2,0,5,1,2,0,3,2,2,0.7867573636214226,0.7794728856552022,0.6111633740187787
ICLR2018-B1wN2f2-G-R2,,"This paper provides a survey of attribute-aware collaborative filtering. In particular, it classifies existing methods into four different categories, according to the representation of the interactions of users, items and attributes. Furthermore, the authors also provide the probabilistic interpretation of the models. In addition, preliminary experiments comparing among different categories are also provided.  There have existed several works which also provide surveys of attribute-aware collaborative filtering . Hence, the contribution of this paper is limited, although the authors claim two differences between their work and the existing ones. In particular, the advantages and disadvantages of different categories are not systematically compared, and hence the readers cannot get insightful comments and suggestions from this survey. n In general, survey papers are not very suitable for publication at conferences.  ",8,126,14.0,6.282258064516129,80,0,126,0.0,0.0076923076923076,-0.2732,38,16,18,10,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 0}",0,1,2,1,0,2,1,1,0,0,0,1,0,2,1,0,0,3,0,0,1,1,0,0.5717685795128972,0.4446787695588243,0.3256166568136814
ICLR2018-B1wN2f2-G-R3,,"This paper reviews the existing literature on attribute-based collaborative filtering.  The author categories the existing works int four categories. While the categorization is reasonable , there is no proposed new work beyond the existing approaches. No new insight is being discussed. Such survey style paper is not appropriate to for ICLR.",6,50,10.0,5.7,37,0,50,0.0,0.0192307692307692,-0.5267,14,8,11,2,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 3, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,1,2,0,2,0,0,0,0,0,1,0,1,1,3,0,0,0,0,1,0,0,0.428880920547069,0.3334235485023696,0.2160202352882493
ICLR2018-B1ydPgTpW-R1,Reject,"The author(s) proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digits. The method uses a learnable character embedding to transform the data, but is an end-to-end approach . The analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted features. Here are my concerns: 1) As the price shows a high skewness in Fig. 1, it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model. That is, making an error of $100 for a plate that is priced $1000 has a huge difference in meaning to that for a plate priced as $10,000.  2) The time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in section 7.2. If so, the evaluation setting of dividing data into three *random* sets of training, validation, and test, in 5.3 doesn't seem to be the right and most appropriate choice. It should however, be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the prediction. ",8,207,20.7,5.094240837696336,120,2,205,0.0097560975609756,0.0333333333333333,0.7964,58,24,37,5,8,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 5, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,1,1,3,5,0,1,1,3,0,0,0,1,0,0,0,1,0,0,7,0,0,0.5725509601247296,0.22595396683128,0.2553722739235593
ICLR2018-B1ydPgTpW-R2,Reject,"Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the luckiness of the license plate number. The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings. Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout. The proposed pipeline converts each character to an embedding with the only sentence of description being Each character is converted by a lookup table to a vector representation, known as character embedding. Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined.  Finally they find a a feature vector for each plate by summing up the output of the last recurrent layer overtime.  and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates. In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better. Specific Comments on Sections:  Comments: Sec 1,2 In these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data. Sec 3 The author does not mention the following reference: Deep learning for stock prediction using numerical and textual information by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks. Sec 4 What are the characters embedded with? This is important to specify. Is it Word2vec or something else? What does the lookup table consist of?  References should be added to the relevant methods. Sec 5 I feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison. LSTMs as well could have been a point of comparison. Sec 6  Nothing too insightful is said about the RNN Model. Sec 7 The ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table. Overall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN. However comparisons are lacking and the paper is not presented very scientifically. The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well. ",21,472,22.476190476190474,4.991130820399113,234,2,470,0.0042553191489361,0.0187110187110187,0.9425,132,42,88,36,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 0, 'DAT': 2, 'MET': 16, 'EXP': 1, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 6, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 4, 'CLA': 0}",0,0,7,0,2,16,1,4,0,1,0,2,2,0,0,0,0,6,2,0,7,4,0,0.575925388780555,0.4491436002882199,0.325841380644246
ICLR2018-B1ydPgTpW-R3,Reject,"The authors present a deep neural network that evaluates plate numbers. The relevance of this problem is that there are auctions for plate numbers in Hong Kong, and predicting their value is a sensible activity in that context.  I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow. There are some typos and grammatical problems (indicated below), but nothing really serious. So, the paper is relevant and well presented. However, I find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality. Even though the significance of the work is apparent given the good results of the proposed neural network, I believe that such material is more appropriate to a focused applied meeting. However, even for that sort of setting I think the paper requires some additional work, as some final parts of the paper have not been tested yet (the interesting part of explanations). Hence I don't think the submission is ready for publication at this moment. Concerning the text, some questions/suggestions: - Abstract, line 1: I suppose In the Chinese society...--- are there many Chinese societies? - The references are not properly formatted; they should appear at (XXX YYY) but appear as XXX (YYY) in many cases, mixed with the main text. - Footnote 1, line 2: an exchange. - Page 2, line 12: prices. Among. - Please add commas/periods at the end of equations. - There are problems with capitalization in the references. ",17,252,15.75,5.109243697478991,138,2,250,0.008,0.0192307692307692,0.9717,71,28,45,18,10,8,"{'ABS': 2, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 8, 'BIB': 2, 'EXT': 1}","{'APR': 2, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 7, 'REC': 1, 'EMP': 0, 'SUB': 1, 'CLA': 3}",2,1,1,2,0,2,0,1,0,1,0,8,2,1,2,1,1,1,7,1,0,1,3,0.7149270411812,0.8894302384843851,0.6278994024373229
ICLR2018-B1zlp1bRW-R1,,"Quality The theoretical results presented in the paper appear to be correct. However, the experimental evaluation is globally limited,  hyperparameter tuning on test which is not fair. Clarity The paper is mostly clear, even though some parts deserve more discussion/clarification (algorithm, experimental evaluation). Originality The theoretical results are original, and the SGD approach is a priori original as well. Significance The relaxed dual formulation and OT/Monge maps convergence results are interesting and can of of interest for researchers in the area, the other aspects of the paper are limited. Pros: -Theoretical results on the convergence of OT/Monge maps -Regularized formulation compatible with SGD Cons -Experimental evaluation limited -The large scale aspect lacks of thorough analysis -The paper presents 2 contributions but at then end of the day, the development of each of them appears limited Comments:  -The weak convergence results are interesting. However, the fact that no convergence rate is given makes the result weak. In particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential. This can be coherent with the problem of Domain Adaptation that can be NP-hard even under the co-variate shift assumption (Ben-David&Urner, ALT2012). Then, I think that the claim of page 6 saying that Domain Adaptation can be performed  early optimally has then to be rephrased. I think that results show that the approach is theoretically justified but optimality is not here yet. Theorem 1 is only valid for entropy-based regularizations, what is the difficulty for having a similar result with L2 regularization? -The experimental evaluation on the running time is limited to one particular problem. If this subject is important, it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations. It is also surprising that the efficiency the L2-regularized version is not evaluated. For a paper interesting in large scale aspects, the experimental evaluation is rather weak. The 2 methods compared in Fig 2 reach the same objective values at convergence, but is there any particular difference in the solutions found? -Algorithm 1 is presented without any discussion about complexity, rate of convergence. Could the authors discuss this aspect? The presentation of this algo is a bit short and could deserve more space (in the supplementary) -For the DA application, the considered datasets are classic but not really large scale, anyway this is a minor remark. The setup is not completely clear, since the approach is interesting for out of sample data, so I would expect the map to be computed on a small sample of source data, and then all source instances to be projected on target with the learned map. This point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point while this is an interesting aspect of the paper: this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification. Hyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup: it seems that the parameters are tuned on test (for all methods), which is not fair since target label information will not be available from a practical standpoint. The authors claim that they did not want to compete with state of the art DA, but the approach of Perrot et al., 2016 seems to a have a similar objective and could be used as a baseline. Experiments on generative optimal transport are interesting and probably generate more discussion/perspectives .  -- After rebuttal -- Authors have answered to many of my comments, I think this is an interesting paper, I increase my score. ",36,619,25.791666666666668,5.277310924369748,268,8,611,0.0130932896890343,0.0414012738853503,0.9687,165,89,110,42,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 4, 'MET': 20, 'EXP': 8, 'RES': 9, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 13, 'CLA': 1}",0,0,2,0,4,20,8,9,0,2,1,3,0,0,0,1,1,0,0,0,19,13,1,0.5775355243928982,0.5680309074825867,0.35974648973277523
ICLR2018-B1zlp1bRW-R2,,"The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems. I like the weak convergence results, but this is just weak convergence. It appears to be an overstatement to claim that the approach  early-optimally transports one distribution to the other (Cf e.g. Conclusion). There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2. Also, near-optimality would refer to some parameters being chosen in the best possible way. I do not see that from the paper. However, the weak convergence results are good. A better result, hinting on how optimal this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist). This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum. I also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ?  After all, the L2 one is just an approximation of the entropic one. Typoes:  1- Kanthorovich -> Kantorovich (Intro) 2- Cal C <-> C (eq. 4)",12,211,16.23076923076923,5.126903553299492,116,2,209,0.0095693779904306,0.0409090909090909,0.9412,55,29,37,10,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 3, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,0,0,1,0,3,3,3,1,0,0,0,0,0,0,0,0,0,0,0,9,1,1,0.3579270367392627,0.3383089928120771,0.17727976173268636
ICLR2018-B1zlp1bRW-R3,,"This paper proposes a new method for estimating optimal transport plans and maps among continuous distributions, or discrete distributions with large support size. First, the paper proposes a dual algorithm to estimate Kantorovich plans, i.e. a coupling between two input distributions minimizing a given cost function, using dual functions parameterized as neural networks. Then an algorithm is given to convert a generic plan into a Monge map, a deterministic function from one domain to the other, following the barycenter of the plan. The algorithms are shown to be consistent, and demonstrated to be more efficient than an existing semi-dual algorithm. Initial applications to domain adaptation and generative modeling are also shown. These algorithms seem to be an improvement over the current state of the art for this problem setting, although more of a discussion of the relationship to the technique of Genevay et al. would be useful: how does your approach compare to the full-dual, continuous case of that paper if you simply replace their ball of RKHS functions with your class of deep networks? The consistency properties are nice, though they don't provide much insight into the rate at which epsilon should be decreased with n or similar properties. The proofs are clear, and seem correct on a superficial readthrough; I have not carefully verified them. The proofs are mainly limited in that they don't refer in any way to the class of approximating networks or the optimization algorithm, but rather only to the optimal solution. Although of course proving things about the actual outcomes of optimizing a deep network is extremely difficult, it would be helpful to have some kind of understanding of how the class of networks in use affects the solutions. In this way, your guarantees don't say much more than those of Arjovsky et al., who must assume that their critic function reaches the global optimum: essentially you add a regularization term, and show that as the regularization decreases it still works, but under seemingly the same kind of assumptions as Arjovsky et al.'s approach which does not add an explicit regularization term at all. Though it makes sense that your regularization might lead to a better estimator, you don't seem to have shown so either in theory or empirically. The performance comparison to the algorithm of Genevay et al. is somewhat limited: it is only on one particular problem, with three different hyperparameter settings. Also, since Genevay et al. propose using SAG for their algorithm, it seems strange to use plain SGD; how would the results compare if you used SAG (or SAGA/etc) for both algorithms? In discussing the domain adaptation results, you mention that the L2 regularization works very well in practice, but don't highlight that although it slightly outperforms entropy regularization in two of the problems, it does substantially worse in the other. Do you have any guesses as to why this might be? For generative modeling: you do have guarantees that, *if* your optimization and function parameterization can reach the global optimum, you will obtain the best map relative to the cost function. But it seems that the extent of these guarantees are comparable to those of several other generative models, including WGANs, the Sinkhorn-based models of Genevay et al. (2017, https://arxiv.org/abs/1706.00292/), or e.g. with a different loss function the MMD-based models of Li, Swersky, and Zemel (ICML 2015) / Dziugaite, Roy, and Ghahramani (UAI 2015). The different setting than the fundamental GAN-like setup of those models is intriguing, but specifying a cost function between the source and the target domains feels exceedingly unnatural compared to specifying a cost function just within one domain as in these other models. Minor:  In (5), what is the purpose of the -1 term in R_e? It seems to just subtract a constant 1 from the regularization term.",23,632,27.47826086956522,5.167763157894737,285,6,626,0.0095846645367412,0.0189274447949526,0.9456,171,73,99,34,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 0, 'MET': 21, 'EXP': 0, 'RES': 7, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 2, 'CLA': 0}",0,1,6,1,0,21,0,7,0,1,0,0,0,0,0,0,0,5,0,0,12,2,0,0.4343119217152985,0.3407501918536874,0.22043122393428416
ICLR2018-B1zlp1bRW-R4,,"This paper explores a new approach to optimal transport. Contributions include a new dual-based algorithm for the fundamental task of computing an optimal transport coupling, the ability to deal with continuous distributions tractably by using a neural net to parameterize the functions which occur in the dual formulation, learning a Monge map parameterized by a neural net allowing extremely tractable mapping of samples from one distribution to another, and a plethora of supporting theoretical results. The paper presents significant, novel work in a straightforward, clear and engaging way. It represents an elegant combination of ideas, and a well-rounded combination of theory and experiments. I should mention that I'm not sufficiently familiar with the optimal transport literature to verify the detailed claims about where the proposed dual-based algorithm stands in relation to existing algorithms. Major comments:  No major flaws. The introduction is particular well written, as an extremely clear and succinct introduction to optimal transport. Minor comments:  In the introduction, for VAEs, it's not the case that f(X) matches the target distribution. There are two levels of sampling: of the latent X and of the observed value given the latent. The second step of sampling is ignored in the description of VAEs in the first paragraph. In the comparison to previous work, please explicitly mention the EMD algorithm, since it's used in the experiments. It would've been nice to see an experimental comparison to the algorithm proposed by Arjovsky et al. (2017), since this is mentioned favorably in the introduction. In (3), R is not defined. Suggest adding a forward reference to (5). In section 3.1, it would be helpful to cite a reference to support the form of dual problem. Perhaps the authors have just done a good job of laying the groundwork, but the dual-based approach proposed in section 3.1 seems quite natural. Is there any reason this sort of approach wasn't used previously, even though this vein of thinking was being explored for example in the semi-dual algorithm? If so, it would interesting to highlight the key obstacles that a naive dual-based approach would encounter and how these are overcome.  In algorithm 1, it is confusing to use u to mean both the parameters of the neural net and the function represented by the neural net. There are many terms in R_e in (5) which appear to have no effect on optimization, such as a(x) and b(y) in the denominator and - 1. It seems like R_e boils down to just the entropy. The definition of F_epsilon is made unnecessarily confusing by the omission of x and y as arguments. It would be great to mention very briefly any helpful intuition as to why F_epsilon and H_epsilon have the forms they do. In the discussion of Table 1, it would be helpful to spell out the differences between the different Bary proj algorithms, since I would've expected EMD, Sinkhorn and Alg. 1 with R_e to all perform similarly. In Figure 4 some of the samples are quite non-physical. Is their any helpful intuition about what goes wrong? What cost is used for generative modeling on MNIST? For generative modeling on MNIST, 784d vector is less clear than 784-dimensional vector. The fact that the variable d is equal to 768 is not explicitly stated. It seems a bit strange to say The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost as if this was an advantage of the proposed method, since arguably there isn't a really natural cost in the generative modeling case (unlike in the domain adaptation case); the latent variable seems kind of conceptually distinct from observation space. Appendix A isn't referred to from the main text as far as I could tell. Just merge it into the main text?    ",30,634,19.8125,5.065436241610739,295,5,629,0.0079491255961844,0.0327102803738317,0.9916,159,79,105,37,11,5,"{'ABS': 0, 'INT': 6, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 20, 'EXP': 2, 'RES': 2, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 3, 'CLA': 1}",0,6,3,1,1,20,2,2,2,2,0,2,0,1,0,1,0,2,0,0,15,3,1,0.7909918401613311,0.56459347521719,0.49530356969540945
ICLR2018-BJ0hF1Z0b-R1,Accept,"Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy. The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example. The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework. Positive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale. The paper uses the right composition tools like moments accountant to get strong privacy guarantees. The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm. Both these contributions are important in the effectiveness of the overall algorithm. Concern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models. I did not find strong algorithmic ideas in the paper. I found the paper to be lacking in that respect.",10,178,17.8,5.505813953488372,100,1,177,0.0056497175141242,0.0168539325842696,0.9705,58,25,30,3,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,1,2,0,5,1,1,0,0,0,1,0,0,0,0,0,1,0,0,5,1,0,0.5010600683169943,0.3358211630727052,0.2535835161968005
ICLR2018-BJ0hF1Z0b-R2,Accept,"Summary of the paper -------------------------------  The authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee. The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed. Clarity, Significance and Correctness --------------------------------------------------  Clarity: Excellent Significance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point. Correctness: The paper is technically correct. Questions --------------  1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline. 2. Clipping is supposed to help with the exploding gradients problem. Do you have an idea why a low threshold hurts the performances? Is it because it reduces the amplitude of the updates (and thus simply slows down the training)? 3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)? Pros ------  1. Nice extensions to FederatedAveraging to provide privacy guarantee. 2. Strong experimental setup that analyses in details the proposed extensions. 3. Experiments performed on public datasets. Cons -------  None Typos --------  1. Section 2, paragraph 3 : is given in Figure 1 -> is given in Algorithm 1   Note -------  Since I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.",16,234,13.764705882352942,5.493087557603687,135,0,234,0.0,0.0277777777777777,0.9674,72,25,43,11,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 9, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,0,2,0,1,9,2,1,1,1,0,4,0,2,0,0,1,2,1,1,8,0,1,0.6451709479538216,0.6711375312677574,0.45163533388061494
ICLR2018-BJ0hF1Z0b-R3,Accept,"This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models. It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model. The idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. This work makes a pretty significant contribution to such topic. It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy. The experiment shows good privacy and utility. The presentation of the paper can be improved a bit. For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper. Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work. ",9,176,17.6,5.971098265895954,103,1,175,0.0057142857142857,0.0112359550561797,0.9792,50,28,26,14,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 3, 'PNF': 3, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,3,2,0,2,2,1,0,0,0,3,0,0,0,0,2,3,3,0,1,0,0,0.4292189297596467,0.4448499440548789,0.24361304944101692
ICLR2018-BJ4prNx0W-R1,Reject,"Quality The paper is well-written and clear, and includes relevant comparisons to previous work (NPI and recursive NPI). Clarity The paper is clearly written. Originality To my knowledge the method proposed in this work is novel. It is the first to study constructing minimal training sets for NPI given a black-box oracle. However, as pointed out by the authors, there is a lot of similar prior work in software testing. Significance The work could be potentially significant, but there are some very strong assumptions made in the paper that could limit the impact. If the NPI has access to a black-box oracle, it is not clear what is the use of training an NPI in the first place. It would be very helpful to describe a potential scenario where the proposed approach could be useful. Also, it is assumed that the number of possible inputs is finite (also true for the recursive NPI paper), and it is not clear what techniques or lessons of this paper might transfer to tasks with perceptual inputs. The main technical contribution is the search procedure to find minimal training sets and pare down the observation size, and the empirical validation of the idea on several algorithmic tasks. Pros - Greatly improves the data efficiency of recursive NPI. - Training and verification sets are automatically generated by the proposed method. Cons - Requires access to a black-box oracle to construct the dataset. - Not clear that the idea will be useful in more complex domains with unbounded inputs. ",15,249,16.6,4.954918032786885,129,1,248,0.0040322580645161,0.0590551181102362,0.9818,62,39,43,15,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 2, 'DAT': 4, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 2, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 3, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 2}",0,0,2,2,4,10,0,2,0,1,2,2,0,0,0,2,3,2,0,0,5,3,2,0.574033612138926,0.6697073279321395,0.388906969538826
ICLR2018-BJ4prNx0W-R2,Reject,"In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle. In particular, they aim at generating a complete set that fully specifies the behavior of the oracle. The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation. The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle. The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class. Their experiments show that this improve technique can produce complete training sets for three programs. It is nice to see the application of ideas from different areas for learning-related questions. However, there is one thing that bothers me again and again. Why do we need a data-generation technique in the paper at all? Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data. If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter. One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter. During research, we have multiple executable oracles and need to produce good training data from them. The authors' technique may let us do this data-generation easily. But this benefit to the researchers does not seem to be strong enough for the acceptance at ICLR'18.   ",14,281,18.73333333333333,5.145985401459854,133,1,280,0.0035714285714285,0.0140845070422535,0.9656,72,20,51,14,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 5, 'MET': 11, 'EXP': 3, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,0,2,5,11,3,4,0,0,1,1,0,1,1,0,0,0,0,1,6,0,0,0.6459022240779648,0.3364431205075482,0.3279977504170337
ICLR2018-BJ4prNx0W-R3,Reject,"Previous work by Cai et al. (2017) shows how to use Neural Programmer-Interpreter (NPI) framework to prove correctness of a learned neural network program by introducing recursion.  It requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem. Moreover, the traces need to be recursive: each function only takes a finite, bounded number of actions. In this paper, the authors show how training set can be generated automatically satisfying the conditions of Cai et al.'s paper. They iteratively explore all possible behaviors of the oracle in a breadth-first manner, and the bounded nature of the recursive oracle ensures that the procedure converges. As a running example, they show how this can be be done for bubblesort. The training set generated in this Fprocess may have a lot of duplicates, and the authors show how these duplicates can possibly be removed. It indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the paper. I am not an expert in this area, so it is difficult for me to judge the technical merit of the work. My feeling from reading the paper is that it is rather incremental over Cai et al. I am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant. But these are also the same set of experiments performed by Cai et al. Given the original number of traces generated is huge, I do not understand, why this method is at all practical. This also explains why the authors have just tested the performance on extremely small sized data. It will not scale. So, I am hesitant accepting the paper. I would have been more enthusiastic if the authors had proposed a way to combine the training space exploration as well as removing redundant traces together to make the whole process more scalable and done experiments on reasonably sized data. ",16,347,18.26315789473684,4.954819277108434,169,2,345,0.0057971014492753,0.0114613180515759,0.9636,87,31,78,22,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 1, 'DAT': 5, 'MET': 6, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,0,5,1,5,6,3,2,0,1,0,1,0,3,0,0,0,4,0,1,5,1,0,0.644821167607617,0.4472837618553862,0.36679219540457847
ICLR2018-BJ6anzb0Z-R1,Reject,"The authors present a study that aims at inferring the emotional tags provided by Thumblr users starting from images and texts in the captions. For text processing the authors use a standard LSTM taking as input GLOVE vectors of words in a sentence. For visual information, authors use a pretrained CNN (with fine tuning). A fully connected layer is used to fuse the multimodal information. Experimental results are reported in a self generated data set. The contribution from the RL perspective is limited, in the sense that the authors simply applied standard models to predict a bunch of labels (in this case, emotion labels). It is interesting the psychological analysis that the authors present in Section 6. Still, I think the contribution in that part is a: sentiment-psychologically inspired analysis of the Thumbrl data set. I think the author's statement on that this study leads to a more plausible psychological model of emotion is not well founded (they also mention to learn to recognize the latent emotional state). Whereas it is true that psychological studies rely on self - filled questionnaires, comparing a questionnaire (produced by expert psychologist) to the tags provided by users in a social network is to ambitious. (in some parts the authors make explicit this is an approximation, this should be stressed in every part of the paper) ",10,221,20.09090909090909,5.2898550724637685,124,2,219,0.0091324200913242,0.0224215246636771,0.8746,64,21,44,8,7,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 4, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,0,0,1,3,4,1,2,0,2,0,2,0,0,0,0,0,0,0,0,2,1,0,0.5009485979025172,0.2228441796570651,0.22518587039361668
ICLR2018-BJ6anzb0Z-R2,Reject,"This paper presents a method for classifying Tumblr posts with associated images according to associated single emotion word hashtags. The method relies on sentiment pre-processing from GloVe and image pre-processing from Inception. My strongest criticism for this paper is against the claim that Tumblr post represent self-reported emotions and that this method sheds new insight on emotion representation and my secondary criticism is a lack of novelty in the method, which seems to be simply a combination of previously published sentiment analysis module and previously published image analysis module, fused in an output layer. The authors claim that the hashtags represent self-reported emotions, but this is not true in the way that psychologists query participants regarding emotion words in psychology studies. Instead these are emotion words that a person chooses to broadcast along with an associated announcement. As the authors point out, hashtags and words may be used sarcastically or in different ways from what is understood in emotion theory. It is quite common for everyday people to use emotion words this way e.g. using #love to express strong approval rather than an actual feeling of love. In their analysis the authors claim: ""The 15 emotions retained were those with high relative frequencies on Tumblr among the PANAS-X scale (Watson & Clark, 1999)"". However five of the words the authors retain: bored, annoyed, love, optimistic, and pensive are not in fact found in the PANAS-X scale:  Reference: The PANAS-X Scale: https://wiki.aalto.fi/download/attachments/50102838/PANAS-X-scale_spec.pdf Also the longer version that the authors cited:  https://www2.psychology.uiowa.edu/faculty/clark/panas-x.pdf It should also be noted that the PANAS (Positive and Negative Affect Scale) scale and the PANAS-X (the ""X"" is for eXtended) scale are questionnaires used to elicit from participants feelings of positive and negative affect, they are not collections of core emotion words, but rather words that are colloquially attached to either positive or negative sentiment. For example PANAS-X includes words like:""strong"" ,""active"", ""healthy"", ""sleepy"" which are not considered emotion words by psychology. If the authors stated goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment they should be aware that this is exactly what PANAS is designed to do - not to infer the latent emotional state of a person, except to the extent that their affect is positive or negative. The work of representing emotions had been an field in psychology for over a hundred years and it is still continuing. https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions.  One of the most popular theories of emotion is the theory that there exist ""basic"" emotions: Anger, Disgust, Fear, Happiness (enjoyment), Sadness and Surprise (Paul Ekman, cited by the authors). These are short duration sates lasting only seconds. They are also fairly specific, for example ""surprise"" is sudden reaction to something unexpected, which is it exactly the same as seeing a flower on your car and expressing ""what a nice surprise. ""  The surprise would be the initial reaction of ""what's that on my car?  Is it dangerous?"" but after identifying the object as non-threatening, the emotion of ""surprise"" would likely pass and be replaced with appreciation. The Circumplex Model of Emotions (Posner et al 2005) the authors refer to actually stands in opposition to the theories of Ekman. From the cited paper by Posner et al :  The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems. This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion.  From my reading of this paper, it is clear to me that the authors do not have a clear understanding of the current state of psychology's view of emotion representation and this work would not likely contribute to a new understanding of the latent structure of peoples' emotions. In the PCA result, it is not clear that the first axis represents valence, as sad has a slight positive on this scale and sad is one of the emotions most clearly associated with negative valence. With respect to the rest of the paper, the level of novelty and impact is ok, but not good enough. This analysis does not seem very different from Twitter analysis, because although Tumblr posts are allowed to be longer than Twitter posts, the authors truncate the posts to 50 characters. Additionally, the images do not seem to add very much to the classification. The authors algorithm also seems to be essentially a combination of two other, previously published algorithms. For me the novelty of this paper was in its application to the realm of emotion theory, but I do not feel there is a contribution here. This paper is more about classifying Tumblr posts according to emotion word hashtags than a paper that generates a new insights into emotion representation or that can infer latent emotional state.             ",29,809,26.966666666666665,5.29559748427673,330,6,803,0.0074719800747198,0.0192076830732292,0.9887,239,94,124,48,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 1, 'MET': 16, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 6, 'IMP': 0, 'CMP': 6, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,1,5,0,1,16,0,4,1,2,0,4,0,0,0,6,0,6,0,0,13,0,0,0.5758314862277737,0.3416081732599896,0.29242714228705496
ICLR2018-BJ6anzb0Z-R3,Reject,"The paper presents a multi-modal CNN model for sentiment analysis that combines images and text. The model is trained on a new dataset collected from Tumblr. Positive aspects: + Emphasis in model interpretability and its connection to psychological findings in emotions + The idea of using Tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative.    Weaknesses: - A deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary.   - The proposed method is not compared with other methods that combine text and image for sentiment analysis. -  The study is limited to just one dataset. The paper presents interesting ideas and findings in an important challenging area. The main novelties of the paper are: (1) the use of Tumblr data, (2) the proposed CNN architecture, combining images and text (using word embedding. I missed a related work section, where authors clearly mention previous works on similar datasets. Some related works are mentioned in the paper, but those are spread in different sections. It's hard to get a clear overview of the previous research: datasets, methods and contextualization of the proposed approach in relation with previous work. I think authors should cite Sentibanks. Also, at some point authors should compare their proposal with previous work. More comments:  - Some figures could be more complete: to see more examples in Fig 1, 2, 3 would help to understand better the dataset and the challenges. - In table 4, for example, it would be nice to see the performance on the different emotion categories. - It would be interesting to see qualitative visual results on recognitions. I like this work, but I think authors should improve the aspects I mention for its publication. ",20,305,16.05263157894737,5.329861111111111,146,3,302,0.009933774834437,0.0218068535825545,0.9825,98,44,46,6,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 6, 'PDI': 2, 'DAT': 6, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,2,6,2,6,5,0,3,2,2,0,3,0,0,0,2,0,1,1,1,6,3,0,0.644775882057683,0.6700349144420427,0.45356148880953073
ICLR2018-BJ78bJZCZ-R1,Reject,"The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor. While the RWA was an interesting idea with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods. On the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original. On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU - except for MultiCopy where it trains faster, but not to better results and it looks like the difference is between few and very-few training steps anyway. The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM. So the results are not strongly convincing, and the paper lacks any mention of newer work on attention. This year strong improvements over state-of-the-art have been achieved using attention for translation (Attention is All You Need) and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition). To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks.",11,224,28.0,5.1900452488687785,135,0,224,0.0,0.0176991150442477,0.9726,63,34,31,17,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,2,2,0,5,0,6,0,0,0,1,0,2,0,1,0,3,0,0,5,0,1,0.5014283937510827,0.4471665992981963,0.2834195467888791
ICLR2018-BJ78bJZCZ-R2,Reject,"This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage. The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions. The proposed method is using Elman nets as the base RNN.  I think the same method can be applied to GRUs or LSTMs . Some parameters might be redundant, however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination. Is there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA? Overall, the proposed method seems to be very useful for the RWA.",8,147,24.5,4.944827586206896,91,3,144,0.0208333333333333,0.0402684563758389,0.7965,40,12,33,6,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,1,0,0,8,2,2,0,2,0,0,0,0,0,0,1,1,0,0,5,0,0,0.3590608190043842,0.3358211630727052,0.17751311492812613
ICLR2018-BJ78bJZCZ-R3,Reject,"Summary: This paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention. The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t. Pros: - Addresses an issue of RWAs. Cons: -The paper addresses a problem with an issue with RWAs.  But it is not clear to me why would that be an important contribution. -The writing needs more work. -The experiments are lacking and the results are not good enough. General Comments:  This paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address. I would still like to have a better justification on why should we care about RWA and fixing that model. The writing of this paper seriously needs more work. The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c. The experiments are only on toyish and small scale tasks. According to the results the model doesn't really do better than a simple LSTM or GRU.",13,209,16.076923076923077,4.73,111,2,207,0.0096618357487922,0.0471698113207547,-0.0266,53,19,43,18,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 3}",0,1,0,2,1,8,2,2,0,0,0,2,0,0,0,2,0,0,0,0,5,2,3,0.5019979611508171,0.447246328418364,0.27552340381967344
ICLR2018-BJ7d0fW0b-R1,Reject,"SIGNIFICANCE AND ORIGINALITY:  The authors propose to accelerate the learning of complex tasks by exploiting traces of experts. Unlike the most common form of imitation learning or behavioral cloning, the authors  formulate their solution in the case where the expert's state trajectory is observable,  but the expert's actions are not. This is an important and useful problem in robotics and other applications. Within this specific setting the authors differentiate their approach from others  by developing a solution that does NOT estimate an explicit dynamics model ( e.g.,  P( S' | S, A ) ). The benefits of not estimating an explicit action model are not really demonstrated in a clear way. The author's articulate a specific solution that provides heuristic guidance rewards that cause the  learner to favor actions that achieve subgoals calculated from expert behavior and refactors the representation of the Q function so that it  has a component that is a function of the subgoal extracted from the expert. These subgoals are linear functions of the expert's change in state (or change in state features). The resultant policy is a function of the expert traces on which it depends. The authors show they can retrain a new policy that does not require the expert traces. As far as I am aware, this is a novel approach to the problem. The authors claim that this factorization is important and useful but the paper doesn't really illustrate this well. They demonstrate the usefulness of the algorithm against a DQN baseline on Doom game problems. The algorithm learns faster than unassisted DQN as shown by learning curve plots. They also evaluate the algorithms on the quality of the final policies for their approach, DQN,  and  a supervised learning from demonstration approach ( LfD ) that requires expert actions. The proposed approach does as well or better than competing approaches. QUALITY  Ablation studies show that the guidance rewards are important to achieving the improved performance of the proposed method which is important confirmation that the architecture is working in the intended way. However, it would also be useful to do an ablation study of the ""factorization"" of action values.  Is this important to achieving better results as well or is the guidance reward enough?  This seems like a key claim to establish. CLARITY  The details of the memory based kernel density estimation and neural gradient training seemed complicated by the way that the process was implemented. Is it possible to communicate the intuitions behind what is going on? I was able to work out the intuitions behind the heuristic rewards, but I still don't clearly get  what the Q-value factorization is providing: To keep my text readable, I assume we are working in feature space instead of state space and use different letters for learner and expert: Learner: S   phi(s)     Expert's i^th state visit:  Ei   phi( hat{s}_i }  where Ei' is the successor state to Ei The paper builds upon approximate n-step discrete-action Q-learning  where the Q value for an action is a linear function of the state features:      Qp(S,a)   Wa S + Ba  where parameters p   ( Wa, Ba ). After observing an experience ( S,A,R,S' ) we use Bellman Error as a loss function to optimize Qp for parameter p. I ignore the complexities of n-step learning and discount factors for clarity. Loss   E[    R + MAXa' Qp(S',a')    -   Qp(S,a)   ]    The authors suggest we can augment the environment reward R  with a heuristic reward Rh proportional to the similarity between  the learner ""subgoal and the expert ""subgoal in similar states. The authors propose to use cosine distance between representations  of what they call the ""subgoals"" of learner and expert. A subgoal is defined as a linear transformation of the distance traveled by an agent during a transition. The heuristic reward is proportional to the cosine distance between the learner and expert ""subgoals     Rh   B  <   Wv LearnerDirectionInStateS,   Wv ExpectedExpertDirectionInStatesSimilarToS   >  The learner's direction in state S is just (S-S') in feature space.  The authors model the behavior of the expert as a kernel density type approximator giving the expected direction of the expert starting from a states similar to the one the learner is in. Let < Wk S, Wk Ej > be a weighted similarity between learner state features S and expert state features Ej and Ej' be the successor state features encountered by the expert. Then the expected expert direction for learner state S is:       SUMj  < Wk S, Wk Ej > ( Ej - Ej' ) Presumably the linear Wk transform helps us pick out the important dimensions of similarity between S and Ej.  Mapping the learner and expert directions into subgoal space using Wv, the heuristic reward is     Rh   B <   Wv (S-S'),                       Wv SUMj  < Wk S, Wk Ej > ( Ej - Ej' ) > I ignore the ReLU here, but I assume that is operates element-wise and just clips negative values? There is only one layer here so we don't have complex non-linear things going on? In addition to introducing a heuristic reward term, the authors propose to alter the Q-function to be specific to the subgoal. Q( s,a,g )   g(S) Wa S + Ba  The subgoal is the same as the first part, namely a linear transform of the expected expert direction in  states similar to state S.      g(S)    Wv   SUMj  < Wk S, Wk Ej >  ( Ej - Ej' )   So in some sense, the Q function is really just a function of S, as g is calculated from S. Q( S,a )   g(S) Wa S + Ba   So this allows the Q-function more flexibility to capture each subgoal in a different linear space? I don't really get the intuition behind this formulation. It allows the subgoal to adjust the value  of the underlying model? Essentially the expert defines a new Q-value problem at every state  for the learner? In some sense are we are defining a model for the action taken by the expert? ADDITIONAL THOUGHTS  While the authors compare to an unassisted baseline, they don't compare to methods that use an action model which is not a fatal flaw but would have been nice. One can imagine there might be scenarios where the local guidance rewards of this  form could be problematic, particularly in scenarios where the expert and learner are not identical and it is possible to return to previous states, such as the grid worlds the authors discuss: If the expert's first few transitions were easily approximable, the learner would get local rewards that cause it to mimic expert behavior. However, if the next step in the expert's path was difficult to approximate,  then the reward for imitating the expert would be lower. Would the learner then just prefer to go back towards those states that it can approximate and endlessly loop? In this case, perhaps expressing heuristic rewards as potentials as described in Ng's shaping paper might solve the problem. PROS AND CONS  Important problem generally. Avoiding the estimation of a dynamics model was stated as a given, but perhaps more could be put into motivating this goal. Hopefully it is possible to streamline the methodology section to communicate the intuitions more easily. ",53,1159,28.26829268292683,5.071428571428571,402,6,1153,0.00520381613183,0.0149812734082397,0.9993,375,122,202,60,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 46, 'EXP': 6, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 25, 'SUB': 0, 'CLA': 0}",0,0,1,4,0,46,6,4,0,1,0,0,0,0,0,4,0,3,1,0,25,0,0,0.4406060579139774,0.4597410707486102,0.2528789664674119
ICLR2018-BJ7d0fW0b-R2,Reject,"The paper presents a method that leverages demonstrations from experts provided in the shape of sequences of states (actually, state transitions are enough, they don't need to come in sequences) to faster learn reinforcement learning tasks. The authors propose to learn subgoals (actually local rewards) to encourage the agent to go towards the same direction as the expert when encountering similar states. The main claimed advantage is that it doesn't require the knowledge of the actions taken by the expert, only observations of states. To me, there is a major flaw in the approach. Ho and Ermon 2016 extensively study the fact that imitation is not possible in stochastic environment without the knowledge of the actions. As the author say, learning the actions from state transitions in a standard stochastic MDP would require to learn the model. Yet, the authors demonstrate their approach in environments where the controlable dynamics is mainly deterministic (if one decides to turn right, the agents indeed turns right). So by subtracting features from successive states, the method mainly encodes the action as it almost encodes the one step dynamics in one shot. Also the main assumption is that there is an easy way to compute similarity between states. This assumption is not met in the HealthGathering environment as several different states may generate very similar vision features. This causes the method not to work. This brings us back to the fact that features encoding the actual dynamics, potentially on many consecutive states (e.g. feature expectations used in IRL or occupancy probability used in Ho and Ermon 2016), are mandatory. The method is also very close to the simplest IRL method possible which consists in placing positive rewards on every state the expert visited. So I would have liked a comparison to that simple method (using similar regression technique to generalize over states with similar features). Finally, I also think that using expert data generated by a pre-trained network makes the experimental section very weak. Indeed, it is unlikely that this kind of data can be obtained and training on this type of data is just a kind of distillation of the optimal network making the weights of the network close to the right optimum. With real data, acquired from humans, the training is likely to end up in a very different minima. Concerning the related work, the authors didn't mention the Universal Value Function Approximation (Schaul et al, @ICML 2015) which precisely extends V and Q functions to generalize over goals. This very much relates to the method used to generalize over subgoals in the paper. Also, the state if the art in IRL and learning from demonstration is lacking a lot of references. For instance, learning via RL + demonstrations was already studied into papers by Farahmand et al (APID, @NIPS 2013), Piot et al (RLED, @ ECML 2014) or Chemali & Lazaric (DPID, @IJCAI 2015) before Hester et al (DQfD @AAAI 2018). Some work is cited in the wrong context. For instance, Borsa et al 2017 doesn't do inverse RL (as said in the related work section) but learn to perform a task only from the extrinsic reward provided by the environment (as said in the introduction). BTW, I would suggest to refer to published papers if they exist instead of their Arxiv version (e.g. Hester et al, DQfD). ",24,553,20.48148148148148,5.009416195856874,261,3,550,0.0054545454545454,0.0215439856373429,0.9622,161,61,95,41,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 8, 'PDI': 3, 'DAT': 2, 'MET': 9, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,8,3,2,9,3,0,0,0,0,1,1,0,0,0,0,4,0,1,5,1,0,0.574320605702559,0.4472837618553862,0.32341216523857547
ICLR2018-BJ7d0fW0b-R3,Reject,"The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations. The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning. The goal of this process is to extract a function that maps any given state to a subgoal. Subgoals are then used to learn different Q-value functions, one per subgoal. To learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal. A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal. Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned). Finally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation. The paper is overall well-written, and the proposed idea seems interesting. However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them. From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration. It is not clear how this method would generalize to new situations. There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration. The architecture of the action value estimator does not seem novel, it's basically just an extension of DQN with an extra parameter (subgoal g). The empirical evaluation seems rather mixed. Figure 3 shows that the proposed method learns faster than DQN, but Table I shows that the improvement is not statistically significant, except in two games, DefendCenter and PredictPosition. Are these the results after all agents had converged? Overall, this is a good paper, but focusing on only a single game (Doom) is a weakness that needs to be addressed because one cannot tell if the choices were tailored to make the method work well for this game. Since the paper does not provide significant theoretical or algorithmic contribution, at least more realistic and diverse experiments should be performed. ",21,468,23.4,5.226164079822617,237,2,466,0.0042918454935622,0.0212314225053078,0.9411,132,48,89,24,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 15, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,0,4,0,15,3,2,0,0,0,2,0,0,0,2,0,1,0,0,8,0,1,0.4325255300341585,0.4488432540728634,0.24402073109254885
ICLR2018-BJ8c3f-0b-R1,Accept,"Update: On further consideration (and reading the other reviews), I'm bumping my rating up to a 7. I think there are still some issues, but this work is both valuable and interesting, and it deserves to be published (alongside the Naesseth et al. and Maddison et al. work). -----------  This paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling. Going beyond the several papers that proposed this simultaneously, the authors observe a key issue: the variance of the gradient of these IWAE-style bounds (w.r.t. the inference parameters) grows with their accuracy. They therefore propose using a more-biased but lower-variance bound to train the inference parameters, and the more-accurate bound to train the generative model. Overall, I found this paper quite interesting. There are a few things I think could be cleared up, but this seems like good work (although I'm not totally up to date on the very recent literature in this area). Some comments:  * Section 4: I found this argument extremely interesting. However, it's worth noting that your argument implies that you could get an O(1) SNR by averaging K noisy estimates of I_K. Rainforth et al. suggest this approach, as well as the approach of averaging K^2 noisy estimates, which the theory suggests may be more appropriate if the functions involved are sufficiently smooth, which even for ReLU networks that are non-differentiable at a finite number of points I think they should be. This paper would be stronger if it compared with Rainforth et al.'s proposed approaches. This would demonstrate the real tradeoffs between bias, variance, and computation. Of course, that involves O(K^2) or O(K^3) computation, which is a weakness. But one could use a small value of K (say, K 5). That said, I could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple examples. Basically, these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail.   * Section 3.3: Claim 1 is an interesting observation. But Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior. I think there's an easier way to make this argument:  Given an unbiased estimator hat{Z} of Z, by Jensen's inequality E[log hat{Z}] u2264 log Z, with equality iff the variance of hat{Z}   0. The only way to get an SMC estimator's variance to 0 is to drive the variance of the weights to 0. That only happens if you perfectly sample each particle from the true posterior, conditioned on all future information. All of which is true as far as it goes, but I think it's a bit of a distraction. The question is not ""what's it take to get to 0 variance"" but ""how quickly can we approach 0 variance"". In principle IS and SMC can achieve arbitrarily high accuracy by making K astronomically large. (Although [particle] MCMC is probably a better choice if one wants extremely low bias.) * Section 3.2: The choice of how to get low-variance gradients through the ancestor-sampling choice seems seems like an important technical challenge in getting this approach to work, but there's only a very cursory discussion in the main text. I would recommend at least summarizing the main findings of Appendix A in the main text. * A relevant missing citation: Turner and Sahani's ""Two problems with variational expectation maximisation for time-series models"" (http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf). They discuss in detail some examples where tighter variational bounds in state-space models lead to worse parameter estimates (though in a quite different context and with a quite different analysis). * Figure 1: What is the x-axis here? Presumably phi is not actually 1-dimensional? Typos etc.:  * ""learn a particular series intermediate"" missing ""of"". * ""To do so, we generate on sequence y1:T"" s/on/a/, I think? * Equation 3: Should there be a (1/K) in Z?",34,659,19.38235294117647,5.156507413509061,316,13,646,0.0201238390092879,0.0384615384615384,0.995,184,87,108,40,11,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 20, 'EXP': 3, 'RES': 3, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 2, 'EMP': 16, 'SUB': 1, 'CLA': 3}",0,1,3,1,1,20,3,3,2,2,0,4,1,0,0,0,0,3,0,2,16,1,3,0.7910969808152731,0.5653013128064533,0.49287734004219097
ICLR2018-BJ8c3f-0b-R2,Accept,"Overall: I had a really hard time reading this paper because I found the writing to be quite confusing. For this reason I cannot recommend publication as I am not sure how to evaluate the paper's contribution. Summary The authors study state space models in the unsupervised learning case. We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters. We want to infer a posterior density given some data. The authors propose an algorithm which uses sequential Monte Carlo + autoencoders. They use a REINFORCE-like algorithm to differentiate through the Monte Carlo. The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters. The authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments. The proposed algorithm seems novel, but I do not understand a few points which make it hard to judge the contribution. Note that here I am assuming full technical correctness of the paper (and still cannot recommend acceptance). Is the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC (that is, should this paper subsume the anonymized pre-print mentioned in the intro)?  This was very unclear to me. The introduction/experiments section of the paper is not well motivated. What is the problem the authors are trying to solve with AESMC (over existing methods)?  Is it scalability? Is it purely to improve likelihood of the fitted model (see my questions on the experiments in the next section)? The experiments feel lacking. There is only one experiment comparing the gains from AESMC, ALT to a simpler (?) method of IWAE. We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge? Does AESMC give a better generative model?). The authors discuss the advantages of SMC and say that is scales better than other methods, it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable.",24,380,22.352941176470587,4.991643454038997,189,1,379,0.0026385224274406,0.0338541666666666,0.9597,101,35,76,24,6,6,"{'ABS': 0, 'INT': 3, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 15, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 1, 'CLA': 1}",0,3,0,1,0,15,4,2,0,0,0,3,0,0,0,1,0,1,0,1,12,1,1,0.4325302829069813,0.6735081984499394,0.30166175525785077
ICLR2018-BJ8c3f-0b-R3,Accept,"[After author feedback] I think the approach is interesting and warrants publication. However, I think some of the counter-intuitive claims on the proposal learning are overly strong, and not supported well enough by the experiments. In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al. [Original review] The authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC. The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure. The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial. The approach is interesting and the paper is well-written, however, I have some comments and questions:  - It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case. This seems to mean that we should not expect for q -> p when K increases? - Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made. If I'm not mistaken K 1 is regular ELBO and not IWAE/AESMC? Have you estimated the probability for positive vs. negative gradient values for  K 10? To me it looks like the probability of it being larger than zero is something like 2/3. K>10 is difficult to see from this plot alone. - Is there a typo in the bound given by eq. (17)? Seems like there are two identical terms. Also I'm not sure about the first equality in this equatiion, is I^2   0 or is there a typo? - The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS. Have you tried this for high-dimensional models as well? Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO. For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1? Might this be a typo in 4.1? You still propose to learn the proposal parameters using SMC but with lower number of particles? I suspect this lower number of particles might be model-dependent. Minor comments: - Section 1, first paragraph, last sentence, that -> than? - Section 3.2, ... using which... formulation in two places in the firsth and second paragraph was a bit confusing - Page 7, second line, just IS? - Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates? - Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K 1 reduces to this?",29,509,24.23809523809524,5.044776119402985,226,10,499,0.0200400801603206,0.0419047619047619,0.6465,136,64,90,27,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 22, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 16, 'SUB': 0, 'CLA': 8}",0,0,2,0,0,22,2,1,1,0,0,1,0,0,0,0,0,1,0,1,16,0,8,0.4339921466549992,0.4543417514630674,0.24116067877201475
ICLR2018-BJ8lbVAfz-R1,,"The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized. The learning dynamics defined for the network results in specific update equations of the weights W (Eqn. 14), which combine elements of supervised learning and self-organizing maps (SOMs). The weights thus change according to the imposed neighborhood relationship and depending on the class labels. What I like about the approach is the investigation of the interplay between unsupervised and hierarchical supervised learning in a biological context. I agree with the authors that an integrated view of self-organization and learning across layers is presumably required to better understand biological learning. The general methodology also makes sense to me. However, I do have concerns including two major concerns: (A) delimitation of results from earlier work; (B) numerical results (especially Tab. 1). (A) The paper derives the main update equation of W which combines self-organization and label-sensitive learning - Eqn. 15. This equation is then discussed and the SOM-like updates and the differences to previous pure SOMs are highlighted. The paper also states (Secs. 1 and 2) that the the network studied here is based on Hartono et al, 2015, with the main difference of the sigmoidal ouput layer being replaced by a softmax layer. What is missing is a discussion of the differences regarding the later numerical experiments, and a clear delimitation to Hartono et al., 2015, when Eqn. 15 is discussed. What is the major structural difference to their Eqn. 13 which is discussed along very similar lines as Eqn. 15 of this paper. Also after reading the abstract of this paper, one may think that this is the first paper discussing the SOM / supervised learning combination. (B) A further difference to Hartono et al, 2015, are comparisons with multi-layer networks, and the presentation and discussion of this comparison is my strongest concern. In the first paragraph of Sec. 3 the competing deep networks are introduced. Then it is stated: the number of hidden neurons, as well as the structures for the deep neural networks were empirically tried, and the results of the best settings were registered for comparison (1st paragraph Sec. 3). What I do not understand are then the high classification errors reported in Tab. 1. It is known that even basic multi-layer perceptrons (MLPs) result in much lower classification errors, e.g., for MNIST. LeCun et al., 1998, is a classical example with less then 3% error on MNIST with many later examples that improve on these. Also the well-known original DBN paper has MNIST as main example (and main selling point) with close to 1% error. Why are the classification errors for DBN and MLP in the Tab 1 so high? And if they are in reality much lower, then competitiveness of s-rRBF in terms of classification results to these systems is questionable. The table makes me having doubts regarding the competitiveness of S-rRBF. I therefore disagree with the conclusion that this paper has shown that S-rRBFs are comparable to the best performer for most of the diverse benchmark applications (last paragraph in Conclusion). The feature of providing auxiliary visual information (also conclusion) is much more convincing (but also a feature of Hartono et al, 2015). More generally, putting the biological arguments aside, why would a 2D neighborhood relationship be helpful? I see a benefit in interpretation which can help. Also, if there is an intrinsic 2D hidden structure in the data, then imposing a 2D representation can help (as a sort of a prior). But in general there may not be a 2D intrinsic property, or there is a higher dimensional hidden structure - so why not 3D or more? Related to this, why not using an objective that would result in a dynamics similar to a growing neural gas instead of an SOM? Minor:  The work is first introduced as multi-layer but only the single hidden layer case is actually discussed. I would suggest to either really add multi-hidden-layer results (which is not really doable in a conference revision), or state multi-layer work as outlook. Fig. 5, bad readability of axes labels. is a hierarchical -> are hierarchical  yields -> yield  twice otherwise after Eqn. 7  are can be viewed  they occurs  can can readily expanded  transfer transform ",32,706,16.80952380952381,5.257668711656442,296,4,702,0.0056980056980056,0.0111265646731571,0.945,196,90,113,47,9,5,"{'ABS': 1, 'INT': 1, 'RWK': 7, 'PDI': 2, 'DAT': 1, 'MET': 13, 'EXP': 2, 'RES': 8, 'TNF': 5, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 11, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 3}",1,1,7,2,1,13,2,8,5,0,0,0,0,0,0,0,0,11,1,0,13,1,3,0.6469709763822198,0.5643529404872791,0.4138600240721935
ICLR2018-BJ8lbVAfz-R2,,"This paper proposes a supervised variant of Kohonen's self-organizing map (SOM), i.e., trained by gradient descent with gradient obtained by backprop, using a grid of RBF neurons which respond to the input only if they are in the grid-neighborhood of the 'winner' neuron (closest to the input). This in itself is not even new, but the authors replace a linear output layer with squared error (proposed in another, earlier paper) by a softmax layer with cross-entropy. Unsuprisingly, this leads to an improvement. The title is misleading. There is nothing deep in this architecture. It is a shallow architecture with a single RBF-like hidden layer. There is a tiny ounce of novelty in that the authors propose to improve a supervised version of the SOM by using what should have been used in the first place according to modern good practice. And the whole paper seems as if written circa 2010 or 2012. Another misleading thing is the term self-organizing used throughout, which is roughly synonym to learning according to me, and not something uniquely belonging to the SOM family of models, as used by the authors. As an example of time-travel to the past, the authors talk about RBMs and stacks of auto-encoders as if that was the deep learning state-of-the-art. The authors even call these methods 'recent'! Clearly not the case.  Unfortunately, it's not just talk, they are also the point of comparison in the experiments, i.e., there are no comparison with modern deep learning methods. Even the datasets are outdated (from the 90s?). Vocabulary is wrong in other places, for example the word semi-supervised is wrongly understood and used.  Semi-supervised means that one combines labeled and unlabeled data. Where the label 'semi-supervised' is used (page 4) is actually wrong: yes the labels are used, but of course it is the *gradients* which show up in the update, not the labels themselves directly. It's also not true that there is little research in understanding the formation of internal representations. There is a whole subfields of papers trying to interpret the features learned by deep networks, and much work designing learning frameworks and objectives to achieve better representations, e.g, to better disentangle the underlying factors. It's also common practice to analyze the representations learned, in many deep learning papers. The paper uses much space to show how to compute gradients in the proposed architecture: there is obviously no need for this in a day and age where gradients are automatically derived by software. The cherry on the sundae are the experimental results. How could the authors get 16% on MNIST with an MLP of any kind? It does not seem right at all. Even a linear regression would get at least half of that. As there are not enough experimental details to judge, it's hard to figure out the problem, but this ppaper is clearly not publishable at any of the quality machine learning venues, for weakness in originality, quality of the writing, and poor experiments. ",25,496,19.84,4.991666666666666,236,2,494,0.0040485829959514,0.0140280561122244,0.1798,125,53,84,36,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 9, 'EXP': 4, 'RES': 4, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 13, 'SUB': 2, 'CLA': 4}",0,2,1,1,2,9,4,4,0,3,0,2,0,0,0,4,0,2,0,1,13,2,4,0.6454408400591667,0.6747327229164104,0.4597294770407097
ICLR2018-BJ8lbVAfz-R3,,"This paper proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function. It is trained with supervised learning, by minimising the cross-entropy error between labels and the softmax output.  The paper's claim of combining unsupervised (self-organising) with supervised training is misleading and confusing. In this model, self-organising is a property of the hidden neurons' activation (eq. 1-3), and the training procedure is entirely supervised. It is misleading to claim any unsupervised or semi-supervised learning based on the *self-organising part* of, for example, eq. 14, which is merely a result of applying chain rule through the hidden neurons' activation. While this model is proposed as an extension of Kohonen's self-organising map (SOM), the paper fails to mention, or compare with, several historically important extension of SOM, which should perhaps at least include the generative topographic mapping (GTM, Bishop et al. 1998), an important probabilistic generalisation of SOM. Finally, the evaluation of the model in comparison with other models is questionable. For example, while the configuration the paper's baseline models are not given, the baseline accuracy of MNIST classification using MLP is 16.2%. This is much worse than the baseline of 12% in LeCun et al. (1998), using simple linear classifier without any preprocessing. The 7% accuracy from the proposed model is not in the range of modern deep learning models (The state-of-art accuracy is <0.3%). Similar problem also exist in results from other datasets. They are therefore unable to support the paper's claim on robust performance Pros: The question of internal representation is interesting. Combining self-organising with classification. Comparing learned representations from different models. Cons: Not clearly written. Mixing the concept of unsupervised/semi-supervised learning is confusing. Model evaluation is questionable. Does not compare existing extensions of SOM.",18,295,13.409090909090908,5.799295774647887,145,1,294,0.0034013605442176,0.027027027027027,-0.9295,98,43,50,13,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 3, 'MET': 8, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 7, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,4,1,3,8,0,3,0,0,0,0,0,0,0,0,0,7,0,0,6,1,1,0.4307478223402791,0.4482572069617991,0.2414718733433811
ICLR2018-BJ8vJebC--R1,Accept,"This paper investigates the impact of character-level noise on various flavours of neural machine translation. It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side. The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables. They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data. Unfortunately, they are not able to show any types of synthetic noise helping address natural noise. However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise. This is a thorough exploration of a mostly under-studied problem. The paper is well-written and easy to follow. The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty. The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work. Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others. In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT. The proposed meanChar architecture doesn't look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn't extremely satisfying. Furthermore, the use of these replacement tables means that even when the noise is natural, it's still kind of artificial. Finally, this paper doesn't seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference. Regarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial. First of all, errors learned from the noisy data sources are constrained to exist within a word. This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words. Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data. This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur). I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.  Also, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6). I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner. On a related note, the line in the abstract stating that ""... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise"" implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren't given the chance. Section 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an ICLR audience. From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies. I have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).  [edited for typos]",27,751,25.89655172413793,5.123796423658872,334,8,743,0.0107671601615074,0.0304232804232804,0.9945,191,97,131,50,10,6,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 7, 'MET': 7, 'EXP': 6, 'RES': 7, 'TNF': 0, 'ANA': 0, 'FWK': 2, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 15, 'SUB': 4, 'CLA': 2}",1,1,1,3,7,7,6,7,0,0,2,3,0,0,2,0,1,0,1,0,15,4,2,0.7169821700640083,0.6757849012612549,0.5029106870082383
ICLR2018-BJ8vJebC--R2,Accept,"This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. The results are not surprising:  * NMT is terrible with noise. * But it improves on each noise type when it is trained on that noise type. What I like about this paper is that:  1) The experiments are very carefully designed and thorough. 2) This problem might actually matter. Out of curiosity, I ran the example (Table 4) through Google Translate, and the result was gibberish. But as the paper shows, it's easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow. So this paper could have real-world impact. 3) Most importantly, it shows that NMT's handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different. So solving the problem of natural noise is not so simple... it's a *real* problem. Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale. So these methods could be applied in the real world. (It would be excellent if an outcome of this paper was that commercial MT providers answered it's call to provide more realistic noise by actually providing examples.) There are no fancy new methods or state-of-the-art numbers in this paper. But it's careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR.",15,256,17.066666666666666,4.927710843373494,132,1,255,0.0039215686274509,0.0153846153846153,0.9768,68,38,44,16,10,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 4, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,1,2,1,3,4,2,1,0,1,3,0,0,1,0,4,0,0,0,8,0,0,0.7152505252590761,0.3378311453225778,0.3632456750945375
ICLR2018-BJ8vJebC--R3,Accept,"This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust. Overall the paper is a clearly written, well described report of several experiments. It shows convincingly that standard NMT models completely break down on both natural  oise and various types of input perturbations. It then tests how the addition of noise in the input helps robustify the charCNN model somewhat. The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations. This study clearly addresses an important issue in NMT and will be of interest to many in the NLP community. The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps) but the impact may be. I wonder if you could put this in the context of training with input noise, which has been studied in Neural Network for a while (at least since the 1990s). I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise. Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet. A few constructive criticisms:  The way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed  oisy training set and adding that to clean data? Or introducing noise on-line as part of the training?  If fixed, what sizes were tried? More information on the experimental design would help. Table 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the Rand line for German, or the Swap/Mid/Rand lines for Czech. It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance. Although the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way. [Response read -- thanks] I agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks.",19,395,20.789473684210527,4.860158311345646,203,4,391,0.0102301790281329,0.0423940149625935,0.9915,102,47,73,24,11,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 8, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,1,1,2,1,3,8,1,1,1,1,2,0,0,1,0,1,0,1,0,11,1,1,0.7869227591537221,0.6728862410150964,0.5438221828841161
ICLR2018-BJB7fkWR--R1,Reject,"In this paper, the authors propose a new approach for learning underlying structure of visually distinct games. The proposed approach combines convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of games. The network architecture is suitably described and seems reasonable to learn simultaneously similar games, which are visually distinct. However, the authors do not explain how this architecture can be used to do the domain adaptation. Indeed, if some games have been learnt by the proposed algorithm, the authors do not precise what modules have to be retrained to learn a new game. This is a critical issue, because the experiments show that there is no gain in terms of performance to learn a shared embedding manifold (see DA-DRL versus baseline in figure 5). If there is a gain to learn a shared embedding manifold, which is plausible, this gain should be evaluated between a baseline, that learns separately the games, and an algorithm, that learns incrementally the games. Moreover, in the experimental setting, the games are not similar but simply the same. My opinion is that this paper is not ready for publication. The interesting issues are referred to future works. ",10,214,19.454545454545453,5.412621359223301,114,1,213,0.0046948356807511,0.0232558139534883,0.5643,56,21,49,13,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 2, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,1,0,1,0,6,3,1,0,2,2,0,0,0,1,0,1,0,0,1,4,2,0,0.5014522025345693,0.5575281043684059,0.31475060159685747
ICLR2018-BJB7fkWR--R2,Reject,"This paper introduces a method to learn a policy on visually different but otherwise identical games. While the idea would be interesting in general, unfortunately the experiment section is very much toy example so that it is hard to know the applicability of the proposed approach to any more reasonable scenario. Any sort of remotely convincing experiment is left to 'future work'. The experimental setup is 4x4 grid world with different basic shape or grey level rendering. I am quite convinced that any somewhat correctly setup vanilla deep RL algorithm would solve these sort of tasks/ ensemble of tasks almost instantly out of the box. Figure 5: Looks to me like the baseline is actually doing much better than the proposed methods? Figure 6: Looking at those 2D PCAs, I am not sure any of those method really abstracts the rendering away. Anyway, it would be good to have a quantified metric on this, which is not just eyeballing PCA scatter plots.",9,162,23.142857142857142,5.0,107,0,162,0.0,0.0308641975308641,0.9581,35,24,28,17,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 0, 'TNF': 2, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,1,2,0,6,3,0,2,1,1,0,0,1,0,0,1,1,0,0,5,1,0,0.644363092832046,0.4469322741838163,0.35936207749211285
ICLR2018-BJB7fkWR--R3,Reject,"- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics. The key idea is that the agent learns a shared representations for tasks with different visual statistics - A lot of important references  touching on very similar ideas are missing. For e.g. Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks, Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping, Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. - This paper has a lot of orthogonal details. For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context. - Only single runs for the results are shown in plots. How statistically valid are the results? - In the last section authors mention the intent to do future work on atari and other env. Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication",10,179,19.88888888888889,5.369942196531792,118,1,178,0.0056179775280898,0.0108108108108108,0.8316,56,34,26,5,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 5, 'CLA': 0}",0,1,0,2,0,2,1,3,0,0,1,1,2,0,1,0,1,0,0,0,2,5,0,0.5718618601944679,0.4454931079125733,0.3249202462815575
ICLR2018-BJDEbngCZ-R1,Reject,"The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control problems, both in deterministic and randomized case, whiling coping with non-convexity of the objective. I found that the paper suffers many shortcomings that must be addressed:  1) The writing and organization is quite cumbersome and should be improved. 2) The authors state in the abstract (and elsewhere): ... showing that (model free) policy gradient methods globally converge to the optimal solution .... This is misleading and NOT true. The authors show the convergence of the objective but not of the iterates sequence. This should be rephrased elsewhere. 3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed.",6,116,14.5,5.901785714285714,74,0,116,0.0,0.042016806722689,0.3365,32,14,19,7,5,3,"{'ABS': 1, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 3}",1,1,0,1,0,3,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,3,0.3576432413704265,0.3334956034750414,0.17609343833642738
ICLR2018-BJDEbngCZ-R2,Reject,"I find this paper not suitable for ICLR. All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION.",2,31,15.5,5.67741935483871,28,0,31,0.0,0.032258064516129,0.3818,9,5,4,4,2,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 0, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0.1428571428571428,0.2222222222222222,0.06354887453930418
ICLR2018-BJDEbngCZ-R3,Reject,"The paper studies the global convergence for policy gradient methods for linear control problems. (1) The topic of this paper seems to have minimal connection with ICRL. It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully. (2) I am not convinced if the main results are novel. The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming. The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration.  I am not sure if it is a good idea to examine the convergence purely from an optimization perspective. (3) The main results of this paper seem technical sound. However, the results seem a bit limited because it does not apply to neural-network function approximator.  It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted. I might have missed something here. I strongly suggest that these results be submitted to a more suitable venue.  ",12,192,14.76923076923077,5.216666666666667,101,3,189,0.0158730158730158,0.0663265306122449,-0.6011,53,20,34,15,6,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 3, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,2,0,4,0,2,0,4,0,0,0,1,0,1,3,1,0,0,0,0,4,1,0,0.4291921448782242,0.4463296485709097,0.2308822468767483
ICLR2018-BJDH5M-AW-R1,Reject,"Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network. Rather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images. Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%. They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction. Main comments: - The idea of building 3D adversarial objects is novel so the study is interesting. However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date. See for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area. - The presentation of the results is not very clear. See specific comments below. - It would be nice to include insights to improve neural nets to become less sensitive to these attacks. Minor comments: Fig1 : a bug with color seems to have been fixed Model section: be consistent with the notations. Bold everywhere or nowhere Results: The tables are difficult to read and should be clarified: What does the l2 metric stands for ?  How about min, max ? Accuracy -> classification accuracy Models -> 3D models Describe each metric (Adversarial, Miss-classified, Correct) ",16,268,20.615384615384617,5.326612903225806,166,1,267,0.0037453183520599,0.0215827338129496,-0.6783,86,32,42,11,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 3, 'DAT': 1, 'MET': 3, 'EXP': 0, 'RES': 4, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,2,2,3,1,3,0,4,2,0,0,0,2,0,0,0,0,0,3,0,5,2,0,0.572356358855224,0.3360509774286334,0.28836586716596324
ICLR2018-BJDH5M-AW-R2,Reject,"The authors present a method to enable robust generation of adversarial visual inputs for image classification. They develop on the theme that 'real-world' transformations typically provide a countermeasure against adversarial attacks in the visual domain, to show that contextualising the adversarial exemplar generation by those very transformations can still enable effective adversarial example generation. They adapt an existing method for deriving adversarial examples to act under a projection space (effectively a latent-variable model) which is defined through a transformations distribution. They demonstrate the effectiveness of their approach in the 2D and 3D (simulated and real) domains. The paper is clear to follow and the objective employed appears to be sound. I like the idea of using 3D generation, and particularly, 3D printing, as a means of generating adversarial examples -- there is definite novelty in that particular exploration for adversarial examples. I did however have some concerns:  1. What precisely is the distribution of transformations used for each experiment? Is it a PCFG? Are the different components quantised such that    they are discrete rvs, or are there still continuous rvs? (For example, is    lighting discretised to particular locations or taken to be (say) a 3D    Gaussian?) And on a related note, how were the number of sampled transformations chosen? Knowing the distribution (and the extent of it's support) can help situate    the effectiveness of the number of samples taken to derive the adversarial    input. 2. While choosing the distance metric in transformed space, LAB is used, but    for the experimental results, l_2 is measured in RGB space -- showing the    RGB distance is perhaps not all that useful given it's not actually being    used in the objective. I would perhaps suggest showing LAB, maybe in    addition to RGB if required. 3. Quantitative analysis: I would suggest reporting confidence intervals;    perhaps just the 1st standard deviation over the accuracies for the true and    'adversarial' labels -- the min and max don't help too much in understanding n   what effect the monte-carlo approximation of the objective has on things. Moreover, the min and max are only reported for the 2D and rendered 3D    experiments -- it's missing for the 3D printing experiment. 4. Experiment power: While the experimental setup seems well thought out and    structured, the sample size (i.e, the number of entities considered) seems a    bit too small to draw any real conclusions from. There are 5 exemplar    objects for the 3D rendering experiment and only 2 for the 3D printing one. While I understand that 3D printing is perhaps not all that scalable to be    able to rattle off many models, the 3D rendering experiment surely can be    extended to include more models? Were the turtle and baseball models chosen    randomly, or chosen for some particular reason? Similar questions for the 5    models in the 3D rendering experiment. 5. 3D printing experiment transformations: While the 2D and 3D rendering    experiments explicitly state that the sampled transformations were random,    the 3D printing one says over a variety of viewpoints. Were these    viewpoints chosen randomly? Most of these concerns are potentially quirks in the exposition rather than any issues with the experiments conducted themselves. For now, I think the submission is good for a weak accept  u2013- if the authors address my concerns, and/or correct my potential misunderstanding of the issues, I'd be happy to upgrade my review to an accept.",28,555,25.227272727272727,5.517647058823529,255,9,546,0.0164835164835164,0.0254372019077901,0.9378,129,64,107,34,8,6,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 10, 'EXP': 15, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 2, 'EMP': 19, 'SUB': 2, 'CLA': 1}",0,2,1,4,0,10,15,1,0,0,0,2,0,1,0,1,0,1,0,2,19,2,1,0.5751358788594881,0.6779883774743268,0.4063266933701037
ICLR2018-BJDH5M-AW-R3,Reject,"The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations. The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results. The experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive. This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting. However, the authors claim that standard techniques require complete control and careful setups (e.g. in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al. This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the HD texture of the turtle). One interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al). If that's the case then complicated transformation sampling and 3D mapping setup would be unnecessary. This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world. Minor comments: Section 1.1: a affine -> an affine Typo in section 3.4: of a of a  It's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse. Moosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017.",12,282,16.58823529411765,5.583969465648855,149,3,279,0.010752688172043,0.0350877192982456,0.8127,88,38,37,20,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,2,2,0,6,2,4,1,0,0,0,1,0,0,0,0,2,1,0,7,0,1,0.5730726212315715,0.4482933516106923,0.3186897565720916
ICLR2018-BJE-4xW0W-R1,Accept,"The paper describes a way of combining a causal graph describing the dependency structure of labels with two conditional GAN architectures (causalGAN and causalBEGAN) that generate  images conditioning on the binary labels. Ideally, this type of approach should allow not only to generate images from an observational distribution of labels (e.g. P(Moustache 1)), but also from unseen interventional distributions (e.g. P(Male 0 | do(Moustache  1)). Maybe I misunderstood something, but one big problem I have with the paper is that for a ""causalGAN"" approach it doesn't seem to do much causality. The (known) causal graph is only used to model the dependencies of the labels, which the authors call the ""Causal Controller"". On this graph, one can perform interventions and get a different distribution of labels from the original causal graph (e.g. a distribution of labels in which women have the same probability as men of having moustaches). Given the labels, the rest of the architecture are extensions of conditional GANs, a causalGAN with a Labeller and an Anti-Labeller (of which I'm not completely sure I understand the necessity) and an extension of a BEGAN. The results are not particularly impressive, but that is not an issue for me. Moreover sometimes the descriptions are a bit imprecise and unstructured. For example, Theorem 1 is more like a list of desiderata and it already contains a forward reference to page 7. The definition of intervention in the Background applies only to do-interventions (Pearl 2009) and not to general interventions (e.g. consider soft, uncertain or fat-hand interventions). Overall, I think the paper proposes some interesting ideas, but it doesn't explore them yet in detail. I would be interested to know what happens if the causal graph is not known, and even worse cannot be completely identified from data (so there is an equivalence class of possible graphs), or potentially is influenced by latent factors. Moreover, I would be very curious about ways to better integrate causality and generative models, that don't focus only on the label space. Minor details: Personally I'm not a big fan of abusing colons ("":"") instead of points ("".""). See for example the first paragraph of the Related Work. EDIT: I read the author's rebuttal, but it has not completely addressed my concerns, so my rating has not changed.",17,378,18.9,5.26628895184136,201,3,375,0.008,0.0365535248041775,-0.5204,106,40,65,33,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 1, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,1,1,2,0,10,0,2,1,3,0,2,0,1,0,0,0,0,3,1,6,1,0,0.6453187312050532,0.4476773694662659,0.35812002526309233
ICLR2018-BJE-4xW0W-R2,Accept,"In their paper CausalGAN: Learning Causal implicit Generative Models with adv. training the authors address the following issue: Given a causal structure between labels of an image (e.g. gender, mustache, smiling, etc.), one tries to learn a causal model between these variables and the image itself from observational data. Here, the image is considered to be an effect of all the labels. Such a causal model allows us to not only sample from conditional observational distributions, but also from intervention distributions. These tasks are clearly different, as nicely shown by the authors' example of do(mustache   1) versus given mustache   1 (a sample from the latter distribution contains only men). The paper does not aim at learning causal structure from data (as clearly stated by the authors). The example images look convincing to me. I like the idea of this paper. IMO, it is a very nice, clean, and useful approach of combining causality and the expressive power of neural networks. The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area. For me, it is not easy to judge the novelty of the approach, but the authors list related works, none of which seems to solve the same task. The presentation of the paper, however, should be improved significantly before publication. (In fact, because of the presentation of the paper, I was hesitating whether I should suggest acceptance.) Below, I give some examples (and suggest improvements), but there are many others. There is a risk that in its current state the paper will not generate much impact, and that would be a pity. I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper. - I believe that I understand the authors' intention of the caption of Fig. 1, but samples outside the dataset is a misleading formulation. Any reasonable model does more than just reproducing the data points. I find the argumentation the authors give in Figure 6 much sharper. Even better: add the expression P(male   1 | mustache   1)   1. Then, the difference is crystal clear. - The difference between Figures 1, 4, and 6 could be clarified.  - The list of prior work on learning causal graphs seems a bit random. I would add Spirtes et al 2000, Heckermann et al 1999, Peters et al 2016, and Chickering et al 2002. - Male -> Bald does not make much sense causally (it should be Gender -> Baldness)... Aha, now I understand:  The authors seem to switch between Gender and Male being random variables.Make this consistent, please.  - There are many typos and comma mistakes. - I would introduce the do-notation much earlier. The paragraph on p. 2 is now written without do-notation (intervening Mustache   1 would not change the distribution). But this way, the statements are at least very confusing (which one is the distribution?). - I would get rid of the concept of CiGM. To me, it seems that this is a causal model with a neural network (NN) modeling the functions that appear in the SCM. This means, it's just using NNs as a model class. Instead, one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN? (This would also clarify the paper's contribution.) - many realizations   one sample (not samples), I think. - Fig 1: which model is used to generate the conditional sample?  - The notation changes between E and N and Z for the noises. I believe that N is supposed to be the noise in the SCM, but then maybe it should not be called E at the beginning. - I believe Prop 1 (as it is stated) is wrong. For a reference, see Peters, Janzing, Scholkopf: Elements of Causal Inference: Foundations and Learning Algorithms (available as pdf), Definition 6.32. One requires the strict positivity of the densities (to properly define conditionals). Also, I believe the Z should be a vector, not a set.  - Below eq. (1), I am not sure what the V in P_V refers to. - The concept of data probability density function seems weird to me. Either it is referring to the fitted model, then it's a bad name, or it's an empirical distribution, then there is no pdf, but a pmf. - Many subscripts are used without explanation. r -> real? g -> generating? G -> generating? Sometimes, no subscripts are used (e.g., Fig 4 or figures in Sec. 8.13) - I would get rid of Theorem 1 and explain it in words for the following reasons. (1) What is an informal theorem? (2) It refers to equations appearing much later. (3) It is stated again later as Theorem 2. - Also: the name P_g does not appear anywhere else in the theorem, I think. - Furthermore, I would reformulate the theorem. The main point is that the intervention distributions are correct (this fact seems to be there, but is hidden in the CIGN notation in the corollary). - Re. the formulation in Thm 2: is it clear that there is a unique global optimum (my intuition would say there could be several), thus: better write _a_ global minimum? - Fig. 3 was not very clear to me. I suggest to put more information into its caption. - In particular, why is the dataset not used for the causal controller? I thought, that it should model the joint (empirical) distribution over the labels, and this is part of the dataset. Am I missing sth? - IMO, the structure of the paper can be improved. Currently, Section 3 is called Background which does not say much. Section 4 contains CIGMs, Section 5 Causal GANs, 5.1. Causal Controller, 5.2. CausalGAN, 5.2.1. Architecture (which the causal controller is part of) etc. An alternative could be:  Sec 1: Introduction  Sec 1.1: Related Work Sec 2: Causal Models Sec 2.1: Causal Models using Generative Models (old: CIGM) Sec 3: Causal GANs Sec 3.1: Architecture (including controller) Sec 3.2: loss functions  ... Sec 4: Empricial Results (old: Sec. 6: Results) - Causal Graph 1 is not a proper reference (it's Fig 23 I guess). Also, it is quite important for the paper, I think it should be in the main part. - There are different references to the Appendix, Suppl. Material, or Sec. 8 -- please be consistent (and try to avoid ambiguity by being more specific -- the appendix contains ~20 pages). Have I missed the reference to the proof of Thm 2? - 8.1. contains copy-paste from the main text. - proposition from Goodfellow -> please be more precise - What is Fig 8 used for? Is it not sufficient to have and discuss Fig 23? - IMO, Section 5.3. should be rewritten (also, maybe include another reference for BEGAN). - There is a reference to Lemma 15. However, I have not found that lemma. - I think it's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model, see Fig 16. This is the case since for the generated realizations, the noise values are known. The authors may think about including a comment on that issue. - Since this paper's main proposal is a methodological one, I would make the publication conditional on the fact that code is released.    ",75,1193,14.373493975903614,4.945065176908752,419,16,1177,0.0135938827527612,0.0365369340746624,0.9931,334,116,228,73,7,9,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 6, 'DAT': 4, 'MET': 27, 'EXP': 0, 'RES': 0, 'TNF': 14, 'ANA': 0, 'FWK': 2, 'OAL': 13, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 2, 'CMP': 3, 'PNF': 17, 'REC': 1, 'EMP': 31, 'SUB': 1, 'CLA': 2}",0,0,4,6,4,27,0,0,14,0,2,13,0,0,1,1,2,3,17,1,31,1,2,0.5080032603819178,1.0,0.4996751834492562
ICLR2018-BJE-4xW0W-R3,Accept,"This should be the first work which introduces in the causal structure into the GAN, to solve the label dependency problem. The idea is interesting and insightful. The proposed method is theoretically analyzed and experimentally tested. Two minor concerns are 1) what is the relationship between the anti-labeler and and discriminator? 2) how the tune related weight of the different objective functions.  ",5,62,12.4,5.533333333333333,46,0,62,0.0,0.03125,0.2023,16,7,10,2,3,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,3,3,0,0,0,0,0,0,0,0,1,0,0,0,0,4,0,0,0.2150177899585567,0.2240880945267511,0.09592425438665853
ICLR2018-BJGWO9k0Z-R1,Accept,"The authors are motivated by two problems: Inputting non-Euclidean data (such as graphs) into deep CNNs, and analyzing optimization properties of deep networks. In particular, they look at the problem of maze testing, where, given a grid of black and white pixels, the goal is to answer whether there is a path from a designated starting point to an ending point. They choose to analyze mazes because they have many nice statistical properties from percolation theory. For one, the problem is solvable with breadth first search in O(L^2) time, for an L x L maze. They show that a CNN can essentially encode a BFS, so theoretically a CNN should be able to solve the problem. Their architecture is a deep feedforward network where each layer takes as input two images: one corresponding to the original maze (a skip connection), and the output of the previous layer. Layers alternate between convolutional and sigmoidal. The authors discuss how this architecture can solve the problem exactly. The pictorial explanation for how the CNN can mimic BFS is interesting but I got a little lost in the 3 cases on page 4. For example, what is r? And what is the relation of the black/white and orange squares? I thought this could use a little more clarity.  Though experiments, they show that there are two kinds of minima, depending on whether we allow negative initializations in the convolution kernels. When positive initializations are enforced, the network can more or less mimic the BFS behavior, but never when initializations can be negative. They offer a rigorous analysis into the behavior of optimization in each of these cases, concluding that there is an essential singularity in the cost function around the exact solution, yet learning succumbs to poor optima due to poor initial predictions in training. I thought this was an impressive paper that looked at theoretical properties of CNNs. The problem was very well-motivated, and the analysis was sharp and offered interesting insights into the problem of maze solving. What I thought was especially interesting is how their analysis can be extended to other graph problems; while their analysis was specific to the problem of maze solving, they offer an approach -- e.g. that of finding bugs when dealing with graph objects -- that can extend to other problems. I would be excited to see similar analysis of other toy problems involving graphs. One complaint I had was inconsistent clarity: while a lot was well-motivated and straightforward to understand, I got lost in some of the details (as an example, the figure on page 4 did not initially make much sense to me). Also, in the experiments, the authors mention multiple attempt with the same settings -- are these experiments differentiated only by their initialization? Finally, there were various typos throughout (one example is  eglect minimua on page 2 should be  eglect minima). Pros: Rigorous analysis, well motivated problem, generalizable results to deep learning theory Cons: Clarity ",25,490,23.33333333333333,5.193965517241379,234,1,489,0.0020449897750511,0.0160965794768611,0.1466,138,57,86,17,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 11, 'EXP': 4, 'RES': 1, 'TNF': 3, 'ANA': 6, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 4}",0,0,0,4,0,11,4,1,3,6,0,3,0,0,0,0,0,0,1,0,8,2,4,0.5031994385391357,0.4491482282092289,0.2854418236664577
ICLR2018-BJGWO9k0Z-R2,Accept,"This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution. They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing bugs in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths. This analysis is conducted by drawing on results from the field of critical percolation in physics. Overall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays. The analysis is very thorough and the methods described may find use in analyzing other tasks. In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms. It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one. This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions. I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds. With that being said, there is some work that needs to be done to make the paper clearer. In particular, many parts are quite technical and may not be accessible to a broader machine learning audience. It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix. Specifically: - I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution. - Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix. - Section 6: there is lots of math here, but the main results don't obviously stand out. I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are. Interested readers can then work through the math if they want to. Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful. Also, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading. They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs. This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs. Here the assumptions of locality and stationarity underlying CNNs are sensible and I don't think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary. However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks). There are lots of small typos, please fix them. Here are a few: - For L 16, batch size of 20, ...: not a complete sentence.  - Right before 6.1.1: when the these such -> when such - Top of page 8: it also have a -> it also has a, when encountering larger dataset  -> ...datasets -  First sentence of 6.2: we turn to the discuss a second ->  we turn to the discussion of a second - etc. Quality: High Clarity: medium-low Originality: high Significance: medium-high  References: [1] https://arxiv.org/pdf/1602.02867.pdf [2] https://arxiv.org/pdf/1612.08810.pdf [3] https://arxiv.org/pdf/1707.03497.pdf",31,709,26.25925925925926,5.205357142857143,313,9,700,0.0128571428571428,0.0219478737997256,0.9931,200,88,122,35,12,8,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 15, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 5, 'FWK': 1, 'OAL': 9, 'BIB': 1, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 7}",0,1,1,1,1,15,0,2,1,5,1,9,1,1,1,1,1,1,4,0,7,1,7,0.8611634766154215,0.8932921506944811,0.7572009179353425
ICLR2018-BJGWO9k0Z-R3,Accept,"This paper studies a toy problem: a random binary image is generated, and treated as a maze (1 wall, 0 freely moveable space). A random starting point is generated. The task is to learn whether the center pixel is reachable from the starting point. A deep architechture is proposed to solve the problem: see fig 1. A conv net on the image is combined with that on a state image, the state being interpreted as rechable pixels. This can work if each layer expands the reachable region (the state) by one pixel if the pixel is not blocked. Two local minima are observed: 1) the network ignores stucture and guesses if the task is solvable by aggregate statistics 2) it works as described above but propagates the rechable region on a checkerboard only. The paper is chiefly concerned with analysing these local minima by expanding the cost function about them. This analysis is hard to follow for non experts graph theory. This is partly because many non-trivial results are mentioned with little or no explanation. The paper is hard to evaluate. The actual setup seems somewhat arbitrary, but the method of analysing the failure modes is interesting. It may inspire more useful research in the future. If we trust the authors, then the paper seems good because it is fairly unusual. But it is hard to determine whether the analysis is correct.",17,232,15.466666666666669,4.876712328767123,131,4,228,0.0175438596491228,0.0344827586206896,0.9226,54,28,53,7,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 4, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,1,0,3,0,7,0,1,1,4,1,3,0,0,0,1,1,0,0,0,8,1,0,0.5732133195929409,0.4487981464883452,0.31912195085248674
ICLR2018-BJIgi_eCZ-R1,Accept,"The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not fully-aware of all levels of abstraction, e.g. word-level, phrase-level, etc. In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD. They also propose an attention mechanism that works better than others (Symmetric + ReLU). Strengths: - The paper is well-written and clear. - I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field. - The multi-level attention is novel and indeed seems to work, with convincing ablations. - Nice engineering achievement, reaching the top of the leaderboard (in early October). Weaknesses: - The paper is long (10 pages) but relatively lacks substances. Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA). - The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard. Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution. Minor: - Probably figure 4 can be drawn better. Not easy to understand nor concrete. - Section 3.2 GRU citation should be Cho et al. [2]. Questions: - Contextualized embedding seems to give a lot of improvement in other works too. Could you perform ablation without contextualized embedding (CoVe)? Reference: [1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015. [2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014.",18,288,11.52,5.446494464944649,174,7,281,0.0249110320284697,0.04,0.9904,89,37,47,25,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 0, 'DAT': 2, 'MET': 7, 'EXP': 0, 'RES': 1, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 1}",0,0,4,0,2,7,0,1,3,0,0,3,2,0,0,2,0,0,4,0,7,2,1,0.5019285336721051,0.559623791028863,0.31046821760765697
ICLR2018-BJIgi_eCZ-R2,Accept,"The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive. That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level . The network proposed here, FusionHet, fixes problem. Importantly, the model achieves state-of-the-art performance of the SQuAD dataset. The paper is very well-written and easy to follow. I found the architecture very intuitively laid out, even though this is not my area of expertise. Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work! What most impressed me, however, was the literature review. Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work. Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2. All in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset.",11,166,16.6,5.2625,108,1,165,0.006060606060606,0.0178571428571428,0.9452,39,21,30,20,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 0, 'DAT': 3, 'MET': 3, 'EXP': 0, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,2,4,0,3,3,0,2,2,0,0,1,0,0,0,0,0,1,2,1,4,0,1,0.5009185481972398,0.5574829967838878,0.31610339623587463
ICLR2018-BJIgi_eCZ-R3,Accept,"(Score before author revision: 4) (Score after author revision: 7)  I think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account, running several ablations as well as reporting nice results on an entirely new dataset (MultiNLI) where they show how their multi level fusion mechanism improves a baseline significantly. I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference). Therefore I would now support accepting this paper. ------------(Original review below) -----------------------  The authors present an enhancement to the attention mechanism called multi-level fusion that they then incorporate into a reading comprehension system. It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores. i.e. the authors form a vector HoW (called history of the word), that is defined as a concatenation of several vectors:  HoW_i   [g_i, c_i, h_i^l, h_i^h]  where g_i   glove embeddings, c_i   COVE embeddings (McCann et al. 2017), and h_i^l and h_i^h are different LSTM states for that word. The attention score is then a function of these concatenated vectors i.e. alpha_{ij}   exp(S(HoW_i^C, HoW_j^Q)) Results on SQuAD show a small gain in accuracy (75.7->76.0 Exact Match). The gains on the adversarial set are larger but that is because some of the higher performing, more recent baselines don't seem to have adversarial numbers. The authors also compare various attention functions (Table 5) showing a particularone (Symmetric + ReLU) works the best. Comments:  -I feel overall the contribution is not very novel. The general neural architecture that the authors propose in Section 3 is generally quite similar to the large number of neural architectures developed for this dataset (e.g. some combination of attention between question/context and LSTMs over question/context). The only novelty is these HoW inputs to the extra attention mechanism that takes a richer word representation into account. -I feel the model is seems overly complicated for the small gain (i.e. 75.7->76.0 Exact Match), especially on a relatively exhausted dataset (SQuAD) that is known to have lots of pecularities (see anonymous comment below). It is possible the gains just come from having more parameters. -The authors (on page 6) claim that that by running attention multiple times with different parameters but different inputs (i.e. alpha_{ij}^l, alpha_{ij}^h, alpha_{ij}^u) it will learn to attend to different regions for different level. However, there is nothing enforcing this and the gains just probably come from having more parameters/complexity.",17,415,18.863636363636363,5.408163265306122,219,4,411,0.0097323600973236,0.0186046511627906,0.9824,144,59,63,28,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 4, 'MET': 10, 'EXP': 1, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 2, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,2,0,4,10,1,5,0,0,0,3,0,0,0,4,0,1,0,2,6,2,0,0.4313255629044939,0.5589271424638115,0.27237659867220154
ICLR2018-BJInEZsTb-R1,Reject,"This paper introduces a generative approach for 3D point clouds. More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent-space GAN (r-GAN and l-GAN as referred to in the paper). In addition, a GMM sampling + GAN decoder approach to generation is also among the experimented variations.  The results look convincing for the generation experiments in the paper, both from class-specific (Figure 1) and multi-class generators (Figure 6). The quantitative results also support the visuals. One question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches. Especially Octree based approaches [1-below] show very convincing and high-resolution shape generation results, whereas the details seem to be washed out for the point cloud results presented in this paper. I would like to see comparison experiments with voxel based approaches in the next update for the paper. [1] @article{tatarchenko2017octree,   title {Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs},   author {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},   journal {arXiv preprint arXiv:1703.09438},   year {2017} }  In light of the authors' octree updates score is updated. I expect these updates to be reflected in the final version of the paper itself as well. ",11,202,18.363636363636363,5.78125,114,0,202,0.0,0.0232558139534883,0.9402,73,29,29,11,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 3, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 3, 'CLA': 0}",0,2,0,0,0,4,2,4,1,1,3,0,0,0,0,0,4,2,0,0,3,3,0,0.5010376587437313,0.4461629848333069,0.2813469024699297
ICLR2018-BJInEZsTb-R2,Reject,"3D data processing is very important topic nowadays, since it has a lot of applications: robotics, AR/VR, etc. Current approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval. The lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds. Considered paper is one of the first approaches to learn GAN-type generative models. Using PointNet architecture and latent-space GAN, the authors obtained rather accurate generative model. The paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures. Thus I think that the paper should be published.",7,125,17.857142857142858,5.694915254237288,82,1,124,0.0080645161290322,0.024,0.5563,45,15,20,6,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,0,1,0,2,1,2,0,0,0,2,0,3,0,1,0,0,0,1,2,0,1,0.4289413975222874,0.4450664018792874,0.24041983533353087
ICLR2018-BJInEZsTb-R3,Reject,"Summary:  This paper proposes generative models for point clouds. First, they train an auto-encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.). Then, they train generative models over the auto-encoder's latent space, both using a latent-space GAN (l-GAN) that outputs latent codes, and a Gaussian Mixture Model. To generate point clouds, they sample a latent code and pass it to the decoder. They also introduce a raw point cloud GAN (r-GAN) that, instead of generating a latent code, directly produces a point cloud. n They evaluate the methods on several metrics. First, they show that the autoencoder's latent space is a good representation for classification problems, using the ModelNet dataset. Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over Chamfer distance. Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled. Pros:  - It is interesting that the latent space models are most successful, including the relatively simple GMM-based model. Is there a reason that these models have not been as successful in other domains? n - The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs. Due to the simplicity of the method, this paper could be a useful baseline for future work. - The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper. Cons:  - How does a model that simply memorizes (and randomly samples) the training set compare to the auto-encoder-based models on the proposed metrics? How does the diversity of these two models differ? - The paper simultaneously proposes methods for generating point clouds, and for evaluating them. The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs. Although the performance on automated metrics is encouraging, it is hard to conclude much about under what circumstances one representation or model is better than another. - The technical approach is not particularly novel. The auto-encoder performs fairly well, but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance. The most successful generative models are based on sampling values in the auto-encoder's latent space using simple models (a two-layer MLP or a GMM). - While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem. n - The paper could possibly be clearer by integrating more of the background section into later sections. Some of the GAN figures could also benefit from having captions. Overall, I think that this paper could serve as a useful baseline for generating point clouds, but I am not sure that the contribution is significant enough for acceptance. ",28,523,21.791666666666668,5.238,228,5,518,0.0096525096525096,0.0205223880597014,0.9917,142,74,84,28,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 20, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 4, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 7, 'CMP': 6, 'PNF': 2, 'REC': 1, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,3,1,1,20,0,4,1,1,4,2,0,0,0,1,7,6,2,1,7,0,0,0.719425257743407,0.671334012876165,0.5073820473657505
ICLR2018-BJInMmWC--R1,Reject,"This paper presented a Generative entity networks (GEN). It is a multi-view extension of variational autoencoder (VAE) for disentangled representation. It uses the image and its attributes. The paper is very well motivated and tackles an important problem. However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished. Pros: 1. This paper tackles an important research question.  Learning a meaningful representation is needed in general. For the application of images, using text description to refine the representation is a natural and important research question. 2. The proposed idea is very well motivated, and the proposed model seems correct. Cons and questions: 1. The presentation of the model is not clear. Figure 2 which is the graphic representation of the model is hard to read. There is no meaningful caption for this important figure. Which notation in the figure corresponds to which variable is not clear at all. This also leads to unclarity of the text presentation of the model, for example, section 3.2. Which latent variable is used to decode which part? 2. Missing important related works. There are a couple of highly related work with multi-view VAE tracking similar problem have been proposed in the past. The paper did not discuss these related work and did not compare the performances. Examples of these related work include [1] and [2] (at the end of the review). Additionally, the idea of factorized representation idea (describable component and indescribable component) has a long history. It can be traced back to [3], used in PGM setting in [4] and used in VAE setting in [1]. This group of related work should also be discussed. 3. Experiment evaluation is not sufficient.  Firstly, only one toy dataset is used for experimental evaluations. More evaluations are needed to verify the method, especially with natural images. Secondly, there are no other state-of-the-art baselines are used. The baselines are various simiplied versions of the proposed model. More state-of-the-art baselines are needed, e.g. [1] and [2]. 4. Maybe overclaiming. In the paper, only attributes of objects are used which is not semi-natural languages. 5. The paper, in general, needs to be polished. There are missing links and references in the paper and un-explained notations, and non-informative captions. 6. Possibility to apply to natural images. This method does not model spatial information. How can the method make sure that  simple adding generated images with each component will lead to a meaningful image in the end? Especially with natural images,  the spacial location and the scale should be critical.  [1] Wang, Weiran, Honglak Lee, and Karen Livescu. Deep variational canonical correlation analysis. arXiv preprint arXiv:1610.03454 (2016). [2] Suzuki, Masahiro, Kotaro Nakayama, and Yutaka Matsuo. Joint Multimodal Learning with Deep Generative Models. arXiv preprint arXiv:1611.01891 (2016). [3] Tucker, Ledyard R. An inter-battery method of factor analysis.  Psychometrika 23.2 (1958): 111-136. [4] Zhang, Cheng, Hedvig Kjellstru00f6m, and Carl Henrik Ek. Inter-battery topic representation learning. European Conference on Computer Vision. Springer International Publishing, 2016.  ",38,500,8.771929824561404,5.470967741935484,215,2,498,0.0040160642570281,0.0255905511811023,0.9342,150,71,89,30,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 5, 'DAT': 1, 'MET': 14, 'EXP': 5, 'RES': 0, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 5, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 15, 'SUB': 6, 'CLA': 1}",0,1,7,5,1,14,5,0,4,0,0,2,5,1,0,0,0,1,4,0,15,6,1,0.7187680065029961,0.5649810489563746,0.4534700121901055
ICLR2018-BJInMmWC--R2,Reject,"**Summary** The paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions. This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task. In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017. The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR. See more detailed points below in Weaknesses. **Strengths** I like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images.  I buy the premise and think the work addresses an important issue. **Weakness**  Approach: * A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative). The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference.  The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper). Clarity: * Eqn. 5, LHS can be written more clearly as hat{a}_k. * It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work:   Efficient inference in occlusion-aware generative models of images, Jonathan Huang, Kevin Murphy. ICLR Workshops, 2016  * It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering. * The claim that the paper works with natural language should be toned down and clarified. This is not natural language, firstly because the language in the dataset is synthetically generated and not ""natural"". Secondly, the approach parses this ""synthetic"" language into structured tuples which makes it even less natural. Also, Page. 3. What does ""partial descriptions"" mean? * Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016). * Sec. 3. 4 mentions that the ""only image"" encoder is used to obtain the representation for the image, but the ""only image"" encoder is expected to capture the ""indescribable component"" from the image, then how is the attribute information from the image captured in this framework? One cannot hope to do image caption association prediction without capturing the image attributes...  *, In general, the writing and presentation of the model seem highly fragmented, and it is not clear what the specifics of the overall model are. For instance, in the decoder, the paper mentions for the first time that there are variables ""z"", but does not mention in the encoder how the variables ""z"" were obtained in the first place (Sec. 3.1). For instance, it is also not clear if the paper is modeling variable length sequences in a similar manner to Eslami, 2016 or not, and if this work also has a latent variable [z, z_pres] at every timestep which is used in a similar manner to Eqn. 2 in Eslami, 2016. Sec. 3.4 ""GEN Image Encoder"" has some typo, it is not clear what the conditioning is within q(z) term. * Comparison to baselines:    1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]? This seems like an important baseline to report for the image caption ranking task. 2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4 ""Gen Image Encoder"" and repeat the rest of the proposed pipeline. Does the proposed approach still show gains over Attend Infer Repeat? 3. The results shown in Fig. 7 are surprising -- in general, it does not seem like a regular VAE would do so poorly. Are the number of parameters in the proposed approach and the baseline VAE similar?  Are the choices of decoder etc. similar? Did the model used for drawing Fig. 7 converge? Would be good to provide its training curve. Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples. If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples. * Page 2: In general the notion of separating the latent space into content and style, where we have labels for the ""content"" is an old idea that has appeared in the literature and should be cited accordingly. See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has ""private"" latent variables for vision similar to this work (this is relevant to Sec. 3.2.) References: [A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. ""From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators."" arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.  [B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014. ""Semi-Supervised Learning with Deep Generative Models."" arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.  [C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. ""Deep Variational Canonical Correlation Analysis."" arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454. [D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. ""Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models."" arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539. ",41,978,14.173913043478262,5.077419354838709,372,4,974,0.0041067761806981,0.0269192422731804,0.9959,333,105,184,48,11,6,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 5, 'DAT': 4, 'MET': 15, 'EXP': 7, 'RES': 1, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 4, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 9, 'PNF': 2, 'REC': 0, 'EMP': 15, 'SUB': 1, 'CLA': 3}",0,1,9,5,4,15,7,1,3,1,0,2,4,0,1,0,0,9,2,0,15,1,3,0.790852718687674,0.6765352102774995,0.5570905099152973
ICLR2018-BJInMmWC--R3,Reject,"Summary: The authors observe that the current image generation models generate realistic images however as the dimensions of the latent vector is fully entangled, small changes to a single neuron can effect every output pixel in arbitrary ways. In this work, they explore the effect of using partial natural language scene descriptions for the task of disentangling the latent entities visible in the image. The proposed Generative Entity Networks jointly generates the natural language descriptions and images from scratch. The core model is Variational Autoencoders (VAE) with an integrated visual attention mechanism that also generates the associated text. The experiments are conducted on the Shapeworld dataset. Strengths: Simultaneous text and image generation is an interesting research topic that is relevant for the community. The paper is well written, the model is formulated with no errors (although it could use some more detail) and supported by illustrations (although there are some issues with the illustrations detailed below). The model is evaluated on tasks that it was not trained on which indicate that this model learns generalizable latent representations. Weaknesses: The paper gives the impression to be rushed, i.e. there are citations missing (page 3 and 6), the encoder model illustration is not as clear as it could be. Especially the white boxes have no labels, the experiments are conducted only on one small-scale proof of concept dataset, several relevant references are missing, e.g. GAN, DCGAN, GAWWN, StackGAN. Visual Question answering is mentioned several times in the paper, however no evaluations are done in this task. Figure 2 is complex and confusing due to the lack of proper explanation in the text. The reader has to find out the connections between the textual description of the model and the figure themselves due to no reference to particular aspects of the figure at all. In addition the notation of the modules in the figure is almost completely disjoint so that it is initially unclear which terms are used interchangeably. Details of the ""white components"" in Figure 2 are not mentioned at all. E.g., what is the purpose of the fully connected layers, why do the CNNs split and what is the difference in the two blocks (i.e. what is the reason for the addition small CNN block in one of the two) The optimization procedure is unclear. What is the exact loss for each step in the recurrence of the outputs (according to Figure 5)? Or is only the final image and description optimized. If so, how is the partial language description as a target handled since the description for a different entity in an image might be valid, but not the current target. (This is based on my understanding that each data point consists of one image with multiple entities and one description that only refers to one of the entities). An analysis or explanation of the following would be desirable: How is the network trained on single descriptions able to generate multiple descriptions during evaluation. How does thresholding mentioned in Figure 5 work? In the text, k suggests to be identical to the number of entities in the image. In Figure 5, k seems to be larger than the number of entities. How is k chosen? Is it fixed or dynamic? Even though the title claims that the model disentangles the latent space on an entity-level, it is not mentioned in the paper. Intuitively from Figure 5, the network generates black images (i.e. all values close to zero) whenever the attention is on no entity and, hence, when attention is on an entity the latent space represents only this entity and the image is generated only showing that particular entity. However, confirmation of this intuition is needed since this is a central claim of the paper. As the main idea and the proposed model is simple and intuitive, the evaluation is quite important for this paper to be convincing. Shapeworlds dataset seems to be an interesting proof-of-concept dataset however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups. First, the visual data is composed of primitive shapes and colors in a black background. Second, the sentences are simple and non-realistic. Third, it is not used widely in the literature, therefore no benchmarks exist on this data. It is not easy to read the figures in the experimental section, no walkthrough of the results are provided. For instance in Figure 4a, the task is described as ""showing the changes in the attribute latent variables"" which gives the impression that, e.g. for the first row the interpolation would be between a purple triangle to a purple rectangle however in the middle the intermediate shapes also are painted with a different color. It is not clear why the color in the middle changes. The evaluation criteria reported on Table 1 is not clear. How is the accuracy measured, e.g. with respect to the number of objects mentioned in the sentence, the accuracy of the attribute values, the deviation from the ground truth sentence (if so, what is the evaluation metric)?  No example sentences are provided for a qualitative comparisons. In fact, it is not clear if the model generates full sentences or attribute phrases. As a summary, this paper would benefit significantly with a more extensive overview of the existing relevant models, clarification on the model details mentioned above and a more through experimental evaluation with more datasets and clear explanation of the findings.",42,914,21.25581395348837,5.062992125984252,347,4,910,0.0043956043956043,0.0316939890710382,0.952,249,90,151,51,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 6, 'DAT': 5, 'MET': 16, 'EXP': 7, 'RES': 5, 'TNF': 15, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 7, 'REC': 0, 'EMP': 22, 'SUB': 6, 'CLA': 1}",0,1,3,6,5,16,7,5,15,3,0,1,0,0,0,0,1,0,7,0,22,6,1,0.71975602659832,0.5695194577716853,0.45338323959316124
ICLR2018-BJJ9bz-0--R1,Reject,"Thanks for all the explanations on my review and the other comments. While I can now clearly see the contributions of the paper, the minimal revisions in the paper do not make the contributions clear yet (in my opinion that should already be clear after having read the introduction). The new section intuitive analysis is very nice. *******************************  My problem with this paper that all the theoretical contributions / the new approach refer to 2 arXiv papers, what's then left is an application of that approach to learning form imperfect demonstrations. Quality        The approach seems sound but the paper does not provide many details on the underlying approach. The application to learning from (partially adversarial) demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly off-policy samples. The experiments are OK  but I would have liked a more thorough analysis. Clarity       The paper reads well, but it is not really clear what the claimed contribution is. Originality           The application seems original. Significance            Having an RL approach that can benefit from truly off-policy samples is highly relevant. Pros and Cons              + good results + interesting idea of using the algorithm for RLfD - weak experiments for an application paper - not clear what's new",17,209,19.0,5.275862068965517,115,2,207,0.0096618357487922,0.0492424242424242,0.9896,86,32,34,19,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 2}",0,0,1,1,0,8,2,1,0,2,0,3,0,1,0,2,1,1,0,0,10,1,2,0.5733816690134902,0.6723905262356256,0.39334995485232443
ICLR2018-BJJ9bz-0--R2,Reject,"This paper proposes a method to learn a control policy from both interactions with an environment and demonstrations. The method is inspired by the recent work on max entropy reinforcement learning and links between Q-learning and policy gradient methods. Especially the work builds upon the recent work by Haarnoja et al (2017) and Schulman et al (2017) (both unpublished Arxiv papers). I'm also not sur to see much differences with the previous work by Haarnoja et al and Schulman et al. It uses demonstrations to learn in an off-policy manner as in these papers. Also, the fact that the importance sampling ration is always cut at 1 (or not used at all) is inherited from these papers too. The authors say they compare to DQfD but the last version of this method makes use of prioritized replay so as to avoid reusing too much the expert transitions and overfit (L2 regularization is also used). It seems this has not been implemented for comparison and that overfitting may come from this method missing. I'm also uncomfortable with the way most of the expert data are generated for experiments. Using data generated by a pre-trained network is usually not representative of what will happen in real life. Also, corrupting actions with noise in the replay buffer is not simulating correctly what would happen in reality. Indeed, a single error in some given state will often generate totally different trajectories and not affect a single transition. So imperfect demonstration have very typical distributions. I acknowledge that some real human demonstrations are used but there is not much about them and the experiment is very shortly described. ",14,273,18.2,5.06083650190114,152,4,269,0.0148698884758364,0.0182481751824817,-0.9276,69,27,55,27,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,4,0,2,7,2,1,0,0,0,1,0,0,0,0,0,4,0,0,6,2,0,0.5018649029214621,0.3369012846874396,0.25391106687411324
ICLR2018-BJJ9bz-0--R3,Reject,"SUMMARY:  The motivation for this work is to have an RL algorithm that can use imperfect demonstrations to accelerate learning. The paper proposes an actor-critic algorithm, called Normalized Actor-Critic (NAC), based on the entropy-regularized formulation of RL, which is defined by adding the entropy of the policy as an additional term in the reward function. Entropy-regularized formulation leads to nice relationships between the value function and the policy, and has been explored recently by many, including [Ziebart, 2010], [Schulman, 2017], [Nachum, 2017], and [Haarnoja, 2017]. The paper benefits from such a relationship and derives an actor-critic algorithm. Specifically, the paper only parametrizes the Q function, and computes the policy gradient using the relation between the policy and Q function (Appendix A.1). Through a set of experiments, the paper shows the effectiveness of the method. EVALUATION:  I think exploring and understanding entropy-regularized RL algorithm is important. It is also important to be able to benefit from off-policy data. I also find the empirical results encouraging. But I have some concerns about this paper:  - The derivations of the paper are unclear. - The relation with other recent work in entropy-regularized RL should be expanded. - The work is less about benefiting from demonstration data and more about using off-policy data. - The algorithm that performs well is not the one that was actually derived. * Unclear derivations: The derivations of Appendix A.1 is unclear. It makes it difficult to verify the derivations. To begin with, what is the loss function of which (9) and (10) are its gradients? To be more specific, the choices of hat{Q} in (15) and hat{V} in (19) are not clear. For example, just after (18) it is said that ""hat{Q} could be obtained through bootstrapping by R + gamma V_Q"". But if it is the case, shouldn't we have a gradient of Q in (15) too? (or show that it can be ignored?)  It appears that hat{Q} and hat{V} are parameterized independently from Q (which is a function of theta). Later in the paper they are estimated using a target network, but this is not specified in the derivations. The main problem boils down to the fact that the paper does not start from a loss function and compute all the gradients in a systematic way. Instead it starts from gradient terms, each of which seems to be from different papers, and then simplifies them. For example, the policy gradient in (8), which is further decomposed in Appendix A.1 as (15) and (16) and simplified, appears to be Eq. (50) of [Schulman et al., 2017] (https://arxiv.org/abs/1704.06440). In that paper we have Q_pi instead of hat{Q} though. I suggest that the authors start from a loss function and clearly derive all necessary steps. * Unclear relation with other papers: What part of the derivations of this work are novel? Currently the novelty is not obvious. For example, having the gradient of both Q and V, as in (9), has been stated by [Haarnoja et al., 2017] (very similar formulation is developed in Appendix B of https://arxiv.org/abs/1702.08165). An algorithm that can work with off-policy data has also been developed by [Nachum, 2017] (in the form of a Bellman residual minimization algorithm, as opposed to this work which essentially uses a Fitted Q-Iteration algorithm as the critic). I think the paper could do a better job differentiating from those other papers. * The claim that this paper is about learning from demonstration is a bit questionable. The paper essentially introduces a method to use off-policy data, which is of course important, but does not cover the important scenario where we only have access to (state,action) pairs given by an expert. Here it appears from the description of Algorithm 1 that the transitions in the demonstration data have the same semantic as the interaction data, i.e., (s,a,r,s'). This makes it different from the work by [Kim et al., 2013], [Piot et al., 2014], and [Chemali et al., 2015], which do not require such a restriction on the demonstration data. * The paper mentions that to formalize the method as a policy gradient one, importance sampling should be used (the paragraph after (12)), but the performance of such a formulation is bad, as depicted in Figure 2. As a result, Algorithm 1 does not use importance sampling. This basically suggests that by ignoring the fact that the data is collected off-policy, and treating it as an on-policy data, the agent might perform better. This is an interesting phenomenon and deservers further study, as currently doing the ""wrong"" things is better than doing the ""right"" thing. I think a good paper should investigate this fact more.",41,765,20.13157894736842,5.048090523338048,277,9,756,0.0119047619047619,0.0282776349614395,-0.8931,229,71,139,40,11,6,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 3, 'MET': 32, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 30, 'SUB': 1, 'CLA': 0}",0,1,6,1,3,32,1,2,1,1,1,5,0,0,0,3,1,3,1,0,30,1,0,0.7941822393582288,0.6850279725605292,0.5634413763027929
ICLR2018-BJJLHbb0--R1,Accept,"1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection 2. It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods 3. Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process? can it be a trainable parameter? 4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning  ",5,96,19.2,5.741573033707865,71,0,96,0.0,0.010204081632653,0.9528,26,19,11,2,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 2, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,0,1,0,1,2,3,0,0,0,0,1,0,0,0,0,0,1,0,0,3,1,0,0.3575654408400592,0.3345772482030192,0.18194265649977692
ICLR2018-BJJLHbb0--R2,Accept,"The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized. The paper is rigorous and ideas are clearly stated. The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods. My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation. I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. 1. The framework uses the class information, i.e., ""only data samples from the normal class are used for training"", but it is still considered unsupervised. Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information. I would like to see a plot of the sample energy as a function of the number of data points. Is there an elbow that indicates the threshold cut? Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data). 2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network? 3. How do the regularization parameters lambda_1 and lambda_2 influence the results? 4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model). Those approaches should at least be discussed in the related work, if not compared against. 5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better. Could you provide a comparison with EM? 6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant?  Does it well describe the new space? Do you normalize the features (the output of the dimension reduction and the representation error are quite different)? Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians. 7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets. 8. The authors mention that ""we can clearly see from Fig. 3a that DAGMM is able to well separate ..."" - it is not clear to me, it does look better than the other ones, but not clear. If there is a clear separation from a different view, show that one instead. We don't need the same view for all methods. 9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic! Minor comments:  1. Fig.1: what dimension reduction did you use? Add axis labels. 2. ""DAGMM preserves the key information of an input sample"" - what does key information mean? 3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE. Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined. They are the best in terms of precision. 4. Is the error in Table 2 averaged over multiple runs? If yes, how many? Quality u2013 The paper is thoroughly written, and the ideas are clearly presented. It can be further improved as mentioned in the comments. Clarity u2013 The paper is very well written with clear statements, a pleasure to read. Originality u2013 Fairly original, but it still needs some work to justify it better. Significance u2013 Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.  ",38,764,16.25531914893617,5.005641748942172,288,4,760,0.0052631578947368,0.0322164948453608,0.9988,195,89,132,53,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 4, 'DAT': 2, 'MET': 19, 'EXP': 2, 'RES': 6, 'TNF': 6, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 7, 'REC': 0, 'EMP': 25, 'SUB': 2, 'CLA': 2}",0,1,3,4,2,19,2,6,6,0,0,4,0,0,0,1,0,4,7,0,25,2,2,0.6483530185184362,0.6825023578964635,0.4570172757645577
ICLR2018-BJJLHbb0--R3,Accept,"Summary  This applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space. The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples. The algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture. The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training. The paper demonstrates improvements in a number of public datasets. Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field. Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection. Criticisms  Based on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method. Little to no detail about these features is included. Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled. Since this is so important to the results, more analysis would be helpful. Why did the choices that were made in the paper yield this success? How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results? Quality  This paper does not set out to produce a novel network architecture. Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM. This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.  ",16,323,21.53333333333333,5.628205128205129,175,2,321,0.0062305295950155,0.0182926829268292,0.9844,102,37,47,10,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 8, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,1,0,1,1,8,3,3,0,1,0,3,0,0,0,2,0,0,0,0,9,2,0,0.5735545492100015,0.3384607769049167,0.2906213282679503
ICLR2018-BJLmN8xRW-R1,Reject,"SUMMARY  This paper addresses the cybersecurity problem of domain generation algorithm (DGA)  detection. A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g. to generate large numbers of rendezvous points.  DGA detection concerns the (automatic) distinction of actual and artificially generated domain names. In this paper, a basic problem formulation and general solution approach is investigated, namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of characters. A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task. A domain name data set with two million instances is used for the experiments. The main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures, since training time and the likelihood for overfitting can potentially be reduced. COMMENTS  The introduction is well-written, clear, and concise. It describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem. The introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used. One suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solution. The methods section (Section 2) provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experiment. In the beginning of the section, it is not clarified why, if a 75 character string is encoded as a 128 byte ASCII sequence, the content has to be stored in a 75 x 128 matrix instead of a vector of size 128. This is clarified later but should perhaps be discussed earlier to allow readers from outside the subarea to grasp the approach. Section 3 describes the experiment settings, the results, and discusses the learned representations and the possible implications of using either the deep architectures or the ""baseline"" Random Forest classifier. Perhaps, the authors could elaborate a little bit more on why Random Forests were trained on a completely different set of features than the deep architectures? The data is stated to be randomly divided into training (80%), validation (10%), and testing (10%). How many times is this procedure repeated? (That is, how many experimental runs were averaged or was the experiment run once?). In summary, this is an interesting and well-written paper on a timely topic. The main conclusion is intuitive. Perhaps the conclusion is even regarded as obvious by some but, in my opinion, the result is important since it was obtained from new, rather extensive experiments on a large data set and through the comparison of several existing (earlier proposed) architectures. Since the main conclusion is that simple models should be prioritised over complex ones (due to that their accuracy is very similar), it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclusion. MINOR COMMENTS  Abstract: ""Little studies"" -> ""Few studies"" Table 1: ""approach"" -> ""approaches"" Figure 1: Use the same y-axis scale for all subplots (if possible) to simplify comparison. Also, try to move Figure 1 so that it appears closer to its inline reference in the text. Section 3: ""based their on popularity"" -> ""based on their popularity""  ",27,600,24.0,5.417989417989418,280,4,596,0.0067114093959731,0.0278688524590163,0.9638,175,66,99,26,9,3,"{'ABS': 1, 'INT': 7, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 6, 'EXP': 7, 'RES': 4, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 6}",1,7,0,1,2,6,7,4,3,0,0,1,0,0,0,0,0,0,0,0,10,2,6,0.645028184091646,0.3394433021095118,0.32404048502195776
ICLR2018-BJLmN8xRW-R2,Reject,"This paper applies several NN architectures to classify url's between benign and malware related URLs. The baseline is random forests and feature engineering. This is clearly an application paper. No new method is being proposed, only existing methods are applied directly to the task. I am not familiar with the task at hand so I cannot properly judge the quality/accuracy of the results obtained but it seems ok. For evaluation data was split randomly in 80% train, 10% test and 10% validation. Given the amount of data 2*10**6 samples, this seems sufficient. I think the evaluation could be improved by using malware URLs that were obtained during a larger time window. Specifically, it would be nice if train, test and validation URLs would be operated chronologically. I.e. all train url precede the validation and test urls. Ideally, the train and test urls would also be different in time. This would enable a better test of the generalization capabilities in what is essentially a continuously changing environment. This paper is a very difficult for me to assign a final rating. There is no obvious technical mistake  and the paper is written reasonably well. There is however a lack of technical novelty or insight in the models themselves. I think that the paper should be submitted to a journal or conference in the application domain where it would be a better fit. For this reason, I will give the score marginally below the acceptance threshold now. But if the other reviewers argue that the paper should be accepted I will change my score.  ",17,261,13.05,5.078189300411522,144,4,257,0.0155642023346303,0.0303030303030303,0.9563,73,25,49,21,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 5, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 2}","{'APR': 2, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 2, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,1,1,2,5,3,0,0,0,0,5,0,2,2,2,0,0,0,2,5,0,1,0.5728288255073802,0.5581179592625789,0.3572378717078664
ICLR2018-BJLmN8xRW-R3,Reject,"This paper proposes to automatically recognize domain names as malicious or benign by deep networks (convnets and RNNs) trained to directly classify the character sequence as such. Pros  The paper addresses an important application of deep networks, comparing the performance of a variety of different types of model architectures. The tested networks seem to perform reasonably well on the task. Cons  There is little novelty in the proposed method/models -- the paper is primarily focused on comparing existing models on a new task. The descriptions of the different architectures compared are overly verbose -- they are all simple standard convnet / RNN architectures. The code specifying the models is also excessive for the main text -- it should be moved to an appendix or even left for a code release. The comparisons between various architectures are not very enlightening as they aren't done in a controlled way -- there are a large number of differences between any pair of models so it's hard to tell where the performance differences come from. It's also difficult to compare the learning curves among the different models (Fig 1) as they are in separate plots with differently scaled axes. The proposed problem is an explicitly adversarial setting and adversarial examples are a well-known issue with deep networks and other models, but this issue is not addressed or analyzed in the paper. (In fact, the intro claims this is an advantage of not using hand-engineered features for malicious domain detection, seemingly ignoring the literature on adversarial examples for deep nets.) For example, in this case an attacker could start with a legitimate domain name and use black box adversarial attacks (or white box attacks, given access to the model weights) to derive a similar domain name that the models proposed here would classify as benign. While this paper addresses an important problem, in its current form the novelty and analysis are limited and the paper has some presentation issues.",11,319,29.0,5.267741935483871,170,1,318,0.0031446540880503,0.0276073619631901,-0.9792,91,41,53,19,6,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,2,0,4,0,5,0,0,1,2,0,1,0,0,0,2,0,4,1,0,4,0,0,0.4298003349220662,0.4467069120050614,0.24042791466647678
ICLR2018-BJMuY-gRW-R1,Reject,"This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017). Differently from (Yogatama et al, 2017), this paper doesn't use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way. Overall, I think it is really an interesting direction and the proposed method sounds reasonable. However, I am concerned about the following points:    - The improvements are really limited on both the SNLI and the Reverse Dictionary tasks. (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it'd be helpful to present results on a diverse set of tasks and see if conclusions can generally hold. Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures. - The computational complexity of this model shouldn't be neglected. If I understand it correctly, the model needs to compute O(N^3) LSTM compositions. This should be at least discussed in the paper. And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM). n - I am wondering about the effects of the temperature parameter t. Is that important for training? Minor: - What is the difference between LSTM and left-branching LSTM? - I am not sure if the attention overt chart is a highlight of the paper or not. If so, better move that part to the models section instead of mention it briefly in the experiments section. Also, if any visualization (over the chart) can be provided, that'd be helpful to understand what is going on.  ",15,290,19.33333333333333,5.037313432835821,150,2,288,0.0069444444444444,0.0566666666666666,0.9581,67,38,56,25,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 7, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,3,1,1,7,4,2,0,0,0,1,0,0,0,0,0,3,2,0,6,2,0,0.5734049891838829,0.4479568021651641,0.31196231334819985
ICLR2018-BJMuY-gRW-R2,Reject,"Summary: The paper proposes to use the CYK chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive NNs . The key idea is to maintain a chart to take into account all possible spans. The paper also introduces an attention method over chart cells. The experimental results show that the propped model outperforms tree-lstm using external parsers. Comment: I kinda like the idea of using chart, and the attention over chart cells. The paper is very well written. - My only concern about the novelty of the paper is that the idea of using CYK chart-based mechanism is already explored in Le and Zuidema (2015). - Le and Zudema use pooling and this paper uses weighted sum. Any differences in terms of theory and experiment? - I like the new attention over chart cells. But I was surprised that the authors didn't use it in the second experiment (reverse dictionary). - In table 2, it is difficult for me to see if the difference between unsupervised tree-lstm and right-branching tree-lstm (0.3%) is ""good enough"". In which cases the former did correctly but the latter didn't? - In table 3, what if we use the right-branching tree-lstm with attention? - In table 4, why do Hill et al lstm and bow perform much better than the others? ",15,215,17.916666666666668,4.873170731707317,119,0,215,0.0,0.0179372197309417,0.8981,60,31,33,10,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 1, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,2,2,0,6,3,1,3,0,0,1,0,0,0,1,0,1,2,0,7,0,1,0.5730383072665651,0.5593488690884167,0.3622607359830797
ICLR2018-BJMuY-gRW-R3,Reject,"The paper presents a model titled the unsupervised tree-LSTM, in which the authors mash up a dynamic-programming chart and a recurrent neural network. As far as I can glean, the topology of the neural network is constructed using the chart of a CKY parser. When combining different constituents, an energy function is computed (equation 6) and the resulting energies are passed through a softmax. The architecture achieves impressive results on two tasks: SNLI and the reverse dictionary of Hill et al. (2016). Overall, I found the paper deeply uninspired. The authors downplay the similarity of their paper to that of Le and Zuidema (2015), which  I did not appreciate. It's true that Le and Zuidema take a parse forest from an existing parser, but it still contains an exponential number of trees, as does the work in here. Note that exposition in Le and Zuidema (2015) discusses the pruned case as well, i.e., a compete parse forest. The authors of this paper simply write Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty.  I would encourage the authors to revisit Le and Zuidema (2015), especially section 3.2, and consider the technical innovations over the existing work. I believe the primary difference (other using an LSTM instead of a convnet) is to replace max-pooling with softmax-pooling. Do these two architectural changes matter? The experiments offer no empirical comparison. In short, the insight of having an end-to-end differentiable function based on a dynamic-programming chart is pretty common -- the idea is in the air. The authors provide yet another instantiation of such an approach, but this time with an LSTM. The technical exposition is also relatively poor. The authors could have expressed their network using a clean recursion, following the parse chart, but opted not to, and, instead,  provided a round-about explanation in English. Thus, despite the strong results, I would not like to see this work in the proceedings, due to the lack of originality and poor technical discussion. If the paper were substantially cleaned-up, I would be willing to increase my rating. ",19,356,17.8,5.194528875379939,186,0,356,0.0,0.0083102493074792,-0.4612,95,45,56,20,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 2, 'IMP': 0, 'CMP': 7, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,6,1,0,11,5,3,0,0,0,3,0,0,1,2,0,7,2,0,7,0,0,0.5034513852181213,0.5600969520160747,0.32158884685225425
ICLR2018-BJNRFNlRW-R1,Accept,"This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem. They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods. Technically, the paper is sound. It mostly leverages the existing literature on primal-dual subgradient methods to modify the GAN training procedure. I think this is a nice contribution that does yield to some interesting insights. However I do have some concerns about the way the paper is currently written and I find some claims misleading. Prior convergence proofs: I think the way the paper is currently written is misleading. The authors quote the paper from Ian Goodfellow: ""For GANs, there is no theoretical prediction as to whether simultaneous gradient descent should converge or not. "". However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf. A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent. Another problem is of course the assumptions required for the proof that typically don't hold in practice (see comment below). Convex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution. This typically yields a non-convex non-concave optimization problem. This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm. Relation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [lapha f_1(D(x_i))]^2. This looks rather similar to some of the recent regularization techniques such as Improved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf Stabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf Can the authors comment on this? I think this would also shed some light as to why this approach alleviates the problem of mode collapse. Curse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality. For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but I'm not sure the empirical evidence provided for the MNIST and CIFAR-10 datasets is sufficient to judge whether or not the method does help with mode collapse. The inception score fails to capture this property.  Could the authors explore other quantitative measure? Have you considered trying your approach on the augmented version of the MNIST dataset used in Metz et al. (2016) and Che et al. (2016)? Experiments Typo: Should say ""The data distribution is p_d(x)   1{x 1}"". ",25,434,18.08333333333333,5.507177033492823,215,3,431,0.0069605568445475,0.0455580865603644,-0.9411,130,55,77,19,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 3}",0,1,5,1,1,10,1,2,0,0,0,4,0,2,0,0,0,4,0,0,10,1,3,0.645632109875759,0.4505558191713092,0.35610569569866196
ICLR2018-BJNRFNlRW-R2,Accept,"This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem. As a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point. Additionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models. But this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1. As follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem. Most important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it. But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR-10 dataset is not large enough to test the extensibility for large-scale cases. ",7,151,18.875,5.3,86,2,149,0.0134228187919463,0.0196078431372549,-0.6067,38,25,23,12,5,2,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 4, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,2,0,3,1,4,0,2,0,0,0,0,0,0,0,0,0,0,0,0,5,1,0,0.3580932540299475,0.2247100519615941,0.16172472272135768
ICLR2018-BJNRFNlRW-R3,Accept,"In this paper, the authors study the relationship between training GANs and primal-dual subgradient methods for convex optimization. Their technique can be applied on top of existing GANs and can address issues such as mode collapse. The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN. Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse. This is a strong contribution In Table 2 the difference between inception scores for DCGAN and this approach seems significant to ignore. The authors should explain more possibly. There is a typo in Page 2 u2013 For all these varaints, -variants. ",8,106,13.25,5.47,73,2,104,0.0192307692307692,0.0373831775700934,-0.3818,35,10,17,2,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 1}",0,1,0,1,0,5,2,0,1,0,0,0,0,0,0,0,0,0,0,0,1,2,1,0.3582298213897044,0.3334400098416548,0.17719158917528696
ICLR2018-BJOFETxR--R1,Accept,"Summary:  The paper applies graph convolutions with deep neural networks to the problem of variable misuse (putting the wrong variable name in a program statement) in graphs created deterministically from source code. Graph structure is determined by program abstract syntax tree (AST) and next-token edges, as well as variable/function name identity, assignment and other deterministic semantic relations. Initial node embedding comes from both type and tokenized name information. Gated Graph Neural Networks (GGNNs, trained by maximum likelihood objective) are then run for 8 iterations at test time. The evaluation is extensive and mostly very good. Substantial data set of 29m lines of code. Reasonable baselines. Nice ablation studies. I would have liked to see separate precision and recall rather than accuracy. The current 82.1% accuracy is nice to see, but if 18% of my program variables were erroneously flagged as errors, the tool would be useless. I'd like to know if you can tune the threshold to get a precision/recall tradeoff that has very few false warnings, but still catches some errors. Nice work creating an implementation of fast GGNNs with large diverse graphs. Glad to see that the code will be released. Great to see that the method is fast---it seems fast enough to use in practice in a real IDE. The model (GGNN) is not particularly novel, but I'm not much bothered by that. I'm very happy to see good application papers at ICLR. I agree with your pair of sentences in the conclusion: Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning. It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses.   I'd like to see work in this area encouraged. So I recommend acceptance. If it had better (e.g. ROC curve) evaluation and some modeling novelty, I would rate it higher still. Small notes: The paper uses the term data flow structure without defining it. Your data set consisted of C# code. Perhaps future work will see if the results are much different in other languages. ",25,364,14.0,5.148148148148148,218,2,362,0.005524861878453,0.0135869565217391,0.9963,107,63,66,24,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 3, 'MET': 9, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 2, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 2, 'EMP': 13, 'SUB': 1, 'CLA': 0}",0,0,1,4,3,9,0,4,1,1,2,3,0,0,0,3,2,0,0,2,13,1,0,0.6453708351286164,0.5631770970633205,0.4113379873949388
ICLR2018-BJOFETxR--R2,Accept,"The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming. The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context. The two experimental tasks were: 1) identifying variable (mis)use, ie. identifying bugs in programs where the wrong variable is used, and 2) predicting a variable's name by consider its semantic context. The paper is generally well written, easy to read and understand, and the results are compelling. The proposed GGNN approach outperforms (bi-)LSTMs on both tasks. Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out. Those results show that as structural information is removed, the GGNN's performance diminishes, as expected. As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open-source project and claimed to find several bugs, at least one of which potentially reduced memory performance. Overall the work is important, original, well-executed, and should open new directions for deep learning in program analysis. I recommend it be accepted.",11,216,19.63636363636364,5.4375,136,0,216,0.0,0.0230414746543778,0.7717,60,25,45,8,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,3,3,1,0,1,1,4,0,0,0,1,1,0,0,1,2,1,0,0.5722376036637098,0.5561775129903985,0.3587731643066951
ICLR2018-BJOFETxR--R3,Accept,"This paper presents a novel application of machine learning using Graph NN's on ASTs to identify incorrect variable usage and predict variable names in context. It is evaluated on a corpus of 29M SLOC, which is a substantial strength of the paper. The paper is to be commended for the following aspects: 1) Detailed description of GGNNs and their comparison to LSTMs 2) The inclusion of ablation studies to strengthen the analysis of the proposed technique 3) Validation on real-world software data 4) The performance of the technique is reasonable enough to actually be used. In reviewing the paper the following questions come to mind: 1) Is the false positive rate too high to be practical? How should this be tuned so developers would want to use the tool? 2) How does the approach generalize to other languages? (Presumably well, but something to consider for future work.) Despite these questions, though, this paper is a nice addition to deep learning applications on software data and I believe it should be accepted.  ",10,171,34.2,5.138364779874214,99,1,170,0.0058823529411764,0.0231213872832369,0.9283,47,16,33,6,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 2, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,0,1,2,4,1,0,0,1,1,3,0,0,0,1,1,0,0,2,6,0,0,0.57230607612592,0.4475740320908243,0.3218631937699675
ICLR2018-BJQPG5lR--R1,Reject,"EDIT: The rating has been changed. See thread below for explanation / further comments. ORIGINAL REVIEW: In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them. I think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance, even though the potential impact of this paper is limited because the training strategy proposed is (by deep learning standards) relatively complicated, requires tuning two additional hyperparameters in the initial value of lambda as well as the step size for updating lambda, and seems to have no significant advantage over just using skip connections throughout training. So my rating based on the message of the paper would be 6/10. However, there appear to be a range of issues. As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6. +++ Section 3.1 problems +++  - I think the toy example presented in section 3.1 is more confusing than it is helpful because the skip connection you introduce in the toy example is different from the skip connection you introduce in VANs. In the toy example, you add (1 - alpha)wx whereas in the VANs you add (1 - alpha)x. Therefore, the type of vanishing gradient that is observed when tanh saturates, which you combat in the toy model, is not actually combated at all in the VAN model. While it is true that skip connections combat vanishing gradients in certain situations, your example does not capture how this is achieved in VANs. - The toy example seems to be an example where Lagrangian relaxation fails, not where it succeeds. Looking at figure 1, it appears that you start out with some alpha < 1 but then immediately alpha converges to 1, i.e. the skip connection is eliminated early in training, because wx is further away from y than tanh(wx). Most of the training takes place without the skip connection. In fact, after 10^4 iterations, training with and without skip connection seem to achieve the same error. It appears that introducing the skip connection was next to useless and the model failed to recognize the usefulness of the skip connection early in training. - Regarding the optimization algorithm involving alpha^* at the end of section 3: It looks to me like a hacky, unprincipled method with no guarantees that just happened to work in the particular example you studied. You motivate the choice of alpha^* by wanting to maximize the reduction in the local linear approximation to mathcal{C} induced by the update on w.  However, this reduction grows to infinity the larger the update is. Does that mean that larger updates are always better? Clearly not. If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes? Hence, the motivation for the algorithm you present is invalid. Here is an example where this algorithm fails: consider the point (x,y,w,alpha,lambda)   (100, sigma(100), 1.0001, 1, 1). Here, w has almost converged to its optimum w*   1. Correspondingly, the derivative of C is a small negative value. However, alpha* is actually 0, and this choice would catapult w far away from w*. If I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues. +++ ResNet scaling +++  There is a crucial difference between VANs and ResNets. In the VAN initial state (alpha   0.5), both the residual path and the skip path are multiplied by 0.5 whereas for ResNet, neither is multiplied by 0.5. Because of this, the experimental results between the two architectures are incomparable. In a question I posed earlier, you claimed that this scaling makes no difference when batch normalization is used. I disagree. Let's look at an example. Consider ResNet first. It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b. Now let's assume we insert a scaling constant after each residual block, say c   0.5. Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B. Therefore, contributions of lower blocks vanish exponentially. This effect is not combated by batch normalization. So the learning dynamics for VAN and ResNet are very different because of this scaling.  Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling? Without this information, the experiments have limited value. In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections. If my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings:   - ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1}   0.5mathcal{F}(x_l) + 0.5x_l - VAN with the following altered equation: x_{l+1}   mathcal{F}(x_l) + (1-alpha)x_l, i.e. please remove the alpha in front of mathcal{F}. Also, initialize alpha to zero. This ensures that VAN starts out as a regular ResNet. +++ writing issues +++  Title:  - VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS This title can be read in two different ways. (A) [Train] [deep feed-forward networks] [without skip-connections] and (B) [Train] [deep feed-forward networks without skip connections]. In (A), the `without skip-connections' modifies the `train' and suggests that training took place without skip connections. In (B), the `without skip-connections' modifies `deep feed-forward networks' and suggests that the network trained has no skip connections. You must mean (B), because (A) is false. Since it is not clear from reading the title whether (A) or (B) is true, please reword it. Abstract:  - Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.  Again, this is ambiguous. To me, this sentence implies that you extend the benefit of avoiding vanishing and exploding gradients to fully-connected networks without skip connections. However, nowhere in your paper do you show that trained VANs have less exploding / vanishing gradients than fully-connected networks trained the old-fashioned way. Again, please reword or include evidence. - where the proposed method is shown to outperform many architectures without skip-connections Again, this sentence makes no sense to me. It seems to imply that VAN has skip connections. But in the abstract you defined VAN as an architecture without skip connections. Please make this more clear. Introduction: - Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations. Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations. In fact, the claim that deep layers don't learn useful representations is directly contradicted by the paper. section 3.1: - replace to to by to in the second line section 4: - This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.  The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers. Therefore, it cannot contribute positively to the performance or ResNet. As mentioned in earlier comments, please reword / clarify your use of activation function. It is generally used a synonym for  onlinearity, so please use it in this way. Change your claim that VAN is equivalent to PReLU. Please include your description of how your method can be extended to networks which do allow for skip connections. +++ Hyperparameters +++  Since the initial values of lambda and eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how lambda evolves for at least some of the experiments.",70,1388,18.26315789473684,5.194211728865194,485,12,1376,0.0087209302325581,0.0234321157822191,0.9744,406,154,260,82,12,6,"{'ABS': 3, 'INT': 7, 'RWK': 2, 'PDI': 1, 'DAT': 3, 'MET': 40, 'EXP': 11, 'RES': 5, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 3, 'EMP': 39, 'SUB': 1, 'CLA': 9}",3,7,2,1,3,40,11,5,1,1,0,3,0,1,0,0,1,1,0,3,39,1,9,0.8683847561598564,0.6909897307018622,0.6227794846030236
ICLR2018-BJQPG5lR--R2,Reject,"UPDATED COMMENT I've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments. I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice). ORIGINAL REVIEW BELOW  The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections). The skip connections are penalized by Lagrange multipliers that are gradually phased out during training. The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets. It's a nice, simple idea. However, I don't think it's sufficient for acceptance. Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it. Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below, and (ii) the results just aren't that good. Comments:  1. Why force the Lag. multipliers to 1 at the end of training?  It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent. I would expect the resulting architecture to perform at least as well as variable action nets. If not, I'd be curious as to why. 2.Similarly, it's not obvious that initializing the multipliers at 0.5 is the best choice. The ""looks linear"" initialization proposed in ""The shattered gradients problem"" (Balduzzi et al) implies that alpha 0 may work better. Did the authors try any values besides 0.5? 3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections. Firstly, it's not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets. Secondly, why not just do it?   ",20,303,15.15,5.096885813148789,171,6,297,0.0202020202020202,0.0485436893203883,0.9832,73,31,57,29,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 2, 'EMP': 14, 'SUB': 0, 'CLA': 0}",0,0,1,3,0,10,3,2,0,0,0,3,0,0,0,0,0,2,0,2,14,0,0,0.43126068841143,0.341555743015647,0.21266641397185326
ICLR2018-BJQPG5lR--R3,Reject,"Update (original review below): The authors have addressed several of the reviewers' comments and improved the paper. The motivation has certainly been clarified, but in my opinion it is still hazy. The paper does use skip connections, but the difference is that they are phased out over training. So I think that the motivation behind introducing this specific difference should be clear. Is it to save the additional (small) overhead of using skip connections? Nevertheless, the additional experiments and clarifications are very welcome. For the newly added case of VAN(lambda 0), please note the strong similarity to https://arxiv.org/abs/1611.01260 (ICLR2017 reviews at https://openreview.net/forum?id Sywh5KYex). In that report alpha_l is a scalar instead of a vector. Although it is interesting, the above case case also calls into question the additional value brought by the use of constrained optimization, a main contribution of the paper. In light of the above, I have increased my score since I find this to be an interesting approach, but in my opinion the significance of the results as they stand is low. The paper demonstrates that it is possible to obtain very deep plain networks (without skip connections) with improved performance  through the use of constrained optimization that gradually removes skip connections, but the value of this demonstration is unclear because a) consistent improvements over past work or the lambda 0 case were not found, and b) The technique still relies on skip connections in a sense so it's not clear that it suggests a truly different method of addressing the degradation problem. Original Review               Summary: The contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections. The paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied. It is proposed to optimize the formulation using the method of Lagrange multipliers. A toy model with a single unit is used to illustrate the basic ideas behind the method. Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets. Quality and significance: The proposed methodology is simple and straightforward. The analysis with the toy network is interesting and helps illustrate the method. However, my main concerns with this paper are related to motivation and experiments. The motivation of the work is not clear at all. The stated goal is to address some of the issues related to the role of depth in deep networks, but I think it should be clarified which specific issues in particular are relevant to this method and how they are addressed. One could additionally consider that removing the skip connections at the end of training reduces the computational expense (slightly), but beyond that the expected utility of this investigation is very hazy from the description in the paper. For MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized. Here, the generalization error is measured instead, which is heavily influenced by regularization. Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer. The general difference between plain and non-plain networks is very likely due to optimization difficulties alone, and due to the above issues further comparisons can not be made from the results. For the CIFAR experiments, the experiment design is reasonable for a general comparison. Similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does not make a rigorous comparison without employing expensive hyper-parameter searches. This is not the fault of the present paper but an unfortunate tradition in the field. Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences. For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do. Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization. Clarity: The paper is generally well-written and easy to read. There are some clarity issues related to the use of the term activation function and a typo in an equation but the authors are already aware of these.",36,761,22.38235294117647,5.386702849389416,317,4,757,0.0052840158520475,0.0309278350515463,0.9982,204,98,133,40,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 2, 'MET': 21, 'EXP': 9, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 22, 'SUB': 1, 'CLA': 5}",0,0,3,2,2,21,9,5,0,2,0,3,0,1,0,0,0,4,0,0,22,1,5,0.649145282440578,0.458181578531133,0.36610138050937846
ICLR2018-BJQRKzbA--R1,Accept,"The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy. Manually designing novel neural architectures is a laborious, time-consuming process. Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field. Overall, the paper is well-written, clear in its exposition and technically sound. While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible. The fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. Nevertheless, while the idea behind the proposed approach is definitely interesting, I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications. In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small. Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3. Another concern I have is regarding the definition of the mutation operators in Section 3.1. While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it). If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A. Finally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations. I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values. References:  [1] Grosse, Roger, et al. Exploiting compositionality to explore a large space of model structures. UAI (2012). [2] Duvenaud, David, et al. Structure discovery in nonparametric regression through compositional kernel search. ICML (2013). ",24,635,21.89655172413793,5.568595041322314,293,4,631,0.0063391442155309,0.028169014084507,0.9929,186,72,95,32,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 13, 'EXP': 3, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,2,1,0,13,3,3,2,0,0,4,1,0,1,2,0,2,0,0,9,0,0,0.6463832858786374,0.4495823740648964,0.36242081520191827
ICLR2018-BJQRKzbA--R2,Accept,"This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL. The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly. The biggest two nitpicks:  > In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs) This is a gross overstatement. The architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution, separable convolution, and pooling layers. Only the topology of the cells themselves are designed. The work is still great, but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste. As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network. It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the [common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search.   > Table 1  Why is the second best method on CIFAR (""Hier. repr-n, random search (7000 samples)"") never tested on ImageNet?  The omission is conspicuous. Just test it and report. Smaller nitpicks:  > ""New state of the art for evolutionary strategies on this task ""  ""Evolutionary Strategies"", at least as used in Salimans 2017, has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do. It may be more clear to change this phrase to ""evolutionary methods"" or similar. > Our evolution algorithm is similar but more generic than the binary tournament selection (K   2) used in a recent large-scale evolutionary method (Real et al., 2017). A K 5% tournament does not seem more generic than a binary K 2 tournament. They're just different.",17,344,21.5,5.363076923076923,187,3,341,0.0087976539589442,0.0139664804469273,0.6746,93,50,58,21,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 0}",0,0,3,1,1,10,1,1,0,2,0,3,0,0,1,0,1,4,1,0,5,3,0,0.5739568332541188,0.6697193370942514,0.4067940536747798
ICLR2018-BJQRKzbA--R3,Accept,"The authors present a novel evolution scheme applied to neural network architecture search. It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm. To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells. Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure. To speed up the search, they focus on finding cells instead of an entire network. In evaluation time, they insert these cells between layers of a network comparable in size to known networks. They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance. The method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. It could lead to new insight on automating design of neural networks for given problems. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The paper presents a good work and is well articulated.  However, it could benefit from additional details and a deeper analysis of the results. The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. The method is also appealing for its use of some kind of emergence between two levels of hierarchy. In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules. Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm. Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy? Are the cells found interpretable? The authors should try to give their opinion about the design obtained. The implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description. During evaluation, what is a step? A batch or an epoch or other? The method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU.  It raises questions on the usability of the method for small labs. At some point, we will have to use insights from this search to stop early, when no improvement is expected. Also, authors claim that their method consume less computation time than reinforcement learning. This should be supported by some quantitative results. The paper would greatly benefit from a deeper comparison over other techniques. For instance, it could describe more the advantages over reinforcement learning. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search.  It could have taken more spaces in the paper. I am also concerned the computational efficiency of the results obtained with this method on current processors. Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand. Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand. Does the solution obtained with the optimization can be run as efficiently? A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper. This is a general comment over this kind of approach, but I think it should be addressed.  ",39,651,16.692307692307693,5.293739967897271,284,4,647,0.0061823802163833,0.0228658536585365,0.9964,183,73,102,27,11,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 25, 'EXP': 1, 'RES': 6, 'TNF': 0, 'ANA': 3, 'FWK': 2, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 4, 'CLA': 0}",0,1,2,1,2,25,1,6,0,3,2,2,0,1,0,1,2,0,0,0,14,4,0,0.7922129731218378,0.4528979572708154,0.44803097234993994
ICLR2018-BJRZzFlRb-R1,Accept,"This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices. The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks. Overall, the paper is well-written. The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation. I think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer. For example, the model does not guarantee to be able to convert resulting indices to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector. As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task. For reader's comprehension, it would like to add results about whole memory consumption of each model as well. Also, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer. There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies. Furthermore, I would like to see two additional analysis. First, if we trained the proposed model with starting from zero (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?",12,309,25.75,5.163879598662208,163,3,306,0.0098039215686274,0.0129449838187702,0.9739,91,28,54,20,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 4, 'CLA': 1}",0,1,1,2,0,7,0,4,1,3,0,1,2,0,0,0,1,1,0,0,5,4,1,0.6446465328459048,0.5583634148198918,0.40916585467284594
ICLR2018-BJRZzFlRb-R2,Accept,"This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings.  It also adopts an interesting multicodebook approach for encoding than binary embeddings. The paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy. The experiments are convincing and solid. Overall I am weakly inclined to accept this paper.",5,79,15.8,5.421052631578948,53,0,79,0.0,0.0125,0.91,22,11,18,3,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,2,1,1,0,0,0,1,0,0,0,1,1,0,0,1,3,0,0,0.4288216206852132,0.4456883593141303,0.24328397663984289
ICLR2018-BJRZzFlRb-R3,Accept,"The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick. The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss. This paper is well-written and easy to follow. The motivation is clear and the idea is simple and effective. n It would be better to provide deeper analysis in Subsection 6.1. The current analysis is too simple.  It may be interesting to explain the meanings of individual components. Does each component is related to a certain topic? Is it meaningful to perform ADD or SUBSTRACT on the leaned code? It may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix. ",10,133,14.77777777777778,5.301587301587301,80,2,131,0.015267175572519,0.037037037037037,0.9679,40,16,25,2,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 1}",0,1,0,2,0,2,0,1,0,5,0,1,0,0,0,0,1,0,0,0,4,3,1,0.428979798069534,0.4465236697656163,0.23767316172305797
ICLR2018-BJRxfZbAW-R1,Reject,"The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features. This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets. A particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a, which leads to a disentangled representation. This is also not dissimilar to ideas used in 'Bayesian Representation Learning With Oracle Constraints' Karaletsos et al 2016 where similar contextual features c are learned to disentangle representations over observations and implicit supervision. The authors provide a clean variational inference algorithm to learn their model. However, a key problem is the following: the nature of the discrete variables being used makes them hard to be inferred with variational inference. The authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach. In fact, it would be interesting to study which level of these variables could be analytically collapsed (such as done in the Semi-Supervised learning work by Kingma et al 2014) and which ones can be sampled effectively using a form of reparametrization. This also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived, the empirical evaluation is on the weak side and the rich properties of the model are not really shown off. It would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy settings. Can the model recover richer structure that was imposed during data generation? How limiting is the learning of a? How does the likelihood of the model behave under the circumstances? The experiments do not really convey how well this all will work in practice.",15,338,30.727272727272727,5.44478527607362,176,0,338,0.0,0.0118343195266272,0.9705,90,30,61,22,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 3, 'DAT': 1, 'MET': 8, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 4, 'CLA': 0}",0,1,2,3,1,8,3,3,0,2,0,0,0,0,0,0,0,1,0,0,9,4,0,0.5736875186006121,0.3386290223370415,0.29262138095388246
ICLR2018-BJRxfZbAW-R2,Reject,"This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185). The generative model defines the process that produces the dataset. This model is first a mixture over contexts followed by i.i.d. generation of the dataset with possibly some unobserved random variable. This corresponds to a mixture of Neural Statisicians. The authors suggest that such a model could help with disentangling factors of variation in data. In the experiments they only consider training the model with the context selection variable and the data variables observed. Unfortunately there is minimal quantitative evaluation (visualizing 264 MNIST samples is not enough). The only quantitative evaluation is in Table 1, and it seems the model is not able to generalize reliably to all rotations and all digits. Clearly, we can't expect perfect performance, but there are some troubling results: 5.2 accuracy on non-rotated 0s, 0.0 accuracy on non-rotated 6s. Every digit has at least one rotation that is not well classified, so this section could use more discussion and analysis. For example, how would this metric classify VAE samples with contexts corresponding only to digit type (no rotations)? How would this metric classify vanilla VAE samples that are hand labeled? Moreover, the context selection variable a should be considered part of the dataset, and as such the paper should report how a was selected. This model is a relatively simple extension of the Neural Statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation. I do think the idea is well-motivated, and represents a promising way to incorporate prior knowledge of concepts into our training of VAEs. Still, the paper as it stands is not complete, and I encourage the authors to followup with more thorough quantitative empirical evaluations. ",15,296,18.5,5.430604982206406,154,3,293,0.0102389078498293,0.0336700336700336,0.5775,80,34,47,18,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 7, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 4, 'CLA': 0}",0,1,0,3,7,8,2,2,1,1,0,1,0,0,0,1,0,1,0,0,6,4,0,0.6450855295011261,0.4478742611436237,0.35909505814430553
ICLR2018-BJRxfZbAW-R3,Reject,"This paper proposes a model for learning to generate data conditional on attributes. Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time. The model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before). However, this paper is mercilessly difficult to read. The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (""concepts"" and ""context"" are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper. For the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine ""contexts"" that were not observed during training. The experiments show that the generation component is quite effective, but this is an obvious missing step. Anyway, some other related work: Lample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes. But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up.",10,255,21.25,5.441295546558704,140,0,255,0.0,0.0196078431372549,0.9725,61,29,50,26,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 4, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,2,0,1,4,3,2,0,0,0,1,0,1,0,0,0,2,0,0,4,1,0,0.5724877291484363,0.3353163681950522,0.28879001330577775
ICLR2018-BJ_QxP1AZ-R1,Reject,"My main concern for this paper is that the description of the Visual Concepts is completely unclear for me. At some point I thought I did understand it, but then the next equation didnt make sense anymore... If I understand correctly, f_p is a representation of *all images* of a specific layer *k* at/around pixel p, (According to last line of page 3). That would make sense, given that then the dimensions of the vector f_p is a scalar (activation value) per image for that image, in layer k, around pixel p. Then f_v is one of the centroids (named VCs). However, this doesnt seem to be the case, given that it is impossible to construct VC activations for specific images from this definition. So, it should be something else, but it does not become clear, what this f_p is. This is crucial in order to follow / judge the rest of the paper. Still I give it a try. Section 4.1 is the second most important section of the paper, where properties of VCs are discussed. It has a few shortcomings.  First, iIt is unclear why coverage should be > 0.8 and firerate ~ 1, according to the motivation firerate should equal to coverage: that is each pixel f_p is assigned to a single VC centroid. Second, VCs tent to occur for a specific class, that seems rather a bold statement from a 6 class, 3 VCs experiment, where the class sensitivity is in the order 40-77%. Also the second experiment, which shows the spatial clustering for the car wheel VC, is unclear, how is the name car wheel assigned to the VC? That has have to be named after the EM process, given that EM is unsupervised. Finally the cost effectiveness training (3c), how come that the same car wheel (as in 3b) is discovered by the EM clustering? Is that coincidence? Or is there some form of supervision involved? Minor remarks - Table 1: the reported results of the Matching Network are different from the results in the paper of Vinyals (2016). - It is unclear what the influence of the smoothing is, and how the smoothing parameter is estimated / set. - The VCs are introduced for few-shot classification, unclear how this is different from previous few-shot methods (sect 5). - 36x36 patches have a plausible size within a 84x84 image, this is rather large, do semantic parts really cover 20% of the image? - How are the networks trained, with what objective, how validated, which training images? What is the influence of the layer on the performance? - Influence of the clustering method on VCs, eg k-means, gaussian, von-mises (the last one is proposed)? On a personal note, I've difficulties with part of the writing.  For example, the introduction is written rather arrogant (not completely the right word, sorry for that), with a sentence, like we have only limited insights into why CNNs are effective seems overkill for the main research body. The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty. Also the authors refer to another paper (about using VCs for detection) which is also under submission (somewhere). Finally, the introduction paragraph of Section 5 is rather bold, resembles the learning process of human beings? Not so sure that is true, and it is not supported by a reference (or an experiment). In conclusion: This paper presents a method for creating features from a (pre-trained) ConvNet. It clusters features from a specific pooling layer, and then creates a binary assignment between per image extracted feature vectors and the cluster centroids. These are used in a 1-NN classifier and a (smoothed) Naive Bayes classifier. The results show promising results, yet lack exploration of the model, at least to draw conclusions like we address the challenge of understanding the internal visual cues of CNNs. I believe this paper needs to focus on the working of the VCs for few-shot experiments, showing the influences of some of the choices (layer, network layout, smoothing, clustering, etc). Moreover, the introduction should be rewritten, and the the background section of VCs (Sect 3) should be clarified. Therefore, I rate the current manuscript as a reject. After rebuttal: The writing of the paper greatly improved, still missing insights (see comments below). Therefore I've upgraded my rating, and due to better understanding now, als my confidence. ",40,722,22.5625,4.927299703264095,315,3,719,0.0041724617524339,0.0272108843537414,0.9644,211,82,122,40,8,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 21, 'EXP': 10, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 2, 'EMP': 25, 'SUB': 1, 'CLA': 5}",0,2,2,2,0,21,10,5,0,0,0,4,0,1,0,0,0,3,0,2,25,1,5,0.5777325465181875,0.5710611998617482,0.3661200755261481
ICLR2018-BJ_QxP1AZ-R2,Reject,"The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015). This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset. The clustered representations are the visual concepts. This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images. A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class .  The results a are convincing, even if they are not state of the art in all the trials. The paper is very easy to follows, and the results are explained in a very simple way. Few comments: The authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general.",8,195,21.666666666666668,4.881720430107527,115,0,195,0.0,0.0050761421319796,0.9348,64,20,27,5,8,1,"{'ABS': 1, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 2, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",1,0,1,2,1,2,1,2,0,0,0,1,0,0,0,0,0,0,0,0,3,0,0,0.5717937652969213,0.112355025980797,0.234172321273298
ICLR2018-BJ_QxP1AZ-R3,Reject,"The paper proposes a method for few-shot learning using a new image representation called visual concept embedding. Visual concepts were introduced in Wang et al. 2015, which are clustering centers of feature vectors in a lattice of a CNN. For a given image, its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concepts. Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions. Experiments are conducted on the Mini-ImageNet dataset and the PASCAL3D+ dataset for few-shot learning. Positives: - The three properties of visual concepts described in the paper are interesting. Negatives: - The novelty of the paper is limited. The idea of visual concept has been proposed in Wang et al. 2015. Using a embedding representation based on visual concepts is straightforward. The two baseline methods for few-shot learning provide limited insights in solving the few-shot learning problem. - The paper uses a hard thresholding  in the visual concept embedding. It would be interesting to see the performance of other strategies in computing the embedding, such as directly using the distances without thresholding.",12,198,14.142857142857142,5.636363636363637,91,0,198,0.0,0.0049504950495049,0.6486,64,24,39,2,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,2,2,2,1,6,1,0,0,1,0,1,0,0,0,2,0,0,0,0,3,2,0,0.57284483869105,0.3347290322958589,0.2930647075944982
ICLR2018-BJ_UL-k0b-R1,Accept,"Summary The paper presents an interesting view on the recently proposed MAML formulation of meta-learning (Finn et al). The main contribution is a) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors, b) insight into the connection between MAML and MAP estimation in non-linear HB models with implicit priors, c) based on these insights, the paper proposes a variant of MALM using a Laplace approximation (with additional approximations for the covariance matrix. The paper finally provides an evaluation on the mini ImageNet problem without significantly improving on the MAML results on the same task. Pro: -            The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few-shot learning. -            Provides valuable insight into the MAML objective and its relation to probabilistic models   Con: -            The paper is generally well-written  but I find (as a non-meta-learner expert) that certain fundamental aspects could have been explained better or in more detail (see below for details). -            The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method (compared to e.g. MAML) -            I do not think the empirical results provide enough evidence that it is a useful/robust method. Especially it does not provide insight into which types of problems (small/large, linear/ non-linear) the method is applicable to. Detailed comments/questions: -            The use of Laplace approximation is (in the paper) motivated from a probabilistic/Bayes and uncertainty point-of-view. It would, however, seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mode. Could the authors perhaps comment on: a) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization (given the psd approximation to the Hessian), or does it only make sense after convergence?  b) the consequence of the approximation errors on the general convergence of the proposed method (consistency and rate)  - Sec 4.1, p5: Last equation: Perhaps useful to explain the term $log(phi_j^* | theta)$ and why it is not in subroutine 4 . Should $phi^*$  be $hat phi$ ? -            Sec 4.2: ""A straightforward..."": I think it would improve readability to refer back to the to the previous equation (i.e. H) such that it is clear what is meant by ""straightforward"". - Sec 4.2: Several ideas are being discussed in Sec 4.2 and it is not entirely clear to me what has actually been adopted here; perhaps consider formalizing the actual computations in Subroutine 4 u2013 and provide a clearer argument (preferably proof) that this leads to consistent and robust estimator of theta. -            It is not clear from the text or experiment how the learning parameters are set. -            Sec 5.1: It took some effort to understand exactly what was going on in the example and particular figure 5.1; e.g., in the model definition in the body text there is no mention of the NN mentioned/used in figure 5, the blue points are not defined in the caption, the terminology e.g.  ""pre-update density"" is new at this point. I think it would benefit the readability to provide the reader with a bit more guidance. -            Sec 5.1: While the qualitative example is useful (with a bit more text), I believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std MAML and possibly compare to a std Bayesian inference method from the HB formulation of the problem (in the linear case) -            Sec 5.2: The abstract clams increased performance over MAML but the empirical results do not seem to be significantly better than MAML ? I find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the significance. -            Sec 5.2: The authors have left out ""Mishral et al"" from the comparison due to the model being significantly larger than others. Could the authors provide insight into why they did not use the ResNet structure from the  tcml paper in their L-MLMA scheme ? -            Sec 6+7: The paper clearly states that it is not the aim to (generally) formulate the MAML as a HB. Given the advancement in gradient based inference for HB the last couple of years (e.g. variational, nested laplace , expectation propagation etc) for explicit models, could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable/useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ? Minor: -            Sec 4.1 ""...each integral in the sum in (2)..."" eq 2 is a product ",31,789,32.875,5.177658142664872,334,9,780,0.0115384615384615,0.0338461538461538,0.9887,228,89,122,46,11,5,"{'ABS': 3, 'INT': 1, 'RWK': 2, 'PDI': 4, 'DAT': 1, 'MET': 21, 'EXP': 1, 'RES': 6, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 3, 'CLA': 3}",3,1,2,4,1,21,1,6,1,2,0,3,0,0,1,0,0,3,0,0,16,3,3,0.7913736024555028,0.5654948653509311,0.49542643888683785
ICLR2018-BJ_UL-k0b-R2,Accept,"The paper reformulates the model-agnostic meta-learning algorithm (MAML) in terms of inference for parameters of a prior distribution in a hierarchical Bayesian model.  This provides an interesting and, as far as I can tell, novel view on MAML. The paper uses this view to improve the MAML algorithm. The writing of the paper is excellent. Experimental evalution is well done against a number of recently developed alternative methods in favor of the presented method, except for TCML which has been exluded using a not so convincing argument. The overview of the literature is also very well done. ",7,97,13.857142857142858,5.25,64,0,97,0.0,0.0101010101010101,0.9482,25,13,15,9,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,2,0,3,1,0,0,0,0,1,0,0,0,1,0,0,0,0,3,0,1,0.4291311126608536,0.3345772482030192,0.21838045197471923
ICLR2018-BJ_UL-k0b-R3,Accept,"MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure. In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation. Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task. Casting MAML as HB seems a good idea. The paper does a good job of explaining the connection, but I think the presentation could be clarified. The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper. The synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method. Regarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review. However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here. In addition, the point made by the TCML authors is fair ( othing prevented you from...) and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote. This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal. The experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods. I am assessing this paper as 7 because despite the merit of the paper, the relevance of the reformulation of MAML, and the technical steps involved in the reformulation, the paper does not eg address other forms (than L-MAML) of the task-specific subroutine ML-..., and the benchmark improvements are quite small. I think the approach is good and fruitful. # Suggestions on readability  * I have the feeling the paper inverts $alpha, beta$ from their use in Finn 2017 (step size for meta- vs task-training). This is unfortunate and will certainly confuse readers; I advise carefully changing this throughout the entire paper (eg Algo 2,3,4, eq 1, last eq in sec3.1, eq in text below eq3, etc)  * I advise avoiding the use of the symbol f, which appears in only two places in Algo 2 and the end of sec 3.1. This is in part because f is given another meaning in Finn 2017, but also out of general parsimony in symbol use. (could leave the output of ML-... implicit by writing ML-...(theta, T)_j in the $sum_j$; if absolutely needed, use another symbol than f)  * Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping (eg avoiding to split them between end of 3.1 and 3.2). * sec6 Machine learning and deep learning: I would definitely avoid this formulation, seems to tail in with all the media nonsense on what's the difference between ML and DL ?. In addition the formulation seems to contrast ML with hierarchical Bayesian modeling, which does not make sense/ is wrong and confusing. # Typos  * sec1 second parag: did you really mean in the architecture or loss function? unclear. * sec2: over a family * common structure, so that (not such that) * orthgonal * sec2.1 suggestion: clarify that theta and phi are in the same space * sec2.2 suggestion: task-specific parameter $phi_j$ is distinct from ... parameters $phi_{j'}, j'  eq j} * unless an approximate ... is provided (the use of the subjunctive here is definitely dated :-) ) * sec3.1 task-specific parameters $phi_j$ (I would avoid writing just phi altogether to distinguish in usage from theta) * Gaussian-noised * approximation of the it objective * before eq9: that solves: well, it doesn't really solve the minimisation, in that it is not a minimum; reformulate this? * sec4.1 innaccurate * well approximated * sec4.2 an curvature * (Amari 1989) * For the the Laplace * O(n^3) : what is n ? * sec5.2 (Ravi and L 2017) * for the the  ",30,673,26.92,5.0095238095238095,311,7,666,0.0105105105105105,0.0154277699859747,0.962,210,79,113,40,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 17, 'EXP': 8, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 6, 'REC': 1, 'EMP': 10, 'SUB': 0, 'CLA': 4}",0,1,2,0,1,17,8,1,1,0,0,3,2,0,0,1,0,0,6,1,10,0,4,0.6476473057429806,0.5617044223007212,0.4114234266109229
ICLR2018-BJ_wN01C--R1,Accept,"In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs. Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning. Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space), but the initial results are quite promising.   It would also be interesting to understand more fully how performance scales to larger networks. If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales. Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?   As a last minor comment, the authors should specify explicitly what the shaded areas are in Fig. 4b,c.",8,177,25.285714285714285,5.554913294797688,120,1,176,0.0056818181818181,0.0165745856353591,0.9493,40,26,35,15,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 0}",0,1,1,1,0,4,0,3,1,0,1,0,0,0,0,1,1,0,1,0,1,2,0,0.5008619801267729,0.555662232063877,0.315897634860002
ICLR2018-BJ_wN01C--R2,Accept,"The authors provide a novel, interesting, and simple algorithm capable of training with limited memory. The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well.  However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs. Second, the relationship to existing work needs to be explained better.   Pro: The algorithm is clearly explained, well-motivated, and empirically supported.   Con: The relationship to stochastic gradient markov chain monte carlo needs to be explained better. In particular, the update form was first introduced in [1], the annealing scheme was analyzed in [2], and the reflection step was introduced in [3].  These relationships need to be explained clearly. The evidence is presented on very small input data.  With something like natural images, the parameterization is much larger and with more data, the number of total parameters is much larger. Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases?  This would require a smaller ratio of included parameters. [1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688). [2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May. Bridging the gap between stochastic gradient MCMC and stochastic optimization. In Artificial Intelligence and Statistics(pp. 1051-1060).    [3] Patterson, S. and Teh, Y.W., 2013. Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems (pp. 3102-3110).    ",19,260,9.62962962962963,5.786610878661088,138,2,258,0.0077519379844961,0.0181818181818181,0.9896,92,43,39,12,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 5, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 8, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 11, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 5, 'CLA': 0}",0,0,3,0,5,6,0,0,0,2,0,0,8,0,0,1,0,11,1,0,2,5,0,0.3588106268905995,0.5577758445955842,0.22641934165687147
ICLR2018-BJ_wN01C--R3,Accept,"This paper presents an iterative approach to sparsify a network already during training. During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold. This is a big advantage when training is performed on hardware with computational limitations, in comparison to post-hoc sparsification methods, that compress the network after training. The method is derived by considering the rewiring of an (artificial) neural network as a stochastic process. This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach.  References to previous work in this area are missing, e.g.  [1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network Models, Neural Computation 2000 [2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011   Especially the stochastic gradient method in [2] is strongly related to the existing approach. Positive aspects  - The presented approach is well grounded in the theory of stochastic processes. The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process  - By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.   - The method is specifically designed for online learning with limited hardware ressources.   Negative aspects  - The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C). Especially the results on MNIST suggest that this method is most advantageous for very high compression levels. However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.   - A detailled discussion of the relation to previously existing very similar work is missing (see above) Technical Remarks  Fig. 1, 2 and 3 are referenced on the pages following the page containing the figure. Readibility could be slightly increased by putting the figures on the respective pages. ",17,339,17.842105263157894,5.7578616352201255,174,0,339,0.0,0.0055710306406685,0.9239,110,41,56,16,10,7,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 2, 'MET': 11, 'EXP': 4, 'RES': 2, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,1,4,1,2,11,4,2,2,1,0,0,1,0,0,2,1,4,2,0,4,1,1,0.7174027770991485,0.780101814262198,0.5725890217079633
ICLR2018-BJaU__eCZ-R1,Reject,Quality  This is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging. Clarity  The paper is well-written. Methods and results are clearly described. The authors state significant improvements in classification using generated data. These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets. Originality  This is one of the first uses of GANs in the context of neuroimaging. Significance   The approach outlined in this paper may spawn a new research direction. Pros  Well-written and original contribution demonstrating the use of GANs in the context of neuroimaging. Cons  The focus on neuroimaging might be less relevant to the broader AI community.,9,115,12.77777777777778,5.79646017699115,73,2,113,0.0176991150442477,0.0327868852459016,0.9571,38,12,20,4,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 3}",0,0,0,1,2,4,0,3,0,0,0,3,0,0,1,2,1,0,0,0,3,0,3,0.3581317656256246,0.5570068481514677,0.22254313366696937
ICLR2018-BJaU__eCZ-R2,Reject,"This paper proposes to use 3D conditional GAN models to generate fMRI scans. Using the generated images, paper reports improvement in classification accuracy on various tasks. One claim of the paper is that a generative model of fMRI data can help to caracterize and understand variability of scans across subjects. Article is based on recent works such as Wasserstein GANs and AC-GANs by (Odena et al., 2016). Despite the rich literature of this recent topic the related work section is rather convincing. Model presented extends IW-GAN by using 3D convolution and also by supervising the generator using sample labels. 1  Major:  - The size of the generated images is up to 26x31x22 which is limited (about half the size of the actual resolution of fMRI data). As a consequence results on decoding learning task using low resolution images can end up worse than with the actual data (as pointed out). What it means is that the actual impact of the work is probably limited. - Generating high resolution images with GANs even on faces for which there is almost infinite data is still a challenge. Here a few thousand data points are used. So it raises too concerns: First is it enough? Using so-called learning curves is a good way to answer this. Second is what are the contributions to the state-of-the-art of the 2 methods introduced? Said differently, as there is no classification results using images produced by an another GAN architecture it is hard to say that the extra complexity proposed here (which is a bit contribution of the work) is actually necessary. Minor:  - Fonts in figure 4 are too small. ",16,271,18.066666666666663,4.918918918918919,149,1,270,0.0037037037037037,0.0323741007194244,-0.1984,72,35,50,16,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 4, 'MET': 6, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,1,2,0,4,6,0,4,1,0,0,0,0,0,0,0,0,2,1,0,6,3,0,0.4301782549405447,0.4478847471924922,0.23977723759539515
ICLR2018-BJaU__eCZ-R3,Reject,"The work is motivated by a real challenge of neuroimaging analysis: how to increase the amount of data to support the learning of brain decoding. The contribution seems to mix two objectives: on one hand to prove that it is possible to do data augmentation for fMRI brain decoding, on the other hand to design (or better to extend) a new model (to be more precise two models). Concerning the first objective the empirical results do not provide meaningful support that the generative model is really effective. The improvement is really tiny and a statistical test (not included in the analysis) probably wouldn't pass a significant threshold. This analysis is missing a straw man. It is not clear whether the difference in the evaluation measures is related to the greater number of examples or by the specific generative model. Concerning the contribution of the model, one novelty is the conditional formulation of the discriminator. The design of the empirical evaluation doesn't address the analysis of the impact of this new formulation. It is not clear whether the supposed improvement is related to the conditional formulation. Figure 3 and Figure 5 illustrate the brain maps generated for Collection 1952 with ICW-GAN and for collection 503 with ACD-GAN. It is not clear how the authors operated the choices of these figures. From the perspective of neuroscience a reader,  would expect to look at the brain maps for the same collection with different methods. The pairwise brain maps would support the interpretation of the generated data. It is worthwhile to remember that the location of brain activations is crucial to detect whether the brain decoding (classification) relies on artifacts or confounds. Minor comments - typos: a first application or this  > a first application of this (p.2) - qualitative quality (p.2)",15,295,19.666666666666668,5.238434163701068,133,2,293,0.0068259385665529,0.0466666666666666,0.9729,83,35,47,11,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 6, 'EXP': 0, 'RES': 2, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 1}",0,1,0,1,3,6,0,2,2,3,0,1,0,2,0,1,1,0,0,0,6,2,1,0.6443180515886588,0.5587720192380918,0.39555815799503585
ICLR2018-BJcAWaeCW-R1,Reject,"The authors try to combine the power of GANs with hierarchical community structure detections. While the idea is sound, many design choices of the system is questionable. The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers. I do not recommend the acceptance of this draft. Compared with GAN, traditional graph analytics is model-specific and non-adaptive to training data. This is also the case for hierarchical community structures. By building the whole architecture on the Louvain method, the proposed method is by no means truly model-agnostic. In fact, if the layers are fine enough, a significant portion of the network structure will be captured by the sum-up module instead of the GAN modules, rendering the overall behavior dominated by the community detection algorithm. The evaluation remains superficial with minimal quantitative comparisons. Treating degree distribution and clustering coefficient (appeared as cluster coefficient in draft) as global features is problematic. They are merely global average of local topological features which is incapable of capturing true long-distance structures in graphs. The writing of the draft leaves much to be desired. The description of the architecture is confusing with design choices never clearly explained. Multiple concepts needs better introduction, including the very name of their model GTI and the idea of stage identification. Not to mention numerous grammatical errors, I suggest the authors seek professional English writing services.",16,231,15.4,5.7368421052631575,146,0,231,0.0,0.0173160173160173,-0.9568,74,33,40,12,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 9, 'SUB': 0, 'CLA': 2}",0,0,1,2,0,8,2,1,1,0,0,5,0,0,0,0,0,1,2,1,9,0,2,0.5020785156822594,0.5606739190289568,0.31802004485095764
ICLR2018-BJcAWaeCW-R2,Reject,"Quality: The work has too many gaps for the reader to fill in. The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator. I am not sure how this is achieved in this work. The matrix is not isomorphic invariant and the different clusters don't share a common model. Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters. How can such a limited technique be generalizing? There is no metric in the results showing how the model generalizes, it may be just overfitting the data. n Clarity: The paper organization needs work; there are also some missing pieces to put the NN training together. It is only in Section 2.3 that the nature of G_i^prime becomes clear, although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation. From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function. If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio's path derivative estimator? This is a key point not discussed in the manuscript. And if I misunderstood the sentence ""turn re_G into a binary matrix"" and the values are continuous, wouldn't the discriminator have an easy time distinguishing the generated data from the real data. And wouldn't the generator start working towards vanishing gradients in its quest to saturate the re_G output? Originality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster. There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work.  There is no contribution in the GAN / neural network aspect. It is also unclear whether the model generalizes. I don't think this is a good fit for ICLR. Significance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases. The work itself, however, falls short of the goal. At best the generator seems to be working but I fear it is overfitting. The contribution for ICLR is rather minimal, unfortunately. Minor:  GTI was not introduced before it is first mentioned in the into. Y. Bengio, N. Leonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432, 2013.  ",26,418,14.413793103448276,5.190839694656488,218,3,415,0.0072289156626506,0.0330969267139479,0.6793,116,52,76,23,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 2, 'NOV': 1, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 1, 'CLA': 1}",0,1,3,1,0,14,2,1,0,0,0,4,0,1,2,1,2,1,0,0,15,1,1,0.5750644969284003,0.7865428844249954,0.4494066516356471
ICLR2018-BJcAWaeCW-R3,Reject,"The proposed approach, GTI, has many free parameters: number of layers L, number of communities in each layer, number of non-overlapping subgraphs M, number of nodes in each subgraph k, etc. No analysis is reported on how these affect the performance of GTI. GTI uses the Louvain hierarchical community detection method to identify the hierarchy in the graph and METIS to partition the communities. How important are these two methods to the success of GTI? Why is it reasonable to restore a k-by-k adjacency matrix from the standard uniform distribution (as stated in Section 2.1)? Why is the stride for the convolutional/deconvoluational layers set to 2 (as stated in Section 2.1)? Equation 1 has a symbol E in it. E is defined (in Section 2.2) to be all the inter-subgraph (community) edges identified by the Louvain method for each hierarchy.   However, E can be intra-community because communities are partitioned by METIS. More discussion is needed about the role of edges in E. Equation 3 sparsifies (i.e. prunes the edges) of a graph -- namely $re_{G}$. However, it is not clear how one selects a $re^{i}{G}$ from among the various i values. The symbol i is an index into $CV_{i}$, the cut-value of the i-th largest unique weight-value. Was the edge-importance reported in Section 2.3 checked against various measures of edge importance such as edge betweenness? Table 1 needs more discussion in terms of retained edge percentage for ordered stages. Should one expect a certain trend in these sequences? Almost all of the experiments are qualitative and can be easily made quantitive by comparing PageRank or degree of nodes. The discussion on graph sampling does not include how much of the graph was sampled. Thus, the comparisons in Tables 2 and 3 are not fair.  The most realistic graph generator is the BTER model.  See http://www.sandia.gov/~tgkolda/bter_supplement/ and http://www.sandia.gov/~tgkolda/feastpack/doc_bter_match.html.  A minor point: The acronym GTI is never defined.",19,315,17.5,5.237288135593221,161,0,315,0.0,0.0093457943925233,0.9486,99,37,51,11,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 13, 'EXP': 1, 'RES': 0, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,0,0,0,0,13,1,0,4,1,0,0,0,0,0,0,0,0,2,0,6,3,0,0.2887864849618215,0.3367180424479944,0.14738671646842497
ICLR2018-BJehNfW0--R1,Accept,"The paper adds to the discussion on the question whether Generative Adversarial Nets (GANs) learn the target distribution. Recent theoretical analysis of GANs by Arora et al. show that of the discriminator capacity of is bounded, then there is a solution the closely meets the objective but the output distribution has a small support. The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally. The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1/2, a uniform sample (with replacement) of size S from a set of  N elements will have a duplicate given S > sqrt{N}. The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists, then estimate the size of the support to be s^2. One should note that the birthday theorem assumes uniform sampling.  In the revised versions, it has been clarified that the tested distribution is not assumed to be uniform but the distribution has effectively small support size using an indistinguishability notion. Given this method to estimate the size of the support, the paper also tries to study the behaviour of estimated support size with the discriminator capacity. Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity. Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above. A result similar to that of Arora et al. is also given for the special case of Encoder-Decoder GAN. Evaluation:  Significance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value. Clarity: The paper is written well and the issues raised are well motivated and proper background is given. Originality: The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new. Quality: The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs.  ",15,355,18.68421052631579,5.123529411764705,147,1,354,0.0028248587570621,0.0276243093922651,0.9969,113,32,66,14,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 5, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,3,5,0,7,2,1,0,0,0,2,0,0,0,0,1,0,0,0,3,0,1,0.5020309425346582,0.3345772482030192,0.2510406040364657
ICLR2018-BJehNfW0--R2,Accept,"This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples. The main goal is to quantify mode collapse in state-of-the-art generative models. The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse. Using the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1). The results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models. Bidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs. Finally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator. This is a very interesting area and exciting work. The main idea behind the proposed test is very insightful.  The main theoretical contribution stimulates and motivates much needed further research in the area. In my opinion both contributions suffer from some significant limitations. However, given how little we know about the behavior of modern generative models, it is a good step in the right direction. 1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity.  The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2. My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity. Anecdotally, real-life distributions are far from uniform, so this should be a common issue. I would still use the test as a part of a suite of measurements, but I would not solely rely on it. I feel that the authors should give a more prominent disclaimer to potential users of the test. 2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing. The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution. As long as they yield collisions at the same rate, these two generative models are 'equally diverse'. Isn't coverage of equal importance? 3. The other main contribution of the paper is Theorem 3, which showsu2014via a very particular construction on the generator and encoderu2014that bidirectional GANs can also suffer from serious mode collapse.  I welcome and are grateful for any theory in the area. This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice.  In light of this, the statement in the introduction that ""encoder-decoder training objectives cannot avoid mode collapse"" might need to be qualified. In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.",26,519,17.3,5.380081300813008,243,4,515,0.0077669902912621,0.0305927342256214,-0.5282,138,65,79,38,8,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 5, 'DAT': 1, 'MET': 15, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 3, 'CLA': 0}",0,2,0,5,1,15,2,2,0,0,1,2,0,0,0,0,1,0,1,0,11,3,0,0.5753812958907638,0.4508773718095171,0.32224675820745263
ICLR2018-BJehNfW0--R3,Accept,"The article Do GANs Learn the Distribution? Some Theory and Empirics considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images. The authors argue that GANs in fact generate the distributions with fairly low support. The proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample. This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017). The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective. The experimental part of the paper considers the CelebA and CIFAR-10 datasets. We definitely see many very similar images in fairly small sample generated. So, the general claim is supported. However, if you look closely at some pictures, you can see that they are very different though reported as similar. For example, some deer or truck pictures. That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates. To sum up, I think that the general idea looks very natural and the results are supportive. On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further.",14,258,17.2,5.238095238095238,148,4,254,0.0157480314960629,0.0348837209302325,0.9612,67,33,43,22,9,2,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 2, 'RES': 3, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,2,1,2,4,2,3,3,2,0,0,0,2,0,0,0,2,0,0,5,0,0,0.6440025407880885,0.2248272145187841,0.28647353688874533
ICLR2018-BJgPCveAW-R1,Reject,"This paper examines sparse connection patterns in upper layers of convolutional image classification networks. Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks. Heuristics for distributing connections among windows/groups and a measure called scatter are introduced to construct the connectivity masks, and evaluated experimentally on CIFAR-10 and -100, MNIST and Morse code symbols. While it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions. I've described many of the points I was confused by in more detailed comments below. Detailed comments and questions:   The distribution of connections in windows are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image. But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure. So are do the windows correspond to spatial windows, and if so, how? Or are they different (maybe arbitrary) groupings over the feature maps? Also a bit confusing is the notation conv2, conv3, etc. These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example). But here it seems just to indicate the number of CL layers: 2.  And p.1 says that the CL layers are those often referred to as FC layers, not conv (though they may be convolutionally applied with spatial 1x1 kernels). The heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but I'm not convinced this will work well in all situations, and may even be sub-optimal for the examined datasets. For example, to distinguish MNIST 1 vs 7 vs 9, it is most important to see the top-left:  whether it is empty, has a horizontal line, or a loop. So some regions are more important than others, and the top half may be more important than an equally spaced global view. So the description of how to space connections between windows makes some intuitive sense, but I'm unclear on whether other more general connections might be even better, including some that might not be as easily analyzed with the scatter metric described. Another broader question I have is in the distinction between lower and upper layers (those referred to as feature extracting and classification in this paper). It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret).  So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the classification function is pushed down to lower layers, as the upper layers are reduced in size. How would they respond to similar reductions? I'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think? I'm also unsure whether the windows are over spatial extent only, or over features.",22,580,30.526315789473685,4.967567567567568,239,17,563,0.0301953818827708,0.0494037478705281,0.301,139,77,94,43,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 4, 'MET': 16, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 13, 'SUB': 3, 'CLA': 0}",0,1,0,0,4,16,2,2,0,0,0,1,0,0,0,0,3,0,3,0,13,3,0,0.4325744801822971,0.4523404978237055,0.23810012298346572
ICLR2018-BJgPCveAW-R2,Reject,"The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers. Numerical experiments show that such sparse networks can have similar performance to fully connected ones. They introduce a concept of ""scatter"" that correlates with network performance. Although  I found the results useful and potentially promising, I did not find much insight in this paper. It was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer. Once the signals from different windows are intermixed, how do you even define the windows? Minor Second line of Section 2.1: ""lesser"" -> less or fewer ",8,119,19.83333333333333,5.300884955752212,88,0,119,0.0,0.0655737704918032,0.5119,34,16,20,10,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,5,0,1,0,0,0,1,0,0,0,0,0,0,0,0,4,0,0,0.3581436255979958,0.11297698341564,0.13814962096135058
ICLR2018-BJgPCveAW-R3,Reject,"The paper seems to claims that 1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters, 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices. I have two problems with these claims: 1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers. 2) The authors reject the technique of 'Deep compression' as being impractical. I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network. p3. What does 'normalized' mean? Batch-norm? p3. Are you using an L2 weight penalty? If not, your fully-connected baseline may be unnecessarily overfitting the training data. p3. Table 1. Where do the choice of CL Junction densities come from? Did you do a grid search to find the optimal level of sparsity at each level? p7-8. I had trouble following the left/right & front/back notation. p8. Figure 7. How did you decide which data points to include in the plots?",13,181,12.928571428571429,5.296969696969697,118,2,179,0.0111731843575419,0.0329670329670329,-0.6966,60,18,35,8,5,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 2, 'MET': 8, 'EXP': 2, 'RES': 0, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,0,0,3,2,8,2,0,3,0,0,0,0,0,0,0,0,0,0,0,11,0,0,0.3591816841160767,0.1173306854595408,0.14376672156636974
ICLR2018-BJgVaG-Ab-R1,Reject,"The paper argues for structured task representations (in TLTL) and shows how these representations can be used to reuse learned subtasks to decrease learning time. Overall, the paper is sloppily put together, so it's a little difficult to assess the completeness of the ideas. The problem being solved is not literally the problem of decreasing the amount of data needed to learn tasks, but a reformulation of the problem that makes it unnecessary to relearn subtasks. That's a good idea, but problem reformulation is always hard to justify without returning to a higher level of abstraction to justify that there's a deeper problem that remains unchanged. The paper doesn't do a great job of making that connection. The idea of using task decomposition to create intrinsic rewards seems really interesting, but does not appear to be explored in any depth. Are there theorems to be had? Is there a connection to subtasks rewards in earlier HRL papers? The lack of completeness (definitions of tasks and robustness) also makes the paper less impactful than it could be. Detailed comments:  learn hierarchical policies -> learns hierarchical policies? games Mnih et al. (2015)Silver et al. (2016),: The citations are a mess. Please proof read. and is hardly reusable -> and are hardly reusable. Skill composition is the idea of constructing new skills with existing skills ( -> Skill composition is the idea of constructing  new skills out of existing skills (.  to synthesis -> to synthesize. set of skills are -> set of skills is.  automatons -> automata. with low-level controllers can -> with low-level controllers that can. the options policy u03c0 o is followed until u03b2(s) > threshold: I don't think that's how options were originally defined... beta is generally defined as a termination probability. The translation from TLTL formula FSA to -> The translation from TLTL formula to FSA? four automaton states Qu03c6   {q0, qf , trap}: Is it three or four? learn a policy that satisfy -> learn a policy that satisfies. HRL, We introduce the FSA augmented MDP -> HRL, we introduce the FSA augmented MDP..  multiple options policy separately ->  multiple options policies separately? Given flat policies u03c0u03c61 and u03c0u03c62 that satisfies  -> Given flat policies u03c0u03c61 and u03c0u03c62 that satisfy . s illustrated in Figure 3 . -> s illustrated in Figure 2 .? , we cam simply -> , we can simply. Figure 4 <newline> . -> Figure 4. .  , disagreement emerge -> , disagreements emerge? The paper needs to include SOME definition of robustness, even if it just informal. As it stands, it's not even clear if larger  values are better or worse. (It would seem that *more* robustness is better than less, but the text says that lower values are  chosen.) with 2 hidden layers each of 64 relu: Missing word?Or maybe a comma? to aligns with -> to align with.  a set of quadratic distance function ->  a set of quadratic distance functions. satisfies task the specification) -> satisfies the task specification). Figure 4: Tasks 6 and 7 should be defined in the text someplace. current frame work i -> current framework i.  and choose to follow ->  and chooses to follow.  this makes ->  making. each subpolicies -> each subpolicy. ",40,503,13.236842105263158,5.201271186440678,231,3,500,0.006,0.0268817204301075,0.9539,166,56,111,29,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 4, 'DAT': 1, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 5, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 23}",0,1,1,4,1,6,0,0,3,1,0,5,2,0,0,0,1,0,3,0,5,2,23,0.6445266671700862,0.5600581712096449,0.4041122350170386
ICLR2018-BJgVaG-Ab-R2,Reject,"I very much appreciate the objectives of this paper:  learning compositional structures is critical for scaling and transfer. The first part of the paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula, and reminds us that we can learn within that restricted MDP. Some previous work is cited, but I would point the authors to much older work of Parr and Russell on HAMs (hierarchies of abstract machines) and later work by Andre and Russell, which did something very similar (though, indeed, not in hybrid domains). The idea of extracting policies corresponding to individual automaton states and making them into options seems novel, but it would be important to argue that those options are likely to be useful again under some task distribution. The second part offers an exciting result:  If we learn policy pi_1 to satisfy objective phi_1 and policy pi_2 to satisfy objective phi_2, then it will be possible to switch between pi_1 and pi_2 in a way that satisfies phi_1 ^ phi_2. This just doesn't make sense to me. What if phi_1 is o ((A v B) Until C) and phi_2 is o ((not A v B) Until C). Let's assume that o(B Until C) is satisfiable, so the conjunction is satisfiable. However, we may find policy pi_1 that makes A true and B false (in general, there is no single optimal policy) and find pi_2 that makes A false and B false, and it will not be possible to satisfy the phi_1 and phi_2 by switching between the policies. But, perhaps I am misunderstanding something. Some other smaller points: - zero-shot skill composition sounds a lot like what used to be called planning or reasoning  - The function rho is originally defined on whole trajectories but in eq 7 it is only on a single s':  I'm not sure exactly what that means. - Section 4:  How is as soon as possible encoded in this objective? - How does the fixed horizon interact with conjoining goals? - There are many small errors in syntax;  it would be best to have this paper carefully proofread.",16,355,29.58333333333333,4.687687687687688,187,4,351,0.0113960113960113,0.0354223433242506,0.989,89,49,65,20,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,0,1,3,0,10,2,2,0,0,0,2,0,1,0,1,0,1,0,0,10,0,1,0.5025576452402422,0.4500420613580311,0.2798713597915956
ICLR2018-BJgVaG-Ab-R3,Reject,"This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition. The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition. The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true.  The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T. The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho. The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy. The proposed method is evaluated on a small Markov chain and a simulated Baxter robot. The main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear. The robustness term rho is essential, but it is not defined. I was also confused by the notation $D_phi^q$, which was described but not defined. The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho. The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored. This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula. It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals. is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals? In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards. ",16,376,23.5,5.260623229461756,169,2,374,0.0053475935828877,0.0264550264550264,0.9529,103,46,64,17,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 0}",0,1,1,1,0,14,2,1,0,1,0,1,0,0,0,0,0,1,1,0,11,2,0,0.5747672646994807,0.4507706953011956,0.32320308464067665
ICLR2018-BJgd7m0xRZ-R1,Reject,"The authors propose a defense against attacks on the security of one-class SVM based anonaly detectors. The core idea is to perform a random projection of the data (which is supposed to decrease the impact from adversarial distortions).  The approach is empirically tested on the following data: MNIST, CIFAR, and SVHN. The paper is moderately well written and structured. Command of related work is ok, but some relevant refs are missing (e.g., Kloft and Laskov, JMLR 2012). The empirical results actually confirm that indeed the strategy of reducing the dimensionality using random projections reduces the impact from adversarial distortions. This is encouraging. What the paper really lacks in my opinion is a closer analysis of *why* the proposed approach works, i.e., a qualitative empirical analysis (toy experiment?)  or theoretical justification. Right now, there is no theoretical justification for the approach, nor even a (in my opinion) convincing movitation/Intuition behind the approach. Also, the attack model should formally introduced. In summary, I d like to encourage the authors to further investigate into their approach, but I am not convinced by the manuscript in the current form. It lacks both in sound theoretical justification and intuitive motivation of the approach. The experiments, however, show clearly advantages of the approach  (again, here further experiments are necessary, e.g., varying the dose of adversarial points). ",15,220,15.714285714285714,5.514150943396227,135,0,220,0.0,0.0223214285714285,0.8133,59,29,38,16,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 4, 'CLA': 1}",0,1,2,2,1,7,3,2,0,2,0,2,1,0,0,0,0,0,1,0,7,4,1,0.7162248863974556,0.4484962185784666,0.40349697662211065
ICLR2018-BJgd7m0xRZ-R2,Reject,"In this paper, the authors explore how using random projections can be used to make OCSVM robust to adversarially perturbed training data. While the intuition is nice and interesting, the paper is not very clear in describing the attack and the experiments do not appropriately test whether this method actually provides robustness. Details: have been successfully in anomaly detection --> have been successfully used in anomaly detectionP The adversary would select a random subset of anomalies, push them towards the normal data cloud and inject these perturbed points into the training set -- This seems backwards. As in the example that follows, if the adversary wants to make anomalies seem normal at test time, it should move normal points outward from the normal point cloud (eg making a 9 look like a weird 7). As s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. -- This seems backwards from Fig 2. From (a) to (b) the red points move closer to the center while in (c) they move further away (why?). The blue points seem to consistently become more dense from (a) to (c). The attack model is too rough. It seems that without bounding D, we can make the model arbitrarily bad, no?  Assumption 1 alludes to this but doesn't specify what is small? Also the attack model is described without considering if the adversary knows the learner's algorithm. Even if there is randomness, can the adversary take actions that account for that randomness? Does selecting a projection based on compactness remove the randomness? Experiments -- why/how would you have distorted test data? Making an anomaly seem normal by distorting it is easy. I don't see experiments comparing having random projections and not. This seems to be the fundamental question -- do random projects help in the train_D | test_C case? Experiments don't vary the attack much to understand how robust the method is.",20,322,24.76923076923077,5.1237785016286646,165,4,318,0.0125786163522012,0.033434650455927,-0.473,81,33,74,23,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 13, 'EXP': 6, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 18, 'SUB': 0, 'CLA': 1}",0,1,0,2,2,13,6,0,1,0,0,0,0,0,0,0,0,1,1,0,18,0,1,0.4321001034971372,0.4550177208367749,0.2423154410637232
ICLR2018-BJgd7m0xRZ-R3,Reject,"Although the problem addressed in the paper seems interesting, but there lacks of evidence to support some of the arguments that the authors make. And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference. Detailed critiques are as following:1. The idea proposed by the authors seems too quite simple. It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM. 2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack}   0.5. It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change. 3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that. And PCA as a non-random projection would a nice baseline to compare against. 4. The paper seems to misuse the term ""False positive rate"" as the y label of figure 3(d/e/f). The definition of false positive rate is FP/(FP+TN), so if the FPR 1 it means that all negative samples are labeled as positive. So it is surprising to see FPR 1 in Figure 3(d) when feature dimension 784 while the f1 score is still high in Figure 3(a). From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified. The paper should come up with a better term for that evaluation. 5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d). Need more experiment on more dimensionality to prove that.  6. In the appendix B results part, sometimes the word 'S_attack' is typed wrong. And the values in  ""distorted/distorted"" columns in Table 5 do not match up with the ones in Figure 3(c).",16,392,17.043478260869566,5.1035422343324255,182,4,388,0.0103092783505154,0.0227272727272727,0.9283,111,43,63,18,8,7,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 2, 'MET': 3, 'EXP': 3, 'RES': 3, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 1}",0,0,1,4,2,3,3,3,3,0,0,1,0,0,1,1,0,1,2,0,8,3,1,0.572473337271851,0.7824064017621248,0.4501790042669158
ICLR2018-BJhxcGZCW-R1,Reject,"SUMMARY.  The paper presents a variational autoencoder for generating entity pairs given a relation in a medical setting. The model strictly follows the standard VAE architecture with an encoder that takes as input an entity pair and a relation between the entities. The encoder maps the input to a probabilistic latent space. The latent variables plus a one-hot-encoding representation of the relation is used to reconstruct the input entities. Finally, a generator is used to generate entity pairs give a relation. ----------  OVERALL JUDGMENT The paper presents a clever use of VAEs for generating entity pairs conditioning on relations. My main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set. If this is the case, all the analysis and results obtained are almost meaningless. I suggest the authors make clear if they used the split training, validation, test. Until then it is not possible to draw any conclusion from this work. Assuming the experimental setting is correct, it is not clear to me the reason of having the representation of r (one-hot-vector of the relation) also in the decoding/generation part. The hidden representation obtained by the encoder should already capture information about the relation. Is there a specific reason for doing so?  ",13,214,15.285714285714286,5.336633663366337,117,1,213,0.0046948356807511,0.045662100456621,0.2045,66,16,39,10,8,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 8, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,0,1,1,8,6,1,0,1,0,1,0,0,0,0,0,0,0,0,8,0,0,0.5736108951836075,0.1154648131550119,0.2262756612857612
ICLR2018-BJhxcGZCW-R2,Reject,"The authors suggest using a variational autoencoder to infer binary relationships between medical entities. The model is quite simple and intuitive and the authors demonstrate that it can generate meaningful relationships between pairs of entities that were not observed before. While the paper is very well-written I have certain concerns regarding the motivation, model, and evaluation methodology followed: 1) A stronger motivation for this model is required. Having a generative model for causal relationships between symptoms and diseases is intriguing yet I am really struggling with the motivation of getting such a model from word co-occurences in a medical corpus. I can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model. 2) The word embeddings used seem to be sufficient to capture the knowledge included in the corpus. An ablation study of the impact of word embeddings on this model is required. 3) The authors do not describe how the data from xywy.com were annotated. Were they annotated by experts in the medical domain or random users? 4) The metric of quality is particularly ad-hoc.",12,206,22.88888888888889,5.461139896373057,116,0,206,0.0,0.0145631067961165,0.8361,59,20,42,9,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 5, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 1}",0,0,0,1,5,7,1,1,0,1,0,1,0,0,0,0,0,0,0,0,7,2,1,0.5016455156422819,0.3371717544507126,0.25480597514846304
ICLR2018-BJhxcGZCW-R3,Reject,"In the medical context, this paper describes the classic problem of knowledge base completion from structured data only (no text). The authors argue for the advantages of a generative VAE approach (but without being convincing). They do not cite the extensive literature on KB completion. They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods. The authors seem unaware of a large literature on knowledge base completion.  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015], The paper claims that operating on pre-structured data only (without using text) is an advantage. I don't find the argument convincing. There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. universal schema [Riedel et al, 2014]. The paper claims that discriminative approaches need to iterate over all possible entity pairs to make predictions.  In their generative approach they say they find outputs by  earest neighbor search. But the same efficient search is possible in many of the classic discriminatively-trained KB completion models also. It is admirable that the authors use an interesting (and to my knowledge novel) data set. But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995. The method is evaluated only against their own VAE-based alternatives.  It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs. ",15,280,15.555555555555555,5.281481481481482,156,0,280,0.0,0.0174825174825174,0.9677,112,44,36,13,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 5, 'DAT': 3, 'MET': 5, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 0}",0,1,3,5,3,5,1,1,0,0,0,0,1,0,0,0,0,2,0,0,4,3,0,0.5728996966156881,0.3355297212116951,0.2896744932724037
ICLR2018-BJij4yg0Z-R1,Accept,"The paper takes a recent paper of Zhang et al 2016 as the starting point to investigate the generalization capabilities of models trained by stochastic gradient descent. The main contribution are scaling rules that relate the batch size k used in SGD with the learning rate epsilon, most notably epsilon/k   const for optimal scaling. First of all, I have to say that the paper is very much focussed on the aforementioned paper, its experiments as well as its (partially speculative) claims. This, in my opinion, is a biased and limited starting point, which ignores much of the literature in learning theory. Chapter 2 provides a sort of a mini-tutorial to (Bayesian) model selection based on standard Bayes factors. I find this of limited usefulness. First of all, I find the execution poor in the details:  (i) Why is omega limited to a scalar? Nothing major really depends on that. Later the presentation switches to a more general case. (ii) What is a one-hot label? One-hot is the encoding of a categorical label. (iii) In which way is a Gaussian prior uncorrelated, if there is just a scalar random variable?  (iv) How can one maximize a probability density function? (v) Why is an incorrect pseudo-set notation used instead of the correct vectorial one? (vi) Exponentially large, reasonably prior model etc. is very vague terminology (vii) No real credit is given for the Laplace approximation presented up to Eq. 10. For instance, why not refer to the seminal paper by Kass & Raferty? Why spend so much time on a step-by-step derivation anyway, as this is all classic and has been carried out many times before (in a cleaner write-up)? (viii) P denotes the number of model parameters (I guess it should be a small p? hard to decipher) (ix) Usually, one should think of the Laplace approximation and the resulting Bayes factors more in terms of a volume of parameters  close to the MAP estimate, which is what the matrix determinant expresses, more than any specific direction of curvature. Chapter 3 constructs a simple example with synthetic data to demonstrate the effect of Bayes factors. I feel the discussion to be too much obsessed by the claims made in Zhang et al 2016 and in no way suprising. In fact, the toy example is so much of a toy that I am not sure what to make of it. Statistics has for decades successfully used criteria for model selection, so what is this example supposed to proof (to whom?). Chapter 4 takes the work of Mandt et al as a starting point to understand how SGD with constant step size effectively can be thought of as gradient descent with noise, the amplitude of which is controlled by the step size and the mini-batch size. Here, the main goal is to use evidence-based arguments to distinguish good from poor local minima. There is some experimental evidence presented on how to resolve the tradeoff between too much noise (underfitting) and too little (overfitting). Chapter 5 takes a stochastic differential equation as a starting point. I see several issues: (i) It seems that you are not doing much with a SDE, as you diredctly jump to the discretized version (and ignore discussions of it's discretization). So maybe one should not feature the term SDE so prominently. (ii) While it is commonly done, it would be nice to get some insights on why a Gaussian approx. is a good assumption. Maybe you can verify this experimentally (as much of the paper consists of experimental findings) (iii) Eq. 13. Maybe you want this form to indicate a direction you want to move towards,  by I find adding and subtracting the gradient in itself not a very interesting manner of illustartion. (iv) I am not sure in whoch way g is measured, but I guess you are determining it by comparing coefficients.  (v) I am confused by the B_opt propto eps statement. It seems you are scaling to mini-batrch gradient to be in expectation equal to the full gradient (not normalized by N), e.g. it scales ~N. Now, if we think of a mini-batch as being a batched version of single pattern updates, then clearly the effective step length should scale with the batch size, which - because of the batch size normalization with N/B - means epsilon needs to scale with B. Maybe there is something deeper going on here, but it is not obvious to me. (vi) The argument why B ~ N is not clear to me. Is there one or are just making a conjecture? Bottom line: The paper may contribute to the current discussion of the Zhang et al 2016 paper, but I feel  it does not make a significant contribution to the state of knowledge in machine learning. On top of that, I feel the execution of the paper leaves much to be desired.  ",41,809,21.289473684210527,4.8083109919571045,354,12,797,0.015056461731493,0.0364520048602673,-0.5377,204,96,145,59,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 25, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 21, 'SUB': 2, 'CLA': 0}",0,1,4,2,0,25,6,0,0,2,0,2,0,1,0,0,0,3,2,0,21,2,0,0.5782350628756213,0.4572861636878086,0.3240230411753547
ICLR2018-BJij4yg0Z-R2,Accept,"Summary: This paper presents a very interesting perspective on why deep neural networks may generalize well, in spite of their high capacity (Zhang et al, 2017).  It does so from the perspective of Bayesian model comparison, where two models are compared based on their marginal likelihood (aka, their evidence --- the expected probability of the training data under the model, when parameters are drawn from the prior).   It first shows that a simple weakly regularized (linear) logistic regression model over 200 dimensional data can perfectly memorize a random training set with 200 points, while also generalizing well when the class labels are not random (eg, when a simple linear model explains the class labels); this provides a much simpler example of a model generalizing well in spite of high capacity, relative to the experiments presented by Zhang et al (2017).  It shows that in this very simple setting, the evidence of a model correlates well with the test accuracy, and thus could explain this phenomena (evidence is low for model trained on random data, but high for model trained on real data). The paper goes on to show that if the evidence is approximated using a second order Taylor expansion of the cost function around a minimia $w_0$, then the evidence is controlled by the cost at the minimum, and by the logarithm of the ratio of the curvature at the minimum compared to the regularization constant (eg, standard deviation of gaussian prior). Thus, Bayesian evidence prefers minima that are both deep and broad. This provides a way of comparing models in a way which is independent of the model parametrization (unfortunately, however, computing the evidence is intractable for large networks). The paper then discusses how SGD can be seen as an algorithmic way of finding minima with large evidence --- the  oise in the gradient estimation helps the model avoid sharp minima, while the gradient helps the model find deep minima.  The paper shows that SGD can be understood using stochastic differential equations, where the noise scale is approximately aN/((1-m)B) (a   learning rate, N   size of training set, B   batch size, m   momentum).  It argues that because there should be an optimal noise scale (which maximizes test performance), the batch size should be taken proportional to the learning rate, as well as the training set size, and proportional to 1/(1-m). These scaling rules are confirmed experimentally (DNN trained on MNIST). Thus, this Bayesian perspective can also help explain the observation that models trained with smaller batch sizes (noisier gradient estimates) often generalize better than those with larger batch sizes (Kesker et al, 2016). These scaling rules provide guidance on how to increase the batch size, which is desirable for increasing the parralelism of SGD training. Review: Quality: The quality of the work is high. Experiments and analysis are both presented clearly. Clarity: The paper is relatively clear, though some of the connections between the different parts of the paper felt unclear to me: 1) It would be nice if the paper were to explain, from a theoretical perspective, why large evidence should correspond to better generalization, or provide an overview of the work which has shown this (eg, Rissanen, 1983). 2) Could margin-based generalization bounds explain the superior generalization performance of the linear model trained on random vs. non-random data? It seems to me that the model trained on meaningful data should have a larger margin. 3) The connection between the work on Bayesian evidence, and the work on SGD, felt very informal. The link seems to be purely intuitive (SGD should converge to minima with high evidence, because its updates are noisy). Can this be formalized? There is a footnote on page 7 regarding Bayesian posterior sampling -- I think this should be brought into the body of the paper, and explained in more detail. 4) The paper does not give any background on stochastic differential equations, and why there should be an optimal noise scale 'g', which remains constant during the stochastic process, for converging to a minima with high evidene. Are there any theoretical results which can be leveraged from the stochastic processes literature? For example, are there results which prove anything regarding the convergence of a stochastic process under different amounts of noise? 5) It was unclear to me why momentum was used in the MNIST experiments. This seems to complicate the experimental setting. Does the generalization gap not appear when no momentum is used? Also, why is the same learning rate used for both small and large batch training for Figures 3 and 4? If the learning rate were optimized together with batch size (eg, keeping aN/B constant), would the generalization gap still appear? Figure 5a seems to suggest that it would not appear (peaks appear to all have the same test accuracy). 6) It was unclear to me whether the analysis of SGD as a stochastic differential equation with noise scale aN/((1-m)B) was a contribution of this paper. It would be good if it were made clearer which part of the mathematical analysis in sections 2 and 5 are original. 7) Some small feedback: The notation $< x_i >   0$ and $< x_i^2 >   1$ is not explained. Is each feature being normalized to be zero mean, unit variance, or is each training example being normalized? Originality: The works seems to be relatively original combination of ideas from Bayesian evidence, to deep neural network research.  However, I am not familiar enough with the literature on Bayesian evidence, or the literature on sharp/broad minima, and their generalization properties, to be able to confidently say how original this work is. Significance: I believe that this work is quite significant in two different ways: 1) Bayesian evidence provides a nice way of understanding why neural nets might generalize well, which could lead to further theoretical contributions. 2) The scaling rules described in section 5 could help practitioners use much larger batch sizes during training, by simultaneously increasing the learning rate, the training set size, and/or the momentum parameter. This could help parallelize neural network training considerably. Some things which could limit the significance of the work: 1) The paper does not provide a way of measuring the (approximate) evidence of a model. It simply says it is prohibitively expensive to compute for large models.  Can the Gaussian approximation to the evidence (equation 10) be approximated efficiently for large neural networks? 2) The paper does not prove that SGD converges to models of high evidence, or formally relate the noise scale 'g' to the quality of the converged model, or relate the evidence of the model to its generalization performance. Overall, I feel the strengths of the paper outweight its weaknesses. I think that the paper would be made stronger and clearer if the questions I raised above are addressed prior to publication.",46,1129,29.71052631578948,5.2080599812558575,378,11,1118,0.0098389982110912,0.0267934312878133,0.9982,311,134,191,57,10,7,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 5, 'MET': 26, 'EXP': 15, 'RES': 1, 'TNF': 3, 'ANA': 2, 'FWK': 1, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 26, 'SUB': 2, 'CLA': 3}",0,0,3,2,5,26,15,1,3,2,1,6,0,0,0,2,2,3,1,0,26,2,3,0.7224103061827324,0.7939231296462277,0.5723341315771772
ICLR2018-BJij4yg0Z-R3,Accept,"This paper builds on Zhang et al. (2016) (Understanding deep learning requires rethinking generalization). Firstly, it shows experimentally that the same effects appear even for simple models such as linear regression.  It also shows that the phenomenon that sharp minima lead to worse result can be explained by Bayesian evidence. Secondly, it views SGD with different settings as introducing different levels of noises that favors different minima. With both theoretical and experimental analysis, it suggests the optimal batch-size given learning rate and training data size. The paper is well written and provides excellent insights.  Pros: 1. Very well written paper with good theoretical and experimental analysis. 2. It provides useful insights of model behaviors which are attractive to a large group of people in the community. 3. The result of optimal batch size setting is useful to wide range of learning methods. Cons and mainly questions: 1. Missing related work. One important contribution of the paper is about optimal batch sizes, but related work in this direction is not discussed.. There are many related works concerning adaptive batch sizes, such as [1] (a summary in section 3.2 of [2]). 2. It will be great if the author could provide some discussions with respect to the analysis of information bottleneck [3] which also discuss the generalization ability of the model. 3. The result of the optimal mini-batch size depends on the training data size. How about real online learning with streaming data where the total number of data points are unknown?. 4. The results are reported mostly concerning the training iterations, not the CPU time such as in figure 3. It will be fair/interesting to see the result for CPU time where small batch maybe favored more. [1] Balles, Lukas, Javier Romero, and Philipp Hennig. Coupling Adaptive Batch Sizes with Learning Rates. arXiv preprint arXiv:1612.05086 (2016).. [2] Zhang, Cheng, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in Variational Inference. arXiv preprint arXiv:1711.05597 (2017). [3] Tishby, Naftali, and Noga Zaslavsky. Deep learning and the information bottleneck principle.  In Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015. u2014u2014u2014u2014u2014- Update: I lowered my rating considering other ppl s review and comments. ",22,360,9.473684210526317,5.397014925373134,192,2,358,0.0055865921787709,0.0192307692307692,0.9858,129,52,55,16,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 3, 'DAT': 3, 'MET': 4, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 2, 'BIB': 4, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 3, 'CLA': 2}",0,0,5,3,3,4,2,3,0,4,0,2,4,0,0,0,0,0,0,1,4,3,2,0.6444748075530701,0.4466048048364703,0.36351107670044763
ICLR2018-BJj6qGbRW-R1,Accept,"This paper proposes to use graph neural networks for the purpose of few-shot learning, as well as semi-supervised learning and active learning. The paper first relies on convolutional neural networks to extract image features. Then, these image features are organized in a fully connected graph. Then, this graph is processed with an graph neural network framework that relies on modelling the differences between features maps, propto phi(abs(x_i-x_j)). For few-shot classification then the cross-entropy classification loss is used on the node. The paper has some interesting contributions and ideas, mainly from the point of view of applications, since the basic components (convnets, graph neural networks) are roughly similar to what is already proposed. However, the novelty is hurt by the lack of clarity with respect to the model design. First, as explained in 5.1 a fully connected graph is used (although in Fig. 2 the graph nodes do not have connections to all other nodes). If all nodes are connected to all nodes, what is the different of this model from a fully connected, multi-stream networks composed of S^2 branches? To rephrase, what is the benefit of having a graph structure when all nodes are connected with all nodes. Besides, what is the effect when having more and more support images? Is the generalization hurt? Second, it is not clear whether the label used as input in eq. (4) is a model choice or a model requirement. The reason is that the label already appears in the loss of the nodes  in 5.1. Isn't using the label also as input redundant? Third, the paper is rather vague or imprecise at points. In eq. (1) many of the notations remain rather unclear until later in the text (and even then they are not entirely clear). For instance, what is s, r, t. The experimental section is also ok, although not perfect. The proposed method appears to have a modest improvement for few-shot learning. However, in the case of  active learning and semi-supervised learning the method is not compared to any baselines (other than the random one), which makes conclusions hard to reach. In general, I tend to be in favor of accepting the paper if the authors have persuasive answers and provide the clarifications required.",21,373,17.761904761904763,4.966386554621849,169,3,370,0.0081081081081081,0.0293333333333333,0.8193,100,45,65,29,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 14, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 0, 'CLA': 1}",0,1,1,2,0,14,1,0,1,0,0,3,0,0,0,1,0,1,0,1,12,0,1,0.5034026349571575,0.5623970873388282,0.3155932090055906
ICLR2018-BJj6qGbRW-R2,Accept,"This paper introduces a graph neural net approach to few-shot learning. Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features. In addition to standard supervised few-shot classification, both semi-supervised and active learning task variants are introduced. The proposed approach captures several popular few-shot learning approaches as special cases. Experiments are conducted on both Omniglot and miniImagenet datasets. Strengths - Use of graph neural nets for few-shot learning is novel. - Introduces novel semi-supervised and active learning variants of few-shot classification. Weaknesses - Improvement in accuracy is small relative to previous work. - Writing seems to be rushed. The originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper. Graph neural nets seem to be a more natural way of representing sets of items, as opposed to previous approaches that rely on a random ordering of the labeled set, such as the FCE variant of Matching Networks or TCML. Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future, and this paper represents a first step in that direction. Regarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help? In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful. Though novel, the motivation behind the semi-supervised and active learning setup could use some elaboration. By including unlabeled examples in an episode, it is already known that they belong to one of the K classes. How realistic is this set-up and in what application is it expected that this will show up? For active learning, the proposed method seems to be specific to the case of obtaining a single label. How can the proposed method be scaled to handle multiple requested labels? Overall the paper is well-structured and related work covers the relevant papers, but the details of the paper seem hastily written. In the problem set-up section, it is not immediately clear what the distinction between s, r, and t is. Stating more explicitly that s is for the labeled data, etc. would make this section easier to follow. In addition, I would suggest stating the reason why t 1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases. Regarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test. Was the same procedure done for the experiments in the paper? If yes, please update 6.1.1 to make this distinction more clear. If not, please update the experiments to be consistent with the baselines. In the experiments, does the varphi MLP explicitly enforce symmetry and identity or is it learned? Regarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods. This should probably be noted. The results for Prototypical Networks appear to be incorrect in the Omniglot and Mini-Imagenet tables. According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet. Moreover, Snell et al. (2017) only used 64 classes for training instead of 80 as utilized in the proposed approach. Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs? For semi-supervised and active-learning results, please include error bars for the miniImagenet results. Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider. Other Comments:  - In Section 4.2, Gc(.) is defined in Equation 2 but not mentioned in the text. - In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful.",40,666,17.526315789473685,5.1864139020537126,269,6,660,0.009090909090909,0.0356083086053412,0.9854,177,89,121,24,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 10, 'PDI': 3, 'DAT': 8, 'MET': 19, 'EXP': 5, 'RES': 6, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 6, 'IMP': 0, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 16, 'SUB': 3, 'CLA': 2}",0,2,10,3,8,19,5,6,2,0,0,3,0,0,0,6,0,4,2,0,16,3,2,0.6492191740661938,0.6769291107947722,0.4535434939838524
ICLR2018-BJj6qGbRW-R3,Accept,"This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors. The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al. This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks. The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art. There are a few typos and the presentation of the paper could be improved and polished more. I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot. I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. ",7,178,22.25,5.211428571428572,108,0,178,0.0,0.0111731843575419,0.9022,57,23,32,4,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 3, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,3,0,3,3,1,0,0,0,0,1,0,0,0,0,0,4,1,0,2,1,0,0.4293049700835528,0.4454178895508573,0.24381561357142184
ICLR2018-BJjBnN9a--R1,Reject,"The paper introduces the notion of continuous convolutional neural networks. The main idea of the paper is to project examples into an RK Hilbert space and performs convolution and filtering into that space. Interestingly, the filters defined in the Hilbert space  have parameters that are learnable. While the idea may be novel and interesting, its motivation is not clear for me. Is it for space? for speed? for expressivity of hypothesis spaces? Most data that are available for learning are in discrete forms and hopefully, they have been digitalized according to Shannon theory. This means that they bring all necessary information for rebuilding their continuous counterpart. Hence, it is not clear why projecting them back into continuous functions is of interest. Another point that is not clear or at least misleading is the so-called Hilbert Maps. As far as I understand, Equation (4) is not an embedding into an Hilbert space but is more a proximity space representation [1]. Hence, the learning framework of the authors can be casted more as a learning with similarity function than learning into a RKHS [2]. A proper embedding would have mapped $x$ into a function belonging to $mH$. In addition, it seems that all computations are done into a ell^2 space instead of in the RKHS (equations 5 and 11). Learning good similarity functions is also not novel [3] and Equations (6) and (7) corresponds to learning these similarity functions. As far as I remember, there exists also some paper from the nineties that learn the parameters of RBF networks but unfortunately I have not been able to google some of them. Part 3 is the most interesting part of the paper, however it would have been great if the authors provide other kernel functions with closed-form convolution  formula that may be relevant for learning. The proposed methodology is evaluated on some standard benchmarks in vision. While results are pretty good, it is not clear how the various cluster sets have been obtained and what are their influence on the performances (if they are randomly initialized, it  would be great to see standard deviation of performances with respect to initializations). I would also be great to have intuitions on why a single continuous filter works betters than 20 discrete ones (if this behaviour is consistent accross initialization). On the overall, while the idea may be of interested, the paper lacks in motivations in connecting to relevant previous works and in providing insights on why it works. However, performance results seem to be competitive and that's the reader may be eager for insights. minor comments ---------------  * the paper employs vocabulary that is not common in ML. eg. I am not sure what occupancy values, or inducing points are. * Supposingly that the authors properly consider computation in RKHS, then Sigma_i should be definite positive right? how update in (7) is guaranteed to be DP? This constraints may not be necessary if instead they used proximity space representation. [1] https://alex.smola.org/papers/1999/GraHerSchSmo99.pdf [2] https://www.cs.cmu.edu/~avrim/Papers/similarity-bbs.pdf [3] A. Bellet, A. Habrard and M. Sebban. Similarity Learning for Provably Accurate Sparse Linear Classification. ",31,511,17.033333333333335,5.264344262295082,232,6,505,0.0118811881188118,0.0462427745664739,0.9952,134,68,96,33,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 4, 'DAT': 2, 'MET': 16, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 3, 'CLA': 0}",0,1,5,4,2,16,0,2,0,0,0,1,1,0,0,1,0,2,0,0,19,3,0,0.5757730969630475,0.4559701938454508,0.3190797506192868
ICLR2018-BJjBnN9a--R2,Reject,"This paper aims to provide a continuous variant of CNN. The main idea is to apply CNN on Hilbert maps of the data. The data is mapped to a continuous Hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix. A convolutional Hilbert layer algorithm is introduced and evaluated on image classification data sets. The paper is well written and provides some new insights on incorporating kernels in CNN. The kernel matrix in Eq. 5 is not symmetric and the kernel function in Eq. 3 is not defined over a pair of inputs. In this case, the projections of the data via the kernel are not necessarily in a RKHS. The connection between Hilbert maps and RKHS in that sense is not clear in the paper. The size of a kernel matrix depends on the sample size. In large scale situations, working with the kernel matrix can be computational expensive. It is not clear how this issue is addressed in this paper. In section 2.2, how mu_i and sigma_i are computed? How the proposed approach can be compared to convolutional kernel networks (NIPS paper) of Mairal et al. (2014)?",13,195,13.0,4.715846994535519,90,0,195,0.0,0.0256410256410256,-0.3875,63,17,32,7,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 10, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,1,2,2,10,0,0,0,0,0,1,0,0,0,1,0,1,0,0,5,1,1,0.4309185481972398,0.5580433852949275,0.2699728635553958
ICLR2018-BJjBnN9a--R3,Reject,"This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases. A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs. The proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop. Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters. Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN. While the idea is interesting and might be a good alternative to standard CNNs, the paper falls short in terms of providing experimental validation that would demonstrate the latter point. It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers. They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results. The CIFAR-10, STL-10, and SVHN results are disappointing. CCNNs do not outperform the prior CNN results listed in Table 2,3,4. Moreover, these tables do not even cite more recent higher-performing CNNs. See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper. The problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not? To be competitive, deeper CCNNs would likely need to be trained. I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers. Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN? Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive. In addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table. This would assist in understanding tradeoffs in the design space. Additional questions:  What is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared? If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs? For CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN?  Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training. Such dependence for CCNNs appears to be a weakness in comparison.",25,447,23.526315789473685,5.529137529137529,208,4,443,0.0090293453724604,0.02,0.7843,131,72,75,20,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 8, 'MET': 9, 'EXP': 5, 'RES': 7, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 7, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 5, 'CLA': 0}",0,1,0,3,8,9,5,7,1,0,0,0,0,0,0,0,0,7,0,0,7,5,0,0.5030517663363346,0.3381947593188169,0.25439491404502157
ICLR2018-BJjquybCW-R1,,"This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks. I summarize the results as follows:  (1) Under certain assumptions, if the network contains a wide"" hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples. (2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data. (3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum. These solutions achieve zero squared-loss. I would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1). Intuitively, (1) is an easy result. Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent. The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small. Result (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima; (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure; (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero. If an optimization algorithm falls onto these solutions, it will be hard to escape. Overall, the paper presents several incremental improvement over existing theories. However, the novelty and the technical contribution are not sufficient for securing an acceptance.  ",15,292,20.857142857142858,5.401459854014599,158,0,292,0.0,0.0101694915254237,0.9567,80,48,36,19,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 6, 'EXP': 1, 'RES': 8, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,0,1,1,3,6,1,8,0,0,0,1,0,0,0,1,0,1,0,1,10,1,0,0.5017130552976764,0.5611531724691423,0.3202212087180119
ICLR2018-BJjquybCW-R2,,"This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.",2,23,11.5,5.227272727272728,19,1,22,0.0454545454545454,0.0869565217391304,0.4767,8,2,5,1,3,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 0, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 1}",0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0.2142857142857142,0.1111111111111111,0.08075862983442111
ICLR2018-BJjquybCW-R3,,"This paper analyzes the loss function and properties of CNNs with one wide layer, i.e., a layer with number of neurons greater than the train sample size. Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums. I like the presentation and writing of this paper. However, I find it uneasy to fully evaluate the merit of this paper, mainly because the wide-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected. The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy. This is not surprising. It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. ",7,146,18.25,5.342657342657343,94,1,145,0.0068965517241379,0.0136054421768707,-0.5188,38,20,23,12,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,0,1,0,4,0,5,0,0,0,1,0,0,0,0,0,0,1,0,3,0,1,0.3581162410550489,0.3345772482030192,0.18160417126223682
ICLR2018-BJk59JZ0b-R1,Accept,"The authors devise and explore use of the hessian of the (approximate/learned) value function (the critic) to update the policy (actor) in the actor-critic  approach to RL. They connect their technique, 'guide actor-critic' or GAC, to existing actor-critic methods (authors claim only two published work use 1st order information on the critic). They show that the 2nd order information can be useful (in several of the 9 tasks, their GAC techniques were best or competitive, and in only one, performed poorly compared to best). The paper has a technical focus. pros:  - Strict generalization of an existing (up to 1st order) actor-critic approaches - Compared to many existing techniques, on 9 tasks cons:  - no mention of time costs, except that for more samples, S > 1, for  taylor approximation, it can be very expensive. - one would expect more information to strictly improve performance,   but the results are a bit mixed (perhaps due to convergence to local optima and both actor and critic being learned at same time,    or the Gaussian assumptions, etc.). - relevance: the work presents a new approach to actor-critique strategy for   reinforcement learning, remotely related to 'representation   learning' (unless value and policies are deemed a form of   representation). Other comments/questions:  - Why does the performance start high on Ant (1000), then goes to 0 (all approaches)? - How were the tasks selected? Are they all the continuous control tasks available in open ai?         ",12,230,28.75,5.240909090909091,133,1,229,0.0043668122270742,0.0190114068441064,0.8146,65,27,37,11,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 9, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,0,0,2,1,9,0,1,0,1,0,1,0,0,0,1,0,1,0,0,5,2,0,0.4306322653435616,0.4470389506921378,0.24305361505713197
ICLR2018-BJk59JZ0b-R2,Accept,"The paper introduces a modified actor-critic algorithm where a ""guide actor"" uses approximate second order methods to aid computation. The experimental results are similar to previously proposed methods. The paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results. However, the method is not properly motivated. As far as I can tell, the paper never answers the questions: Why do we need a guide actor? What problem does the guide actor solve? The paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and (2) it's not clear why we should want to use second-order methods in reinforcement learning in the first place. Using second order methods is not an end in itself. The experimental results show the authors have found a way to use second order methods without making performance *worse*. Given the high variability of deep RL, they have not convincingly shown it performs better. The paper does not discuss the computational cost of the method. How does it compare to other methods? My worry is that the method is more complicated and slower than existing methods, without significantly improved performance. I recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm.  ",15,220,18.33333333333333,5.184834123222749,124,1,219,0.0045662100456621,0.027027027027027,-0.2458,56,32,45,18,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,1,0,0,0,12,3,3,0,1,0,0,0,0,0,0,0,3,0,0,9,1,1,0.3601787657633247,0.4496544290375682,0.20219405039059912
ICLR2018-BJk59JZ0b-R3,Accept,"The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor. This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form. The paper is mostly clear and well-presented, except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3); and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.  The presented method itself seems to be an important contribution, even if the results are not overwhelmingly positive. It'd be interesting to see a more elaborate analysis of why it works well in some domains but not in others. More trials are also needed to alleviate any suspicion of lucky trials. There are some other issues with the presentation of the method, but these don't affect the merit of the method: 1. Returns are defined from an initial distribution that is stationary for the policy. While this makes sense in well-mixing domains, the experiment domains are not well-mixing for most policies during training, for example a fallen humanoid will not get up on its own, and must be reset. 2. The definition of beta(a|s) as a mixture of past actors is inconsistent with the sampling method, which seems to be a mixture of past trajectories. 3. In the first paragraph of Section 3.3: [...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation.  What else does it depend on? Then: [...] the action a_0 should be in a local vicinity of a. ; and [...] the action a_0 should be similar to actions sampled from pi_theta(a|s). What do you mean should? In order for the Taylor approximation to be good? 4. The line before (19) is confusing, since (19) is exact and not an approximation. For the approximation (20), it isn't clear if this is a good approximation. Why/when is the 2nd term in (19) small? 5. The parametrization nu of hat{Q} is never specified in Section 3.6. This is important in order to evaluate the complexities involved in computing its Hessian. ",21,398,18.09090909090909,4.873315363881401,186,2,396,0.005050505050505,0.0222222222222222,0.7893,101,39,71,23,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 12, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 1}",0,1,0,2,0,12,1,2,0,2,0,2,0,0,0,1,0,0,2,0,10,2,1,0.5029372531948634,0.5613214179012671,0.3172101896101339
ICLR2018-BJk7Gf-CZ-R1,Accept,"The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks. The paper is an extension of Kawaguchi'16. It also provides some sufficient conditions for the non-linear cases. I think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16. I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it. As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique. Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum. This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points. ",8,168,18.666666666666668,5.050314465408805,100,3,165,0.0181818181818181,0.0588235294117647,-0.2022,39,30,28,11,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 3, 'CLA': 0}",0,1,3,2,0,4,0,1,0,0,1,2,0,0,0,1,1,2,0,0,1,3,0,0.5010162708160283,0.5558860711293885,0.3029903991257823
ICLR2018-BJk7Gf-CZ-R2,Accept,"Summary: The paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks. In particular: - in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion. This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are . - in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach. Quality: The quality is very good. The paper is technically correct and nontrivial. All proofs are provided and easy to follow. Clarity: The paper is very clear. Related work is clearly cited, and the novelty of the paper well explained. The technical proofs of the paper are in appendices, making the main text very smooth. Originality: The originality is weak. It extends a series of recent papers correctly cited. There is some originality in the proof which differs from recent related papers. Significance: The result is not completely surprising, but it is significant given the lack of theory and understanding of deep learning. Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people.",16,255,17.0,5.199170124481328,134,0,255,0.0,0.0193798449612403,0.9022,67,45,38,14,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 8, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 3, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 2}",0,1,4,1,0,1,0,4,0,0,0,8,2,1,0,2,3,1,3,0,2,0,2,0.5721674654750429,0.6676340779013843,0.40249631972649386
ICLR2018-BJk7Gf-CZ-R3,Accept,"-I think title is misleading, as the more concise results in this paper is about linear networks I recommend adding linear in the title i.e. changing the title to ... deep LINEAR networks - Theorems 2.1, 2.2 and the observation (2) are nice! - Theorem 2.2 there is no discussion about the nature of the saddle point is it strict? Does this theorem imply that the global optima can be reached from a random initialization? Regardless of if this theorem can deal with these issues, a discussion of the computational implications of this theorem is necessary. - I'm a bit puzzled by Theorems 4.1 and 4.2 and why they are useful. Since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result?  Further discussion would be helpful. ",8,140,23.33333333333333,4.961538461538462,83,1,139,0.0071942446043165,0.0547945205479452,0.8091,36,17,27,1,4,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 3, 'CLA': 0}",0,1,0,0,0,5,0,3,0,2,0,0,0,0,1,0,1,0,1,0,3,3,0,0.2868511773354595,0.5570128234418844,0.17437041150728175
ICLR2018-BJlrSmbAZ-R1,Reject,"This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization. There is a lot going on in this paper. Although the overall presentation is clean, there are few key shortfalls (see below). Overall, the reported functionality is nice, although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive). Some open questions that I find crucial:  * How exactly is the ""stochastic forward-pass"" performed that gives rise to the moment estimates? This step is the real meat of the paper, yet I struggle to find a concrete definition in the text. Is this really just an average over a few recent weights during optimization? If so, how is this method specific to batch normalization? Maybe I'm showing my own lack of understanding here, but it's worrying that the actual sampling technique is not explained anywhere. This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here? In Bayesian Dropout, there is an explicit variational objective. Here, this is replaced by an implicit regularizer. The argument in Section 3.3 seems rather weak to me. To paraphrase it: If the prior vanishes, so does the regularizer. Fine. But what's the regularizer that's vanishing? The sentence that the influence of the prior diminishes as the size of the training data increases is debatable for something as over-parametrized as a DNN. I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior. * I'm confused about the statements made about the ""constant uncertainty"" baseline. First off, how is this (constant) width of the predictive region chosen? Did I miss this, or is it not explained anywhere? Unless I misunderstand the definition of CRPS and PLL, that width should matter, no? Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is ""quite a reasonable baseline"". That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?). On a related point: What would Figure 2 look like for the constand uncertainty setting? Just a horizontal line in blue and red? But at which level? I like this paper. It is presented well (modulo the above problems), and it makes some strong points. But I'm worried about the empirical evaluation, and the omission of crucial algorithmic details. They may hide serious problems.",32,445,22.25,5.110070257611241,232,3,442,0.006787330316742,0.0513392857142857,-0.9768,104,63,79,32,9,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 15, 'EXP': 3, 'RES': 1, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 5, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 0}",0,1,2,2,1,15,3,1,3,0,0,6,0,0,0,0,0,0,5,0,16,0,0,0.6469457017594513,0.2317978594400801,0.28506605963785553
ICLR2018-BJlrSmbAZ-R2,Reject,"*Summary*  The paper proposes using batch normalisation at test time to get the predictive uncertainty. The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each layer. This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time. Several experiments show Monte Carlo prediction at test time using batch norm is better than dropout. *Originality and significance*  As far as I understand, almost learning algorithms similar to equation 2 can be recast as variational inference under equation 1. However, the critical questions are what is the corresponding prior, what is the approximating density, what are the additional approximations to obtain 2, and whether the approximation is a good approximation for getting closer to the posterior/obtain better prediction. It is not clear to me from the presentation what the q(w) density is -- whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w). From a Bayesian perspective, it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small lambda] -- what is the rationale of this?  Can this be explained by comparing the variational free-energy. The experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g. Louizos and Welling, 2016, 2017). The VI method included in the PBP paper (Hernandez-Lobato and Adams, 2015) does not use the reparameterisation trick, which has been found to reduce variance and improve over Graves' VI method. *Clarity* The paper is in general well written and easy to understand. *Additional comments*  Page 2: Monte Carlo Droput --> Dropout Page 3 related work: (Adams, 2015) should be (Hernandez-Lobato and Adams, 2015)",13,328,25.23076923076923,5.459807073954984,165,0,328,0.0,0.0298507462686567,0.9609,101,46,62,14,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 2}",0,1,3,1,1,6,6,0,0,0,0,1,0,0,0,0,1,3,1,0,4,0,2,0.5018429153322347,0.5577368880453185,0.31299064278402394
ICLR2018-BJlrSmbAZ-R3,Reject,"The authors show how the regularization procedure called batch normalization, currently being used by most deep learning systems, can be understood as performing approximate Bayesian inference. The authors compare this approach to Monte Carlo dropout (another regularization technique which can also be considered to perform approximate Bayesian inference). The experiments performed show that the Bayesian view of batch normalization performs similarly as MC dropout in terms of the estimates of uncertainty that it produces. Quality:  I found the quality to be low in some aspects. First, the description of what is the prior used by batch normalization in section 3.3 is unsatisfactory. The authors basically refer to Appendix 6.4 for the case in which the weight decay penalty is not zero. The details in that Appendix are almost none, they just say it is thus possible to derive the prior.... The results in Table 2 are a bit confusing. The authors should highlight in bold face the results of the best performing method. The authors indicate that they do not need to compare to variational methods because Gal and Ghahramani 2015 compare already to those methods. However, Gal and Ghahramani's code used Bayesian optimization methods to tune hyper-parameters and this code contains a bug that optimizes hyper-parameters by maximizing performance on the test data. In particular for hyperparameter selection, they average performance across (subsets of) 5 of the training sets from the 20x train/test split, and then using the tau which got the best average performance for all of 20x train/test splits to evaluate performance:  https://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54 Therefore, the claim that   Since we have established that MCBN performs on par with MCDO, by proxy we might conclude that MCBN outperforms those VI methods as well.  is not valid. At the beginning of section 4.3 the authors indicate that they follow in their experiments the setup of Gal and Ghahramani (2015). However, Gal and Ghahramani (2015) actually follow Hernu00e1ndez-Lobato and Adams, 2015 so the correct reference should be the latter one. Clarity:  The paper is clearly written and easy to follow and understand. I found confusing how to use the proposed method to obtain estimates of uncertainty for a particular test data point x_star. The paragraph just above section 4 says that the authors sample a batch of training data for this, but assume that the test point x_star has to be included in this batch. How is this actually done in practice? Originality:  The proposed contribution is original.  This is the first time that a Bayesian interpretation has been given to the batch normalization regularization proposal. Significance:  The paper's contributions are significant. Batch normalization is a very popular regularization technique and showing that it can be used to obtain estimates of uncertainty is relevant and significant. Many existing deep learning systems can use this to produce estimates of uncertainty in their predictions. ",24,471,19.625,5.503311258278146,211,1,470,0.002127659574468,0.0187110187110187,0.905,143,42,83,23,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 2, 'MET': 10, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",0,1,4,1,2,10,5,2,0,0,0,4,0,0,0,1,2,4,0,0,7,1,1,0.5744945963833746,0.6707979355957423,0.4056852482631553
ICLR2018-BJluxbWC--R1,Reject,"The main goal of this paper is to cluster images from classes unseen during training. This is an interesting extension of the open-world paradigm, where at test time, the classifier has to identify images beloning to the C seen classes during training, but also identify (reject) images which were previously unseen. These rejected images could be clustered to identify the number of unseen classes; either for revealing the underlying structure of the unseen classes, or to reduce annotation costs. In order to do so, an extensive framework is proposed, consisting of 3 ConvNet architectures, followed by a hierarchical clustering approach. The 3 ConvNets all have a different goal: 1. an Open Classification Network (per class sigmoid, trained 1vsRest, with thresholds for rejection) 2. Pairwise Classification Network, (binary sigmoid, trained on pairs of images of same/different classes) 3. Auto encoder network  These network are jointly trained, and the joint-loss is simply the addition of a cross-entropy loss (from OCN), the binary cross-entropy loss (from PCN) and a pixel wise loss (from AE).  Remarks: - it is unclear if the ConvNet weights of the first layers are shared). - it is unclear how joint training might help, given that the objectives do not influence each other - Eq 1:    *label y_i has two different semantics (L_ocn it is the class label, while in L_pcn it is the label of an image pair being from the same class or not) * s_j is undefined * relation between the p(y_i   1) (in PCN) and g(x_p,x_q) in Eq 2 could be made more explicit, PCN depends on two images, according to Eq 1, it seems just a sum over single images. - It is unclear why the Auto Encoder network is added, and what its function is. - It is unclear wether OCN requires/uses unseen class examples during training. - Last paragraph of 3.1 The 1-vs-rest ... rejected, I don't see why you need 1vsRest classifiers for this, a multi-class (softmax) output can also be thresholded to reject an test image from the known classes and to assign it to the unknown class. Experimental evaluation The experimental evaluation uses 2 datasets, MNIST and EMNIST, both are very specific for character recognition. It is a pity that not also more general image classification has been considered (CIFAR100, ImageNet, Places365, etc), that would provide insights to the more general behaviour of the proposed ideas. My major concern is that the clustering task is not extensively explored. Just a single setting (with a single random sampling of seen/unseen classes) has been evaluated. This is -in part- due to the nature of the chosen datasets, in a 10 class dataset it is difficult to show the influence of the number of unseen classes. So, I'd really urge the authors to extend this evaluation. Will the method discover more classes when 100 unknown classes are used? What kind of clusters are discovered? Are the types of classes in the seen/unseen classes important, I'd expect at least multiple runs of the current experiments on (E)MNIST. Further, I miss some baselines and ablation study. Questions which I'd like to seen answered: how good is the OCN representation when used for clustering compared to the PCN representation? What is the benefit of joint-training? How important is the AE in the loss? Remaining remarks - Just a very simple / non-standard ConvNet architecture is trained. Will a ResNet(32) show similar performance? - In Eq 4, |C_i || y_j| seems a strange notation for union. Conclusion This paper brings in an interesting idea, is it possible to cluster the unseen classes in an open-world classification scenario?  A solution using a pairwise convnet followed by hierarchical clustering is proposed. This is a plausible solution, yet in total I miss an exploration of the solution. Both in terms of general visual classification (only MNIST is used, while it would be nice to see results on CIFAR and/or ImageNet as in Bendale&Boult 2016), as in exploration of different scenarios (different number of unseen classes, different samplings) and ablation of the method (independent training, using OCN for hierarchical clustering, influence of Auto Encoder). Therefore, I rate this paper as a (weak) reject: it is just not (yet) good enough for acceptance.",35,689,24.607142857142858,5.101226993865031,286,3,686,0.0043731778425655,0.0154929577464788,-0.8789,211,91,115,31,8,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 5, 'MET': 27, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 28, 'SUB': 2, 'CLA': 0}",0,0,1,2,5,27,4,1,0,2,0,1,0,0,0,0,0,0,0,0,28,2,0,0.5784205359641447,0.2391217494713039,0.26611537802655977
ICLR2018-BJluxbWC--R2,Reject,"This paper concerns open-world classification. The open-world related tasks have been defined in many previous works. This paper had made a good survey. The only special point of the open-word classification task defined in this paper is to employ the constraints from the similarity/difference expected for examples from the same class or from different classes. Unfortunately, this paper is lack of novelty. Firstly, the problem context and setting is kinda synthesized. I cannot quite imagine in what kind of applications we can get ""a set of pairs of intra-class (same class) examples, and the negative training data consists of a set of pairs of inter-class"". Secondly, this model is just a direct combination of the recent powerful algorithms such as DOC and other simple traditional models. I do not really see enough novelty here. Thirdly, the experiments are only on the MNIST and EMNIST; still not quite sure any real-world problems/datasets can be used to validate this approach. I also cannot see the promising performance. The clustering results of rejected examples are still far from the ground truth, and comparing the result with a total unsupervised K-means is a kind of unreasonable. ",12,192,14.76923076923077,5.300546448087432,118,0,192,0.0,0.0466321243523316,-0.6845,55,28,28,19,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 4, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,1,2,2,4,1,2,0,0,0,3,0,0,0,2,0,1,0,0,7,0,0,0.5009925064519137,0.3371101855269093,0.24642783819515612
ICLR2018-BJluxbWC--R3,Reject,"This paper focuses on the sub-problem of discovering previously unseen classes for open-world classification. It employs a previously proposed system, Open Classification Network, for classifying instances into known classes or rejecting as belonging to an unseen class, and applies hierarchical clustering to the rejected instances to identify unseen classes. The key novel idea is to learn a pairwise similarity function using the examples from the known classes to apply to examples of unknown classes. The argument is that we tend to use the same notion of similarity and dissimilarity to define classes (known or unknown) and one can thus expect the similarity function learned from known classes to carry over to the unknown classes. This concept is not new. Similar idea has been explored in early 2000 by Finley and Joachims in their ICML paper titled Supervised Clustering with Support Vector Machines. But to the best of my knowledge, this is the first paper that applies this concept to the open world classification task. Once we learn the similarity function, the rest of the approach is straightforward, without any particular technical ingenuity. It simply applies hierarchical clustering on the learned similarities and use cross-validation to pick a stopping condition for deciding the number of clusters. I find the experiments to be limited, only on two hand-written digits/letters datasets. Such datasets are too simplistic. For example, simply applying kmeans to PCA features of the images on the MNIST data can get you pretty good performance. Experiments on more complex data is desired, for example on Imagenet classes. Also the results do not clearly demonstrate the advantage of the proposed method, in particular the benefit of using PCN. The number of clusters found by the algorithm is not particularly accurate and the NMI values obtained by the proposed approach does not show any clear advantage over baseline methods that do not use PCN. Some minor comments: When applied to the rejected examples, wouldn't the ground truth # of clusters no longer be 4 or 10 because there are some known-class examples mixed in? For the base line Encoder+HC, was the encoder trained independently? Or it's trained jointly with PCN and OCN?  It is interesting to see the impact of incorporating PCN into the training of OCN and encoder. Does that have any impact on accuracy of OCN? It seems that one of the claimed benefit is that the proposed method is effective at identifying the k. If so, it would be necessary to compared the proposed method to some classic methods for identifying k with kmeans, such as the elbow method, BIC, G-means etc, especially since kmeans seem to give much better NMI values.   ",22,441,23.21052631578948,5.127314814814815,214,1,440,0.0022727272727272,0.0112107623318385,0.9862,117,44,88,25,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 4, 'MET': 11, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 1, 'CLA': 0}",0,0,2,1,4,11,5,2,0,0,0,1,0,0,0,2,0,3,0,0,12,1,0,0.5030911663194553,0.4515654089266153,0.28747216709294204
ICLR2018-BJuWrGW0Z-R1,Accept,"This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program. Three NN architectures are explored, which leverage program semantics rather than pure syntax. The approach is validated using programming assignments from an online course, and compared against syntax based approaches as a baseline. The problem considered by the paper is interesting, though it's not clear from the paper that the approach is a substantial improvement over previous work. This is in part due to the fact that the paper is relatively short, and would benefit from more detail. I noticed the following issues:  1) The learning task is based on error patterns, but it's not clear to me what exactly that means from a software development standpoint. 2) Terms used in the paper are not defined/explained. For example, I assume GRU is gated recurrent unit, but this isn't stated. 3) Treatment of related work is lacking. For example, the Cai et al. paper from ICLR 2017 is not considered 4) If I understand dependency reinforcement embedding correctly, a RNN is trained for every trace. If so, is this scalable? I believe the work is very promising, but this manuscript should be improved prior to publication.",14,214,17.833333333333332,5.199004975124378,127,0,214,0.0,0.055813953488372,0.8304,60,23,45,12,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 5, 'CLA': 0}",0,1,4,3,0,4,1,0,0,0,0,4,1,0,1,0,0,2,0,0,4,5,0,0.5012468961963691,0.4468541853394492,0.2726363942739045
ICLR2018-BJuWrGW0Z-R2,Accept,"Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax. The application is to predict errors made by students on programming tasks. This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements. The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds).  ---  Quality: The experiments compare the three proposed neural network architectures with two syntax-based architectures. It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings. Clarity: The paper is clearly written. Originality: This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know). Significance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand. ---  Some questions/comments: - Do we need to add the print statements for any new programs that the students submit? What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated? ---  References   Cai, J., Shin, R., & Song, D. (2017). Making Neural Programming Architectures Generalize via Recursion. In International Conference on Learning Representations (ICLR).",14,257,16.0625,5.566801619433198,150,0,257,0.0,0.0148148148148148,0.9286,87,19,52,10,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 4, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,1,0,2,1,4,3,1,0,0,1,1,3,0,0,2,1,0,0,0,2,1,1,0.6438641966534445,0.5562226205749167,0.40703355829854043
ICLR2018-BJuWrGW0Z-R3,Accept,"The authors present 3 architectures for learning representations of programs from execution traces. In the variable trace embedding, the input to the model is given by a sequence of variable values. The state trace embedding combines embeddings for variable traces using a second recurrent encoder. The dependency enforcement embedding performs element-wise multiplication of embeddings for parent variables to compute the input of the GRU to compute the new hidden state of a variable. The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204.1X (an introduction to C# offered on edx) and problems on the Microsoft CodeHunt platform. They additionally use their embeddings to decrease the search time for the Sarfgen program repair system. This is a fairly strong paper. The proposed models make sense and the writing is for the most part clear, though there are a few places where ambiguity arises: - The variable Evidence in equation (4) is never defined. - The authors refer to predicting the error patterns, but again don't define what an error pattern is. The appendix seems to suggest that the authors are simply performing multilabel classification based on a predefined set of classes of errors, is this correct? - It is not immediately clear from Figures 3 and 4 that the architectures employed are in fact recurrent. - Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct? n Assuming that the authors can address these clarity issues, I would in principle be happy for the paper to appear. ",14,265,22.08333333333333,5.413654618473895,141,3,262,0.0114503816793893,0.0333333333333333,0.2319,85,21,49,9,5,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 2}",0,2,0,2,0,7,0,0,2,0,0,2,0,0,0,0,0,0,1,0,2,2,2,0.3587975453854935,0.4452542134584629,0.1992736825564444
ICLR2018-BJubPWZRW-R1,Reject,"This paper presents a so-called cross-view training for semi-supervised deep models. Experiments were conducted on various data sets and experimental results were reported.  Pros: * Studying semi-supervised learning techniques for deep models is of practical significance. Cons: * The novelty of this paper is marginal. The use of unlabeled data is in fact a self-training process. Leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retrieval. * The proposed approach suffers from a technical weakness or flaw. For the self-labeled data, the prediction of each view is enforced to be same as the assigned self-labeling. However, since each view related to a sub-region of the image (especially when the model is not so deep), it is less likely for this region to contain the representation of the concepts (e.g., some local region of an image with a horse may exhibit only grass); enforcing the prediction of this view to be the same self-labeled concepts (e.g,""horse"") may drive the prediction away from what it should be ( e..g, it will make the network to predict grass as horse). Such a flaw may affect the final performance of the proposed approach. * The word ""view"" in this paper is misleading. The ""view"" in this paper is corresponding to actually sub-regions in the images * The experimental results indicate that the proposed approach fails to perform better than the compared baselines in table 2, which reduces the practical significance of the proposed approach.  ",12,245,18.846153846153847,5.206751054852321,124,4,241,0.0165975103734439,0.0354330708661417,-0.34,72,29,46,9,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 6, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,0,2,2,6,1,2,1,0,0,2,0,0,0,2,1,0,2,0,5,0,0,0.5014474718714326,0.4470389506921378,0.27841275414437616
ICLR2018-BJubPWZRW-R2,Reject,"The paper proposes a 'Cross View training' approach to semi-supervised learning. In the teacher-student framework for semi-supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model. The auxiliary softmax layers take different views of the input for prediction. Pros: 1. A simple approach to encourage better representations learned from unlabeled examples. 2. Experiments are comprehensive. Cons:  0. The whole paper just presented strategies and empirical results. There are no discussions of insights and why the proposed strategy work, for what cases it will work, and for what cases it will not work? Why? 1. The addition of auxiliary layers improves Sequence Tagging results marginally. 2. The claim of cross-view for sequence tagging setting is problematic. Because the task is per-position tagging, those added signals are essentially not part of the examples, but the signals of its neighbors. 3. Adding n^2 linear layers for image classification essentially makes the model much larger. It is unfair to compare to the baseline models with much fewer parameters. 4. The CVT, no noise should be compared to CVT, random noise, then to CVT, adversarial noise. The current results show that the improvements are mostly from VAT, instead of CVT.    ",13,212,10.095238095238097,5.487562189054726,118,0,212,0.0,0.0230414746543778,-0.7851,67,28,31,10,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 0}",0,1,0,2,0,10,1,2,0,1,0,0,0,0,0,0,0,1,0,0,7,3,0,0.4309381593500557,0.3372784309590341,0.2170660216121906
ICLR2018-BJubPWZRW-R3,Reject,"This paper proposes a multi-view semi-supervised method. For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap. Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data. To be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here). As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term. Otherwise, the model can't learn anything. My understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss). In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance? If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1. One obvious merit is that the unlabeled data is utilized more efficiently, k times better.   ",10,228,22.8,5.047169811320755,128,1,227,0.0044052863436123,0.0129870129870129,0.7708,62,31,38,17,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 9, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,1,0,2,9,0,0,0,0,0,0,0,0,0,1,0,0,0,0,3,0,0,0.2877519133644565,0.2234661370919081,0.13065729427772343
ICLR2018-BJvVbCJCb-R1,Reject,"This paper presents an algorithm for clustering using DNNs. The algorithm essentially alternates over two steps: a step that trains the DNN to predict random targets, and another step that reassigns the targets based on the overall matching with the DNN outputs. The second step also shrinks the number of targets over time to achieve clustering. Intuitively, the randomness in target may achieve certain regularization effect. My concerns: 1. There is no analysis on what the regularization effect is. What advantage does the proposed algorithm offer to an user that a more deterministic algorithm cannot? 2. The delete-and-copy step also introduces randomness, and since the algorithm removes targets over time, it is not clear if the algorithm consistently optimizes one objective throughout. Without a consistent objective function, the algorithm seems somewhat heuristic. 3. Due to the randomness from multiple operations, the experiments need to be run multiple times, and see if the output clustering is sensitive to it. If it turns out the algorithm is quite robust to the randomness, it is then an interesting question why this is the case. 4. Does the  Hungarian algorithm used for matching scales to much larger datasets? 5. While the algorithm empirically improve over k-means, I believe at this point combinations of DNN with classical clustering algorithms already exist and comparisons with such stronger baselines are missing.  The authors have listed a few related algorithms in the last paragraph on page 1. I think the following one is also relevant: -- Law et al. Deep spectral clustering learning. ICML 2015.  ",15,256,12.8,5.419753086419753,144,3,253,0.0118577075098814,0.0421455938697318,0.8972,69,31,45,15,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 0}",0,1,2,0,1,10,1,0,0,1,0,0,0,0,0,0,0,1,0,0,5,3,0,0.4309036454978745,0.3360345160893481,0.2126767541800522
ICLR2018-BJvVbCJCb-R2,Reject,"This ms presents a new clustering method which combines deep autoencoder and a recent unsupervised representation learning approach (NAT; Bojanowski and Joujin 2017). The proposed method can jointly learn latent features and the cluster assignments. Then the method is tested in several image and text data sets. I have the following concerns:  1) The paper is not self-contained. The review of NAT is too brief and makes it too hard to understand the remaining of the paper. Because NAT is a fundamental starting point of the work, it will be nice to elaborate the NAT method to be more understandable. 2) Predicting the noise has no guarantee that the data items are better clustered in the latent space. Especially, projecting the data points to a uniform sphere can badly blur the cluster boundaries. 3) How should we set the parameter lambda? Is it data dependent? 4) The experimental results are a bit less satisfactory: a) It is known that unsupervised clustering methods can achieve 0.97 accuracy for MNIST. See for example [Ref1, Ref2, Ref3]. b) Figure 3 is not satisfactory. Actually t-SNE on raw MNIST pixels is not bad at all. See https://sites.google.com/site/neighborembedding/mnist c) For 20 Newsgroups dataset, NATAC achieves 0.384 NMI. By contrast, the DCD method in [Ref3] can achieve 0.54. 5) It is not clear how to set the number of clusters. More explanations are appreciated. [Ref1] Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, Erkki Oja. Clustering by Nonnegative Matrix Factorization Using Graph Random Walk.In NIPS 2012. [Ref2] Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht. Multiclass Total Variation Clustering. In NIPS 2013. [Ref3] Zhirong Yang, Jukka Corander and Erkki Oja.  Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis. Journal of Machine Learning Research, 17(187): 1-25, 2016.",24,291,12.125,5.271375464684015,169,0,291,0.0,0.0136518771331058,0.7541,107,42,47,15,9,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 8, 'MET': 8, 'EXP': 0, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 5, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,0,1,0,8,8,0,3,1,1,0,1,5,1,0,0,0,0,0,0,8,1,0,0.6450220986376579,0.226575924266123,0.29359716033075467
ICLR2018-BJvVbCJCb-R3,Reject,"This paper proposes a neural clustering model following the Noise as Target technique. Combining with an reconstruction objective and delete-and-copy trick, it is able to cluster the data points into different groups and is shown to give competitive results on different benchmarks. It is nice that the authors tried to extend the  oise as target to the clustering problem, and proposed the simple delete-and-copy technique to group different data points into clusters. Even tough a little bit ad-hoc, it seems promising based on the experiment results. However, it is unclear to me why it is necessary to have the optimal matching here and why the simple nearest target would not work. After all, the cluster membership is found based on the nearest target in the test stage. Also, the authors should provide more detailed description regarding the scheduling of the alpha and lambda values during training, and how sensitive it is to the final clustering performance. The authors cited the no requirement of a predefined number of clusters as one of the contributions, but the tuning of alpha seems more concerning. I like the authors experimented with different benchmarks, but lack of comparisons with existing deep clustering techniques is definitely a weakness. The only baseline comparison provided is the k-means clustering, but the comparisons were somewhat unfair. For all the text datasets, there were no comparisons with k-means on the features learned from the auto-encoders or clusterings learned from similar number of clusters. The comparisons for the Twitter dataset were even based on character-level with word-level. It is more convincing to show the superiority of the proposed method than existing ones on the same ground. Some other issues regarding quantitative results: - In Table 1, there are 152 clusters for 10-d latent space after convergence, but there are 61 clusters for 10-d latent space in Table 2 for the same MNIST dataset. Are they based on different alpha and lambda values? - Why does NATAC perform much better than NATAC-k? Would NATAC-k need a different number of clusters than the one from NATAC? The number of centroids learned from NATAC may not be good for k-means clustering. - It seems like the performance of AE-k is increasing with increase of dimensionality of latent space for Fashion-MNIST. Would AE-k beat NATAC with a different dimensionality of latent space and k?",20,386,22.705882352941178,5.238605898123325,173,4,382,0.0104712041884816,0.0205128205128205,0.9573,111,50,57,13,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 5, 'MET': 9, 'EXP': 8, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 0}",0,1,0,0,5,9,8,3,1,1,0,0,0,0,0,0,0,5,0,0,11,1,0,0.5028606741972309,0.3400215579105229,0.25463264011590214
ICLR2018-BJvWjcgAZ-R1,Reject,"This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a u00ab backward u00bb update (i.e. from end to start of episode). The targets' update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta 0 corresponds to regular DQN with no such reward propagation). This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta 1, and on DQN and its optimality tightening variant on Atari games with beta 0.5. The intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL. Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay (u00ab  Programming Robots Using Reinforcement Learning and Teaching u00bb, Lin, 1991), something that is not mentioned here. That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin's algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments). In the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation. Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time? Or maybe the average is a bad performance measure due to outliers? I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method). Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here). The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm. Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance. A few additional small remarks and questions: - u00ab Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. u00bb: should u00ab unless u00bb be replaced by u00ab if u00bb? - In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze? - Typo in eq. 3: the - in the max should be a comma - There is a good amount of typos and grammar errors, though they do not harm the readability of the paper - Citations for u00ab Deep Reinforcement Learning with Double Q-learning u00bb and u00ab Dueling Network Architectures for Deep Reinforcement Learning u00bb could refer to their conference versions - u00ab epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner u00bb: please specify the exact formula - Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it",21,635,30.23809523809524,4.9983108108108105,296,2,633,0.0031595576619273,0.0295031055900621,0.9865,168,85,111,38,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 13, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 4}",0,1,2,2,1,13,2,3,1,1,0,2,0,0,0,2,0,2,0,0,10,1,4,0.7176708258005481,0.5615588478234126,0.4494689371879206
ICLR2018-BJvWjcgAZ-R2,Reject,"The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update. The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards. This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.). Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max. The paper is written in a clear way. The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there. I think this should have been one of the baselines to compare to for that reason. The experimental results seem promising in the illustrative MNIST domain. Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious.  ",9,195,19.5,5.374331550802139,123,3,192,0.015625,0.0656565656565656,0.9318,55,24,32,13,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,1,2,1,3,2,2,0,0,0,1,0,0,0,0,0,1,0,0,4,0,1,0.5721301532024147,0.3351992056378622,0.2755885922982448
ICLR2018-BJvWjcgAZ-R3,Reject,"This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode. The paper is interesting, but it lacks the proper comparisons to previously published techniques. The results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA. In the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case. There has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case. In the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.",13,221,17.0,5.170616113744076,124,0,221,0.0,0.0135746606334841,0.9169,59,25,36,12,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 0}",0,1,5,2,0,5,0,2,2,1,0,1,0,0,0,0,0,5,0,0,2,2,0,0.5728895912085178,0.3345306175052577,0.29051318703213214
ICLR2018-BJy0fcgRZ-R1,Reject,Quality  This paper demonstrates that human category representations can be inferred by sampling deep feature spaces. The idea is an extension of the earlier developed MCMC with people approach where samples are drawn in the latent space of a DCGAN and a BiGAN. The approach is thoroughly validated using two online behavioural experiments. Clarity  The rationale is clear and the results are straightforward to interpret. In Section 4.2.1 statements on resemblance and closeness to mean faces could be tested. Last sentences on page 7 are hard to parse. The final sentence probably relates back to the CI approach. A few typos.  Originality  The approach is a straightforward extension of the MCMCP approach using generative models. Significance   The approach improves on previous category estimation approaches by embracing the expressiveness of recent generative models. Extensive experiments demonstrate the usefulness of the approach. Pros  Useful extension of an important technique backed up by behavioural experiments. Cons  Does not provide new theory but combines existing ideas in a new manner.,12,166,12.76923076923077,5.716981132075472,106,1,165,0.006060606060606,0.0114942528735632,0.8292,53,24,31,5,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,4,2,0,5,5,1,0,0,0,1,0,0,0,2,0,4,0,0,5,1,1,0.5016463151909811,0.5584399805510155,0.3188883519526357
ICLR2018-BJy0fcgRZ-R2,Reject,"This paper presents a method based on GANs for visualizing how humans represent visual categories. Authors perform experiments on two datasets: Asian Faces Dataset and ImageNet Large Scale Recognition Challenge dataset. Positive aspects: + The idea of using GANs for this goal is smart and interesting + The results seem interesting too Weaknesses: - Some aspects of the paper are not clear and presentation needs improvement. - I miss a clearer results comparison with previous methods, like Vondrick et al. 2015. Specific comments and questions:  -  Figure 1 is not clear. Authors should clarify how they use the inference network and what the two arrows from this inference network represent. - Figure 2 is also not clear. Just the FLD projections of the MCMCP chains are difficult to interpret. The legend of the figure is too tiny. The right part of the figure should be better described in the text or in the caption, I don't understand well what this illustrates. - Regarding to the human experiments with AMT: how do the authors deal with noise on the workers performance? Is any qualification task used? What are the instructions given to the workers? - In section 4.2. the authors state We also simultaneously learn a corresponding inference network, .... granular human biases captured. This seems interesting but I didn't find any result on that in the paper. Can you give more details or refer to where in the paper it is discussed/tested? - Figure 4 shows most interpretable mixture components. How this most interpretable were selected? - In second paragraph Section 4.3, it should be Table 1 instead of Figure 1. - It would be interesting to see a discussion on why MCMCP Density is better for group 1 and MCMCP Mean is better for group 2. To see the confusion matrixes could be useful. I like this paper. The addressed problem is challenging and the proposed idea seems interesting. However, the aspects mentioned make me think the paper needs some improvements to be published. ",27,323,14.043478260869565,5.180327868852459,167,3,320,0.009375,0.0384615384615384,0.9767,95,34,61,18,10,8,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 2, 'RES': 6, 'TNF': 6, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 6, 'REC': 1, 'EMP': 10, 'SUB': 2, 'CLA': 1}",0,1,1,2,1,7,2,6,6,0,1,3,0,0,1,0,1,1,6,1,10,2,1,0.7164181995051682,0.8949010269298137,0.6206104326398885
ICLR2018-BJy0fcgRZ-R3,Reject,"The idea of using MCMCP with GANs is well-motivated and well-presented in the paper, and the approach is new as far as I know. Figures 3 and 5 are convincing evidence that MCMCP compares favorably to direct sampling of the GAN feature space using the classification images approach. However, as discussed in the introduction, the reason an efficient sampling method might be interesting would be to provide insight on the components of perception. On these insights, the paper felt incomplete. For example, it was not investigated whether the method identifies classification features that generalize. The faces experiment is similar to previous work done by Martin (2011) and Kontsevich (2004) but unlike that previous work does not investgiate whether classification features have been identified that can be added to an arbitrary image to change the attribute happy vs sad or male vs female. Similarly, the second experiment in Table 1 compares classification accuracy between different sampling methods, but it does not provide any comparison as done in Vondrick (2015) to a classifier trained in a conventional way (such as an SVM), so it is difficult to discern whether the learned distributions are informative. Finally, the effect of choosing GAN features vs a more  aive feature space is not explored in detail. For example, the GAN is trained on an image data set with many birds and cars but not many fire hydrants. Is the method giving us a picture of this data set?",11,242,24.2,5.134199134199134,131,1,241,0.0041493775933609,0.0246913580246913,0.8078,66,24,49,12,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 9, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 4, 'CLA': 0}",0,1,2,1,2,9,1,1,1,1,0,1,0,0,0,1,0,2,1,0,4,4,0,0.716403829838269,0.5578586199422388,0.4491744211129511
ICLR2018-BJypUGZ0Z-R1,Reject,"This paper explores the use of simple models for predicting the final validation performance of a neural network, from intermediate values during training. It uses support vector regression to show that a relatively small number of samples of hyperparameters, architectures, and validation time series can lead to reasonable predictions of eventual performance. The paper performs a modest evaluation of such simple models and shows surprisingly good r-squared values. The resulting simple prediction framework is then used for early stopping, in particular within the Hyperband hyperparameter search algorithm. There's a lot that I like about this paper, in particular the ablation study to examine which pieces matter, and the evaluation of a couple of simple models. Ultimately, however, I felt like the paper was somewhat unsatisfying as it left open a large number of obvious questions and comparisons: - The use of the time series is the main novelty. In the AP, HP and   AP+HP cases of Table 2, it is essentially the same predictive setup of SMAC, BO, and other approaches that are trying to model the map   from these choices to out-of-sample performance. Doesn't the good   performance without TS on, e.g., ResNets in Table 2 imply that the   Deep ResNets subfigure in Figure 3 should start out at 80+? - In light of the time series aspect being the main contribution, a   really obvious question is: what does it learn about the time   series? The linear models do very well, which means it should be   possible to look at the magnitude of the weights. Are there any   surprising long-range dependencies?  The fact that LastSeenValue   doesn't do as well as a linear model on TS alone would seem to   indicate that there are higher order autoregressive coefficients. That's surprising and the kind of thing that a scientific   investigation here should try to uncover; it's a shame to just put   up a table of numbers and not offer any analysis of why this works. - In Table 1 the linear SVM uniformly outperforms the RBF SVM, so why   use the RBF version? - Given that the paper seeks to use uncertainty in estimates and the   entire regression setup could be trivially made Bayesian with no   significant computational cost over a kernelized SVM or OLS,   especially if you're doing LOOCV to estimate uncertainty in the   frequentist models. Why not include Bayesian linear regression and   Gaussian process regression as baselines? - Since the model gets information from the AP and HP before doing any   iterations, why not go on and use that to help select candidates? - I don't understand how speedup is being computed in Figure 4. - I'd like a more explicit accounting of whether 0.00006 seconds vs   0.024 seconds is something we should care about in this kind of   work, when the steps can take minutes or hours on a GPU. - How useful is r-squared as a measure of performance in this setting? My experience has been that most of the search space has very poor   performance and the objective is to find the small regions that work   well. Minor things:  - y' (prime) gets overloaded in Section 3.1 as a derivative and then   in Section 4 as a partial learning curve. - ... is more computationally and ...  - ... our results for performing final ... ",25,533,26.65,5.049800796812749,270,0,533,0.0,0.0233333333333333,0.9615,148,62,83,36,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 24, 'EXP': 3, 'RES': 7, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 0, 'CLA': 0}",0,0,1,0,0,24,3,7,2,1,0,0,0,0,0,1,0,2,0,0,19,0,0,0.4348557480888565,0.3446457297176968,0.22059462891177936
ICLR2018-BJypUGZ0Z-R2,Reject,"This paper shows a simple method for predicting the performance that neural networks will achieve with a given architecture, hyperparameters, and based on an initial part of the learning curve. The method assumes that it is possible to first execute 100 evaluations up to the total number of epochs. From these 100 evaluations (with different hyperparameters / architectures), the final performance y_T is collected. Then, based on an arbitrary prefix of epochs y_{1:t}, a model can be learned to predict y_T. There are T different models, one for each prefix y_{1:t} of length t. The type of model used is counterintuitive for me; why use a SVR model? Especially since uncertainty estimates are required, a Gaussian process would be the obvious choice. The predictions in Section 3 appear to be very good, and it is nice to see the ablation study. Section 4 fails to mention that its use of performance prediction for early stopping follows exactly that of Domhan et al (2015) and that this is not a contribution of this paper; this feels a bit disingenious and should be fixed. The section should also emphasize that the models discussed in this paper are only applicable for early stopping in cases where the function evaluation budget N is much larger than 100. The emphasis on the computational demand of 1-3 minutes for LCE seems like a red herring: MetaQNN trained 2700 networks in 100 GPU days, i.e., about 1 network per GPU hour. It trained 20 epochs for the studied case of CIFAR, so 1-3 minutes per epoch on the CPU can be implemented with zero overhead while the network is training on the GPU. Therefore, the following sentence seems sensational without substance: Therefore, on a full meta-modeling experiment involving thousands of neural network configurations, our method could be faster by several orders of magnitude as compared to LCE based on current implementations.   The experiment on fast Hyperband is very nice at first glance, but the longer I think about it the more questions I have. During the rebuttal I would ask the authors to extend f-Hyperband all the way to the right in Figure 6 (left) and particularly in Figure 6 (right). Especially in Figure 6 (right), the original Hyperband algorithm ends up higher than f-Hyperband. The question this leaves open is whether f-Hyperband would reach the same performance when continued or not.  I would also request the paper not to casually mention the 7x speedup that can be found in the appendix, without quantifying this. This is only possible for a large number of 40 Hyperband iterations, and in the interesting cases of the first few iterations speedups are very small. Also, do the simulated speedup results in the appendix account for potentially stopping a new best configuration, or do they simply count how much computational time is saved, without looking at performance? The latter would of course be extremely misleading and should be fixed. I am looking forward to a clarification in the rebuttal period. For relating properly to the literatue, the experiment for speeding up Hyperband should also mention previous methods for speeding up Hyperband by a model (I only know one by the authors' reference Klein et al (2017)). Overall, this paper appears very interesting. The proposed technique has some limitations, but in some settings it seems very useful. I am looking forward to the reply to my questions above; my final score will depend on these. Typos / Details:  - The range of the coefficient of determination is from 0 to 1. Table 1 probably reports 100 * R^2? Please fix the description. - I did not see Table 1 referenced in the text. - Page 3: more computationally and -> more computationally efficient and  - Page 3: for performing final -> for predicting final    Points in favor of the paper: - Simple method - Good prediction results - Useful possible applications identified Points against the paper: - Methodological advances are limited / unmotivated choice of model - Limited applicability to settings where >> 100 configurations can be run fully - Possibly inflated results reported for Hyperband experiment",37,668,23.857142857142858,5.230145867098866,296,7,661,0.010590015128593,0.0303030303030303,0.9914,183,76,110,43,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 20, 'EXP': 7, 'RES': 7, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 18, 'SUB': 5, 'CLA': 4}",0,1,2,0,1,20,7,7,4,1,0,3,0,0,0,0,0,1,0,1,18,5,4,0.6487033985261652,0.5667989431937341,0.4080833782883774
ICLR2018-BJypUGZ0Z-R3,Reject,"This paper proposes the use of an ensemble of regression SVM models to predict the performance curve of deep neural networks. This can be used to determine which model should be trained (further). The authors compare their method, named Sequential Regression Models (SRM) in the paper, to previously proposed methods such as BNN, LCE and LastSeenValue and claim that their method has higher accuracy and less time complexity than the others. They also use SRM in combination with a neural network meta-modeling method and a hyperparameter optimization one and show that it can decrease the running time in these approaches to find the optimized parameters. Pros: The paper is proposing a simple yet effective method to predict accuracy. Using SVM for regression in order to do accuracy curve prediction was for me an obvious approach, I was surprised to see that no one has attempted this before. Using features sur as time-series (TS), Architecture Parameters (AP) and Hyperparameters (HP) is appropriate, and the study of the effect of these features on the performance has some value. Joining SRM with MetaQNN is interesting as the method is a computation hog that can benefit from such refinement. The overall structure of the paper is appropriate. The literature review seems to cover and categorize well the field. Cons: I found the paper difficult to read. In particular, the SRM method, which is the core of the paper, is not described properly, I am not able to make sense of the description provided in Sec. 3.1. The paper is not talking about the weaknesses of the method at all. The practicability of the method can be controversial, the number of attempts require to build the (meta-)training set of runs can be huge and lead to something that would be much more costful that letting the runs going on for more iterations. Questions: 1. The approach of sequential regression SVM is not explained properly. Nothing was given about the combination weights of the method. How is the ensemble of (1-T) training models trained to predict the f(T)? 2.  SRM needs to gather training samples which are 100 accuracy curves for T-1 epochs. This is the big challenge of SRM because training different variations of a deep neural networks to T-1 epochs can be a very time consuming process. Therefore, SRM has huge preparing training dataset time complexity that is not mentioned in the paper. The other methods use only the first epochs of considered deep neural network to guess about its curve shape for epoch T. These methods are time consuming in prediction time. The authors compare only the prediction time of SRM with them which is really fast. By the way still, SRM is interesting method if it can be trained once and then be used for different datasets without retraining.  Authors should show these results for SRM. 3. Discussing about the robustness of SRM for different depth is interesting and I suggest to prepare more results to show the robustness of SRM to violation of different hyperparameters. 4. There is no report of results on huge datasets like big Imagenet which takes a lot of time for deep training and we need automatic advance stopping algorithms to tune the hyper parameters of our model on it. 5. In Table 2 and Figure 3 the results are reported with percentage of using the learning curve.  To be more informative they should be reported by number of epochs, in addition or not to percentage. 6. In section 4, the authors talk about estimating the model uncertainty in the stopping point and propose a way to estimate it. But we cannot find any experimental results that is related to the effectiveness of proposed method and considered assumptions. There are also some  typos. In section 3.3 part Ablation Study on Features Sets, line 5, the sentence should be ""Ap are more important than HP"". ",33,647,16.175,4.944983818770226,260,2,645,0.0031007751937984,0.0153374233128834,0.8966,195,60,117,27,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 21, 'EXP': 6, 'RES': 5, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 25, 'SUB': 1, 'CLA': 2}",0,1,1,0,2,21,6,5,1,0,0,4,0,0,0,0,0,0,1,0,25,1,2,0.5772585473976911,0.4594525579515298,0.3310950154293533
ICLR2018-BJyy3a0Ez-R1,,"Pros: * asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models * the paper is clearly written and easy to understand Cons: * weak experiments: performance of algorithms are not analyzed in terms of wall-clock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm * weak theory: although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent, neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice; the proof is straightforward corollary of Lin et al. Summary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously. Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers. While asynchronous updates of parameters in stochastic gradient descent has been explored (dating back to [1] in 1986, and authors should also be referring to [2]), to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studied. Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine. Unfortunately, the practical usefulness of the algorithm has not been demonstrated. It remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures, or in which situations this algorithm will be more useful than existing algorithms. Experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm. What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs. Also, authors should compare against mini-batch gradient descent, because this is the most popular way of training deep neural networks; authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method. Authors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent, but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm. The algorithm operates in discrete time horizon, and continuous time is not used anywhere. Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon. Authors argue in page 1 that Continuous Propagation is statistically superior to mini-batch gradient descent, but I cannot find statistical superiority of the method. Also, the upper bound of the time-delay T slows down the convergence rate (Proposition in the appendix), so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient descent. I think which algorithm is faster depends on values of L, T and M. Authors do not provide enough citations. Continuous-time characterization of gradient descent has a long history, and authors should provide citation of it, for example when (5) is introduced. Authors should provide more discussion of the history of model-parallel asynchronous SGD (such as [1] and [2]), and when mentioning alternatives like Czarnecki et al (2017), authors should discuss what advantages and disadvantages the proposed algorithm has against these alternatives. [1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986) [2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011) ",23,617,30.85,5.830252100840336,262,2,615,0.0032520325203252,0.0337620578778135,0.981,185,89,110,40,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 15, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 4, 'CLA': 1}",0,0,5,1,0,15,5,0,0,0,1,1,3,0,0,0,0,5,0,0,10,4,1,0.5041942103790304,0.4508307411117555,0.28147122303788386
ICLR2018-BJyy3a0Ez-R2,,"The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system. Because the layer-wise decoupling, it can easily be applied for distributed training of the model. The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results. Although I found the proposed method is interesting enough to investigate more thoroughly, it is a shame to see the overall quality of the paper very weak. The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP). The experiments are also very weak. Important information on the experiment settings are missing, e.g., how the model is parallelized. - Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD. It needs to be better defined.",10,138,15.333333333333334,5.856060606060606,90,1,137,0.0072992700729927,0.0071942446043165,0.2942,37,18,27,11,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 2}",0,1,0,0,0,6,4,1,0,0,0,2,0,0,0,0,0,1,0,0,3,1,2,0.3586978238949571,0.4457694943849844,0.20459973691309497
ICLR2018-BJyy3a0Ez-R3,,"# Summary of paper The authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples. # Summary of review The idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem. However, this contribution is far from correctly addressing the problem. The Algorithm is poorly described and crucial parts of the algorithm are very confusing. Mathematical rigor in the proof and discussion is lacking. Proof has mathematical errors. # Detailed comments * Key definitions are scattered across the paper, making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a variable. To make things worse, some variables are simply not defined. For example, I can't find the definition of D. From the context it seems to be the number of layers in the network (I shouldn't need to guess). * From Algorithm 1, the bracket notation is used for both indexing and specifying the size of the variables? This is nonstandard and confusing. * Again, from Algorithm 1, it is not clear which parts can be performed asynchronously. It is even not clear to me if the algorithm can be run asynchronously (as some of the other reviewers seem to imply) or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their continuous-propagation factorization? * Eq. (3) and (4): I doubt this is true without some assumptions on the distribution of the data generating process. * The proof, despite being a trivial application of existing work, has obvious flaws. After equation (17) it is stated that the left-hand side is independent of x_{k, m, l} which is not true since Theta_{k+1} is computed **precisely** using x_{k, m, l} and so is not independent (this is actually done correctly in Lian 2015, where the expectation is correctly carried on that term).  * The proof relies on an inequality (16) in which key quantities are not defined (what is L? is L   L_d?) and which is impossible to verify in practice (T is not known). This crucial detail is only mentioned in the appendix, giving the impression in the main text that the algorithm is always convergent. It should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quantities. * As mentioned in the other reviews, key references are lacking, e.g., for ODE interpretation, Eq. (3) and (4). In appendix:   * Assumption 3, 4: Why is upper superindex d? In any case, be consistent, most of the time these are used but then its stated for all Theta (whithout superindex)  * Proposition: what is L? is L   L_d?    Other    * Assumption 5: decay -> delay?  ",22,458,19.91304347826087,5.046620046620046,214,2,456,0.0043859649122807,0.0388548057259713,0.306,114,55,97,35,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 13, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 7, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 0}",0,1,0,2,0,13,0,0,0,0,0,3,1,0,0,0,0,0,7,0,11,2,0,0.36029529998623,0.3400289977329045,0.17912755770909125
ICLR2018-Bk-ofQZRb-R1,Reject,"Summary: This paper tackles the issue of combining TD learning methods with function approximation. The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule. Results with linear and non-linear function approximation highlight the attributes of the method. Quality: The quality of the writing, notation, motivation, and results analysis is low . I will give a few examples to highlight the point. The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed. There are many missing references: ETD, HTD, mirror-prox methods, retrace, ABQ. Q-sigma. This is a very active area of research and the paper needs to justify their approach. The paper has straightforward technical errors and naive statements: e.g. the equation for the loss of TD takes the norm of a scalar.  The paper claims that it is not well-known that TD with function approximation ignores part of the gradient of the MSVE. There are many others. The experiments have serious issues. Exp1 seems to indicate that the new method does not converge to the correct solution. The grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discussed. Again exp3 provides little information about the experimental setup. Clarity: The clarity of the text is fine, though errors make things difficult sometimes. For example The Bhatnagar 2009 reference should be Maei. Originality: As mentioned above this is a very active research area, and the paper makes little effort to explain why the multitude of existing algorithms are not suitable.  Significance: Because of all the things outlined above, the significance is below the bar for this round.",18,310,14.761904761904765,5.231023102310231,167,2,308,0.0064935064935064,0.0319488817891373,0.8393,104,31,54,12,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 5, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 2}",0,1,0,0,0,7,4,2,0,1,0,5,2,1,0,0,1,0,1,0,8,0,2,0.5734380816161544,0.4488792815591992,0.3200830088192946
ICLR2018-Bk-ofQZRb-R2,Reject,"This paper proposes adding a constraint to the temporal difference update to minimize the effect of the update on the next state's value. The constraint is added by projecting the original gradient to the orthogonal of the maximal direction of change of the next state's value. It is shown empirically that the constrained update does not diverge on Baird's counter example and improves performance in a grid world domain and cart pole over DQN. This paper is reasonably readable. The derivation for the constraint is easy to understand and seems to be an interesting line of inquiry that might show potential. The key issue is that the justification for the constrained gradients is lacking. What is the effect, in terms of convergence, in modifying the gradient in this way? It seems highly problematic to simply remove a whole part of the gradient, to reduce effect on the next state. For example, if we are minimizing the changes our update will make to the value of the next state, what would happen if the next state is equivalent to the current state (or equivalent in our feature space)?  In general, when we project our update to be orthogonal to the maximal change of the next states value, how do we know it is a valid direction in which to update? I would have liked some analysis of the convergence results for TD learning with this constraint, or some better intuition in how this effects learning. At the very least a mention of how the convergence proof would follow other common proofs in RL. This is particularly important, since GTD provides convergent TD updates under nonlinear function approximation; the role for a heuristic constrained TD algorithm given convergent alternatives is not clear. For the experiments, other baselines should be included, particularly just regular Q-learning. The primary motivation comes from the use of a separate target network in DQN, which seems to be needed in Atari (though I am not aware of any clear result that demonstrates why, rather just from informal discussions). Since you are not running experiments on Atari here, it is invalid to simply assume that such a second network is needed. A baseline of regular Q-learning should be included for these simpler domains. The results in Baird's counter example are discouraging for the new constraints. Because we already have algorithms which better solve this domain, why is your method advantageous? The point of showing your algorithm not solve Baird's counter example is unclear. There are also quite a few correctness errors in the paper, and the polish of the plots and language needs work, as outlined below.  There are several mistakes in the notation and background section. 1. ""If we consider TD-learning using function approximation, the loss that is minimized is the squared TD error. "" This is not true; rather, TD minimizes the mean-squared project Bellman error. Further, L_TD is strangely defined: why a squared norm, for a scalar value?  2. The definition of v and delta_TD w.r.t. to v seems unnecessary, since you only use Q. As an additional (somewhat unimportant) point, the TD-error is usually defined as the negative of what you have. 3. In the function approximation case the value function and q functions parameterized by theta are only approximations of the expected return. 4. Defining the loss w.r.t. the state, and taking the derivative of the state w.r.t. to theta is a bit odd. Likely what you meant is the q function, at state s_t? Also, are ignoring the gradient of the value at the next step? If so, this further means that this is not a true gradient. There is a lot of white space around the plots, which could be used for larger more clear figures. The lack of labels on the plots makes them hard to understand at a glance, and the overlapping lines make finding certain algorithm's performance much more difficult. I would recommend combining the plots into one figure with a drawing program so you have more control over the size and position of the plots. Examples of odd language choices: t-t""The idea also does not immediately scale to nonlinear function approximation. Bhatnagar et al. (2009) propose a solution by projecting the error on the tangent plane to the function at the point at which it is evaluated. "" - The paper you give exactly solves for the nonlinear function approximation case. What do you mean does not scale to nonlinear function approximation? Also Maei is the first author on this paper. t-t""Though they do not point out this insight as we have"" - This seems to be a bit overreaching. - ""the gradient at s_{t+1} that will change the value the most""  - This is too colloquial. I think you simply mean the gradient of the value function, for the given s_t, but its not clear. ",43,802,18.227272727272727,4.893368010403121,322,10,792,0.0126262626262626,0.031980319803198,0.3095,214,90,136,46,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 3, 'MET': 25, 'EXP': 6, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 5, 'REC': 0, 'EMP': 31, 'SUB': 0, 'CLA': 1}",0,1,6,1,3,25,6,1,4,0,0,2,2,0,0,0,0,4,5,0,31,0,1,0.7213236306618043,0.4637009308565168,0.4086293503239205
ICLR2018-Bk-ofQZRb-R3,Reject,"This is an interesting idea, and written clearly. The experiments with Baird's and CartPole were both convincing as preliminary evidence that this could be effective.  However, it is very hard to generalize from these toy problems. First, we really need a more thorough analysis of what this does to the learning dynamics itself. Baring theoretical results, you could analyze the changes to the value function at the current and next state with and without the constraint to illustrate the effects more directly. I think ideally, I would want to see this on Atari or some of the continuous control domains often used. If this allows the removing of the target network for instance, in those more difficult tasks, then this would be a huge deal. Additionally, I do not think the current gridworld task adds anything to the experiments, I would rather actually see this on a more interesting linear function approximation on some other simple task like Mountain Car than a neural network on gridworld. The reason this might be interesting is that when the parameter space is lower dimensional (not an issue for neural nets, but could be problematic for linear FA) the constraint might be too much leading to significantly poorer performance.  I suspect this is the actual cause for it not converging to zero for Baird's, although please correct me if I'm wrong on that. As is, I cannot recommend acceptance given the current experiments and lack of theoretical results. But I do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it. Pros: Simple, interesting idea Works well on toy problems, and able to prevent divergence in Baird's counter-example Cons: Lacking in theoretical analysis or significant experimental results.",14,292,22.46153846153846,5.139285714285714,157,6,286,0.0209790209790209,0.0442176870748299,0.7103,72,39,45,24,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 6, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,0,0,3,1,6,5,3,0,3,0,0,0,0,0,0,1,0,0,1,11,1,1,0.4304466145775495,0.5617751299039853,0.2657034270981822
ICLR2018-Bk346Ok0W-R1,Reject,"This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism. Pros: One of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on CHiME-3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions. This analysis is quite intuitive, and also shows the effectiveness of the proposed method in this practical setup. Cons: The idea seems to be simple and does not have significant originality. Also, the paper does not clearly mention the attention mechanism part, and needs some improvement. Comments: -tThe paper mainly focuses on the soft sensor selection. However, in an array signal processing context (and its application to multichannel speech recognition), it would be better to mention beamforming techniques, where the compensation of the delays of sensors is quite important. -tIn addition, there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by Ochiai et al, A Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming, IEEE Journal of Selected Topics in Signal Processing.  This paper uses the same CHiME-3 database, and also showing a similar analysis of channel selection. It's better to discuss about this paper as well as a reference. -tSection 2: better to explain about how to obtain attention scores z in more details. -tFigure 3, experiments of Double audio/video clean conditions: I cannot understand why they are improved from single audio/video clean conditions.Need some explanations. -tSection 3.1: 39-dimensional Mel-frequency cepstral coefficients (MFCCs) -> 13 -dimensional Mel-frequency cepstral coefficients (MFCCs) with 1st and 2nd order delta features. -tSection 3.2 Dataset ""As for TIDIGIT"": ""As for GRID""(?) -tSection 4 Models ""The parameters of the attention modules are either shared across sensors (STAN-shared) or not shared across sensors (STAN- default). "": It's better to explain this part in more details, possibly with some equations. It is hard to understand the difference.  ",16,327,19.23529411764705,5.704761904761905,173,2,325,0.0061538461538461,0.0150602409638554,0.9751,109,48,45,18,11,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 7, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 3, 'REC': 0, 'EMP': 3, 'SUB': 5, 'CLA': 0}",0,1,1,2,3,7,3,1,1,2,0,0,2,1,0,1,0,3,3,0,3,5,0,0.7875564237075073,0.557583639420514,0.4984276361834098
ICLR2018-Bk346Ok0W-R2,Reject,"The manuscript introduces the sensor transformation attention networks, a generic neural architecture able to learn the attention that must be payed to different input channels (sensors) depending on the relative quality of each sensor with respect to the others. Speech recognition experiments on synthetic noise on audio and video, as well as real data are shown. First of all, I was surprised on the short length of the discussion on the state-of-the-art. Attention models are well known and methods to merge information from multiple sensors also (very easily, Multiple Kernel Learning, but many others). Second, from a purely methodological point of view, STANs boil down to learn the optimal linear combination of the input sensors. There is nothing wrong about this, but perhaps other more complex (non-linear) models to combine data could lead to more robust learning. Third, the experiments with synthetic noise are significant to a reduced extend. Indeed, adding Gaussian noise to a replicated input is too artificial to be meaningful. The network is basically learning to discard the sensor when the local standard deviation is high. But this is not the kind of noise found in many applications, and this is clearly shown in the performances on real data (not always improving w.r.t state of the art). The interesting part of these experiments is that the noise is not stationary, and this is quite characteristic of real-world applications. Also, to be fair when discussion the results, the authors should say that simple concatenation outperforms the single sensor paradigm. I am also surprised about the baseline choice. The authors propose a way to merge/discard sensors, and there is no comparison with other ways of doing it (apart from the trivial sensor concatenation). It is difficult to understand the benefit of this technique if no other baseline is benchmarked. This mitigates the impact of the manuscript. I am not sure that the discussion in page corresponds to the actual number on Table 3, I did not understand what the authors wrote.",17,332,19.529411764705884,5.180124223602484,170,1,331,0.0030211480362537,0.0271084337349397,0.9708,90,47,55,23,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 5, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 1}",0,1,2,2,0,6,5,2,1,0,1,0,0,0,0,0,1,4,2,0,6,2,1,0.5732198048212787,0.6702961869445763,0.4011987738572862
ICLR2018-Bk346Ok0W-R3,Reject,"Summary:   The authors consider the use of attention for sensor, or channel, selection. The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels. Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features. The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data. Review:  The paper reads well, but as a standard application of attention lacks novelty. The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017). Furthermore, while their approach is sold as a general sensor fusion technique, most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase. Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better! The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better!  http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed...  Overall Assessment:    In summary, the paper lacks novelty wrt technique, and as an ""application-of-attention"" paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed. As such, I recommend that the paper be rejected. Additional comments:   -tThe experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised? Were the baselines with concatenated features optimized independently? Why is there no multi-channel baseline for the GRID results? -tIssue with noise bursts plot (Input 1+2 attention does not sum to 1) -tA concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)... ",19,344,22.933333333333334,5.443786982248521,190,0,344,0.0,0.0056179775280898,-0.4605,118,37,57,19,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 5, 'MET': 5, 'EXP': 2, 'RES': 7, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 2, 'CLA': 1}",0,1,2,1,5,5,2,7,0,1,0,3,0,0,0,2,0,1,0,1,12,2,1,0.6445940069383059,0.6736599825427789,0.4625074555906263
ICLR2018-Bk6qQGWRb-R1,Reject,"The authors propose a new algorithm for exploration in Deep RL. They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action. Posterior weights are sampled to select actions during execution (Thompson Sampling style). I generally liked the paper and the approach, here are some more detailed comments. Unlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets. It's not immediately clear what the semantics of this posterior are then. Take for example the case where a particular transition (s,a,r,s') gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced. Previous applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode. But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling? It would be useful to comment on that aspect. The method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN). DDQN wasn't proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods). The authors argue they could not reproduce Osband's bootstrapped DQN, which is also TS-based, but you could at least have reported their scores. On these games versus (their implementation of) DDQN, the results seem encouraging. But it would be good to know whether the approach works well across games and is competitive against other stronger baselines. Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice. In addition, your reported score on Atlantis of ~2M seems too big. Did you cap the max episode time to 30mins? As is done in the baselines usually. Minor things: -""TS finds the true Q-function very fast"" But that contradicts the previous statements, I think you mean something different. If TS does not select certain actions, the Q-function would not be updated for these actions. It might find the optimal policy quickly though, even though it doesn't resolve the entire value function completely. -Which epsilon did you use for evaluation of DDQN in the experiments? It's a bit suspicious that it doesn't achieve 20+ in Pong. -The history of how to go from a Bellman equation to a sample-based update seems a bit distorted. Sample-based RL did not originate in 2008. Also, DQN does not optimize the Bellman residual, it's a TD update.  ",27,452,18.08,5.169373549883991,247,6,446,0.0134529147982062,0.026431718061674,0.9887,112,59,90,31,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 0, 'DAT': 0, 'MET': 21, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 1, 'CLA': 0}",0,0,6,0,0,21,2,5,0,0,0,1,0,0,0,0,0,4,0,0,16,1,0,0.3628581422930169,0.3430141825275478,0.18320292710690805
ICLR2018-Bk6qQGWRb-R2,Reject,"The authors describe how to use Bayesian neural networks with Thompson sampling for efficient exploration in q-learning. The Bayesian neural networks are only Bayesian in the last layer.  That is, the authors learn all the previous layers by finding point estimates. The Bayesian learning of the last layer is then tractable since it consists of a linear Gaussian model. The resulting method is called BDQL. The experiments performed show that the proposed approach, after hyper-parameter tuning, significantly outperforms the epsilon-greedy exploration approaches such as DDQN. Quality:  I am very concern about the authors stating on page 1 we sample from the posterior on the set of Q-functions. I believe this statement is not correct. The Bayesian posterior distribution is obtained by combining an assumed generative model for the data, data sampled from that model and some prior assumptions. In this paper there is no generative model for the data and the data obtained is not actually sampled from the model. The data are just targets obtained by the q-learning rule. This means that the authors are adapting Q-learning methods so that they look Bayesian, but in no way they are dealing with a principled posterior distribution over Q-functions. At least this is my opinion, I would like to encourage the authors to be more precise and show in the paper what is the exact posterior distribution over Q-functions and show how they approximate that distribution, taking into account that a posterior distribution is obtained as $p(theta|D) propto p(D|theta)p(theta)$. In the case addressed in the paper, what is the likelihood $p(D|theta)$ and what are the modeling assumptions that explain how $D$ is generated by sampling from a model parameterized by theta? I am also concerned about the hyper-parameter tuning for the baselines. In section 5 (choice of hyper-parameters) the authors describe a quite exhaustive hyper-parameter tuning procedure for BDQL. However, they do not mention whether they perform a similar hyper-parameter tuning for DDQN, in particular for the parameter epsilon which will determine the amount of exploration. This makes me wonder if the comparison in table 2 is fair. Especially, because the authors tune the amount of data from the replay-buffer that is used to update their posterior distribution. This will have the effect of tuning the width of their posterior approximation which is directly related to the amount of exploration performed by Thompson sampling. You can, therefore, conclude that the authors are tuning the amount of exploration that they perform on each specific problem. Is that also being done for the baseline DDQN, for example, by tuning epsilon in each problem? The authors also report in table 2 the scores obtained for DDQN by Osband et al. 2016. What is the purpose of including two rows in table 2 with the same method? It feels a bit that the authors want to hide the fact that they only compare with a singe epsilon-greedy baseline (DDQN). Epsilon-greedy methods have already been shown to be less efficient than Bayesian methods with Thompson sampling for exploration in q learning (Lipton et al. 2016). The authors do not compare with variational approaches to Bayesian learning (Lipton et al. 2016). They indicate that since Lipton et al. do not investigate the Atari games, we are not able to have their method as an additional baseline. This statement seems completely unjustified. The authors should clearly include a description of why Lipton's approach cannot be applied to the Atari games or include it as a baseline. The method proposed by the authors is very similar to Lipton's approach. The only difference is that Lipton et al. use variational inference with a factorized Gaussian distribution to approximate the posterior on all the network weights. The authors by contrast, perform exact Bayesian inference, but only on the last layer of their neural network. It would be very useful to know whether the exact linear Gaussian model in the last layer proposed by the authors has advantages with respect to a variational approximation on all the network weights. If Lipton's method would be expensive to apply to large-scale settings such as the Atari games, the authors could also compare with that method in smaller and simpler problems. The plots in Figure 2 include performance in terms of episodes. However, it would also be useful to know how much is the extra computational costs of the proposed method. One could imagine that computing the posterior approximation in equation 6 has some additional cost. How do BDQN and DDQN compare when one takes into account running time and not episode count into account? Clarity:  The paper is clearly written. However, I found a lack of motivation for the specific design choices made to obtain equations 9 and 10. What is a_t in equation 9? The parameters theta are updated just after equation 10 by following the gradient of the loss in which the weights of the last layer are fixed to a posterior sample, instead of the posterior mean. Is this update rule guaranteed to produce convergence of theta? I could imagine that at different times, different posterior samples of the weights will be used to compute the gradients. Does this create any instability in learning? I found the paragraph just above section 5 describing the maze-like deterministic game confusing and not very useful.  The authors should improve this paragraph. Originality:  The proposed approach in which the weights in the last layer of the neural network are the only Bayesian ones is not new. The same method was proposed in  Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... & Adams, R. (2015, June). Scalable Bayesian optimization using deep neural networks. In International Conference on Machine Learning (pp. 2171-2180).  which the authors fail to cite. The use of Thompson sampling for efficient exploration in deep Q learning is also not new since it has been proposed by Lipton et al. 2016. The main contribution of the paper is to combine these two methods (equations 6-10) and evaluate the results in the large-scale setting of ATARI games, showing that it works in practice. Significance:  It is hard to determine how significant the work is since the authors only compare with a single baseline and leave aside previous work on efficient exploration with Thompson sampling based on variational approximations. As far as the method is described, I believe it would be impossible to reproduce their results because of the complexity of the hyper-parameter tuning performed by the authors. I would encourage the authors to release code that can directly generate Figure 2 and table 2. ",57,1092,17.901639344262296,5.179363548698168,359,1,1091,0.0009165902841429,0.0163191296464188,0.9943,296,129,194,50,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 13, 'PDI': 0, 'DAT': 4, 'MET': 46, 'EXP': 1, 'RES': 3, 'TNF': 5, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 15, 'PNF': 2, 'REC': 0, 'EMP': 19, 'SUB': 5, 'CLA': 1}",0,0,13,0,4,46,1,3,5,4,0,1,0,0,0,1,0,15,2,0,19,5,1,0.5840400973672638,0.679990451251589,0.41723139081878374
ICLR2018-Bk6qQGWRb-R3,Reject,"(Last minute reviewer brought in as a replacement).  This paper proposed Bayesian Deep Q-Network as an approach for exploration via Thompson sampling in deep RL. This algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty. The agent then samples from this posterior for an approximate Thompson sampling. Experimental results show that this outperforms an epsilon-greedy baseline. There are several things to like about this paper: - The problem of efficient exploration with deep RL is important and under-served by practical algorithms. This seems like a good algorithm in many ways. - The paper is mostly clear and well written. - The experimental results are impressive in their outperformance. However, there are also some issues, many of which have already been raised: - The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example). - There are some loose and misleading descriptions of the algorithm computing the posterior when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away. - The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI. The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph.  RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method. - This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets. Overall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network. However, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach. For example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example). In it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly.                                                                  Revising my review following the rebuttal period and also the (ongoing) revisions to the paper. I've been disappointed by the authors have incorporated the feedback/reviews - I expected something a little more clear / honest. Given the ongoing review decisions/issues I'm putting my review slightly below accept.. ## Relation to literature on randomized value functions It's really wrong to present BDQN as is if it's the first attempt at large-scale approximations to Thompson sampling (and then slip in a citation to RLSVI as a BDQN-like algorithm). This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this. Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm! If you say something like they did not estimate a true posterior then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior. In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN. ## Quality/science of experiments The experimental results have been updated, and the performance of the baseline now seems much more reasonable. However, the procedure for selecting arbitrary number of frames to report performance seems really unnecessary.... it would be clear that BDQN is outperforming DDQN.. you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific. This type of stuff smells like overfitting!.",31,713,19.80555555555556,5.236574746008708,312,7,706,0.0099150141643059,0.0314465408805031,0.9958,174,97,124,56,9,5,"{'ABS': 0, 'INT': 2, 'RWK': 8, 'PDI': 0, 'DAT': 2, 'MET': 20, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 9, 'PNF': 0, 'REC': 1, 'EMP': 15, 'SUB': 1, 'CLA': 1}",0,2,8,0,2,20,5,3,0,1,0,3,0,2,0,0,0,9,0,1,15,1,1,0.6487921706414602,0.565200260100877,0.4070215589787301
ICLR2018-Bk7wvW-C--R3,Reject,"The authors build on the work of Tang et al. (2017), who made a minor change to the skip-thought model by decoding only the next sentence, rather than the previous one also. The additional minor change in this paper is to use a CNN, rather than RNN, decoder .  I am sympathetic to the goals of the work, and believe this sort of work should be carried out, but I see the contribution as too minor to constitute a paper at the conference track of a leading international conference such as ICLR. Given the incremental nature of the work, I think this would be a good fit for something like a short paper at *ACL .  I found the more theoretical motivation of the CNN decoder not terribly convincing, and somewhat post-hoc . I feel as though analogous arguments could just as easily be made for an RNN decoder . Ultimately I see these questions - such as CNN vs. RNN for the decoder - as empirical ones.  Finally, the authors have admirably attempted a thorough comparison with existing work, in the related work section, but this section takes up a large chunk of the paper at the end, and again I would have preferred this section to be much shorter and more concise. Summary: worthwhile empirical goal, but the paper could have been easily written using half as much space.  ",7,225,18.75,4.732057416267943,121,1,224,0.0044642857142857,0.0211864406779661,0.9819,57,32,33,19,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 2, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,1,3,0,2,2,0,0,0,0,2,0,1,1,0,0,1,0,0,2,0,1,0.429071835008684,0.4450664018792874,0.241170538937478
ICLR2018-Bk8ZcAxR--R1,Accept,"The paper extends the idea of eigenoptions, recently proposed by Machado et al. to domains with stochastic transitions and where state features are learned. An eigenoption is defined as an optimal policy for a reward function defined by an eigenvector of the matrix of successor representation (SR), which is an occupancy measure induced here by a uniform policy. In high-dimensional state space, the authors propose to approximate that matrix with a convolutional neural network (CNN). The approach is evaluated in a tabular domain (i.e., rooms) and Atari games. Overall the paper is well-written and quite clear. The proposed ideas for the extension seem natural (i.e., use of SR and CNN). The theorem stated in the paper seems to provide an interesting link between SR and the Laplacian. However, a few points are not clear to me: - Is the result new or not?  If I understand correctly, Stachenfeld et al. discussed this result, but didn't prove it. Is that correct? So the provided proof is new? - Besides, how are D and W exactly defined? - Finally, as the matrix is not symmetric, do real eigenvalues always exist? The execution of the proposed ideas in the experiments was a bit disappointing to me. The approximated eigenoption was simply computed as a one-step greedy policy. Besides, the eigenoptions seem to help for exploration (as a uniform policy was used) as indicated by plot 3(d), but could they help for other tasks (e.g., learn to play Atari games faster or better)?  I think that would be a more useful measure for the learned eigenoptions. During learning SR and the features, what would be the impact if the gradient for SR estimation were also propagated? In Figure 4, the trajectories generated by the different eigenoptions are barely visible. Some typos: - Section 2.1: in the definition of G_t, the expectation is taken over p as well I_w and T_w should be a subset of S - in (2), the hat is missing over Psi in the definition of v_pi(s), r only depends on s'? This seems inconsistent with the previous definition of psi  - p. 6: in the definition of L_{SR}(s, s'), why psi takes phi(s) as argument?  - in conclusion: that that",23,363,22.6875,4.887573964497041,184,3,360,0.0083333333333333,0.0374331550802139,0.9569,103,43,61,21,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 15, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 5}",0,1,3,2,0,15,1,2,0,0,0,1,0,0,0,1,0,0,0,0,10,0,5,0.5037786671523184,0.3392554905303363,0.250625390105487
ICLR2018-Bk8ZcAxR--R2,Accept,"- This paper shows an equivalence between proto value functions and successor representations. It then derives the idea of eigen options from the successor representation as a mechanism for option discovery. The paper shows that even under a random policy, the eigen options can lead to purposeful options   - I think this is an important conceptual paper. Automatic option discovery from raw sensors is perhaps one of the biggest open problems in RL research. This paper offers a new conceptual setup to look at the problem and consolidates different views (successor repr, proto values, eigen decomposition) in a principled manner. - I would be keen to see eigen options being used inside of the agent. Have authors performed any experiments ? - How robust are the eigen options for the Atari experiments? Basically how hand picked were the options? - Is it possible to compute eigenoptions online?? This seems crucial for scaling up this approach?",12,149,21.285714285714285,5.356643356643357,95,3,146,0.0205479452054794,0.0318471337579617,0.9226,53,15,24,5,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,0,2,0,3,4,0,0,0,0,2,0,1,0,1,0,0,0,0,5,0,0,0.4294351188440303,0.2247100519615941,0.19158538542768977
ICLR2018-Bk8ZcAxR--R3,Accept,"Eigenoption Discovery Through the Deep Successor Representation The paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called ""eigenoptions"". In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options. They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features. The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one. Interestingly, the way they do so is through the successor representation (SR). Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP. Thus, if we have the SR matrix we can replace the Laplacian mentioned above. This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics.  On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs. Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case . The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation. The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over). This is a well-written paper with interesting (and potentially useful) insights. I only have a few comments regarding some aspects of the paper that could perhaps be improved, such as the way eigenoptions are evaluated. One question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options). In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions. The way the authors propose to do it is to have the agent follow a random policy. So, in order to have options that lead to more direct, purposeful behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space. This problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue. One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk.  First, although this metric makes intuitive sense, it is unclear to me how much it reflects control performance, which is what we ultimately care about. Perhaps more important, measuring performance using the same policy used to build the options (the random policy) seems somewhat unsatisfactory to me. To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space. In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper. Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible). More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance). Another point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2. In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than ""random options"" built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals. This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well. Interestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero).  The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative. As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies. Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them.",34,958,24.56410256410257,4.986970684039088,346,11,947,0.0116156282998944,0.0207253886010362,0.9959,238,84,185,66,8,2,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 4, 'DAT': 1, 'MET': 21, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 21, 'SUB': 0, 'CLA': 0}",0,1,6,4,1,21,3,2,0,0,0,2,0,0,0,0,0,1,0,0,21,0,0,0.5772862650859293,0.2346613709190816,0.2630316198352283
ICLR2018-Bk9nkMa4G-R1,,"The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution, ie for classes with relatively few annotated examples. In order to do so, they constrain the final softmax layer, using weights and biases based on the class means, in a nearest-class-mean style layer. In practice the class means are learned, yet regularised towards the batch class means. My main concern with the paper is in the theoretical underpinning of the work. From the title a Bayesian approach is suggested, while in practice a rather standard softmax classifier is learned, albeit with a different regulariser (last layer is regularised towards batch class means). Also the Gaussian Mixture Model, is not a true mixture model, in the sense that normally GMMs are used for describing a distribution of unlabelled data, in this case, each class is described with a Gaussian, and thus the class probabilities are the reseponsibilities proportional to the class Gaussian. To take this one further, it is assumed that there is equal class probabilities and each class has a the same Identity matrix as covariance matrix. Taking away a large part of the Gaussian distribution. The relation (Eq 6) with Softmax is insightful, yet already discussed in eg Mensink et al 2013 (already cited for the Nearest Class Mean classifier).  A second concern is the experimental exploration. First of all, it is unclear if the method works much better for the tail than the standard softmax. That is not apparent from the results. For example, Fig 4 shows -except for CIFAR10- not a clear relation between class index and proposed relative improvement, it is also unclear if there is just a difficult class (eg at index 150), or that the experiment has been repeated several times. Moreover, when the performance becomes more stable for the classes in the tail, I'd have expected that the standard deviation of the mean class accuracy would decrease, from the results there is no difference between Softmax and the proposed method: 44 +/-1 for Softmax (miniImageNet) to 41 +/-1 for the proposed NCM approach. In the final experiment is the regularised version  compared to an unregularised one, which shows that the first performs better. However, I'm a little unsure about these conclusions, what is the unregularized version exactly doing, how is it different from a standard softmax? Remaining (minor) remarks: - It is unclear how iCaRL has been used - it has been proposed as an iterative classification method. - Eq 2: how would this perform on a learned Softmax representation? Preferably including the (co)variance and class priors? - Figure 4: Gain -> Relative performance - The batch size must have a great influence on the functioning of the regularisation (especially when there are many classes, in that case just a single example counts for the class mean). This is not explored in the paper.",19,480,26.666666666666668,5.157079646017699,208,0,480,0.0,0.0245901639344262,0.8809,139,57,78,25,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 14, 'EXP': 5, 'RES': 5, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,1,2,1,1,14,5,5,2,1,0,0,0,0,0,1,0,3,1,0,10,1,0,0.6467810169371067,0.5613874975835222,0.40699671400850934
ICLR2018-Bk9nkMa4G-R2,,"The paper aims to address a common issue in many classification applications: that the number of training data from different classes is much unbalanced. The paper proposes a Bayesian framework to address it with a Gaussian mixture model. Overall the math looks reasonable. I am not sure about the novelty of the paper, as it is a relatively standard definition of Bayesian math. Essentially, instead of computing a softmax prediction which is the discrimination probability of each class given the input, one uses a logistic regression type interpretation (equation 1). This has been used in multi-class classification before. For example, many early SVM papers deal with multi-class classification by training 1-vs-all classifiers on each class and then choose the one having the highest score (possibly with a class-prior adjustment). n Note that this actually changes the underlying assumption a bit: softmax basically assumes the classes are mutually exclusive, while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple classes. This probably does not match the assumption of many of the datasets being tested upon (CIFAR, MNIST) but I don't consider that a fundamental issue. I am quite a bit concerned about the experimentation protocol as well. The datasets are relatively smaller scale, and datasets such as MNIST and CIFAR are known to overfit. As a result, although there are approaches taken to generate unbalanced datasets out of them (e.g. MNIST). Regardless, the results seem to suggest that the proposed method is similar to softmax performance - which is expected as they are similar - but I am not sure if it accurately evaluated / analyzed the possible application and performance gain of the proposed method.",13,282,20.142857142857142,5.471698113207547,156,2,280,0.0071428571428571,0.0524475524475524,0.6933,71,33,57,22,8,4,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 1, 'DAT': 3, 'MET': 8, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 6, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,2,4,1,3,8,2,1,0,0,0,0,0,1,0,3,0,6,0,0,6,1,0,0.5736052761330271,0.4482302595736454,0.3134764365250286
ICLR2018-Bk9nkMa4G-R3,,"This paper presents a method based on a Bayesian classifier that improves classification of rare classes in datasets with long tail class distributions. The method is based on balance the class-priors to generalize well for rare classes. By using a Gaussian Mixture Model (GMM), authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models, such as current deep learning classifiers. Authors, also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail. They face class imbalance problems, particular long tail distributions, by fixing: i) The covariance matrices of all the classes to be the identity, and ii) The priors over each class to be uniform. So all classes, popular and rare, have equal weight for Bayesian classification. To me, this is not a fundamental way to solve the long-tail problem, in the sense that by fixing isotropic likelihoods and flat priors, authors are also ignoring information that can be relevant in some classification problems, where a good prior can be useful to disambiguate confusing situations. On other hand, using a unimodal function to model each class is an over-simplification that ignores intra-class complexity. The datasets used by the authors are balanced, so they artificially transform them into long-tailed, it will be good to test directly on real long-tailed datasets. The experimentation is only performed using small to medium datasets (< 80K instances), it will be good to show if the benefits of the proposed approach can also be present in the case of large datasets. In this sense, I agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy, however, the protocol proposed requires to train the model several times, therefore, it does not scale properly to large datasets that are the common rule in the deep learning world. Results respect to similar state-of-the-art techniques shows a reasonable improvement (depends of the dataset, approx. 1-3%).",13,336,25.846153846153847,5.427244582043343,172,0,336,0.0,0.0089020771513353,0.9616,98,44,57,16,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 5, 'MET': 8, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 0}",0,1,0,1,5,8,3,2,0,0,0,0,0,0,0,0,0,1,0,0,4,3,0,0.4306952298036219,0.3354125586545052,0.21962272723221932
ICLR2018-Bk9zbyZCZ-R1,Accept,"The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments. The memory consists of a 2D array and includes trainable read/write mechanisms. The RL agent's policy is a function of the context read, read, and next step write vectors (which are functions of the observation). The effectiveness of the proposed architecture is evaluated via reinforcement learning (% of mazes solved). The evaluation included 1000 test mazes--which sets a good precedent for evaluation in this subfield. My main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architecture. The downsampling experiment in Section 5.1 seems to indicate the contrary: downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own location. I'm concerned whether the proposed agent is actually employing a navigation strategy, as seems to be suggested, or is simply a good agent architecture for this task (e.g. for optimization reasons). The short experiment in Appendix E seems to try and answer this question, but it's results are anecdotal at best. If good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation (world centric) into memory and use something like a value iteration network or shortest path planning to plan routes. My point is that there are classical algorithms to solve navigation even in partially observable 2D grid worlds, why bother with deep RL here? ",11,260,21.666666666666668,5.254032258064516,150,4,256,0.015625,0.0534351145038167,0.9549,80,26,53,10,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,1,0,1,2,4,4,3,0,0,0,0,0,0,0,1,0,0,0,0,5,2,0,0.429728086813221,0.3359278395810267,0.20969317219256947
ICLR2018-Bk9zbyZCZ-R2,Accept,"This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity. The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory. Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector. The position {x,y} is a binned representation of discrete or continuous coordinates. The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves. The experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment. The most important aspect is the capability to build a feature map of previously unseen environments. This paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited. It has been improved since, through different sets of experiments and apparently a clearer presentation, but the ideas are the same. I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there). What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research, or are they ignorant, and is the peer review system broken? I quite like the formulation of the NIPS ratings: if this paper does not get accepted, I am considering boycotting the conference. * The noise model experiment in Appendix D is commendable,  but the noise model is somewhat unrealistic (very small variance, zero mean Gaussian) and assumes only drift in x and y, not along the orientation. While this makes sense in grid world environments or rectilinear mazes, it does not correspond to realistic robotic navigation scenarios with wheel skid, missing measurements, etc...  Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix).",17,413,29.5,5.10025706940874,229,1,412,0.0024271844660194,0.014388489208633,-0.6411,121,44,75,27,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 1, 'MET': 5, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 2}","{'APR': 1, 'NOV': 2, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,4,3,1,5,4,0,0,0,0,2,1,2,1,2,0,3,2,0,5,1,0,0.6445313089944786,0.66949549802874,0.45651904042732516
ICLR2018-Bk9zbyZCZ-R3,Accept,"# Summary This paper presents a new external-memory-based neural network (Neural Map) for handling partial observability in reinforcement learning. The proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memory. The results on several memory-related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN/FRMQN. [Pros] - The overall direction toward more flexible/scalable memory is an important research direction in RL. - The proposed memory architecture is new.  - The paper is well-written. [Cons] - The proposed memory architecture is new but a bit limited to 2D/3D navigation tasks. - Lack of analysis of the learned memory behavior. # Novelty and Significance The proposed idea is novel in general. Though [Gupta et al.] proposed an ego-centric neural memory in the RL context, the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory, whereas [Gupta et al.] designed the memory specifically for predicting free space. On the other hand, the proposed method is also specific to navigation tasks in 2D or 3D environment, which is hard to apply to more general memory-related tasks in non-spatial environments. But, it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment. # Quality The experiment does not show any analysis of the learned memory read/write behavior especially for ego-centric neural map and the 3D environment. It is hard to understand how the agent utilizes the external memory without such an analysis. # Clarity The paper is overall clear and easy-to-follow except for the following. In the introduction section, the paper claims that the expert must set M to a value that is larger than the time horizon of the currently considered task when mentioning the limitation of the previous work. In some sense, however, Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task. ",17,331,18.38888888888889,5.342679127725857,155,0,331,0.0,0.0029239766081871,0.9775,97,57,46,14,8,7,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 3, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 2}",0,2,1,4,0,7,1,1,0,3,0,2,0,0,0,3,3,2,1,0,4,3,2,0.5732280001954452,0.7802415891929257,0.4587939305753489
ICLR2018-BkA7gfZAb-R1,Reject,"The paper deals with ""fixing GANs at the computational level"", in a similar sprit to f-GANs and WGANs.  The fix is very specific and restricted. It relies on the logistic regression model as the discriminator, and the dual formulation of logistic regression by Jaakkola and Haussler. Comments:  1) Experiments are performed by restricting alternatives to also use a linear classifier for the discriminator. It is mentioned that results are expected to be lower than those produced by methods with a multi-layer classifier as the discriminator (e.g. Shen et al., Wasserstein distance guided representation learning for domain adaptation, Ganin et al., Domain-adversarial training of neural networks?). 2) Considering this is an unsupervised domain adaption problem, how do you set the hyper-parameters lambda and the kernel width? The ""reverse validation"" method described in Ganin et al., Domain-adversarial training of neural networks, JMLR, 2016 might be helpful. Minor comments: on the upper-bound of the distance, alpha_i instead of alpha^top, and please label the axes in your figures. ",8,164,18.22222222222222,5.518987341772152,104,1,163,0.0061349693251533,0.0119760479041916,-0.6832,57,20,23,3,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,3,1,0,5,1,2,1,0,0,0,0,0,0,0,0,2,1,0,4,0,0,0.5012174461526161,0.3353163681950522,0.2547748850344387
ICLR2018-BkA7gfZAb-R2,Reject,"This paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss. This allows the authors to turn the standard min max problem of adversarial training into a single minimization problem, which is easier to  The method is demonstrated on a toy example and on the task of unsupervised domain adaptation. Strengths: - The derivation of the dual formulation is novel - The dual formulation simplifies adversarial training - The experiments show the better behavior of the method compared to adversarial training for domain adaptation Weaknesses: - It is unclear that this idea would generalize beyond a logistic regression classifier, which might limit its applicability in practice - It would have been nice to see results on other tasks than domain adaptation, such as synthetic image generation, for which GANs are often used - It would be interesting to see if the DA results with a kernel classifier are better (comparable to the state of the art) - The mathematical derivations have some errors Detailed comments: - The upper bound used to derive the formulation applies to a logistic regression classifier. While effective, such a classifier might not be as powerful as multi-layer architectures that are used as discriminators.  I would be interested to know if they authors see ways to generalize to better classifiers. - The second weakness listed above might be related to the first one. Did the authors tried their approach to non-DA tasks, such as generating images, as often done with GANs? Showing such results would be more convincing. However, I wonder if the fact that the method has to rely on a simple classifier does not limit its ability to tackle other tasks. - The DA results are shown with a linear classifier, for the comparison to the baselines to be fair, which I appreciate. However, to evaluate the effectiveness of the method, it would be interesting to also report results with a kernel-based classifier, so as to see how it compares to the state of the art. - There are some errors and unclear things in the mathematical derivations: * In the equation above Eq. 2, alpha should in fact be alpha_i, and it is not a vector (no need to transpose it) * In Eq. 2, it should be alpha_i alpha_j instead of alpha^Talpha * In Eq. 3, it is unclear to me where the constraint 0 leq alpha leq 1 comes from. The origin of the last equality constraints on the sums of alpha_A and alpha_B is also unclear to me. * In Eq. 3, it is also not clear to me why the third term has a different constant weight than the first two. This would have an impact on the relationship to the MMD - The idea of sample reweighting within the MMD was in fact already used for DA, e.g., Huang et al., NIPS 2007, Gong et al., ICML 2013. What is done here is quite different, but I think it would be worth discussing these relationships in the paper. - The paper is reasonably clear, but could be improved with some more details on the mathematical derivations (e.g., explaining where the constraints on alpha come from), and on the experiments (it is not entirely clear how the distributions of accuracies were obtained). ",29,533,25.38095238095238,4.868369351669941,223,6,527,0.0113851992409867,0.0488245931283905,0.6549,134,65,93,23,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 20, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 1}",0,1,4,2,0,20,3,3,0,1,0,1,0,0,0,1,0,3,0,0,19,1,1,0.576766780528324,0.566985114497109,0.3562378098301999
ICLR2018-BkA7gfZAb-R3,Reject,"This paper proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression (where the dual function can be obtained in closed form when the discriminator is linear). They motivate their approach by repeating the previously made claim that the naive gradient approach is non-convergent for generic saddle point problems (Figure 1); while a gradient approach often works well for a minimization formulation. The dual problem of regularized logistic regression is an entropy-regularized concave quadratic objective problem where the Hessian is y_i y_j <x_i, x_j>, thus highlighting the pairwise similarities between the points x_i & x_j; here the labels represent whether the point x comes from the samples A from the target distribution or B from the proposal distribution. This paper then compare this objective with the MMD distance between the samples A & B. It points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the MMD distance, an interesting analogy (but also showing the limited power of the adversarial logistic distance for getting good generating distributions, given e.g. that the MMD has been observed in the past to perform poorly for face generation [Dziugaite et al. UAI 2015]).  From this analogy, one could expect the method to improves over MMD, but not necessarily significantly in comparison to an approach which would use more powerful discriminators. This paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation, with experiments on a visual adaptation task. They observe better performance for their approach in comparison to ADDA, improved WGAN and MMD, when restricting the discriminators to be a linear classifier.    Evaluation   I found this paper quite clear to read and enjoyed reading it. The observations are interesting, despite being on the toyish side. I am not an expert on GANs for domain adaptation, and thus I can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper). One concern about the paper (but this is an unfortunate common feature of most GAN papers) is that it ignores the vast knowledge on saddle point optimization coming from the optimization community. The instability of a gradient method on non-strongly convex-concave saddle point problems (like the bilinear form of Figure 1) is a well-known property, and many alternative *stable* gradient based algorithms have been proposed to solve saddle point problems which do not require transforming them to a minimization problem as suggested in this paper. Moreover, the transformation to the minimization form crucially required the closed form computation of the dual function (with w* just defined above equation (2)), and this is limited to linear discriminators,  thus ruling out the use of the proposed approach to more powerful discriminators like deep neural nets. Thus the significance appears a bit limited to me.    Other comments  1) Note that d(A, B'_theta) is *equal* to min_alpha max_w  (...)  above equation (2) (it is not just an upper bound). This is a standard result coming from the fact that the Fenchel dual problem to regularized maximum likelihood is the maximum entropy problem with a quadratic objective as (2). See e.g. Section 2.2 of [Collins et al. JMLR 2008] (this is for the more general multiclass logistic regression problem, but (2) is just the binary special case of equation (4) in the [Collins ... ] reference). And note that the w(u) defined in this reference is the lambda*w*(alpha) optimal relationship defined in this paper (but without the 1/lambda factor because of just slightly different writing; the point though is that strong duality holds there and thus one really has equality). [Collins et al. JMLR 2008] Michael Collins, Amir Globerson, Terry Koo , Xavier Carreras, Peter L. Bartlett, Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks, , JMLR 2008.   [Dziugaite et al. UAI 2015] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015",20,686,21.4375,5.493827160493828,299,2,684,0.0029239766081871,0.0112994350282485,0.9692,229,93,97,39,8,5,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 4}",0,2,4,3,0,8,2,2,0,0,0,3,1,0,0,0,1,5,0,0,3,1,4,0.5737982338857622,0.5575115258665636,0.3644866467891433
ICLR2018-BkBCjzp7G-R1,,"The paper proposes a novel workflow for acceleration and compression of CNNs. The proposed workflow consists of the novel two-pass decomposition of a group of layers, and the fine-tuning of the remaining network. This process is applied iteratively to different groups of layers. The authors also propose a way to determine the target rank of each layer given the target overall acceleration. The authors report the highest measured acceleration of VGG16 using low-rank approximation techniques (6.2x vs 5x previously) with a similar accuracy drop (1.2% vs 1.0% previously). The paper is well-structured, and the proposed method is clearly described. However it would be nice to see the difference to other related methods more clearly. Some results are counterintuitive if the reader is not familiar with related works (e.g. the Zhang et al. 2016 achieves a lower acceleration with much lower ranks). The main concern is the motivation of the two-pass decomposition. It is not clear why the the optimized full-rank tensor is more easy to decompose if it was initialized with a low-rank tensor. There are no theoretical results regarding this question in the paper, and the empirical justification is also lacking. It would be necessary to see the tensor reconstruction error during the following 2 scenarios: We apply the CP decomposition to a pretrained network We apply the CP decomposition to a pretrained network, then restore it back into the dense format, optimize it, and then apply the CP decomposition again What is the reconstruction error in case 1? What is the reconstruction error during the second CP decomposition in 2? What is the accuracy drop after fine-tuning in both scenarios? Figure 4 could have answered this question, however it is not clear from the paper whether the CP-ALS procedure was followed by fine-tuning or not. If it wasn't, then the comparison is unfair, as the results for CP-ALS are drastically underestimated. It would also be nice to see the full learning curves for all experiments, where different stages (decompose->optimize->decompose->finetune->...) are explicitly marked. The reported tables seem to ignore a lot of the relevant information . Also Astrid and Lee 2017 do not seem to report the instabilities during fine-tuning of the decomposed layers, and argue that these layers should not be freezed. As they use a very similar iterative fine-tuning workflow, it is not clear why the two-pass decomposition + freezing should work better than one-pass decomposition + iterative fine-tuning with no freezing. These two methods seem to be closely related and should be thoroughly compared. The improvement w.r.t. other methods seems marginal. The previous SotA result on VGG16 was 5x acceleration with 1% accuracy drop, and here the reported result is 6.2x acceleration with 1.2% accuracy drop. The authors claim that the previous SotA result was carefully fine-tuned with a low learning rate, and that in this paper they used only default fine-tuning with a high learning rate. Is it possible to further improve the accuracy by a more careful fine-tuning? Right now the results are not very convincing. I would be glad to reconsider my grade if the questions regarding the motivation of the two-pass decomposition and the comparison with Astrid and Lee 2017 are answered. Other comments and remarks: The meaning of the following sentence is not clear, it probably should be rephrased: ""We observed that if the network is trained in the restored dense form, the training result can be more stable because of its smoother convex. "". What does ""smoother convex"" mean? It should be stated more clearly how the results from Figure 4 were obtained. It would be interesting to see the accuracy of the fitness approximation during the rank selection procedure. Is it possible to perform the CP decomposition by minimizing the activation reconstruction loss (like proposed by Zhang et al. 2016), and not the tensor reconstruction loss (as usual)? It seems as a more natural way to do it. The convergence constraint procedure from Table 4 is not clear. ""our experiment is extended with additional epochs to fine-tune until the accuracy improvement is smaller than 0.1%. "" - what does ""the accuracy improvement is smaller than 0.1%"" mean?",38,682,19.485714285714284,5.239567233384853,260,3,679,0.0044182621502209,0.0494186046511627,0.9705,190,84,111,48,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 17, 'EXP': 4, 'RES': 18, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 7, 'PNF': 1, 'REC': 0, 'EMP': 26, 'SUB': 1, 'CLA': 1}",0,0,5,2,0,17,4,18,3,0,0,1,0,0,0,0,0,7,1,0,26,1,1,0.5056364407645463,0.5718074667697697,0.31362775111940366
ICLR2018-BkBCjzp7G-R2,,"* Paper Summary This paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks (DNNs). It makes use of the CP or rank-1 tensor decomposition to define the meaning of rank for a tensor. When a tensor is decomposed as a sum of rank-1 tensors (outer products), the number of operations in a DNN forward pass decreases leading to a faster testing runtime. This form of network compression has been worked on before. The contribution of this paper seems to be the specific way the decomposition is used in training the DNN. It seems that this training process follows a projected gradient descent procedure, where the filter weights of the network are iteratively updated using regular (stochastic) gradient descent and then they are projected onto the set of rank-R tensors. The authors devise a heuristic way (based on an innovated measure that combines computational complexity with performance) to select the tensor rank to be used. Experiments are conducted on the task of image classification for a couple well-known DNN architectures (VGG and Resnet) to show a speedup of runtime in testing, significant compression of the network, and minimal degradation in performance. * Related Work The authors do a good job describing and listing the papers most related to the current submission. * Technical Novelty One main limitation of the paper is the lack of technical contribution. The idea of using rank-1 tensor decomposition for training low-rank filtering operations in DNNs has already been proposed and used in several other work. From what I understood from the paper, the only technical contribution is the use of a so called 2-pass decomposition, which is simply an implementation of projected gradient descent on the set of rank-R tensors. In particular, if we seek to minimize f(W) such that W belongs to asset that can be easily projected on, then projected gradient descent would apply traditional gradient descent on the current iterate, followed by a projection step onto this set. This paper seems to be applying this exact same strategy in training for a cross-entropy classification loss f(.). This iterative projection tends to perform better than iteratively optimizing f(W) and then applying the projection step only once at the very end of the optimization (assumedly the CP-ALS method that is used for comparison). Put in this light, the proposed paper does not contribute much. * Paper Presentation In the reviewer's opinion, the primary limitation of the paper is how it is written and organized. The paper is badly written. It is riddled with grammar, choice of word, and spelling mistakes. The paper organization needs to be revamped with emphasis on the proposed ideas of the paper and how it differs from the rich related work. These issues make the paper hard to read. For example, the authors spend quite a bit of space focusing on the rank-1 (CP) decomposition, which is well known, as opposed to focusing on the merits of their technical contributions. Also, the experiments are not clearly explained. It is hard to understand the experimental setup of each experiment and what the conclusions are. For example, it is unclear whether the baseline in Table 3 also uses the two-pass decomposition or not. Also, the authors should provide a clear and standard description of the experimental setup for each experiment (e.g. which network, which dataset, which task/loss, which measure, etc.). * Experimental Results From what I understood from the experiments, it seems that using the ""two-pass decomposition"" (i.e. projected gradient descent) is better than CP-ALS (gradient descent ended with a single projection step). This conclusion seems to be intuitive and expected. However, as mentioned earlier, the paper writing and organization makes it hard to understand what exactly is being shown. For example, Table 1 shows that the baseline method uses less filters than the proposed method that selects the number of filters through an innovated heuristic measure. Then in Table 3, we see that the baseline is less stable (i.e. its performance decreases across the different iterations of projected gradient descent). Isn't this expected since the baseline uses less filters? It is unclear from the text if this is the case. The authors should do a better job explaining and comparing the overall experimental results. For example, it seems that the proposed projected gradient descent method leads to better speedup results in VGG as opposed to Resnet, with very similar reduction in accuracy. The authors do not comment on this. It would be nice for them to explain the circumstances under which the proposed method is best suited and any potential failure cases (e.g. cases when the low-rank decomposition leads to a significant decrease in performance). All of this analysis provided more insight into the method and helps the reader understand its extents.  ",38,795,18.928571428571427,5.184655396618986,310,6,789,0.0076045627376425,0.0224438902743142,0.9528,222,97,145,35,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 3, 'DAT': 0, 'MET': 16, 'EXP': 10, 'RES': 6, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 8, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 6, 'PNF': 7, 'REC': 0, 'EMP': 10, 'SUB': 3, 'CLA': 10}",0,1,5,3,0,16,10,6,3,1,0,8,0,0,0,0,0,6,7,0,10,3,10,0.6484693750638529,0.5630519674522416,0.4096171141945122
ICLR2018-BkBCjzp7G-R3,,"General comment                Low-rank decomposing convolutional filters has been used to speedup convolutional networks at the cost of a drop in prediction performance. The authors a) extended existing decomposition techniques by an iterative method for decomposition and fine-tuning convolutional filter weights, and b) and algorithm to determine the rank of each convolutional filter. The authors show that their method enables a higher speedup and lower accuracy drop than existing methods when applied to VGG16. The proposed method is a useful extension of existing methods but needs to evaluated more rigorously. The manuscript is hard to read due to unclear descriptions and grammatical errors. Major comments               1. The authors authors showed that their method enables a higher speedup and lower drop in accuracy than existing methods when applied to VGG16.. The authors should analyze if this also holds true for ResNet and Inception, which are more widely used than VGG16. 2. The authors measured the actual speedup on a single CPU (Intel Core i5).. The authors should measure the actual speedup also on a single GPU. 3. It is unclear how the actual speedup was measured. Does it correspond to the seconds per update step or the overall training time?. In the latter case, how long were models trained?. 4. How and which hyper-parameters were optimized? The authors should use the same hyper-parameters for all methods (Jaderberg, Zhang, Rank selection). The authors should also analyze the sensitivity of speedup and accuracy drop depending on the learning rate for 'Rank selection'. 5. Figure 4: the authors should show the same plot for more convolutional layers at varying depth from both VGG and ResNet. 6. The manuscript is hard to understand and not written clearly enough. In the abstract, what does 'two-pass decomposition', 'proper ranks', 'the instability problem', or 'systematic' mean? What are 'edge devices', 'vanilla parameters'? The authors should also avoid uninformative adjectives, clutter, and vague terms throughout the manuscript such as 'vital importance' or 'little room for fine-tuning'. Minor comments               1. The authors should use 'significantly' only if a statistical hypothesis was performed. 2. The manuscript contains several typos and grammatical flaws, e.g. 'have been widely applied to have the breakthrough', 'The CP decomposition factorizes the tensors into a sum of series rank-one tensors. ', 'Our two-pass decomposition provides the better result as compared with the original CP decomposition'. 3. For clarity, the authors should express equation 5 in terms of Y_1, Y_2, Y_3, and Y_4. 4. Equation 2, bottom: C_in, W_f, H_f, and C_out are undefined at this point.",25,417,12.636363636363637,5.438461538461539,198,0,417,0.0,0.0195227765726681,-0.7083,128,49,66,15,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 18, 'EXP': 2, 'RES': 4, 'TNF': 1, 'ANA': 4, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 3, 'CLA': 4}",0,0,0,0,2,18,2,4,1,4,0,4,0,0,0,0,0,4,1,0,13,3,4,0.5047530727100703,0.5638272906744463,0.32001691662968135
ICLR2018-BkCV_W-AZ-R1,Reject,"This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE. The main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions). The claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN. There are several things to like about this paper: - The authors do a good job of reviewing/referencing several papers in the field of regret minimization that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives. - The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in deep reinforcement learning. - The experimental results show that ARM outperforms DQN on a suite of deep RL tasks. However, there are also some negatives: - Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused. - The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc. At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed. - The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability) but we only see results compared to DQN ... it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods. Yes A3C and TRPO seem like they perform very poorly compared to ARM ... but I'm left wondering how/why? I feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a paper. A lot of the cited literature was also new to me, so it could be that I'm missing something about why this is so interesting. However, I came away from this paper quite uncertain about the real benefits/differences of ARM versus other similar policy gradient methods ... I also didn't feel the experimental evaluations drove a clear message except ARM did better than all other methods on these experiments ... I'd want to understand how/why and whether we should expect this universally. The focus on regret minimization perspectives didn't really get me too excited ...  Overall I would vote against acceptance for this version. ",20,416,19.80952380952381,5.17632241813602,218,5,411,0.0121654501216545,0.0510440835266821,0.985,103,59,74,36,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 6, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 6, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 2, 'CLA': 0}",0,1,2,1,0,9,6,2,0,0,0,1,2,1,0,0,0,6,0,1,8,2,0,0.6454382415258942,0.4494906357826166,0.3536741760143115
ICLR2018-BkCV_W-AZ-R2,Reject,"This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL. Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better. The results is interesting and novel. This paper should be accepted. The presentation of the paper can be improved a bit. Much of the notation introduced in section 3.1 is not used later on. There seems to be a bit of a disconnect before and after section 3.3. The algorithm in deep RL could be explained a bit better. There are some papers that could be connected. Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments. It could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals. The argument the authors made against recurrent value functions is that recurrent value could be hard to train. An experiment illustrating this effect could be illuminating. Can the proposed approach help when we have recurrent value functions? Since recurrence does not guarantee that all information needed is captured. Finally some miscellaneous points:  One interesting reference: Memory-based control with recurrent neural networks by Heess et al. Potential typos: in the 4th bullet point in section 3.1, should it be rho^{pi}(h, s')?",17,222,13.875,5.311627906976744,127,1,221,0.004524886877828,0.0224215246636771,0.9756,57,22,47,13,7,7,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 1}",0,1,2,0,0,5,2,1,0,0,0,2,2,0,0,1,0,1,3,1,6,3,1,0.5012252639621192,0.7812240558162422,0.39380590454230807
ICLR2018-BkCV_W-AZ-R3,Reject,"Quality and clarity:  The paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization. The paper claims that the approach can deal with the partial observable domain better than the standard methods. However the results only show that the algorithm converges, in some cases, faster than the previous work  reaching asymptotically to a same or worse performance. Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs. The paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain. To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no Markov assumption . The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle. This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy. To me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more efficiently. Originality and Significance:  The proposed algorithm seems original. However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification). I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence. However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones. Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling. More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence.  ",15,404,25.25,5.2292191435768265,197,4,400,0.01,0.0355450236966824,0.9865,104,50,58,22,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 1}",0,1,3,1,0,14,0,1,0,0,1,0,0,0,0,1,1,4,0,0,5,3,1,0.4319849018553972,0.6697193370942514,0.2996349653539169
ICLR2018-BkDB51WR--R1,Reject,"Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay. Unclear if this would work at all in higher-dimensional time series. It is also unclear to me what are the sources of the uncertainties captured. n  The author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series. The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts. Instead, the conditional density p(y_t|y_{1:t-1|, theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM. The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels. Therefore, the uncertainties produced by the model appear to be a black-box. It is probably unlikely that the discretisation method can be generalised to high-dimensional setting? Quality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth.  In the experiments with real data, it's unclear how good the uncertainties produced by the model are. It may be useful to compare to the uncertainty produced by a GP with suitable kernels. In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that? Clarity: The paper is well-written. The presentations of the ideas are pretty clear. Originality: Above average. I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever. Significance: Average.  It would be excellent if the authors can extend this to higher dimensional time series. I'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC.",20,307,16.157894736842106,5.537162162162162,156,3,304,0.0098684210526315,0.0354838709677419,0.9547,81,40,58,10,9,7,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 4, 'EXP': 6, 'RES': 6, 'TNF': 1, 'ANA': 0, 'FWK': 2, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 1}",0,0,0,2,2,4,6,6,1,0,2,5,0,1,0,1,3,1,1,0,6,2,1,0.6445984044561515,0.7810903147572099,0.4998129547442158
ICLR2018-BkDB51WR--R2,Reject,"The papers proposes a recurrent neural network-based model to learn the temporal evolution of a probability density function. A Monte Carlo method is suggested for approximating the high dimensional integration required for multi-step-ahead prediction. The approach is tested on two artificially generated datasets and on two real-world datasets, and compared with standard approaches such as the autoregressive model, the Kalman filter, and a regression LSTM. n The paper is quite dense and quite difficult to follow, also due to the complex notation used by the authors. n The comparison with other methods is very week, the authors compare their approach with two very simple alternatives, namely a first-order autoregressive mode and the Kalman filter. More sophisticated should have been employed.",6,120,20.0,5.831858407079646,78,0,120,0.0,0.0166666666666666,0.2732,31,21,18,8,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,1,2,1,1,4,0,0,0,0,0,1,0,0,0,0,0,2,1,0,0,0,0,0.4294024928151665,0.2223393847794121,0.19404059871301524
ICLR2018-BkDB51WR--R3,Reject,"This work proposes an LSTM based model for time-evolving probability densities. The model does not assume an explicit prior over the underlying dynamical systems, instead only uncertainty over observation noise is explicitly considered. Experiments results are good for given synthetic scenarios but less convincing for real data.  Clarity: The paper is well-written.  Some notations in the LSTM section could be better explained for readers who are unfamiliar with LSTMs. Otherwise, the paper is well-structured and easy to follow. n Originality: I'm not familiar with LSTMs, it is hard for me to judge the originality here. Significance: Average. The work would be stronger if the authors can extend this to higher dimensional time series. There are also many papers on this topic using Gaussian process state-space (GP-SSM) models where an explicit prior is assumed over the underlying dynamical systems. The authors might want to comment on the relative merits between GP-SSMs and DE-RNNs. The SMC algorithm used is a sequential-importance-sampling (SIS) method. I think it's correct but may not scale well with dimensions.",14,172,13.23076923076923,5.42603550295858,118,3,169,0.0177514792899408,0.0459770114942528,0.9804,49,29,31,10,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,1,1,1,4,1,0,0,2,1,3,0,2,0,1,2,1,2,0,5,0,1,0.7151611756919428,0.6692641019782898,0.49068139557356494
ICLR2018-BkIkkseAZ-R1,Reject,"The paper studies the theoretical properties of the two-layer neural networks. To summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data. The paper shows that  a) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank b) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank.  It's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global. But the paper cannot establish such a result.  The paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al. The paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W. result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate. As the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum. It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question.  --------------------  additional review after seeing the author's response:   The author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree. However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.) The key technical limitation is the dependency of the local minima on the weight parameters . Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea. Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights. The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned. I don't also see the value of extension to other activation function . To some extent this is not consistent with the empirical observation that relu is very important for deep learning. Regarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary. Gradient descent can converge to a first order optimal solution.  (Indeed I have a typo in my previous review regarding w.r.t. k-th sample, which should be w.r.t. k-th update. ) Moreover, to justify the effect of the randomness, the paper should have empirical experiments. I think the writing of the paper is also misleading in several places.  ",24,516,19.11111111111111,4.701431492842536,222,1,515,0.0019417475728155,0.0263653483992467,0.9215,147,56,80,41,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 4, 'NOV': 4, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 9, 'CLA': 2}",0,1,4,0,1,5,2,5,0,5,0,1,0,0,4,4,0,0,0,0,0,9,2,0.5730786844758736,0.4455433120683292,0.32060216437246725
ICLR2018-BkIkkseAZ-R2,Reject,"This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution. I think there are some interesting arguments made in the paper e.g. Lemmas 4.1, 5.1, 5.2, and 5.3. However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims. Of course there is a chance that I might be misunderstanding some things and happy to adjust my score based on the discussions here. Detailed comments: 1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paper Example 1 from abstract:  ""We show that for a wide class of differentiable activation functions (this class involved ""almost"" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular.""  This is certainly not proven and in fact not formally stated anywhere in the paper. Closest result to this is Lemma 4.1 however, because the optimal solution is data dependent this lemma can not be used to conclude this. Example 2 from intro when comparing with other results on page 2: The authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data (e.g. do not require Gaussianity). However as explained above the final conclusions are also significantly weaker than this prior literature so it's a bit of apples vs oranges comparison. 2) Page 2 minor typos We study training problem -->we study the training problem In the regime training objective--> in the regime the training objective 3) the basic idea argument and derivative calculations in section 3 is identical to section 4 of Soltan...et al 4) Lemma 4.1 is nice, well done! That being said it does not seem easy to make it (1) quantifiable (2) apply to all W. It would also be nice to compare with Soudry et. al. 5) Argument on top of page 6 is incorrect as the global optima is data dependent and hence lemma 4.1 (which is for a fixed matrix) does not apply 6) Section 5 on page 6. Again the stated conclusion here that the iterates do not lead to singular W is much weaker than the claims made early on. 7) I haven't had time yet to verify correctness of Lemmas 5.1, 5.2, and Lemma 5.3 in detail but if this holds is a neat argument to side step invertibility w.r.t. W, Nicely done! 8) What is the difference between Lemma 5.4 and Lemma 6.12 of Soltan...et al  9) Theorem 5.9. Given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and W is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent Gaussian noise is injected in every iteration over W. By adjusting the noise variance across time one can ensure a result of the kind in Theorem 5.9 (Of course in the absence of a quantifiable version of Lemma 4.1 which can apply to all W that result will also suffer from the same issues).",19,559,27.95,4.906746031746032,249,2,557,0.0035906642728904,0.0088967971530249,0.963,142,57,94,39,9,3,"{'ABS': 3, 'INT': 4, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 6, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",3,4,2,0,1,8,4,6,0,2,0,0,0,1,0,0,0,2,0,0,10,0,1,0.6453303690805635,0.33904811280411,0.33027105676384577
ICLR2018-BkIkkseAZ-R3,Reject,"I only got access to the paper after the review deadline; and did not have a chance to read it until now. Hence the lateness and brevity. The paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al).  However, the paper does not properly relate their results, assumptions in the context of the existing literature. Much explanation is needed in the author reply in order to clear these questions. The work should not be evaluated from a practical perspective as it is of a theoretical nature. I agree with most of the criticism raised by other reviewers . However, I also believe the authors managed to clear essentially of the criticism in they reply. The paper lacks in clarity as currently written. The results are interesting, but more explanation is needed for the main message to be conveyed more clearly. I suggest 7, but the paper has a potential to become 8 in my eyes in a future resubmission. ",10,168,14.0,4.924050632911392,100,0,168,0.0,0.0175438596491228,0.7928,39,17,35,12,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 2}",0,1,1,0,0,1,0,2,0,0,0,4,0,3,0,0,0,0,0,0,3,0,2,0.4287820430246039,0.2235472721627622,0.1938243127004086
ICLR2018-BkJ3ibb0--R1,Accept,"This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE. The GAN is a WGAN trained on the train set (only to keep the generator). The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x. + The paper is easy to follow. + It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses. + Simple/directly applicable approach that seems to work experimentally; but - A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set. - Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST. - Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image. ? MagNet results were very often worse than no defense in Table 4, could you comment on that? - In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control.",11,216,24.0,4.773869346733668,116,4,212,0.0188679245283018,0.0223214285714285,-0.8881,69,23,37,9,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,1,1,4,1,0,1,0,0,1,0,0,0,0,0,1,0,0,3,0,1,0.5721791477699254,0.3345772482030192,0.287639499260507
ICLR2018-BkJ3ibb0--R2,Accept,"This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs.  Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution).  This projected input is then used to produce the classification probabilities. The authors test their method on various adversarially constructed inputs (with varying degrees of noise). Questions/Comments:  - I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method.  Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data?  If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable?  - Is there anything special about the GAN approach, versus other generative approaches?  - In the black-box vs. white-box scenarios, can the attacker know the GAN parameters? Is that what is meant by the defense network (in experiments bullet 2)? - How computationally expensive is this approach take compared to MagNet or other adversarial approaches?  Quality: The method appears to be technically correct. Clarity: This paper clearly written; both method and experiments are presented well.  Originality: I am not familiar enough with adversarial learning to assess the novelty of this approach. Significance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold. I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).",17,279,23.25,5.7407407407407405,152,2,277,0.0072202166064981,0.0136986301369863,0.504,80,36,55,11,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 12, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,2,2,1,12,3,0,0,0,1,1,0,1,0,2,0,4,1,0,3,0,1,0.6459214354564312,0.5571960656813295,0.4091882483183594
ICLR2018-BkJ3ibb0--R3,Accept,"The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM). They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation. In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution. The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator. Utilizing a trained GAN, the authors propose the following defense at inference time. Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z*   argmin_z ||G(z) - x||_2. This is done by SGD. Then apply any classifier trained on the true distribution on the resulting x*   G(z*). In the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective. In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model. Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM. The authors provide less-convincing evidence that the defense is effective against white-box attacks. In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks. However, it is not clear to me that the method is invulnerable to novel white-box attacks. In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream. Nevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks  (which is arguably of great practical value). It is novel and should generate further research with respect to understanding its vulnerabilities more completely. Minor Comments: The sentence starting ""Unless otherwise specified..."" at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed.",17,392,20.63157894736842,5.495890410958904,187,3,389,0.0077120822622107,0.0276381909547738,0.9206,108,51,75,20,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 13, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,13,3,0,1,0,1,0,0,0,0,1,0,0,1,0,8,0,0,0.3603175540916905,0.3376870353772341,0.1807354901018442
ICLR2018-BkLhaGZRW-R1,Accept,"The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points, even when it would be easy to discriminate between these manifolds using only a fraction of its capacity, so that the discriminator provides a more informative signal to the generator. The regularizer rewards high entropy in the signs of discriminator activations. Experiments show that this helps to prevent mode collapse on synthetic Gaussian mixture data and improves Inception scores on CIFAR10. The high-level idea of guiding model capacity by rewarding high-entropy activations  is interesting and novel to my knowledge (though I am not an expert in this space). Figure `1 is a fantastic illustration that presents the core idea very clearly. That said I found the intuitive story a little bit difficult to follow; -- it's true that in Figure 1b the discriminator won't communicate the detailed structure of the data manifold to the generator, but it's not clear why this would be a problem; -- the gradients should still pull the generator *towards* the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finer-grained details. Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap? But in that case much of the theoretical story goes out the window. I'd also appreciate further discussion of the relationship of this approach to Wasserstein GANs, which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap. More generally I'd like to better understand what effect we'd expect this regularizer to have. It appears to be motivated by improving training dynamics, which is understandably a significant concern. Does it also change the location of the Nash equilibria? (or equivalently, the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483). I'd expect that it would but the effects of this changed objective are not discussed in the paper. The experimental results seem promising, although not earthshattering. I would have appreciated a comparison to other methods for guiding discriminator representation capacity, e.g. autoencoding (I'd also imagine that learning an inference network (e.g. BiGAN) might serve as a useful auxiliary task?). Overall this feels like an cute hack, supported by plausible intuition but without deep theory or compelling results on real tasks (yet). As such I'd rate it as borderline; though perhaps interesting enough to be worth presenting and discussing. A final note: this paper was difficult to read due to many grammatical errors and unclear or misleading constructions, as well as missing citations (e.g. sec 2.1). From the second paragraph alone: impede their wider applications in new data domain -> domains extreme collapse and heavily oscillation -> heavy oscillation modes of real data distribution -> modes of the real data distribution while D fails to exploit the failure to provide better training signal to G -> should be this failure to refer to the previously-described generator mode collapse, or rewrite entirely even when they are their Jensen-Shannon divergence -> even when their Jensen-Shannon divergence  I'm sympathetic to the authors who are presumably non-native English speakers; many good papers contain mistakes, but in my opinion the level in this paper goes beyond what is appropriate for published work. I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.",24,578,26.27272727272728,5.477558348294434,295,5,573,0.0087260034904013,0.0221465076660988,0.9868,165,75,97,35,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 2, 'MET': 11, 'EXP': 5, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 11, 'SUB': 0, 'CLA': 4}",0,1,2,0,2,11,5,0,2,0,0,5,1,0,0,1,1,2,1,1,11,0,4,0.5745968275684391,0.7843579198959597,0.4529945242432606
ICLR2018-BkLhaGZRW-R2,Accept,"The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the model capacity of the discriminators over the (potentially multi-modal) generated / real data points, which might in turn helps with learning a more faithful generator. The paper is in general very well written, with intuitions and technical details well explained and empirical studies carefully designed and executed. Some detailed comments / questions: 1. It seems the concept of binarized activation patterns, which the proposed regularizer is designed upon, is closely coupled with rectifier nets. I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract. 2. In order for the paper to be more self-contained, maybe list at least once the formula for rectifier net (sth. like a^T max(0, wx + b) + c) ? This might also help the readers better understand where the polytopes in Figure 1 come from. 3. In section 3.1, when presenting random variables (U_1, ..., U_d), I find the word Bernourlli a bit misleading because typically people would expect U_i to take values from {0, 1} whereas here you assume {-1, +1}. This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations. Also, K is already used to denote the mini-batch size, so it's a slight abuse to reuse k to denote the kth marginal. 4. In section 3.2, it may be clearer to explicitly point out the use of the 3-sigma rule for Gaussian distributions here. But I don't find it justified anywhere why leave 99.7% of i, j pairs unpenalized is sth. to be sought for here? 5. In section 3.3, when presenting Corollary 3.3 of Gavinsky & Pudlak (2015),   abruptly appears without proper introduction / context. 6. For the empirical study with 2D MoG, would an imbalanced mixture make it harder for the BRE-regularized GAN to escape from modal collapse? 7. Figure 3 is missing the sub-labels (a), (b), (c), (d).",16,334,15.904761904761903,5.171140939597316,189,6,328,0.0182926829268292,0.0231884057971014,0.7215,90,39,56,29,8,3,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 2}",1,1,1,0,0,10,1,0,2,0,0,1,1,0,0,0,0,0,2,0,11,0,2,0.5737035984133401,0.3396956116764204,0.2896345887753171
ICLR2018-BkLhaGZRW-R3,Accept,"The paper presents a method for improving the diversity of Generative Adversarial Network (GAN) by promoting the Gnet's weights to be as informative as possible. This is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra node. Numerical experiments that demonstrate the diversity increment on the generated samples are shown. Concerns.  The paper is hard do tear and it is deficit to identify the precise contribution of the authors. Such contribution can, in my opinion, be summarized  in a potential of the form  with  $$ R_BRE   a R_ME+ b R_AC   a sum_k  sum_i s_{ki}^2   +  b sum_{<k,l>} sum_i { s_{ki} s_{li} }    $$ (Note that my version of R_ME is different to the one proposed by the authors, but it could have the same effect)  Where a and b are parameters that weight the relative contribution of each term  (maybe computed as suggested in the paper). In this formulation:  Then R_ME has a high response if the node has saturated responses -1's or 1``s, as one desire such saturated responses, a should be negative. The R_AC, penalizes correlation between responses of different nodes. The point is,   a) The second term will introduce  low correlation in saturated vectors, then the will be informative. b) why the authors use the softsign instead the tanh:  $tahnh in C^2 $! Meanwhile the derivative id softsign is discontinuous. c)  It is not clear is the softsign is used besides the activation function: In page 5 is said ""R_BRE can be applied on ant rectified layer before the nolinearity""  . This seems tt the authors propose to add a second activation function (the softsign), why not use the one is in teh layer? d) The authors found hard to regularize the gradient $ abla_x D(x)$, even they tray tanh and cosine based activations. It seems that effectively, the  introduce their additional softsign in the process. e) En the definition of R_AC, I denoted by <k,l> the pair of nodes (k  e l). However, I think that it should be for pair in the same layer. It is not clear in the paper. f) It is supposed that the L_1 regularization motes the weights to be informative, this work is doing something similar. How is it compared  the L_1 regularization vs. the proposal? Recommendation I tried to read the paper several times and I accept that it was very hard to me. The most difficult part is the lack of precision on the maths, it is hard to figure out what the authors contribution indeed are. I think there is some merit in the work. However, it is not very well organized and many points are not defined. In my opinion, the paper is in a preliminary stage and should be refined. I recommend a ""SOFT"" REJECT.",23,458,19.08333333333333,4.781176470588235,204,5,453,0.0110375275938189,0.0365111561866125,-0.9213,135,50,94,22,4,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 15, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 12, 'SUB': 0, 'CLA': 3}",0,1,0,0,0,15,1,0,0,0,0,6,0,0,0,0,0,1,1,1,12,0,3,0.2894440693475238,0.5625593574805364,0.18018971754975588
ICLR2018-BkM27IxR--R1,Reject,"[Main comments]  * I would advice the authors to explain in more details in the intro what's new compared to Li & Malik (2016) and Andrychowicz et al. (2016). It took me until section 3.5 to figure it out. * If I understand correctly, the only new part compared to Li & Malik (2016) is section 3.5, where block-diagonal structure is imposed on the learned matrices. Is that correct? * In the experiments, why not comparing with Li & Malik (2016)? (i.e., without   block-diagonal structure) * Please clarify whether the objective value shown in the plots is wrt the training   set or the test set. Reporting the training objective value makes little sense to me, unless the time taken to train on MNIST is taken into account in the comparison. * Please clarify what are the hyper-parameters of your meta-training algorithm   and how you chose them. I will adjust my score based on the answer to these questions. [Other comments]  * Given this state of affairs, perhaps it is time for us to start practicing   what we preach and learn how to learn  This is in my opinion too casual for a scientific publication... * aim to learn what parameter values of the base-level learner are useful   across a family of related tasks  If this is essentially multi-task learning, why not calling it so?  Learning what to learn does not mean anything. I understand that the authors wanted to have what, which and how sections but this is not clear at all. What is a base-level learner? I think it would be useful to define it more precisely early on. * I don't see the difference between what is described in Section 2.2   (learning which model to learn) and usual machine learning (searching for the best hypothesis in a hypothesis class). * Typo: p captures the how -> p captures how * The L-BFGS results reported in all Figures looked suspicious to me. How do you   explain that it converges to a an objective value that is so much worse? Moreover, the fact that there are huge oscillations makes me think that the authors are measuring the function value during the line search rather than that at the end of each iteration.",18,358,23.866666666666667,4.754437869822485,182,3,355,0.0084507042253521,0.0230179028132992,0.951,84,42,78,16,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 3}",0,1,2,0,0,12,0,2,0,0,0,2,1,1,0,1,0,3,0,0,9,0,3,0.5029337218547754,0.4498166991792762,0.28351277732668445
ICLR2018-BkM27IxR--R2,Reject,"This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks. This work is an extended version of [1], aiming to address the high-dimensional problem. Strengths:  The proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithms. The proposed method has better robustess in different tasks and different batch size setting. The invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of LQG. Weaknesses:  1. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch. More iterations should be taken and the log-scale style figure is suggested. 2. In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments. 3. ""The average recent iterate"" described in section 3.6 uses recent 3 iterations to compute the average, the reason to choose ""3"", and the effectiveness of different choices should be discussed, as well as the ""24"" used in state features. 4. Since the block-diagonal structure imposed on A_t, B_t, and F_t, how to choose a proper block size? Or how to figure out a coordinate group? 5. The caption in Figure 1,3, ""with 48 input and hidden units"" should clarify clearly. The curves of different methods are suggested to use different lines (e.g., dashed lines) to denote different algorithms rather than colors only. 6. typo: sec 1 parg 5, ""current iterate"" -> ""current iteration"". Conclusion:  Since RL based framework has been proposed in [1] by Li & Malik, this paper tends to solve the high-dimensional problem. With the new observation of invariant in coordinates permutation in neural networks, this paper imposes the block-diagonal structure in the model to reduce the complexity of LQG algorithm. Sufficient experiment results show that the proposed method has better convergence rate than [1]. But comparing to [1], this paper has limited contribution. [1]: Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016.",19,340,13.6,5.426751592356688,165,0,340,0.0,0.0144927536231884,0.8796,116,41,57,7,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 0, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 2}",0,2,4,0,0,10,3,0,4,0,0,1,1,0,0,0,0,4,1,0,7,1,2,0.5027609971260666,0.5597199229070374,0.3190481627011343
ICLR2018-BkM27IxR--R3,Reject,"Summary of the paper --------------------------- The paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets training. The main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems (related to machine learning applications as learning a neural network) sharing the same properties. For this sake, the paper casts the problem into reinforcement learning framework and relies on guided policy search (GPS) to explore the space of states and actions. The states are represented by the iterates, the gradients, the objective function values, derived statistics and features, the actions are the update directions of parameters to be learned.  To make the formulated problem tractable, some simplifications are introduced (the policies are restricted to gaussian distributions family, block diagonal structure is imposed on the involved parameters). The mean of the stationary non-linear policy of GPS is modeled as a recurrent network with parameters to be learned. A hatch of how to learn the overall process is presented. Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach .  Comments ------------- - The overall idea of the paper, learning how to optimize, is very seducing and the experimental evaluations (comparison to normal optimizers and other meta-learners) tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problems. - Materials of the paper sometimes appear tedious to follow, mainly in sub-sections 3.4 and 3.5. It would be desirable to sum up the overall procedure in an algorithm. Page 5, the term $omega$ intervening in the definition of the policy $pi$ is not defined. - The definitions of the statistics and features (state and observation features) look highly elaborated. Can authors provide more intuition on these precise definitions? How do they impact for instance changing the time range in the definition of $Phi$) in the performance of the meta-learner? - Figures 3 and 4 illustrate some oscillations of the proposed approach. Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does? How long should be the training to ensure a good and stable convergence of the method? - An interesting experience to be conducted and shown is to train the meta-learner on another dataset (CIFAR for example) and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method.",19,402,26.8,5.383631713554987,203,1,401,0.0024937655860349,0.0072815533980582,0.9626,111,40,79,9,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 13, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,1,0,1,2,13,3,0,1,0,0,1,0,0,0,0,0,0,1,0,9,0,1,0.5032107876887267,0.3383089928120771,0.25776197530362666
ICLR2018-BkM3ibZRW-R1,,"I was asked to contribute this review rather late in the process, and in order to remain unbiased I avoided reading other reviews. I apologize if some of these comments have already been addressed in replies to other reviewers. This paper proposes a regularization strategy for autoencoders that is very similar to the adversarial autoencoder of Makhzani et al. The main difference appears to be that rather than using the classic GAN loss to shape the aggregate posterior of an autoencoder to match a chosen, fixed distribution, they instead employ a Wasserstein GAN loss (and associated weight magnitude constraint, presumably enforced with projected gradient descent) on a system where the matched distribution is instead learned via a parameterized sampler (generator in the GAN lingo). Gradient steps that optimize the encoder, decoder and generator are interleaved. The authors apply an extension of this method to topic and sentiment transfer and show moderately good latent space interpolations between generated sentences. The difference from the original AAE is rather small and straightforward, making the novelty mainly in the choice of task, focusing on discrete vectors and sequences. The exposition leaves ample room for improvement. For one thing, there is the irksome and repeated use of discrete structure when discrete *sequences* are considered almost exclusively (with the exception of discretized MNIST digits). The paper is also light on discussion of related work other than Makhzani et al -- the wealth of literature on combining autoencoders (or autoencoder-like structures such as ALI/BiGAN) and GANs merits at least passing mention. The empirical work is somewhat compelling, though I am not an expert in this task domain. The annealed importance sampling technique of Wu et al (2017) for estimating bounds on a generator's log likelihood could be easily applied in this setting and would give (for example, on binarized MNIST) a quantitative measurement of the degree of overfitting, and this would have been preferable than inventing new heuristic measures. The Reverse PPL metric requires more justification, and it looks an awful lot like the long-since-discredited Parzen window density estimation technique used in the original GAN paper. High-level comments:  - It's not clear why the optimization is done in 3 separate steps. Aside from the WGAN critic needing to be optimized for more steps, couldn't the remaining components be trained jointly, with a weighted sum of terms for the encoder? - In section 2, This [pre-training or co-training with maximum likelihood]   precludes there being a latent encoding of the sentence.  It is not at all   clear to me why this would be the case. - One benefit of the ARAE framework is that it compresses the input to a   single code vector.  This is true of any autoencoder. - It would be worth explaining, in a sentence, the approach in Shen et al for   those who are not familiar with it, seeing as it is used as a baseline. - We are told that the encoder's output is l2-normalized but the generator's   is not, instead output units of the generator are squashed with the tanh   activation. The motivation for this choice would be helpful. Shortly   thereafter we are told that the generator quickly learns to produce norm 1   outputs as evidence that it is matching the encoder's distribution, but this   is something that could have just as easily have been built-in, and is a   trivial sort of distribution matching  - In general, tables that report averages would do well to report error bars as   well. In general some more nuanced statistical analysis of these results   would be worthwhile, especially where they concern human ratings. - The dataaset fractions chosen for the semi-supervised experience seem   completely arbitrary. Is this protocol derived from some other source? Putting these in a table along with the results would improve readability. - Linear interpolation in latent space may not be the best choice here   seeing as e.g. for a Gaussian code the region near the origin has rather low   probability. Spherical interpolation as recommended by White (2016) may   improve qualitative results. - For the interpolation results you say we output the argmax, what is meant? Is beam search performed in the case of sequences? - Finally, a minor point: I will challenge the authors to justify their claim   that the learned generative model is useful (their word). Interpolating   between two sentences sampled from the prior is a neat parlour trick, but the   model as-is has little utility. Even some speculation on how this aspect   could be applied would be appreciated (admittedly, many GAN papers could use   some reflection of this sort).",36,747,24.096774193548388,5.256588072122053,381,5,742,0.0067385444743935,0.0186567164179104,0.9897,201,95,129,41,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 0, 'DAT': 4, 'MET': 16, 'EXP': 5, 'RES': 4, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 3, 'CLA': 1}",0,1,6,0,4,16,5,4,2,3,0,0,0,3,0,0,0,5,0,0,16,3,1,0.6477240179987296,0.4544558092124918,0.3684348139326597
ICLR2018-BkM3ibZRW-R2,,"the paper presents a way to encode discrete distributions which is a challenging problem. they propose to use a latent variable gan with one continuous encoding and one discrete encoding. two questions linger around re practices: 1. gan is known to struggle with discriminating distributions with different supports. the problem also persists here as the gan is discriminating between a continuous and a discrete distribution.  it'll interesting to see how the proposed approach gets around this issue. 2. the second question is related. it is unclear how the optimal distribution would look like with the latent variable gan. ideally, the discrete encoding be simply a discrete approximation of the continuous encoding. but optimization with two latent distributions and one discriminator can be hard. what we get in practice is pretty unclear. also how this could outperform classical discrete autoencoders is unclear. gan is an interesting idea to apply to solve many problems; it'll be helpful to get the intuition of which properties of gan solves the problem in this particular application to discrete autoencoders.",12,174,12.428571428571429,5.542168674698795,90,2,172,0.0116279069767441,0.0285714285714285,0.8608,39,24,36,6,4,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,0,1,0,7,6,1,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0.2876464173555371,0.1148428557201689,0.11535374327053614
ICLR2018-BkM3ibZRW-R3,,"This paper introduces a model for learning robust discrete-space representations with autoencoders. The proposed method jointly trains an RNN encoder with a GAN to produce latent representations which are designed to better encode similarity in the discrete input space. A variety of experiments are conducted that demonstrate the efficacy of the proposed methodology. Generally speaking, I like the overall idea, which, as far as I know, is a novel approach for dealing with discrete inputs. The generated textual samples look good and offer strong support for the model. However, I would have preferred to see more quantitative evaluation and less qualitative evaluation, but I understand that doing so is challenging in this domain. I will refrain from adding additional detailed commentary in this review because I am unable to judge this paper fairly with respect to other submissions owing to its large deviation from the suggested length limits. The call for papers states that we strongly recommend keeping the paper at 8 pages, yet the current submission extends well into its 10th page. In addition (and more importantly), the margins appear to have been reduced relative to the standard latex template. Altogether, it seems like this paper contains a significant amount of additional text beyond what other submissions enjoyed. I see no strong reason why this particular paper needed the extra space. In fact, there are obvious places where the exposition is excessively verbose, and there are clear opportunities to reduce the length of the submission. While I fully understand that the length suggestions are not requirements, in my opinion this paper did not make an adequate effort to abide by these suggestions. Moreover, as a result, I believe this extra length has earned this paper an unfair advantage relative to other submissions, which themselves may have removed important content in order to abide by the length suggestions. As such, I find it difficult or impossible to judge this paper fairly relative to other submissions. I regrettably cannot recommend this paper for acceptance owing to these concerns. There are many good ideas and experiments in this paper and I would strongly encourage the authors to resubmit this work to a future conference, making sure to reorganize the paper to adhere to the relevant formatting guidelines.",17,375,22.058823529411764,5.430985915492958,207,4,371,0.0107816711590296,0.0213333333333333,0.9931,96,50,68,25,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 10, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 1, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,6,2,2,0,1,0,10,0,1,1,1,0,0,4,1,2,1,0,0.573230198954368,0.6674733308729196,0.4023922571610527
ICLR2018-BkM3ibZRW-R4,,"The authors present a new variation of autoencoder, in which they jointly train (1) a discrete-space autoencoder to minimize reconstuction loss, and (2) a simpler continuous-space generator function to learn a distribution for the codes, and (3) a GAN formulation to constrain the distributions in the latent space to be similar. The paper is very clearly written, very clearly presented, addresses an important issue, and the results are solid. My primary suggestion is that I would like to know a lot more (even qualitatively, does not need to be extensively documented runs) about how sensitive the results were--- and in what ways were they sensitive--- to various hyperparameters. Currently, the authors mention in the conclusion that, as is known to often be the case with GANS, that the results were indeed sensitive. More info on this throughout the paper would be a valuable contribution.  Clearly the authors were able to make it work, with good results. When does it not work? Any observations about how it breaks down? It is interesting how strong the denoising effect is, as simply a byproduct of the adversarial regularization. Some of the results are quite entertaining indeed. I found the yelp transfer results particularly impressive. (The transfer from positive->negative on an ambiguous example was interesting: Original service is good but not quick -> service is good but not quick, but the service is horrible, and service is good, and horrible, is the same and worst time ever. I found it interesting to see what it does with the mixed signals of the word but: on one hand, keeping it helps preserve the structure of the sentence, but on the other hand, keeping it makes it hard to flip the valence. I guess the most accurate opposite would have been The service is quick but not good... ) I really like the reverse perplexity measure. Also, it was interesting how that was found to be high on AAE due to mode-collapse. Beyond that, I only have a list of very insignificant typos: -p3, end of S3, this term correspond to minimizing -p3, S4, to approximate Wasserstein-1 term --> to approximate the Wasserstein-1 term -Figure 1, caption which is similarly decoded to $mathbf{~x}$ . I would say that it is similarly decoded to $mathbf{c}$, since it is mathbf{c} that gets decoded. Unless the authors meant that it is similarly decoded to produce $mathbf{~x}$. Alternately, I would just say something like to produce a code vector, which lies in the same space as mathbf{c}, since the decoding of the generated code vector does not seem to be particularly relevant right here. -p5, beginning of Section 6.1:  to regularize the model produce --> to regularize the model to produce ? -p6, end of first par. is quite high for the ARAE than in the case --> quite a bit higher than? etc... -p7, near the bottom shown in figure 6. --> table, not figure... -p8  ability mimic -->ability to mimic  -p9 Fig 3 -- the caption is mismatched with the figure.. top/bottom/left/right/etc.... Something is confusing there... -p9 near the bottom The model learns a improved --> The model learns an improved -p14 left side, 4th cell up, Cross-AE-->ARAE This is a very nice paper with a clear idea (regularize discrete autoencoder using a flexible rather than a fixed prior), that makes good sense and is very clearly presented. In the words of one of the paper's own examples: It has a great atmosphere, with wonderful service. :) Still, I wouldn't mind knowing a little more about what happened in the kitchen...  ",29,582,20.06896551724138,4.983271375464684,250,2,580,0.0034482758620689,0.006677796327212,0.9967,144,70,116,49,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 14, 'TNF': 3, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 13, 'SUB': 3, 'CLA': 11}",0,1,0,2,0,7,1,14,3,3,0,1,0,0,0,0,0,0,3,0,13,3,11,0.5738091832610037,0.4530557752353503,0.32986216540483865
ICLR2018-BkN_r2lR--R1,Accept,"This paper presents an image-to-image cross domain translation framework based on generative adversarial networks. The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved. The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision. Negatives: 1.) The task of exact correspondence identification seems contrived. It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing. 2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN's matches is better than an unsupervised method.  More transfer experiments of this kind would greatly benefit the paper and support the conclusion that ""our self-supervised method performs similarly to the fully supervised method. ""   Positives: 1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework. 2.) The paper shows promising results on applying a supervised method on top of AN-GAN's matches. Minor comments: 1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases. 2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al.",11,248,17.714285714285715,5.390557939914163,135,3,245,0.0122448979591836,0.0238095238095238,0.9836,76,30,38,13,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 6, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,1,0,0,1,6,2,2,1,0,0,1,1,0,0,0,0,0,1,0,8,1,1,0.5728214296819129,0.4487981464883452,0.32232574021464144
ICLR2018-BkN_r2lR--R2,Accept,"The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not). The algorithm is tested on three datasets. While the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts),; the paper lacks clarity and sufficient details. Instead of the longer intro and related work discussion,; I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. It is hard to understand sufficiently well what the formalism means without more insigh t.  Also, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean.",10,174,19.33333333333333,5.392857142857143,109,0,174,0.0,0.0171428571428571,0.9579,44,28,29,9,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,2,2,1,0,4,3,1,2,0,0,0,0,0,0,0,0,1,3,0,4,0,1,0.5010522727171774,0.44643345459658,0.2831568628737872
ICLR2018-BkN_r2lR--R3,Accept,"This paper adds an interesting twist on top of recent unpaired image translation work. A domain-level translation function is jointly optimized with an instance-level matching objective. This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches. I think this is a promising direction, but the current paper has unconvincing results, and it's not clear if the method is really solving an important problem yet. My main criticism is with the experiments and results. The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets. Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches. This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches. It feels rather contrived to focus so much on the datasets with exact matches since,; 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly,; 2) it's hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios); 3) when exact matches exist, simpler methods may be sufficient, such as matching edges. There is no comparison to any such simple baselines. I think finding analogies that are not exact matches is much more compelling. Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results. I'd like to see far more results, and some attempt at a metric. One option would be to run user studies where humans judge the quality of the matches. The results shown in Figure 2 don't convince me, not just because they are qualitative and few, but also because I'm not sure I even agree that the proposed method is producing better results: for example, the DiscoGAN results have some artifacts but capture the texture better in row 3. I was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it's no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data. In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful. What I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches. For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real. I'd like to see more discussion of why the second stage supervised problem is beneficial. Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)? Minor comments: 1. In the intro, it would be useful to have a clear definition of ""analogy"" for the present context. 2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017. 3. Page 3: ""Weakly Supervised Mapping"" u2014 I wouldn't call this weakly supervised. Rather, I'd say it's just another constraint / prior, similar to cycle-consistency, which was referred to under the ""Unsupervised"" section. 4. Page 4 and throughout: It's hard to follow which variables are being optimized over when. For example, in Eqn. 7, it would be clearer to write out the min over optimization variables. 5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017. 6. Page 7: The following sentence is confusing and should be clarified: ""This shows that the distribution matching is able to map source images that are semantically similar in the target domain. "" 7. Page 7: ""This shows that a good initialization is important for this task. "" u2014 Isn't this more than initialization? Rather, removing the distributional and cycle constraints changes the overall objective being optimized. 8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function? 9. Throughout the paper, some citations are missing enclosing parentheses.",38,720,16.0,5.137313432835821,301,5,715,0.0069930069930069,0.0235131396957123,0.9937,179,98,135,55,10,5,"{'ABS': 0, 'INT': 3, 'RWK': 2, 'PDI': 0, 'DAT': 5, 'MET': 16, 'EXP': 6, 'RES': 6, 'TNF': 2, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 20, 'SUB': 3, 'CLA': 5}",0,3,2,0,5,16,6,6,2,4,0,1,3,0,0,0,0,2,1,0,20,3,5,0.7191462152473936,0.5680278026748211,0.45603946836518267
ICLR2018-BkPrDFgR--R1,Reject,"The paper studies methods for verifying neural nets through their piecewise linear structure. The authors survey different methods from the literature, propose a novel one, and evaluate them on a set of benchmarks. A major drawback of the evaluation of the different approaches is that everything was used with its default parameters. It is very unlikely that these defaults are optimal across the different benchmarks. To get a better impression of what approaches perform well, their parameters should be tuned to the particular benchmark. This may significantly change the conclusions drawn from the experiments. Figures 4-7 are hard to interpret and do not convey a clear message. There is no clear trend in many of them and a lot of noise.  It may be better to relate the structure of the network to other measures of the hardness of a problem, e.g. the phase transition. Again parameter tuning would potentially change all of these figures significantly, as would e.g. a change in hardware. Given the kind of general trend the authors seem to want to show here, I feel that a more theoretic measure of problem hardness would be more appropriate here. The authors say of the proposed TwinStream dataset that it may not be representative of real use-cases. It seems odd to propose something that is entirely artificial. The description of the empirical setup could be more detailed. Are the properties that are being verified different properties, or the same property on different networks? The tables look ugly. It seems that the header data set should be approach or something similar. In summary, I feel that while there are some issues with the paper, it presents interesting results and can be accepted.",17,283,14.894736842105264,5.099630996309963,152,5,278,0.0179856115107913,0.0528169014084507,0.4464,69,36,49,14,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 1, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 1, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,10,2,1,4,1,0,1,0,0,0,0,0,1,4,1,10,0,0,0.5738363901264619,0.4502267681294412,0.314113437875862
ICLR2018-BkPrDFgR--R2,Reject,"Summary:  This paper: - provides a compehensive review of existing techniques for verifying properties of neural networks. - introduces a simple branch-and-bound approach. - provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one. Relevance: Although there isn't any learning going on, the paper is relevant to the conference. Clarity: Writing is excellent, the content is well presented and the paper is enjoyable read. Soundness: As far as I can tell, the work is sound. Novelty: This is in my opinion the weakest point of the paper. There isn't really much novelty in the work. The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is). The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well). Significance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods.  Unfortunately, there isn't much insight to be gained from them. I couldn't see any emerging trend/useful recommendations (like if your problem looks like X, then use algorithm B). This is unfortunately often the case when dealing with combinatorial search/optimization.",14,221,15.785714285714286,5.287735849056604,133,3,218,0.0137614678899082,0.0132743362831858,0.9703,57,29,43,21,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 3, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,1,0,0,3,4,3,0,0,0,5,0,1,1,3,1,2,0,0,4,0,1,0.5010520506203165,0.6687399166974218,0.35502051252130706
ICLR2018-BkPrDFgR--R3,Reject,"The paper compares some recently proposed method for validation of properties of piece-wise linear neural networks and claims to propose a novel method for the same. Unfortunately, the proposed branch and bound method does not explain how to implement the bound part (compute lower bound) -- and has been used  several times in the same application,; incl.:  Ruediger Ehlers. Planet. https://github.com/progirep/planet, Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis Alessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351 Specifically, the authors say: In our experiments, we use the result of  minimising the variable corresponding to the output of the network, subject  to the constraints of the linear approximation introduced by Ehlers (2017a) which sounds a bit like using linear programming relaxations, which is what the approaches using branch and bound cited above use. If that is the case, the paper does not have any original contribution. If that is not the case, the authors may have some contribution to make, but have not made it in this paper, as it does not explain the lower bound computation other than the one based on LPs. Generally, I find a jarring mis-fit between the motivation (deep learning for driving, presumably involving millions or billions of parameters) and the actual reach of the methods proposed (hundreds of parameters). This reach is NOT inherent in integer programming, per se. Modern solvers routinely solve instances with tens of millions of non-zeros in the constraint matrix, but require a strong relaxation. The authors may hence consider improving the LP relaxation, noting that the big-M constraint are notorious for producing weak relaxations.",10,282,20.142857142857142,5.44043321299639,165,3,279,0.010752688172043,0.0138408304498269,0.9649,91,35,44,12,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 5, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,2,1,0,6,0,0,0,0,0,2,0,1,0,5,0,0,0,0,5,0,0,0.4299482958507865,0.2248904822996666,0.19533472219565723
ICLR2018-BkQCGzZ0--R1,Reject,"The topic is interesting however the description in the paper is lacking clarity. The paper is written in a procedural fashion - I first did that, then I did that and after that I did third. Having proper mathematical description and good diagrams of what you doing would have immensely helped. Another big issue is the lack of proper validation in Section 3.4. Even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations, such as listening tests in speech synthesis. Given that I see only one example I can not objectively know if your model produces examples like that 'each' time so having just one example is as good as having none. ",6,132,18.857142857142858,4.935483870967742,85,0,132,0.0,0.074074074074074,0.6808,32,12,30,14,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,0,1,1,0,3,1,0,1,0,0,1,0,0,0,0,0,1,1,0,2,1,1,0.4290718127989979,0.5561775129903985,0.25562914444082696
ICLR2018-BkQCGzZ0--R2,Reject,"This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder. However, the experiments are too weak to demonstrate the effectiveness of using discrete representations. The design of the experiments on language model is problematic. There are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper. Generally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations. However, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart.  The proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people. How is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model? The qualitative evaluation on 'Deciperhing the Latent Code' is not enough either. In addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively. One suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case. Overall, this paper is more suitable for the workshop track . It also needs a lot of more studies on related work.",13,249,20.75,5.726530612244898,142,2,247,0.0080971659919028,0.0278884462151394,0.4818,69,33,40,20,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,7,4,1,0,0,0,1,0,0,0,0,0,1,0,1,9,0,0,0.4304116565316466,0.3383089928120771,0.2159749952166832
ICLR2018-BkQCGzZ0--R3,Reject,"The authors describe a method for encoding text into a discrete representation / latent space . On a measure that they propose, they should it outperforms an alternative Gumbel-Softmax method for both language modeling and NMT. The proposed method seems effective, and the proposed DSAE metric is nice, though it's surprising if previous papers have not used metrics similar to normalized reduction in log-ppl . The datasets considered in the experiments are also large, another plus. However, overall, the paper is difficult to read and parse, especially since low-level details are weaved together with higher-level points throughout, and are often not motivated. The major critique would be the qualitative nature of results in the sections on ""Decipering the latent code"" and (to a lesser extent) ""Mixed sample-beam decoding. "" These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3. Some quantitative or aggregate results are needed, and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decoding.",8,177,22.125,5.4678362573099415,122,2,175,0.0114285714285714,0.0331491712707182,0.7022,43,26,33,10,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,0,1,4,1,2,0,0,0,1,0,0,0,0,0,0,0,0,3,0,1,0.5008062782340634,0.2234661370919081,0.22285833977037936
ICLR2018-BkQqq0gRb-R1,Accept,"Overall, the idea of this paper is simple but interesting. Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty. The paper is written well, and literature review is sufficient. My comment is mainly about its importance for large-scale computer vision applications. The neural networks in the experiments are shallow.  ",6,66,11.0,5.7846153846153845,53,0,66,0.0,0.0147058823529411,0.8338,21,12,8,2,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,1,1,0,1,2,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0.4286576243631372,0.2222222222222222,0.19406145005248157
ICLR2018-BkQqq0gRb-R2,Accept,"This paper proposes a new method, called VCL, for continual learning. This method is a combination of the online variational inference for streaming environment with Monte Carlo method. The authors further propose to maintain a coreset which consists of representative data points from the past tasks. Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning. Extensive experiments shows that VCL performs very well, compared with some state-of-the-art methods. The authors present two ideas for continual learning in this paper: (1) Combination of online variational inference and sampling method, (2) Use of coreset to deal with the catastrophic forgetting problem. Both ideas have been investigated in Bayesian literature, while (2) has been recently investigated in continual learning. Therefore, the authors seems to be the first to investigate the effectiveness of (1) for continual learning. From extensive experiments, the authors find that the first idea results in VCL which can outperform other state-of-the-art approaches, while the second idea plays little role.  The finding of the effectiveness of idea (1) seems to be significant.  The authors did a good job when providing a clear presentation, a detailed analysis about related work, an employment to deep discriminative models and deep generative models, and a thorough investigation of empirical performance. There are some concerns the authors should consider: - Since the coreset plays little role in the superior performance of VCL, it might be better if the authors rephrase the title of the paper. When the coreset is empty, VCL turns out to be online variational inference [Broderich et al., 2013; Ghahramani & Attias, 2000]. Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well. - It is unclear about the sensitivity of VCL with respect to the size of the coreset. The authors should investigate this aspect. - What is the trade-off when the size of the coreset increases? ",16,323,19.0,5.477272727272728,146,3,320,0.009375,0.0303030303030303,0.7425,98,46,44,8,8,7,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 1, 'DAT': 5, 'MET': 7, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 4, 'CLA': 1}",0,2,5,1,5,7,2,0,0,2,0,0,1,0,0,1,1,3,1,0,5,4,1,0.5735324727820297,0.780819962156494,0.44689849528750825
ICLR2018-BkQqq0gRb-R3,Accept,"The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning. The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations. The paper is well written, the problem is described neatly in conjunction with the past work, and the proposed algorithm is supported by experiments. The work is a useful addition to the communit y.  My main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017).  Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem.",8,148,14.8,5.006993006993007,83,0,148,0.0,0.0066666666666666,-0.7861,49,18,18,5,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,1,3,0,0,3,3,0,0,0,0,2,1,0,0,0,1,1,0,0,4,1,1,0.4294505989952338,0.5574214278600844,0.27403654576571
ICLR2018-BkS3fnl0W-R1,Reject,"The idea of using GANs for outlier detection is interesting and the problem is relevant. However, I have the following concerns about the quality and the significance: - The proposed formulation in Equation (2) is questionable. The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence. However, then what is learned though the proposed formulation? Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable. - In addition to the above point, I guess the expectation is needed as the original formulation of GAN. Otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defined. - In experiments, although the authors say lots of datasets are used, only two datasets are used, which is not enough to examine the performance of outlier detection methods. Moreover, outliers are artificially generated in these datasets, hence there is no evaluation on pure real-world datasets. To achieve the better quality of the paper, I recommend to add more real-world datasets in experiments. - As discussed in Section 2, there are already many outlier detection methods, such as distance-based outlier detection methods, but they are not compared in experiments. Although the authors argue that distance-based outlier detection methods do not work well for high-dimensional data, this is not always correct .   Please see the paper:   -- Zimek, A., Schubert, E., Kriegel, H.-P., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining (2012)   This paper shows that the performance gets even better for higher dimensional data if each feature is relevant. I recommend to add some distance-based outlier detection methods as baselines in experiments. - Since parameter tuning by cross validation cannot be used due to missing information of outliers, it is important to examine the sensitivity of the proposed method with respect to changes in its parameters (a_new, lambda, and others). Otherwise in practice how to set these parameters to get better results is not obvious. * The clarity of this paper is not high as the proposed method is not well explained. In particular, please mathematically formulate each proposed technique in Section 4. * Since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough, the originality is not high. Minor comments: - P.1, L.5 in the third paragraph: architexture -> architecture - What does Cor of CorGAN mean? AFTER REVISION Thank you to the authors for their response and revision. Although the paper has been improved, I keep my rating due to the insufficient experimental evaluation.",23,447,22.35,5.43287037037037,202,1,446,0.0022421524663677,0.021505376344086,0.979,128,60,85,29,7,7,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 11, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 2, 'CLA': 2}",0,0,1,2,3,11,6,0,0,0,0,2,1,0,0,1,1,2,0,1,9,2,2,0.5031098002460833,0.783058411392887,0.3963752930067515
ICLR2018-BkS3fnl0W-R2,Reject,"The idea of the paper is to use a GAN-like training to learn a novelty detection approach. In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution. The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers . To establish such a behavior, the authors propose early stopping as well as other heuristics. I like the idea of the paper,; however, this paper needs a revision in various aspects, which I simply list in the following:; * The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc * The model selection using the AUC of inlier accepted fraction is not well motivated in my opinion. This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data. The latter is important for the GAN-like training. * The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison.  * With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5) * There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators) * A cross-entropy loss for the autoencoder does not make much sense in my opinion (?) Minor comments: * Citations should be fixed (use citep to enclose them in ()) * The term AI-related task sounds a bit too broad * The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection. * Where is Table 1? * There are quite a lot of typos. *After revision statement* I thank the authors for their revision, but I keep my rating. The clarity of the paper has improved; but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)",21,386,27.571428571428573,5.226158038147139,193,0,386,0.0,0.0049875311720698,0.9818,117,45,64,30,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 2, 'MET': 8, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 4, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 2}",0,0,3,2,2,8,2,0,1,1,0,4,1,0,0,0,0,3,4,1,4,2,2,0.6450873062760131,0.669139382436161,0.46106430221507017
ICLR2018-BkS3fnl0W-R3,Reject,"This paper addresses the problem of one class classification. The authors suggest a few techniques to learn how to classify samples as negative (out of class) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class. The suggested techniques are nice and show promising results. But I feel a lot can still be done to justify them, even just one of them. For instance, the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values. But, in the experimental section results are shown only for a  single value, alpha_new 0.9 The authors also suggest early stopping but again (as far as I understand) only a single value for the number of iterations was tested. The writing of the paper is also very unclear, with several repetitions and many typos e.g.:  'we first introduce you a' 'architexture' 'future work remain to' 'it self'  I believe there is a lot of potential in the approach(es) presented in the paper. In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.",8,200,25.0,5.032258064516129,118,0,200,0.0,0.0344827586206896,0.7477,52,25,30,15,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,0,1,0,4,2,1,0,0,0,1,0,0,0,0,0,0,1,0,5,0,1,0.4294082007044912,0.3358211630727052,0.21352339129512155
ICLR2018-BkSDMA36Z-R1,Accept,"The authors propose a mechanism for learning task-specific region embeddings for use in text classification. Specifically, this comprises a standard word embedding an accompanying local context embedding. The key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size. Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings. The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways. Region embeddings are then composed (summed) and fed through a standard model. Strong points --- + The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization. Further, the work is clearly presented. + At the very least the model does seem comparable in performance to various recent methods (as per Table 2), however as noted below the gains are marginal and I have some questions on the setup. + The authors perform ablation experiments, which are always nice to see. Weak points --- - I have a critical question for clarification in the experiments.  The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach? - The gains here appear to be consistent, but they seem marginal. The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range). Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see A sensitivity analysis of (and practitioners guide to) CNNs... Zhang and Wallace, 2015.) - The related work section seems light. For instance, there is no discussion at all of LSTMs and their application to text classificatio (e.g., Tang et al., EMNLP 2015) -- although it is noted that the authors do compare against D-LSTM,  or char-level CNNs for the same (see Zhang et al., NIPs 2015). Other relevant work not discussed includes Iyyer et al. (ACL 2015). In their respective ways, these papers address some of the same issues the authors consider here. - The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc. I would have appreciated more intuition behind these approaches. Small comments --- There is a typo in Figure 4 -- Howerver should be However *** Update after author response ***  Thanks to the authors for their responses. My score is unchanged.",21,485,21.08695652173913,5.167410714285714,253,3,482,0.0062240663900414,0.0297619047619047,0.9789,152,60,84,26,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 2, 'MET': 12, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 3, 'CLA': 2}",0,1,3,2,2,12,2,3,1,1,0,2,0,0,0,0,0,2,0,1,9,3,2,0.7175372123290409,0.5609428656789863,0.4489923146840629
ICLR2018-BkSDMA36Z-R2,Accept,"The authors present a model for text classification. The parameters of the model are an embedding for each word and a local context unit.  The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word). After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task. The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task. The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings. Here the embedding of word i is combined with the elements of the context units of words in the context. To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units). The method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy. the choice of baselines is convincing. What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task? The introduction was fine. Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing. The related work section only makes sense *after* there is at least a minimal explanation of what the local context units do. A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear. Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones. The authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}. The included baselines are extensive and the proposed method outperforms existing methods on most datasets. In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper. Figure 2 and 3 could be next to each other to save space. I found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best? Qualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level. It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position. Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? Pros:  + simple model  + strong quantitative results Cons:  - notation (i.e. precise definition of r_{i,c}) - qualitative analysis could be extended  - writing could be improved  ",33,655,22.58620689655172,5.152380952380953,244,1,654,0.0015290519877675,0.0059880239520958,0.9954,199,67,108,27,8,5,"{'ABS': 0, 'INT': 5, 'RWK': 4, 'PDI': 0, 'DAT': 2, 'MET': 19, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 6, 'REC': 0, 'EMP': 9, 'SUB': 5, 'CLA': 3}",0,5,4,0,2,19,0,1,1,4,0,1,0,0,0,0,0,3,6,0,9,5,3,0.5763858177828515,0.56166236094269,0.36961118287090916
ICLR2018-BkSDMA36Z-R3,Accept,"() Summary In this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark.  The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words. In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a context unit of size d x K, where d is the embedding size and K the region size. Thus, the model also have a vector representation for pair of word and position in the region. Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the context unit of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and context vectors are exchanged) . The max-pooling operation is then used to obtain a vector representation of size d. Then a linear classifier is applied on top of the sum of the region embeddings.  The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015). They obtain state of the art results on most of the datasets. They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the context units vector by a scalar. The authors also provide some visualisation of the parameters of their model. () Discussion Overall, I think that the proposed method is sound and well justified. The empirical evaluations, analysis and comparisons to existing methods are well executed. I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art. I suspect that the model is also computationally efficient: can the authors report training time for different datasets? I think that it would make the paper stronger. One of the main limitations of the model, as stated by the authors, is its number of parameters. Could the authors also report these? While the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed. Indeed, the papers contains many typos (I have listed a few), as well as ungrammatical sentences. I also think that a discussion of the attention is all you need paper by Vaswani et al. is needed, as both articles seem strongly related. As a minor comment, I advise the authors to use a different letter for word embeddings and the projected word embeddings (equation at the bottom of page 3). It would also make the paper more clear. () Pros / Cons: + simple yet powerful method for text classification + strong experimental results + ablation study / analysis of influence of parameters - writing of the paper - missing discussion to the attention is all you need paper, which seems highly relevant () Typos: Page 1 a support vectors machineS -> a support vector machine  performs good -> performs well  the n-grams was widely ->  -grams were widely  to apply large region size -> to apply to large region size are trained separately -> do not share parameters Page 2 convolutional neural networks(CNN) -> convolutional neural networks (CNN)  related works -> related work  effective in Wang and Manning -> effective by Wang and Manning  applied on text classification -> applied to text classification  shard(word independent) -> shard (word independent) Page 3 can be treat -> can be treated fixed length continues subsequence -> fixed length contiguous subsequence w_i stands for the -> w_i standing for the which both the unit -> where both the unit in vocabulary -> in the vocabulary  etc...",39,611,25.45833333333333,5.09965034965035,240,6,605,0.0099173553719008,0.0138674884437596,0.993,213,71,102,31,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 3, 'MET': 19, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 4, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 16}",0,1,5,1,3,19,1,1,1,4,0,4,0,0,0,1,0,5,0,0,7,1,16,0.7193937199891617,0.5609729764561842,0.4573798785636775
ICLR2018-BkTQ8UckG-R1,,"The paper describes learning joint embedding of sentences and images. The main point is in using a triplet loss that is applied to hardest-negatives, instead of averaging over all triplets. This led to improvement over SOTA in a task of caption ranking on MS-COCO, nd good performance on flickr30K. The main issue with this paper is novelty. Using hard negatives is routinely  used in many embedding tasks, and has been discussed in many publications. For instance recently Wu et all in ICCV2017, bu also many other papers. When used in practice wit real-world datasets, taking the max (hardest negative) tends to be very sensitive to label noise, since the hardest negative is sometime just a positive sample with incorrect label. In these cases focusing on the hardest negative reduces performance. While it is good to know that using hard negatives improves recall measures on coco, it is not clear that this paper provides enough novel insight to be interesting enough for the ICLR audience.",9,164,18.22222222222222,4.975155279503106,103,0,164,0.0,0.0303030303030303,0.4617,42,24,32,13,7,2,"{'ABS': 0, 'INT': 3, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,3,1,2,1,3,0,2,0,0,0,1,0,0,0,2,0,0,0,0,1,0,0,0.5006664238407654,0.2222673298067403,0.22316342668763167
ICLR2018-BkUDW_lCb-R1,Reject,"The paper claims to develop a novel method to map natural language queries to SQL. They claim to have the following contributions:   1. Using a grammar to guide decoding 2. Using a new loss function for pointer / copy mechanism. For each output token, they aggregate scores for all positions that the output token can be copied from. I am confident that point 1 has been used in several previous works. Although point 2 seems novel, I am not convinced that it is significant enough for ICLR. I was also not sure why there is a need to copy items from the input question, since all SQL query nouns will be present in the SQL table in some form. What will happen if we restrict the copy mechanism to only copy from SQL table. The references need work. There are repeated entries for the same reference (one form arxiv and one from conference). Please cite the conference version if one is available, many arxiv references have conference versions. Rebuttal Response: I am still not confident about the significance of contribution 1, so keeping the score the same.",11,186,14.307692307692308,4.84393063583815,109,1,185,0.0054054054054054,0.0476190476190476,0.5792,54,19,38,8,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 1, 'NOV': 3, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,0,0,1,6,1,0,0,0,0,1,2,0,1,3,1,0,2,1,2,0,0,0.4298347821451891,0.6674404081943492,0.2938949139382744
ICLR2018-BkUDW_lCb-R2,Reject,"This paper proposes a model for solving the WikiSQL dataset that was released recently. The main issues with the paper is that its contributions are not new. * The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning).  Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017.  Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time. I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular. Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017. So at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution. * The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017. If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results. Moreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold. the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens.  The authors re-invent this and find it works better than randomly choosing a gold token or taking the max. But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing. * As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance. In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision.",16,437,20.80952380952381,4.583732057416268,198,2,435,0.0045977011494252,0.0358744394618834,0.9612,99,45,93,37,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 3, 'DAT': 2, 'MET': 8, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 7, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,7,3,2,8,0,2,0,1,0,0,0,0,0,7,0,2,0,0,8,0,0,0.502444664567111,0.338074843441533,0.25005625625920824
ICLR2018-BkUDW_lCb-R3,Reject,"This paper presents a neural architecture for converting natural language queries to SQL statements. The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary.  The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization). Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model.  Overall, even though the architecture is not very novel,; the paper is well-written and the results are strong; as such, I'd recommend the paper for acceptance. Some questions: - How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)? Could the output grammar be extended to support joins, for instance?  As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful. - How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.? In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?",12,218,27.25,5.353488372093024,141,1,217,0.0046082949308755,0.0089285714285714,0.9715,64,31,34,10,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 1, 'MET': 7, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,3,0,1,7,2,2,0,0,0,2,0,0,0,1,0,2,0,1,6,0,1,0.5018494449799447,0.5587825052869604,0.31994659721675056
ICLR2018-BkUHlMZ0b-R1,Accept,"The work claims a measure of robustness of networks that is attack-agnostic. Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function. That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples. Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation. The paper closely follows Hein and Andriushchenko (2017). There is a slight modification that enlarges the class of functions for which the theory is applicable (Lemma 3.3). As far as I know, the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of samples. This is a rather simple idea that is shown to be effective in Figure 3. The following section (the part starting from 5.3) presents the key to the success of the proposed measure. This is an important problem and the paper attempts to tackle it in a computationally efficient way. The fact that the norms of attacks are slightly above the proposed score is promising, however, there is always the risk of finding a lower bound that is too small (zeros and large gaps in Figure 3).  It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score. Finally, a minor point: Definition 3.1 has a confusing notation, f is a K-valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1. I believe this is just a typo. Edit: Thanks for the fixes and clarification of essential parts in the paper.",14,308,20.53333333333333,4.961538461538462,155,0,308,0.0,0.0064724919093851,0.8338,78,25,54,19,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,1,2,1,10,1,0,2,0,0,1,0,0,0,1,0,0,0,0,6,1,1,0.5737628982751958,0.4475542316186593,0.32809328764079154
ICLR2018-BkUHlMZ0b-R2,Accept,"Summary           The authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point. This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network. The method proposed in the paper already exists for classical function, they only transpose it to neural networks. Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions. Clarity        The paper is clear and well-written. Originality            This idea is not new: if we search for Lipschitz constant estimation in google scholar, we get for example Wood, G. R., and B. P. Zhang. Estimation of the Lipschitz constant of a function. (1996) which presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull). Technical quality                 The main theoretical result in the paper is the analysis of the lower-bound on delta, the smallest perturbation to apply on a data point to fool the network. This result is obtained almost directly by writing the bound on Lipschitz-continuous function  | f(y)-f(x) | < L || y-x || where x   x_0 and y   x_0 + delta. Comments: - Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity? Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x 0). Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement. - (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g   f_c - f_j for fixed g. However, once g   min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be  g(x)   min_{k  eq c} f_c(x) - f_k(x). Thus its Lipschitz constant is different, potentially equal to L_q   max_{k} | L_q^k |,  where L_q^k is the Lipschitz constant of f_c-f_k. If the theorem remains unchanged after this modification, you should clarify the proof. Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened. - Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated. Numerical experiments                       Globally, the numerical experiments are in favor of the presented method. The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example. Moreover, the numerical experiments look to be realized in the context of targeted attack. To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack. #######################################################  Post-rebuttal review ---------------------------  Given the details the authors provided to my review, I decided to adjust my score. The method is simple and shows to be extremely effective/accurate in practice. Detailed answers:  1) Indeed, I was not aware that the paper only focuses on one dimensional functions. However, they still work with less assumption, i.e., with no differential functions. I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from slope to gradient norm. In any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point. 2)  Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work.  This is right. I am just surprised is has not been done before, since it requires only few lines of derivation. I searched a bit but it is not possible to find any kind of similar results. Moreover, this leads to good performances, so there is no needs to have something more complex. 3) The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward Indeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider. Quickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E. Let || . || the norm of space E. Then, || . ||* is the dual norm of ||.||, and also the norm of E*. In that case, Lipschitz continuity writes f(x)-f(y) <  L || x-y ||, with L >  max_{x in E*} || f'(x) ||* In the case where || . || is an ell-p norm, then || . ||* is an ell-q norm; with 1/p+1/q   1. If you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton's method on convex problems, by Yurii Nesterov. I have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper.",35,809,16.18,5.0,321,2,807,0.0024783147459727,0.0149253731343283,0.9916,265,96,125,46,9,5,"{'ABS': 0, 'INT': 2, 'RWK': 7, 'PDI': 1, 'DAT': 0, 'MET': 23, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 3, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 2, 'EMP': 22, 'SUB': 0, 'CLA': 1}",0,2,7,1,0,23,5,3,0,5,0,3,3,0,0,2,0,4,0,2,22,0,1,0.6495405038045192,0.5690330574155111,0.4158700700908547
ICLR2018-BkUHlMZ0b-R3,Accept,"In this work, the objective is to analyze the robustness of a neural network to any sort of attack. This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function. This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community. This is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea.",4,94,18.8,4.9010989010989015,49,0,94,0.0,0.0531914893617021,0.4779,26,7,15,6,3,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,3,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0.2147860985132836,0.1111562186956292,0.08373084026634708
ICLR2018-BkUp6GZRW-R1,Accept,"This paper studies a new architecture DualAC. The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible). There are several drawbacks of the current format of the paper: 1. The algorithm is vague. Alg 1 line 5: 'closed form': there is no closed form in Eq(14). It is just an MC approximation. line 6: Decay O(1/t^beta). This is indeed vague albeit easy to understand. The algorithm requires that every step is crystal clear. 2. Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract,'Dual-AC ' (an extra space). There are many format errors like this throughout the paper. The author is suggested to do a careful format check. 3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief. 4. The experimental part is ok to me, but not very impressive. Overall, this seems to be a nice paper to me.",13,181,9.526315789473683,4.929411764705883,101,2,179,0.0111731843575419,0.0329670329670329,0.6151,43,31,31,7,6,3,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",1,1,1,0,0,7,1,0,0,0,0,3,0,0,0,0,0,1,3,0,7,0,0,0.4301634188702377,0.3371882157899978,0.21451643665540954
ICLR2018-BkUp6GZRW-R2,Accept,"The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper. Each extension to the Dual Actor-Critic is well motivated and clear in context. Perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice; multi-step updates, regularized against MC returns, stochastic mirror descent. The practical implementation section losses some of this clear organization, and could certainly be clarified each part tied into Algorithm 1, and this was itself made less high-level. But these are minor gripes overall. Turning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO. There were a few things that jumped out to me that I was surprised by. The difference in performance for Dual-AC between Figure 1 and Figure 2b is significant, but the only difference seems to be a reduce batch size, is this right? This suggests a fairly significant sensitivity to this hyperparameter if so. Reproducibility in continuous control is particularly problematic. Nonetheless, in recent work PPO and TRPO performance on the same set of tasks seem to be substantively different than what the authors get in their experiments. I'm thinking in particular of:  Proximal Policy Optimization Algorithms (Schulman et. al., 2017) Multi-Batch Experience Replay for Fast Convergence of Continuous Action Control (Han and Sung, 2017) In both these cases the results for PPO and TRPO vary pretty significantly from what we see here, and an important one to look at is the InvertedDoublePendulum-v1 task, which I would think PPO would get closer to 8000, and TRPO not get off the ground. Part of this could be the notion of an iteration, which was not clear to me how this corresponded to actual time steps. Most likely, to my mind, is that the parameterization used (discussed in the appendix) is improving TRPO and hurting PPO. With these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved,; which may beg the question if the algorithmic contributions are buying much for their added complexity? Pros: Well written, thorough treatment of the approaches. Improvements on top of Dual-AC with ablation study show improvement. Cons: Empirical gains might not be very large.",20,396,23.294117647058822,5.184210526315789,222,9,387,0.0232558139534883,0.0453400503778337,0.9876,105,48,63,23,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 10, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 3}",0,0,3,0,0,6,10,1,1,0,1,2,2,0,0,0,2,3,1,0,11,0,3,0.5736740817405287,0.5622197618085213,0.3538740803550589
ICLR2018-BkUp6GZRW-R3,Accept,"This paper proposes a method, Dual-AC, for optimizing the actor(policy) and critic(value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems.  The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms. Overall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score. For the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time"". There is a statement that ""Chen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization"". Is you Theorem 2 somehow an extension? Is Theorem 3 completely new? This is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*}   V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.  There is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.  For the experiments, the following should be addressed. 1. It would have been better to also show the performance graphs with and without the improvements for multiple domains. 2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains. 3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO). 4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful. Minor comments: 1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint.",36,688,18.5945945945946,5.391371340523883,310,3,685,0.0043795620437956,0.0244604316546762,0.9825,190,80,125,30,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 0, 'MET': 26, 'EXP': 6, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 5, 'PNF': 1, 'REC': 1, 'EMP': 20, 'SUB': 0, 'CLA': 1}",0,1,5,0,0,26,6,3,1,0,0,2,1,0,0,2,1,5,1,1,20,0,1,0.5785931274347369,0.7901087268530723,0.45771549894054403
ICLR2018-BkVf1AeAZ-R1,Reject,"The paper proposes to add an embedding layer for labels that constrains normal classifiers in order to find label representations that are semantically consistent. The approach is then experimented on various image and text tasks. The description of the model is laborious and hard to follow. Figure 1 helps but is only referred to at the end of the description (at the end of section 2.1), which instead explains each step without the big picture and loses the reader with confusing notation. For instance, it only became clear at the end of the section that E was learned. One of the motivations behing the model is to force label representations to be in a semantic space (where two labels with similar meanings would be nearby). The assumption given in the introduction is that softmax would not yield such a representation, but nowhere in the paper this assumption is verified.  I believe that using cross-entropy with softmax should also push semantically similar labels to be nearby in the weight space entering the softmax. This should at least be verified and compared appropriately. Another motivation of the paper is that targets are given as 1s or 0s while soft targets should work better . I believe this is true, but there is a lot of prior work on these, such as adding a temperature to the softmax, or using distillation, etc. None of these are discussed appropriately in the paper. Section 2.2 describes a way to compress the label embedding representation, but it is not clear if this is actually used in the experiments. h is never discussed after section 2.2. Experiments on known datasets are interesting, but none of the results are competitive with current state-of-the-art results (SOTA), despite what is said in Appending D. For instance, one can find SOTA results for CIFAR100 around 16% and for CIFAR10 around 3%. Similarly, one can find SOTA results for IWSLT2015 around 28 BLEU . It can be fine to not be SOTA as long as it is acknowledged and discussed appropriately.",14,337,18.72222222222222,4.962264150943396,150,3,334,0.0089820359281437,0.0294117647058823,0.927,77,30,68,25,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 0, 'DAT': 1, 'MET': 9, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 3}",0,2,3,0,1,9,4,2,1,1,0,0,0,0,0,0,0,4,0,0,6,0,3,0.5739308923407677,0.3369568783208262,0.2871641794601743
ICLR2018-BkVf1AeAZ-R2,Reject,"This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks. Pros: Good empirical results. Cons:  There is not much technical contribution. The proposed approach is neither well motivated, nor well presented/justified.  The presentation of the paper needs to be improved. 1. Part of the motivation on page 1 does not make sense. In particular, for paragraph 3, if the classification task is just to separate A from B, then (1,0) separation should be better than (0.8, 0.2).  2. Label embedding learning has been investigated in many previous works. The authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in Section 2.1 without clear motivation and justification. This assumption is not very natural u2014 though label embeddings can capture semantic information and label correlations, it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of labels. The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works. 3. The proposed model is not well explained. (1) By using the objective in eq.(14), how to learn the embeddings E? (2) The authors state ""In back propagation, the gradient from z2 is kept from propagating to h"".  This makes the learning process quite arbitrary under the objective in eq.(14).  (3) The label embeddings are not directly used for the classification (H(y, z'_1)), but rather as auxiliary part of the objective.  How to decide the test labels?",13,264,13.894736842105264,5.419087136929461,140,0,264,0.0,0.0148148148148148,-0.6193,75,23,49,16,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,3,3,0,8,0,1,0,0,0,0,0,0,0,0,0,2,1,0,9,0,0,0.3591737774678293,0.3384261553692671,0.18262185917864737
ICLR2018-BkVf1AeAZ-R3,Reject,"The paper proposes a method which jointly learns the label embedding (in the form of class similarity) and a classification model. While the motivation of the paper makes sense, the model is not properly justified, and I learned very little after reading the paper. There are 5 terms in the proposed objective function. There are also several other parameters associated with them: for example, the label temperature of z_2'' and and parameter alpha in the second last term etc. For all the experiments, the same set of parameters are used, and it is claimed that ""the method is robust in our experiment and simply works without fine tuning"". While I agree that a robust and fine-tuning-free model is ideal 1) this has to be justified by experiment. 2) showing the experiment with different parameters will help us understand the role each component plays. This is perhaps more important than improving the baseline method by a few point, especially given that the goal of this work is not to beat the state-of-the-art.",6,171,21.375,5.024844720496894,100,1,170,0.0058823529411764,0.0116959064327485,0.9628,43,17,31,10,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,1,1,1,0,3,3,0,0,0,0,0,0,0,0,0,0,1,0,0,3,1,1,0.3578156329538438,0.4456883593141303,0.2031633458586617
ICLR2018-BkVsWbbAW-R1,Reject,"This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting. They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory. The main novelty of this work are 1-balancing mechanism for the replay memory. 2-Using multiple models for short and long term memory.  The most interesting aspect of the paper is using a generate model as replay buffer which has been introduced before. As explained in more detail below, it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forgetting. The paper claims using the task ID (either from Oracle or from a HMM) is an advantage of the model. It is not clear to me as why is the case, if anything it should be the opposite. Humans and animal are not given task ID and it's always clear distinction between task in real world. Deep Generative Replay section and description of DGDMN are written poorly and is very incomprehensible. It would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechanism. Using the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting. Baselines: why is Shin et al. (2017) not included as one of the baselines? As it is the closet method to this paper it is essential to be compared against. I disagree with the argument in section 4.2.  A good robust model against catastrophic forgetting would be a model that still can achieve close to SOTA. Overfitting to the latest task is the central problem in catastrophic forgetting which this paper avoids it by limiting the model capacity. 12 pages is very long, 8 pages was the suggested page limit. It's understandable if the page limit is extend by one page, but 4 pages is over stretching.",17,348,18.31578947368421,4.844311377245509,159,0,348,0.0,0.0369318181818181,-0.759,92,40,64,19,5,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 13, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 2}",0,1,2,0,0,13,0,0,0,0,0,1,1,0,0,2,0,1,1,0,8,0,2,0.3602256504106571,0.5600355002548285,0.22359454033092302
ICLR2018-BkVsWbbAW-R2,Reject,"This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime. Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction. The method is tested on standard sequential MNIST variants as long as a class incremental variant. Superior performance to recent baselines (e.g. EWC) is reported in several cases. Interesting parallels with human cortical and hippocampal learning and memory are discussed. Unfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines. The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though. There are several example of successful multitask learning, but it does not follow that a random grouping of several tasks immediately leads to successful MTL. Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains. In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to 'negative interference' between tasks [1, 2]. I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting. The requirement of task labels also rules out important use cases such as following a non-stationary objective function, which is important in several realistic domains, including deep RL. [1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: t Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016. [2] Andrei A. Rusu, Sergio Gomez Colmenarejo, u00c7aglar Gu00fclu00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016.",13,304,15.2,5.771626297577854,187,0,304,0.0,0.006578947368421,0.6785,106,55,44,16,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,2,0,1,10,1,0,0,0,0,0,1,0,0,0,0,2,0,0,7,0,0,0.4309036454978745,0.22607112938847,0.19742853816746728
ICLR2018-BkVsWbbAW-R3,Reject,"This paper introduces a neural network architecture for continual learning. The model is inspired by current knowledge about long term memory consolidation mechanisms in humans. As a consequence, it uses: -tOne temporary memory storage (inspired by hippocampus) and a long term memory -tA notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks. Overall, although the result are not very surprising, the approach is well justified and extensively tested. It provides some insights on the challenges and benefits of replay based memory consolidation. Comments: t 1-tThe results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach. 2-tIt is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume). 3-tSo the main benefit of the approach seems to point towards the direction of what possibly happens in real brains. It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep. 4-tDue to these differences, I my view, this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting, which relates more to finding optimal plasticity rules for the network in an unsupervised setting. 5-tThe writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages.",11,345,31.363636363636363,5.474474474474475,193,2,343,0.0058309037900874,0.0231884057971014,0.9294,99,53,55,16,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,9,0,2,0,1,0,1,0,0,0,0,0,0,1,0,6,0,1,0.3592000959458439,0.3364431205075482,0.1807535169248123
ICLR2018-BkXmYfbAZ-R1,Accept,"Summary: This paper proposes a different approach to deep multi-task learning using ""soft ordering. ""  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across. Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks. The idea of ""soft ordering"" enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular. The methods reviewed prior work which the authors refer to as ""parallel order"", which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn't be the case. They authors then extend the approach to ""permuted order"" and finally present their proposed ""soft ordering"" approach. The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance. The ""soft ordering"" approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task. The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to ""parallel ordering"" and ""permuted ordering"" and show the performance gain. Positives:  - The paper is clearly written and easy to follow. - The idea is novel and impactful if its evaluated properly and consistently. - The authors did a great job summarizing prior work and motivating their approach. Negatives:  - Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation). CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection. u2028 (TODO) I would recommend that the authors test their approach on such setting. - Figure 6 is a bit confusing, the authors do not explain why the ""Permuted Order"" performs worse than ""Parallel Order"".  Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task. u2028(TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader. - Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)? How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL. u2028 (TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare. Notation and Typos: - Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Please clarify. 1- If I assume that the Y-Axis is incorrectly labeled and it is Training Error instead, then the permuted order is doing worse than the parallel order. u20282- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense. - Figure 4 is very small and not easy to read the text. Does single task mean average performance over the tasks? - In eq.(3) Choosing sigma_i for a task-specific permutation of the network is a bit confusing, since it could be thought of as a sigmoid function, I suggest using a different symbol. u2028Conclusion: I would suggest that the authors address the concerns mentioned above. Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication.",29,606,22.444444444444443,5.3478260869565215,264,1,605,0.0016528925619834,0.0258064516129032,0.9928,174,78,114,24,10,7,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 4, 'DAT': 4, 'MET': 8, 'EXP': 5, 'RES': 4, 'TNF': 6, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 3, 'REC': 1, 'EMP': 14, 'SUB': 0, 'CLA': 2}",0,1,2,4,4,8,5,4,6,0,0,1,0,1,0,1,1,2,3,1,14,0,2,0.7170320974383347,0.7861846599063872,0.5642507891014686
ICLR2018-BkXmYfbAZ-R2,Accept,"This paper proposes a new approach for multi-task learning. While previous approaches assumes the order of shared layers are the same between tasks, this paper assume the order can vary across tasks, and the (soft) order is learned during training. They show improved performance on a number of multi-task learning problems. My primary concern about this paper is the lack of interpretation on permuting the layers. For example, in standard vision systems, low level filters V1 learn edge detectors (gabor filters) and higher level filters learn angle detectors [1]. It is confusing why permuting these filters make sense. They accept different inputs (raw pixels vs edges). Moreover, if the network contains pooling layers, different locations of the pooling layer result in different shapes of the feature map, and the soft ordering strategy Eq. (7) does not work. It makes sense that the more flexible model proposed by this paper performs better than previous models. The good aspect of this paper is that it has some performance improvements. But I still wonder the effect of permuting the layers. The paper also needs more clarifications in the writing. For example, in Section 3.3, how each s_(i, j, k) is sampled from S? The parallel ordering terminology also seems to be arbitrary... [1] Lee, Honglak, Chaitanya Ekanadham, and Andrew Y. Ng. Sparse deep belief net model for visual area V2. Advances in neural information processing systems. 2008.",15,234,12.31578947368421,5.199095022624435,142,1,233,0.0042918454935622,0.0128205128205128,0.7047,84,32,38,7,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,1,1,0,0,9,1,2,0,0,0,1,1,0,0,0,0,1,0,0,7,0,2,0.5020572388029868,0.3371462130132452,0.2554533564121928
ICLR2018-BkXmYfbAZ-R3,Accept,"- The paper proposes to learn a soft ordering over a set of layers for multitask learning (MTL) i.e.   at every step of the forward propagation, each task is free to choose its unique soft (`convex')   combination of the outputs from all available layers. This idea is novel and interesting. - The learning of such soft combination is done jointly while learning the tasks and is not set   manually cf. setting permutations of a fixed number of layer per task. - The empirical evaluation is done on intuitively related, superficially unrelated, and a real world   task. The first three results are on small datasets/tasks, O(10) feature dimensions, and number of   tasks and O(1000) images; (i) distinguish two MNIST digits, (ii) 10 UCI tasks with feature sizes   4--30 and number of classes 2--10, (iii) 50 different character recognition on Omniglot dataset. The last task is real world -- 40 attribute classification on the CelebA face dataset of 200K   images. While the first three tasks are smaller proof of concept, the last task could have been   more convincing if near state-of-the-art methods were used. The authors use a Resnet-50 which is a   smaller and lesser performing model, they do mention that benefits are expected to be    complimentary to say larger model, but in general it becomes harder to improve strong models. While this does not significantly dilute the message, it would have made it much more convincing   if results were given with stronger networks. - The results are otherwise convincing and clear improvements are shown with the proposed method. - The number of layers over which soft ordering was tested was fixed however.  It would be   interesting to see what would the method learn if the number of layers was explicitly set to be   large and an identity layer was put as one of the option. In that case the soft ordering could   actually learn the optimal depth as well, repeating identity layer beyond the option number of   layers. Overall, the paper presents a novel idea, which is well motivated and clearly presented. The  empirical validation, while being limited in some aspects, is largely convincing.",15,347,20.41176470588235,5.02416918429003,178,0,347,0.0,0.0077720207253886,0.9959,95,46,63,20,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 8, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,1,1,2,3,8,3,2,0,0,0,0,0,0,0,2,0,0,1,0,9,2,0,0.5021109196142622,0.4495718880160278,0.2873269936767648
ICLR2018-Bk_fs6gA--R1,Reject,"Learning to solve combinatorial optimization problems using recurrent networks is a very interesting research topic. However, I had a very hard time understanding the paper. It certainly doesn't help that I'm not familiar with the architectures the model is based on, nor with state-of-the-art integer programming solvers. The architecture was described but not really motivated. The authors chose to study only random instances which are known to be bad representatives of real-world problmes, instead of picking a standard benchmark problem.  Furthermore, the insights on how the network is actually solving the problems and how the proposed components contribute to the solution are minimal, if any.   The experimental issues (especially regarding the baseline) raised by the anonymous comments below were rather troubling; it's a pity they were left unanswered.   Hopefully other expert reviewers will be able to provide constructive feedback.",8,139,17.375,5.791044776119403,106,0,139,0.0,0.0277777777777777,-0.9228,31,18,35,15,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 4}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 0}",0,1,0,2,1,1,1,0,0,0,0,1,0,4,0,1,0,0,0,0,1,2,0,0.500087284066327,0.3334400098416548,0.24974612037737381
ICLR2018-Bk_fs6gA--R2,Reject,"# Summary This paper proposes a neural network framework for solving binary linear programs (Binary LP). The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP). In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations. This network is trained through supervised learning for the output and reinforcement learning for discrete operations. The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline. [Pros] - The idea of approximating a binary linear program solver using neural network is new. [Cons] - The paper is not clearly written (e.g., problem statement, notations, architecture description). So, it is hard to understand the core idea of this paper. - The proposed method and problem setting are not well-justified. - The results are not very convincing. # Novelty and Significance - The problem considered in this paper is new, but it is unclear why the problem should be formulated in such a way.. To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem. I do not see why this should be formulated as a sequential decision problem. Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks. This does not require any memory because all necessary information is available to the network. - The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications. # Quality - The proposed reward function for training the memory controller sounds a bit arbitrary. The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network. In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective. It would be good to justify (empirically) the proposed reward function. - The results are not fully-convincing. If my understanding is correct, the LTMN is trained to predict the baseline solver's output. But, the LTMN significantly outperforms the baseline solver even in the training set. Can you explain why this is possible? # Clarity - The problem statement and model description are not described well. 1) Is the network given a sequence of program/solution input? If yes, is it given during evaluation as well? 2) Many notations are not formally defined. What is the output (o_t) of the network? Is it the optimal solution (x_t)? 3) There is no mathematical definition of memory addressing mechanism used in this paper. - The overall objective function is missing. [Reference] - Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",34,485,17.321428571428573,5.376873661670236,197,2,483,0.0041407867494824,0.022,0.9076,161,51,92,22,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 12, 'DAT': 0, 'MET': 11, 'EXP': 5, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 18, 'SUB': 4, 'CLA': 2}",0,1,4,12,0,11,5,6,0,0,0,2,1,0,0,2,0,4,2,0,18,4,2,0.5754929661924157,0.6780992718347071,0.4080241149148636
ICLR2018-Bk_fs6gA--R3,Reject,"This paper proposes using long term memory to solve combinatorial optimization problems with binary variables. The authors do not exhibit much knowledge of combinatorial optimization literature (as has been pointed out by other readers) and ignore a lot of previous work by the combinatorial optimization community. In particular, evaluating on random instances is not a good measure of performance,  as has already been pointed out. The other issue is with the baseline solver, which also seems to be broken since their solution quality seems extremely poor. In light of these issues, I recommend reject.",5,94,18.8,5.461538461538462,68,2,92,0.0217391304347826,0.0421052631578947,-0.5266,27,13,20,7,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,1,1,0,2,0,1,0,0,0,1,0,0,0,0,0,1,0,1,2,1,0,0.4288216206852132,0.4450664018792874,0.23604020922406047
ICLR2018-BkabRiQpb-R1,Accept,"This paper proposes a novel adaptive learning mechanism to improve results in ergodic cooperation games. The algorithm, tagged 'Consequentialist Conditional Cooperation', uses outcome-based accumulative rewards of different strategies established during prior training. Its core benefit is its adaptiveness towards diverse opposing player strategies (e.g. selfish, prosocial, CCC) while maintaining maximum reward. While the contribution is explored in all its technical complexity, fundamentally this algorithm exploits policies for selfish and prosocial strategies to determine expected rewards in a training phase. During operation it then switches its strategy depending on a dynamically-calculated threshold reward value (considering variation in agent-specific policies, initial game states and stochasticity of rewards) relative to the total reward of the played game instance. The work is contrasted to tit-for-tat approaches that require complete observability and operate based on expected future rewards. In addition to the observability, approximate Markov TFT (amTFT) methods are more processing-intense, since they fall back on a game's Q-function, as opposed to learned policies, making CCC a lightweight alternative. Comments:  The findings suggest the effectiveness of that approach.  In all experiments CCC-based agents fare better than agents operating based on a specific strategy. While performing worse than the amTFT approach and only working well for larger number of iterations, the outcome-based evaluation shows benefits. Specifically in the PPD game, the use of CCC produces interesting results; when paired with cooperate agents in the PPD game, CCC-based players produce higher overall reward than pairing cooperative players (see Figure 2, (d) & (e)). This should be explained. To improve the understanding of the CCC-based operation, it would further be worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance. However, when paired with non-cooperative players in the risky PPD game, CCC players lead to an improvement of pay-offs by around 50 percent (see Figure 2, (e)), compared to payoff received between non-cooperative players (-28.4 vs. -18, relative to -5 for defection). This leads to the question: How much CCC perform compared to random policy selection? Given its reduction of processing-intensive and need for larger number of iterations, how much worse is the random choice (no processing, independent of iterations)? This is would be worthwhile to appreciate the benefit of the proposed approach. Another point relates to the fishing game. The game is parameterized with the rewards of +1 and +3. What is the bases for these parameter choices? What would happen if the higher reward was +2, or more interestingly, if the game was extended to allow agents to fish medium-sized fish (+2), in addition to small and large fish. Here it would be interesting to see how CCC fares (in all combinations with cooperators and defectors). Overall, the paper is well-written and explores the technical details of the presented approach. The authors position the approach well within contemporary literature, both conceptually and using experimental evaluation, and are explicit about its strengths and limitations. Presentation aspects: - Minor typo: Page 2, last paragraph of Introduction: `... will act act identically. ' - Figure 2 should be shifted to the next page, since it is not self-explanatory and requires more context. ",26,521,19.296296296296298,5.786,265,1,520,0.0019230769230769,0.0113421550094517,0.9962,163,77,82,15,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 23, 'EXP': 2, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 19, 'SUB': 0, 'CLA': 2}",0,2,0,0,0,23,2,3,2,0,0,1,0,1,0,1,0,0,1,0,19,0,2,0.5057506429704123,0.455720813342472,0.2901570725110409
ICLR2018-BkabRiQpb-R2,Accept,"This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information. The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. From a game-theoretic point of view, the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect information. It is basically a straightforward generalization of the idea of punishing, which is common in folk theorems from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be the natural solution but in general it is far from clear why all players would want to maximize the total payoff. The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques. In contrast, the paper Coco-Q: Learning in Stochastic Games with Side Payments by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting. It should also be noted that I was asked to review another ICLR submission entitled MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING which amazingly introduced the same Pong Player's Dilemma game as in this paper. Notice the following suspiciously similar paragraphs from the two papers:  From MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING: We also look at an environment where strategies must be learned from raw pixels.  We use the method of Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a point they receive a reward of 1 and the other player receives u22122. We refer to this game as the Pong Player's Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully cooperative agent can be exploited by a defector. From CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION: To demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong  which makes the game into a social dilemma. In what we call the Pong Player's Dilemma (PPD) when an agent  scores they gain a reward of 1 but the partner receives a reward of u22122. Thus, in the PPD the only (jointly) winning move is not to play, but selfish agents are again tempted to defect and try to score points even though this decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this game.",24,585,20.17241379310345,5.189530685920578,238,5,580,0.0086206896551724,0.0203735144312393,0.9909,156,79,96,32,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 6, 'DAT': 0, 'MET': 14, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,1,7,6,0,14,1,0,0,1,0,1,0,0,0,0,0,3,0,0,10,0,0,0.5040319242027833,0.2280541642501889,0.22822232364516437
ICLR2018-BkabRiQpb-R3,Accept,"The main result specifies a (trigger) strategy (CCC) and corresponding algorithm that leads to an efficient outcome in social dilemmas, the theoretical basis of which is provided by theorem 1. This underscores an algorithm that uses a prosocial adjustment of the agents rewards to encourage efficient behaviour. The paper makes a useful contribution in demonstrating that convergence to efficient outcomes in social dilemmas without the need for agents to observe each other's actions. The paper is also clearly written and the theoretical result is accompanied by some supporting experiments. The numerical experiments show that using CCC strategy leads to an increase in the proportion of efficient equilibrium outcomes. However, in order to solidify the experimental validation, the authors could consider a broader range of experimental evaluations.  There are also a number of items that could be added that I believe would strengthen the contribution and novelty, in particular:  Some highly relevant references on (prosocial) reward shaping in social dilemmas are missing, such as Babes, Munoz de cote and Littman, 2008 and for the (iterated) prisoner's dilemma; Vassiliades and Christodoulou, 2010 which all provide important background material on the subject. In addition, it would be useful to see how the method put forward in the paper compares with other (reward-shaping) techniques within MARL (especially in the perfect information case in the pong players' dilemma (PPD) experiment) such as those already mentioned. The authors could, therefore, provide more detail in relating the contribution to these papers and other relevant past work and existing algorithms. The paper also omits any formal discussion on the equilibrium concepts being used in the Markov game setting (e.g. Markov Perfect Equilibrium or Markov-Nash equilibrium) which leaves a notable gap in the theoretical analysis. There are also some questions that to me, remain unaddressed namely:  i.] the model of the experiments, particularly a description of the structure of the pong players' dilemma in terms of the elements of the partially observed Markov game described in definition 1. In particular, what are the state space and transitions? ii. the equilibrium concepts being considered i.e. does the paper consider Markov perfect equilibria. Some analysis on the conditions that under which the continuation equilibria e.g. cooperation in the social dilemma is expected to arise would also be beneficial. iii. Although the formal discussion is concerned with Markov games (i.e. repeated games with stochastic transitions with multiple states) the experiments (particularly the PPD) appear to apply to repeated games (this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used). iv. In part 1 of the proof of the main theorem, it seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability (equation at the top of the page). As this is an important component of the proof of the main result, the paper would benefit from an explanation of this step? ",17,491,21.347826086956523,5.426470588235294,232,1,490,0.0020408163265306,0.0181818181818181,0.994,147,56,72,19,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 4, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 4, 'CLA': 2}",0,0,4,2,0,6,4,4,0,1,0,1,1,0,0,0,0,2,0,0,8,4,2,0.5734059886197569,0.4493164736413537,0.32464382325239466
ICLR2018-BkbOsNeSM-R1,,"This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks. The authors claimed to obtain efficiency improvement and better numerical stability. This is a short paper that contains five pages. The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices. In terms of the overall time complexity, the improvement seems quite limited considering that the normalization is not the bottleneck operations in the training. In addition, it is not very clear how the proposed approach benefits the mini-batch training of the network. In terms of numerical stability, though experimental results were reported, there is no theoretical analysis. The experiments are quite limited. ",8,128,14.22222222222222,5.543307086614173,72,1,127,0.0078740157480314,0.0465116279069767,0.7196,41,13,21,6,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 0}",0,1,0,2,0,3,2,3,0,1,0,1,0,0,0,0,2,0,0,0,1,2,0,0.5007572836665527,0.3334880464901026,0.24545114822614086
ICLR2018-BkbOsNeSM-R2,,"This paper proposes a computationally fast method to train neural networks with normalized weights. Experiments demonstrate that their method is promising compared to the competitor ""NormProp"" which explicitly normalizes the weights of neural networks. Pros:  (1) The paper is easy to follow. (2) Authors use figures that are easy to understand to explain their core idea, i.e., maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weights. Cons: (1) If we count the matrix multiplication operation in fc layer along with normalization (in common cases normalization should follow a weighted layer), the whole computation complexity becomes O(mn) rather than O(n+m), so I doubt how fast it could be in the common case. (2) Authors did a MNIST experiment with a 2-fc layer neural network for comparing their FastNorm to NormProp.  It is a bit strange that they do not show the difference of speed, but show that FastNorm can outperform NormProp in terms of classification accuracy with a higher learning rate. Since the efficiency is one of the main contributions, I suggest authors add this comparison.   (3) The proposed FastNorm improves the stability by observing the standard deviation of validation accuracies in training phase. The authors attribute this to the reduction of accumulated rounding error in training process, which is somewhat against the community's consensus, i.e., float precision is not that important so we can use float32 or even float16 to train/do inference for neural networks. I'm curious if this phenomenon still holds if authors use float64 in the experiments. Some typos: First line in page 3: ""brining"" should be ""bringing"" n Overall, I think the current version of the paper is not ready for ICLR conference.  Authors need more experiments to show their approach's effectiveness. For example, batching and convolution as mentioned by authors would be more significant.  ",15,304,20.266666666666666,5.397212543554007,169,1,303,0.0033003300330033,0.0257234726688102,0.7844,93,34,54,13,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 7, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 3, 'SUB': 5, 'CLA': 1}",0,1,0,1,1,6,7,1,1,1,0,2,0,0,1,0,0,0,2,1,3,5,1,0.6446706969843687,0.6683988564934418,0.45097640584547755
ICLR2018-BkbOsNeSM-R3,,"The paper consider a method for weight normalization of layers of a neural network.  The weight matrix is maintained normalized, which helps accuracy. However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root). The paper proposes FastNorm, which is a way to implicitly maintain the normalized weight matrix using much less computation. Essentially, a normalization vector is maintained an updated separately. Pros:   Natural method to do weight normalization efficeintly   Cons:   A very natural and simple solution that is fairly obvious. Limited experiments   ",8,93,13.285714285714286,5.873563218390805,61,1,92,0.0108695652173913,0.0097087378640776,0.807,28,14,18,8,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,0,0,0,6,1,2,0,0,0,0,0,0,0,0,0,0,0,0,2,1,0,0.2870209481759184,0.2228441796570651,0.1306720363834061
ICLR2018-BkeC_J-R--R1,Reject,"This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy. It provides two main contributions: pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions.  The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning. This paper is easy to understand but has a couple shortcomings and some fatal (but reparable) flaws:. 1) When using RL please try to standardize your notation to that used by the community, it makes things much easier to read. I would strongly suggest avoiding your notation a(x|Theta) and using pi(x) (subscripting theta or making conditional is somewhat less important).  Your a(.) function seems to be the policy here, which is invariable denoted pi in the RL literature. There has been recent effort to clean up RL notation which is presented here: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf. You have no obligation to use this notation but it does make reading of your paper much easier on others in the community. This is more of a shortcoming than a fundamental issue. 2) More fatally, you have failed to compare your algorithm's performance against benchline implementations of similar algorithms. It is almost trivial to run DDPG on Torcs using the openAI baselines package [https://github.com/openai/baselines]. I would have loved, for example, to see the effects of simply pre-training the DDPG actor on supervised data, vs. adding your mixture loss on the critic. Using the baselines would have (maybe) made a very compelling graph showing DDPG, DDPG + actor pre-training, and then your complete method.   3) And finally, perhaps complementary to point 2), you really need to provide examples on more than one environment. Each of these simulated environments has its own pathologies linked to determenism, reward structure, and other environment particularities. Almost every algorithm I've seen published will often beat baselines on one environment and then fail to improve or even be wors on others, so it is important to at least run on a series of these. Mujoco + AI Gym should make this really easy to do (for reference, I have no relatinship with OpenAI). Running at least cartpole (which is a very well understood control task), and then perhaps reacher, swimmer, half-cheetah etc. using a known contoller as your behavior policy (behavior policy is a good term for your data-generating policy.)   4) In terms of state of the art you are very close to Todd Hester et. al's paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper.  Please also have a look at some more recent work my Matej Vecerik, Todd Hester & Jon Scholz: 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards' for an approach that is pretty similar to yours.   Overall I think your intuitions and ideas are good, but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods. The idea of pre-training the policy net has been tried before (although I can't find a published reference) and in my experience will help on certain problems, and hinder on others, primarily because the policy network is already 'overfit' somewhat to the expert, and may have a hard time moving to a more optimal space.  Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment.",26,596,22.923076923076923,5.241258741258742,298,7,589,0.0118845500848896,0.0213464696223316,0.9911,168,61,110,43,10,5,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 2, 'DAT': 2, 'MET': 16, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 3, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 5, 'PNF': 8, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 0}",0,2,5,2,2,16,5,2,0,0,0,3,3,3,0,0,3,5,8,0,4,3,0,0.7190162219547188,0.5586304868690064,0.45267206116876596
ICLR2018-BkeC_J-R--R2,Reject,"This paper proposes to combine reinforcement learning with supervised learning to speed up learning. Unlike their claim in the paper, the idea of combining supervised and RL is not new. A good example of this is a supervised actor-critic by Barto (2004). I think even alphaGo uses some form of supervision. However, if I understand correctly, it seems that combining supervision of RL at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel. Having the luxury of some supervised episodes is of course useful. The first step of building a supervised initial model looks straight forward. The next step of the algorithm is less easy to follow, and presentation of the ideas could be much better. This part of the paper leaves me already with many questions such as why is it essential to consider only a deterministic case and also to consider greedy optimization? Doesn't this prevent exploration? What are the network parameters (e.g. size of layers) etc. I am not sure I could redo the work from the provided information.   Overall, it is unclear to me what the advantage of the algorithm is over pure supervised learning, and I don't think a compelling case has been made. Since the influence of the supervision is increased by increasing alpha, it can be expected that results should be better for increasing alpha. The results seem to indicate that an intermediate level of alpha is best, though I would even question the statistical significance by looking at the curves in Figure 3. Also, what is the epoch number, and why is this 1 for alpha 0? If the combination of supervised learning with RL is better, than this should be clearly stated. Some argument is made that pure supervision is overfitting, but would one then not simply add some other regularizer? The presentation could also be improved with some language edits. Several articles are wrongly placed and even some meaning is unclear. For example, the phrase ""continuous input sequence"" does not make sense; maybe you mean ""input sequence of real valued quantities"".   In summary, while the paper contains some good ideas, I certainly think it needs more work to make a clear case for this method.  ",20,374,18.7,5.0084507042253525,189,6,368,0.0163043478260869,0.0605263157894736,0.9798,86,41,82,29,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 5, 'DAT': 0, 'MET': 12, 'EXP': 1, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 3, 'PNF': 7, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,1,1,5,0,12,1,3,1,0,0,3,0,2,0,2,0,3,7,0,9,1,1,0.6460580250258744,0.6722911723871285,0.4381906888776627
ICLR2018-BkeC_J-R--R3,Reject,"The paper was fairly easy to follow, but I would not say it was well written.  These are minor annoyances; there were some typos and a strange citation format. There is nothing wrong with the fundamental idea itself, but given the experimental results it just is not clear that it is working. n The bot performance significantly better than the fully trained agent. This leads to a few questions:  1. What was the performance of the regression policy, that was learned during the supervised pretraining phase? n2. Given enough time would the basic RL agent reach similar performance? (Guessing no...) Why not? 3. Considering the results of Figure 3 (right) shouldn't the conclusion be that the RL portion is essentially contributing nothing? Pros: The regularization of the Q-values w.r.t. the policy of another agent is interesting   Cons: Not very well setup experiments Performance is lower than you would expect just using supervised training Not clear what parts are working and what parts are not   ",11,164,18.22222222222222,5.089743589743589,95,1,163,0.0061349693251533,0.0701754385964912,-0.3132,39,18,32,18,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 2}",0,0,1,1,0,3,2,3,1,1,0,3,0,0,0,1,0,1,2,0,6,0,2,0.5722173928493695,0.5588080467244279,0.3431444806065305
ICLR2018-BkfEzz-0--R1,Reject,"This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning). The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem. This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology. This work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning. Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks, there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims. These issues may need a significant amount of effort to fix as I will elaborate more below. 1. Introduction There are several important concepts, such as reward distribution, credit assignment, which are used (from the very beginning of the paper) without explanation until the final part of the paper. The motivation of the work is not very clear. There seems to be a gap between the first paragraph and the second paragraph. The authors mentioned that ""From a micro perspective, the abstraction capability of each unit contribute to the return of the entire system. Therefore, we address the following questions. Will reinforcement learning work even if we consider each unit as an autonomous agent "" Is there any citation for the claim ""From a micro perspective, the abstraction capability of each unit contribute to the return of the entire system"" ?  It seems to me this is a very general claim. Even RL methods with linear function approximations use abstractions. Also, it is unclear to me why this is an interest question. Does it have anything to do with existing issues in DRL? Moreover, The definition of autonomous agent is not clear, do you mean learning agent or policy execution agent? ""it uses epsilon-greedy as a policy, ..."" Do you mean exploration policy? I also have some concerns regarding the claim that ""We confirm that optimization with the framework of NaaA leads to better performance of RL"". Since there are only two baselines are compared to the proposed method, this claim seems too general to be true. It is not clear to why the authors mention that ""negative result that the return decreases if we naively consider units as agents"". What is the big picture behind this claim? ""The counterfactual return is that by which we extend reward ..."" need to be rewritten. The last paragraph of introduction discussed the possible applications of the proposed methods without any substantiation, especially neither citations nor any related experiments of the authors are provided. 2 Related Work   ""POSG, a class of reinforcement learning with multiple .."" -> reinforcement learning framework ""Another one is credit assignment. Instead of reward.. "" Two sentences are disconnected and need to be rewritten. ""This paper unifies both issues"" sounds very weird. Do you mean ""solves/considers both issues in a principled way""? The introduction of GAN is very abrupt. Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs. "",which we propose in a later part of this paper"" -> which we propose in this paper 3. Background   ""a function from the state and the action of an agent to the real value"" -> a reward function Should provide a citation for DRQN There is a big gap between the last two paragraphs of section 3. 4. Neuro as an agent ""We add the following assumption for characteristics of the v_i"" -> assumptions for characterizing v_i ""to maximize toward maximizing its own return"" -> to maximize its own return We construct the framework of NaaA from the assumptions -> from these assumptions   ""indicates that the unit gives additional value to the obtained data. ..."" I am not sure what this sentence means, given that rho_ijt is not clearly defined. 5. Optimization   ""NaaA assumes that all agents are not cooperative but selfish"" Why? Is there any justification for such a claim? What is the relation between rho_jit and q_it ? ""A buyer which cannot receive the activation approximates x_i with ..."" It is unclear why a buyer need to do so given that it cannot receive the activation anyway. ""Q_it maximizing the equation is designated as the optimal price."" Which equation? e_j and 0 are not defined in equation 8 6 Experiment setare -> set are what is the std for CartPole in table 1 It is hard to judge the significance of the results on the left side of figure 2. It might be better to add errorbars to those curves More description should be provided to explain the reward visualization on the right side of figure 2. What reward? External/internal?   ""Specifically, it is applicable to various methods as described below ..."" Related papers should be cited.",53,832,23.11111111111111,5.208020050125313,331,6,826,0.0072639225181598,0.0290697674418604,0.9916,239,90,147,36,10,7,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 5, 'DAT': 0, 'MET': 25, 'EXP': 1, 'RES': 3, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 13, 'SUB': 5, 'CLA': 9}",0,2,5,5,0,25,1,3,4,1,0,4,1,0,0,1,1,1,3,0,13,5,9,0.7211925490945111,0.7864401914436185,0.5658203268144287
ICLR2018-BkfEzz-0--R2,Reject,"In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward. The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks. For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline. The idea proposed in the paper is quite interesting, but the presentation is severely lacking. In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works. For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). Many equations were unclear to me for similar reasons to the point I decided to only skim those parts. Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later. Like, what does it mean when we say, ""environment that the multi-agent system itself touches""? Overall, I think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.",12,244,24.4,5.185344827586207,152,1,243,0.0041152263374485,0.0163934426229508,0.7308,62,30,44,14,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,1,3,0,8,0,0,0,1,0,1,0,0,0,1,0,1,2,0,7,0,0,0.3590128016630612,0.4482377579773056,0.2034056706312326
ICLR2018-BkfEzz-0--R3,Reject,"The authors consider a Neural Network where the neurons are treated as rational agents. In this model, the neurons must pay to observe the activation of neurons upstream. Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task). While this is an interesting idea on its surface, the paper suffers from many problems in clarity, motivation, and technical presentation. It would require very major editing to be fit for publication. The major problem with this paper is its clarity. See detailed comments below for problems just in the introduction.  More generally, the paper is riddled with non sequiturs. The related work section mentions Generative Adversarial Nets. As far as I can tell, this paper has nothing to do with GANs. The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells. There is also a general lack of attention to detail. For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task. This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}). It is never explained how this reward is allocated even in the authors' own experiments. The authors state that all units playing NOOP is an equilibrium. While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs sigma_{jit}, and the network topology. None of this is discussed. The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary. This is pervasive throughout the paper, and is detrimental to the reader's understanding. While this might be lost because of the clarity problems described above, the model itself is also never really motivated. Why is this an interesting problem? There are many ways to create rational incentives for neurons in a neural net. Why is paying to observe activations the one chosen here? The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction. Is there an economic motivation? Is it just a different way to train a NN? Detailed Comments: ""In the of NaaA""  > remove ""of""? ""passing its activation to the unit as cost""  > Unclear. What does this mean? ""performance decreases if we naively consider units as agents""  > Performance on what? "".. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium""  > Perhaps, this should be rewritten as ""Agents maximize their counterfactual return in equilibrium. ""Subsequently, we present that learning counterfactual return leads the model to learning optimal topology""  > Do you mean u2028""maximizing"" instead of learning. Optimal with respect to what task? ""pure-randomly""  > ""randomly""  ""with adaptive algorithm""  > ""with an adaptive algorithm"" ""the connection""  > ""connections"" ""In game theory, the outcome maximizing overall reward is named Pareto optimality. ""  > This is simply incorrect. ",38,499,16.633333333333333,5.319755600814664,248,3,496,0.0060483870967741,0.0095785440613026,0.9922,138,55,95,31,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 5, 'DAT': 0, 'MET': 16, 'EXP': 4, 'RES': 4, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 15, 'SUB': 3, 'CLA': 6}",0,1,1,5,0,16,4,4,0,4,0,4,0,0,0,0,0,0,2,0,15,3,6,0.5760547602020194,0.4538324458269626,0.3304504182909515
ICLR2018-Bki1Ct1AW-R1,Reject,"This study proposes the use of non-negative matrix factorization accounting for baseline by subtracting the pre-stimulus baseline from each trial and subsequently decompose the data using a 3-way factorization thereby identifying spatial and temporal modules as well as their signed activation. The method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data. Pros: The paper is well written, the analysis interesting and the application of the Tucker2 framework sound. Removing baseline is a reasonable step and the paper includes analysis of several spike-train datasets. The analysis of the approaches in terms of their ability to decode is also sound and interesting. Cons: I find the novelty of the paper limited:  The authors extend the work by (Onken et al. 2016) to subtract baseline (a rather marginal innovation) of this approach.  To use a semi-NMF type of update rule (as proposed by Ding et al .2010) and apply the approach to new spike-train datasets evaluating performance by their decoding ability (decoding also considered in Onken et al. 2016).   Multiplicative update-rules are known to suffer from slow-convergence and I would suspect this also to be an issue for the semi-NMF update rules. It would therefore be relevant and quite easy to consider other approaches such as active set or column wise updating also denoted HALS which admit negative values in the optimization, see also the review by N. Giles https://arxiv.org/abs/1401.5226 as well as for instance: Nielsen, Su00f8ren Fu00f8ns Vind, and Morten Mu00f8rup. Non-negative tensor factorization with missing data for the modeling of gene expressions in the human brain. Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on. IEEE, 2014. It would improve the paper to also discuss that the non-negativity constrained Tucker2 model may be subject to local minima solutions and have issues of non-uniqueness (i.e. rotational ambiguity). At least local minima issues could be assessed using multiple random initializations. The results are in general only marginally improved by the baseline corrected non-negativity constrained approach. For comparison the existing methods ICA, Tucker2 should also be evaluated for the baseline corrected data, to see if it is the constrained representation or the preprocessing influencing the performance. Finally, how performance is influenced by dimensionality P and L should also be clarified.   It seems that it would be naturally to model the baseline by including mean values in the model rather than treating the baseline as a preprocessing step. This would bridge the entire framework as one model and make it potentially possible to avoid structure well represented by the Tucker2 representation to be removed by the preprocessing.     Minor:  The approach corresponds to a Tucker2 decomposition with non-negativity constrained factor matrices and unconstrained core - please clarify this as you also compare to Tucker2 in the paper with orthogonal factor matrices.  Ding et al. in their semi-NMF work provide elaborate derivation with convergence guarantees. In the present paper these details are omitted and it is unclear how the update rules are derived from the KKT conditions and the Lagrange multiplier and how they differ from standard semi-NMF, this should be better clarified.   ",20,523,18.678571428571427,5.4547244094488185,254,2,521,0.0038387715930902,0.0222634508348794,0.9876,159,66,89,26,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 1, 'DAT': 4, 'MET': 14, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 2, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 8, 'PNF': 3, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,1,10,1,4,14,1,3,0,4,0,2,3,0,0,2,0,8,3,0,8,1,1,0.7186266418510441,0.672008752043022,0.5062786704783561
ICLR2018-Bki1Ct1AW-R2,Reject,"In this paper, the authors present an adaptation of space-by-time non-negative matrix factorization (SbT-NMF) that can rigorously account for the pre-stimulus baseline activity.  The authors go on to compare their baseline-corrected (BC) method with several established methods for dimensionality reduction of spike train data.   Overall, the results are a bit mixed. The BC method often performs similarly to or is outperformed by non-BC SbT-NMF. The authors provide a possible mechanism to explain these results, by analyzing classification performance as a function of baseline firing rate. The authors posit that their method can be useful when sensory responses are on the order of magnitude of baseline activity; however, this doesn't fully address why non-BC SbT-NMF can strongly outperform the BC method in certain tasks (e.g. the step of light, Fig. 3b). Finally, while this method introduces a principled way to remove mean baseline activity from the sensory-driven response, this may also discount the effect that baseline firing rate and fast temporal fluctuations can have on the response (Destexhe et al., Nature Reviews Neuroscience 4, 2003; Gutnisky DA et al., Cerebral Cortex 27, 2017).",7,182,20.22222222222222,5.526011560693641,114,2,180,0.0111111111111111,0.0162162162162162,0.2263,60,24,25,9,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,3,1,1,5,0,2,1,1,0,0,0,0,0,1,0,3,0,0,3,0,0,0.5726460175811875,0.3348115733173992,0.28970498513602205
ICLR2018-Bki1Ct1AW-R3,Reject,"In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train. Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition. The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients. An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques).    The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2. The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability.  A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization.",7,158,22.571428571428573,6.013333333333334,96,1,157,0.0063694267515923,0.0245398773006134,0.9062,50,22,21,4,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,3,1,1,5,2,2,1,0,0,0,0,0,0,0,0,2,0,0,3,1,0,0.5727322133728963,0.3346944107602093,0.28731316827853287
ICLR2018-Bki4EfWCb-R1,Reject,"* EDIT: Increased score from 5 to 6 to reflect improvements made in the revision. The authors break down the inference gap in VAEs (the slack in the variational lower bound) into two components: 1. the amortization gap, measuring what part of the slack is due to amortizing inference using a neural net encoder, as compared to separate optimization per example. 2. the approximation gap: the part of the slack due to using a restricted parametric form for the posterior approximation. They perform various experiments to analyze how these quantities depend on modeling decisions and data sets. Breaking down the inference gap into its components is an interesting idea and could potentially provide insights when analyzing VAE performance and for further improving VAEs. I enjoyed reading the paper, but I think its contribution is on the small side for a conference paper. It would be a good workshop paper. The main limitation of the proposed method of analysis I think is that the two parts of the inference gap are not really separable: Because the VAE encoder is trained jointly with the decoder, the different limitations of the encoder and decoder all interact. E.g. one could imagine cases where jointly training the VAE encoder and decoder finds a local optimum where inference is perfect, but which is still much worse than the optimum that could be achieved if the encoder would have been more flexible. The authors do seem to realize this and they provide experiments examining this interaction. I think these experiments should be elaborated on. For example: What happens when the decoder is trained separately using more flexible inference (e.g. Hamiltonian MC) and the encoder is trained later? What happens when the encoder is optimized separately for each data point during training as well as testing?",14,297,18.5625,5.186619718309859,152,3,294,0.010204081632653,0.0268456375838926,0.9672,80,28,60,15,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 7, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,0,0,3,1,7,4,0,0,4,0,4,0,0,1,0,0,0,0,1,6,1,0,0.4306601829189746,0.4475542316186593,0.24129685390844507
ICLR2018-Bki4EfWCb-R2,Reject,"        Update:  The new version addresses some of my concerns. I think this paper is still pretty borderline, but I increased my rating to a 6.          This article examines the two sources of loose bounds in variational autoencoders, which the authors term ""approximation error"" (slack due to using a limited variational family) and ""amortization error"" (slack due to the inference network not finding the optimal member of that family). The existence of amortization error is often ignored in the literature, but (as the authors point out) it is not negligible. It has been pointed out before in various ways, however: * Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class). * The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. That this approach works well implies that amortization error cannot be ignored. * The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network. * The very recent paper by Krishnan et al. (posted to arXiv days before the ICLR deadline, although a workshop version was presented at the NIPS AABI workshop last year; http://approximateinference.org/2016/accepted/KrishnanHoffman2016.pdf) examines amortization error as a core cause of training failures in VAEs. They also observe that the gap persists at test time, although it does not examine how it relates to approximation error. Since these earlier results existed, and approximation-amortization decomposition is fairly simple (although important!), the main contributions of this paper are the empirical studies. I will try to summarize the main novel (i.e., not present elsewhere in the literature) results of these: Section 5.1: Inference networks with FFG approximations can produce qualitatively embarrassing approximations. Section 5.2: When trained on a small dataset, training amortization error becomes negligible. I found this surprising, since it's not at all clear why dataset size should lead to ""strong inference"". It seems like a more likely explanation is that the decoder doesn't have to work as hard to memorize the training set, so it has some extra freedom to make the true posterior look more like a FFG. Also, I think it's a bit of an exaggeration to call a gap of 2.71 nats ""much tighter"" than a gap of 3.01 nats. Section 5.3: Amortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset. (This is totally consistent with Krishnan et al.'s finding that eliminating amortization error gave a bigger improvement for more complex datasets than for MNIST.) Section 5.4: Using a restricted variational family causes the decoder to learn to induce posteriors that are easier to approximate with that variational family. This idea has been around for a long time (although I'm having a hard time coming up with a reference). These results are interesting, but given the empirical nature of this paper I would have liked to see results on more interesting datasets (Celeb-A, CIFAR-10, really anything but MNIST). Also, it seems as though none of the full-dataset MNIST models have been trained to convergence, which makes it a bit difficult to interpret some results. A few more specific comments:  2.2.1: The cdot seems extraneous to me. 5.1: What dataset/model was this experiment done on? Figure 3: This can be inferred from the text (I think), but I had to remind myself that ""IW train"" and ""IW test"" refer only to the evaluation procedure, not the training procedure. It might be good to emphasize that you don't train on the IWAE bound in any experiments. Table 2: It would be good to see standard errors on these numbers; they may be quite high given that they're only evaluated on 100 examples. ""We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound. "": Why not just compare the optimal with the AIS evaluation? If you trust the AIS estimate, then the result will be the actual KL divergence between the FFG and the true posterior.",32,692,22.32258064516129,5.17351598173516,318,13,679,0.0191458026509572,0.0279329608938547,0.9842,204,82,114,40,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 2, 'DAT': 7, 'MET': 6, 'EXP': 7, 'RES': 9, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 15, 'SUB': 1, 'CLA': 0}",0,0,8,2,7,6,7,9,1,0,0,1,0,1,0,1,0,2,0,1,15,1,0,0.6459101529358984,0.5643801222005471,0.40630640751537855
ICLR2018-Bki4EfWCb-R3,Reject,"This paper studies the amortization gap in VAEs. Inference networks, in general, have two sources of approximation errors. One due to the function family of variational posterior distributions used in inference and the other due to choosing to amortize inference rather than doing per-data-point inference as in SVI. They consider learning VAEs using two different choices of inference networks with (1) fully factorized Gaussian and (2) normalizing flows. The former is the de-facto choice of variational approximation used in VAEs and the latter is capable of expressing complex multi-modal distributions. The inference gap is log p(x) - L[q], the approximation gap is log p(x) - L[q^* ] and the amortization gap is L[q^* ] - L[q]. The amortization gap is easily evaluated. To evaluate the first two, the authors use estimates (lower bounds of log p(x)) given from annealed importance sampling and the importance sampling based IWAE bound (the tighter of the two is used). There are several different observations made via experiments in this work but one of the more interesting ones is quantifying that a deep generative model, when trained with a fully factorized gaussian posterior, realizes a true posterior distribution that is (more) approximately Gaussian. While this might be (known) intuition that people rely on when learning deep generative models, it is important to be able to test it, as this paper does. The authors study several discrete questions about the aforementioned inference gaps and how they vary on MNIST and FashionMNIST. The concerns I have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators. Questions: * How did you optimize the variational parameters for q^* and the flow parameters in terms of learning rate, stopping criteria etc. * In Section 5.2, what is strong inference? This is not defined previously. * Have you evaluated on a larger dataset such as CIFAR? FashionMNIST and MNIST are similar in many ways. * Which kind of error would using a convolution architecture for the encoder decrease? Do you have insights on the role played by the architecture of the inference network and generative model? I have two specific concerns: * Did you perform any checks to verify whether the variance in the estimators use to bound log p(x) is controlled (for the specific # samples you use)? I'm concerned since the evaluation is only done on 100 points. * In Section 5.2.1, L_iw is used to characterize encoder overfitting where the argument is that L_ais is not a function of the encoder, but L_iw is, and so the difference between the two summarizes how much the inference network has overfit. How is L_iw affected by the number of samples used in the estimator? Presumably this statement needs to be made while also keeping mind the number of importance samples. For example, if I increase the number of importance samples, even if I'm overfitting in Fig 3(b), wouldn't the green line move towards the red simply because my estimator depends less on a poor q? Overall, I think this paper is interesting and presents a quantitative analysis of where the errors accrue due to learning with inference networks. The work can be made stronger by addressing some of the questions above such as what role is played by the neural architecture and whether the results hold up under evaluation on a larger dataset.",26,555,27.75,5.101123595505618,241,4,551,0.0072595281306715,0.0088183421516754,0.9821,175,66,108,21,10,2,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 6, 'MET': 16, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 2, 'CLA': 0}",0,2,1,1,6,16,4,2,1,2,0,1,0,0,0,0,0,0,0,0,13,2,0,0.7185835772697189,0.2297923879486594,0.3297240413977612
ICLR2018-BkiIkBJ0b-R1,Reject,"The paper evaluates one proposed Deep RL-based model (Mirowski et al. 2016) on its ability to generally navigate. This evaluation includes training the agent on a set of training mazes and testing it's performance on a set of held-out test mazes. Evaluation metrics include repeated latency to the goal and comparison to the shortest route. Although there are some (minor) differences between the implementation with Mirowski et al. 2016, I believe the conclusions made by the authors are mostly valid. I would firstly like to point out that measuring generalization is not standard practice in RL. Recent successes in Deep RL--including Atari and AlphaGo all train and test on exactly the same environment (except for random starts in Atari and no two games of Go being the same). Arguably, the goal of RL algorithms is to learn to exploit their environment as quickly as possible in order to attain the highest reward. However, when RL is applied to navigation problems it is tempting to evaluate the agent on unseen maps in order to assess weather the agent has learned a generic mapping & planning policy. In the case of Mirowski et al. this means that the LSTM has somehow learned to do general SLAM in a meta-learning sense. To the best of my knowledge, Mirowski et al. never made such a bold claim (despite the title of their paper). Secondly, there seems to be a big disconnect between attaining a high score in navigation tasks and perfectly solving them by doing general SLAM & optimal path planning.  Clearly if the agent receives the maximal possible reward for a well designed navigation task it must, by definition, be doing perfect SLAM & path planning. However at less than optimal performance the reward fails to quality  the agent's ability to do SLAM. The relationship between reward and ability to do general SLAM is not clear. Therefore it is my opinion that reinforcement learning approaches to SLAM lack a concrete goal in what they are trying to show. Minor points: Section 5.3 Square map: how much more reward will the agent gain by taking the optimal path? Perhaps not that much? Wrench map: the fact that the paths taken by the agent are not distributed evenly makes me suspicious. Could the authors generate many wrench maps (same topology, random size, random wall textures) to make sure there is no bias? ",20,394,18.761904761904763,4.892105263157895,209,2,392,0.0051020408163265,0.025,0.984,117,45,66,25,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,3,1,0,11,1,3,0,0,0,0,0,0,0,0,0,1,0,0,9,0,0,0.4313457292994621,0.227197881700966,0.19424258312277456
ICLR2018-BkiIkBJ0b-R2,Reject,"Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results. It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction. However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place, by 2) reproducing the results in the same way as in the original work, 3) by avoiding introducing false claims based on a misunderstanding of terminology  and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks. This paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, ""Learning to Navigate in Complex Environments"") and one of the architectures (NavA3C+D1D2L) from that paper. It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method. For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique. The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call ""static maze""), and in fixed mazes with changing goal environments (what they call ""environments with dynamic elements"" or ""random goal mazes""). This submission claims that: [a] ""[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms"", [b] ""following training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal"", [c] ""when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning"", [d] ""this state-of-the-art result is shown to be successful on only one map, which brings into question the repeatability of the results"", [e] ""Do DRL-based navigation algorithms really 'learn to navigate'? Our results answer this question negatively. "" [f] ""we are the first to evaluate any DRL-based navigation method on maps with unseen structures ""  The paper also conducts an extensive analysis of the performance of a different version of the NavA3C+D1D2L algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions). While the experimental setup does not match (Mirowski et al, 2016), thereby invalidating claim [d], the experiments are thorough and do show that that architecture does not generalize to unseen mazes. The use of attention heat maps is interesting. The main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results. Regarding the former, I could not find any claim that the methods in (Mirowski et al, 2017) learn mapping and path planning in unseen environments, that could support claim [a]. More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument. Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, "" Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation""). This widely accepted definition of navigation does not preclude being limited to known environments only. Regarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns. The experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (Mirowski et al, 2016) achieved by training on static maps such as their ""I-maze"". Moreover, claim [d] about repeatability is also invalidated by the fact thatu00a0the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes. It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable. Additionally, some statements made by the authors are demonstrably untrue. First, the authors claim that they are the first to train DRL agents in all random mazes [f], but this has been already shown in at least two publications (Mnih et al, 2016 and Jaderberg et al, 2016). Second, the title of the submission, ""Do Deep Reinforcement Learning Algorithms Really Learn to Navigate"" makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper). Why did the authors not cite and consider (Parisotto et al, 2017, ""Neural Map: Structured Memory for Deep Reinforcement Learning""), which explicitly claims that their method is ""capable of generalizing to environments that were not seen during training""? It seems that the authors need to revise both their bibliography and their logical reasoning: one cannot invalidate a broad set of algorithms for a broad goal, simply by taking a specific example and showing that it does not fit a particular interpretation of navigation *in previously unseen environments*. ",33,1054,42.16,5.354838709677419,375,7,1047,0.0066857688634192,0.0094339622641509,0.8859,304,130,183,59,9,3,"{'ABS': 0, 'INT': 2, 'RWK': 12, 'PDI': 2, 'DAT': 0, 'MET': 14, 'EXP': 4, 'RES': 5, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 7}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 7, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,2,12,2,0,14,4,5,0,1,0,0,1,7,0,1,0,7,0,0,9,0,0,0.6476171894086449,0.3390119681552169,0.3312514552810933
ICLR2018-BkiIkBJ0b-R3,Reject,"This paper proposes to re-evaluate some of the methods presented in a previous paper with a somewhat more general evaluation method. The previous paper (Mirowski et al. 2016) introduced a deep RL agent with auxiliary losses that facilitates learning in navigation environments, where the tasks were to go from a location to another in a first person viewed fixed 3d maze, with the starting and goal locations being either fixed or random. This proposed paper rejects some of the claims that were made in Mirowski et al. 2016, mainly the capacity of the deep RL agent to learn to navigate in such environments. The proposed refutation is based on the following experiments: - an agent trained on random maps does much worse on fixed random maps that an agent trained on the same maps its being evaluated on (figure 4) - when an agent is trained on fixed number of random map, its performance on random unseen maps doesn't increase with the number of training maps beyond ~100 maps. (figure 5). The authors argue that the reason for those diminishing returns is that the agent is actually learning a trivial wall following strategy that doesn't benefit from more maps. - when evaluated on hand designed small maps, the agent doesn't perform very well (figure 6). There is addition experimental data reported which I didn't find very conclusive nor relevant to the analysis, particularly the attention heat map and the effect of apples and texture. I don't think any of the experiments reported actually refute any of the original paper's claim. All of the reported results are what you would expect. It boils down to these simple commonly known facts about deep RL agents: - When evaluated outside of its training distribution, it might not generalized very well (figure 4/6) - It has a limited capacity so if the distribution of environments is too large, its performance will plateau (figure 5). By the way to me results presented in figure 5 are not enough to claim that the agent trained on random map is implementing a purely reactive wall-following strategy. In fact, an interesting experiment here would have been to do ablation studies e.g. by replacing the LSTM with a feed forward fully connected network. To me the reported performance plateau with number of map size is normal expected behavior, only symptomatic that this deep RL agent has finite capacity. I think this paper does not provide compelling pieces of evidence of unexpected pathological behavior in the previous paper, and also does not provide any insight of how to improve upon and address the obvious limitations of previous work. I therefore recommend not to accept this paper in its current form.",18,445,23.42105263157895,5.052132701421801,211,3,442,0.006787330316742,0.02,-0.8049,118,46,88,33,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 1, 'EXP': 8, 'RES': 3, 'TNF': 5, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,0,4,1,1,1,8,3,5,0,0,2,0,0,0,0,0,1,0,1,8,1,0,0.5725234201139802,0.4487981464883452,0.323387660078277
ICLR2018-BkisuzWRW-R1,Accept,"The authors propose an approach for zero-shot visual learning.  The robot learns inverse and forward models through autonomous exploration. The robot then uses the learned parametric skill functions to reach goal states (images) provided by the demonstrator. The ""zero-shot"" refers to the fact that all learning is performed before the human defines the task. The proposed method was evaluated on a mobile indoor navigation task and a knot tying task. The proposed approach is well founded and the experimental evaluations are promising.  The paper is well written and easy to follow. I was expecting the authors to mention ""goal emulation"" and ""distal teacher learning"" in their related work. These topics seem sufficiently related to the proposed approach that the authors should include them in their related work section, and explain the similarities and differences. Learning both inverse and forward models is very effective. How well does the framework scale to more complex scenarios, e.g., multiple types of manipulation together? Do you have any intuition for what kind of features or information the networks are capturing? For the mobile robot, is the robot learning some form of traversability affordances, e.g., recognizing actions for crossings, corners, and obstacles? The authors should consider a test where the robot remains stationary with a fixed goal, but obstacles are move around it to  see how it affects the selected action distributions. How much can change between the goal images and the environment before the system fails? In the videos, it seems that the people and chairs are always in the same place. I could imagine a network learning to ignore features of objects that tend to wander over time. The authors should consider exploring and discussing the effects of adding/moving/removing objects on the performance. I am very happy to see experimental evaluations on real robots, and even in two different application domains. Including videos of failure cases is also appreciated. The evaluation with the sequence of checkpoints was created by using every fifth image. How does the performance change with the number of frames between checkpoints? In the videos, it seems like the robot could get a slightly better view if it took another couple of steps. I assume this is an artifact of the way the goal recognizer is trained. For the videos, it may be useful to indicate when the goal is detected, and then let it run a couple more steps and stop for a second. It is difficult to compare the goal image and the video otherwise.   ",26,416,18.90909090909091,5.138613861386139,217,4,412,0.0097087378640776,0.0355450236966824,0.8748,121,32,84,16,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 11, 'EXP': 9, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 1, 'CLA': 1}",0,1,2,3,0,11,9,3,0,1,0,1,0,0,0,0,0,3,0,0,16,1,1,0.5749305503116019,0.454008131081469,0.3214083261410755
ICLR2018-BkisuzWRW-R2,Accept,"Summary: The authors present a paper about imitation of a task presented just during inference, where the learning is performed in a completely self-supervised manner. During training, the agent explores by itself related (but different) tasks, learning a) how actions affect the world state, b) which action to perform given the previous action and the world state, and c) when to stop performing actions. This learning is done without any supervision, with a loss that tries to predict actions which result in the state achieved through self exploration (forward consistency loss). During testing, the robot is presented with a sequence of goals in a related but different task. Experiments show that the system achieves a better performance than different subparts of the system (through an ablation study), state of the art and common open source systems. Positive aspects: The paper is well written and clear to understand. Since this is not my main area of research I cannot judge its originality in a completely fair way, but it is original AFAIK. The idea of learning the basic relations between actions and state through self exploration is definitely interesting. This line of work is specially relevant since it attacks one of the main bottlenecks in learning complex tasks, which is the amount of supervised examples. The experiments show clearly that a) the components of the proposed pipeline are important since they outperform ablated versions of it and b) the system is better than previous work in those tasks Negative aspects: My main criticism to the paper is that the task learning achieved through self exploration seems relatively shallow. From the navigation task, it seems like the system mainly learns a discover behavior that is better than random motion. It definitely does not seem able to learn higher level concepts like certain scenes being more likely to be close to each other than others (e.g. it is likely to find an oven in the same room as a kitchen sink but not in a toilet). It is not clear whether this is achievable by the current system even with more training data. Another aspect that worries me about the system is how it can be extended to higher dimensional action spaces. Extending control laws through self-exploration under random disturbances has been studied in character control (e.g. Domain of Attraction Expansion for Physics-based Character Control by Borno et al.), but the dimensionality of the problem makes this exploration very expensive (even for short time frames, and even in simulation). I wonder if the presented ideas won't suffer from the same curse of dimensionality. In terms of experiments, it is shown that the system is more effective than others but not so much *how* it achieves this efficiency. It would be good to show whether part of its efficiency comes from effective image-guided navigation: does a partial image match entail with targetted navigation (e.g. matches in the right side of the image make the robot turn right)? A couple more specific comments: - I think that dealing with multimodal distributions of actions with the forward consistency loss is effective for achieving the goal, but not necessarily good for modeling multimodality. Isn't it possible that the agent learns only one way of achieving such goal? - It is not clear how the authors achieve to avoid the problem of starting from scratch by pre-train the forward model and PSF separately by blocking gradient flow. Isn't it still challenging to update them independently, given that at the beginning both components are probably not very accurate? Conclusion: I think the paper presents an interesting idea which should be exposed to the community. The paper is easy to read and its experiments show the effectiveness of the method. The relevance of the method to achieve a deeper sense of learning and performing more complex tasks is however unclear to me.",28,640,25.6,5.123176661264181,289,7,633,0.0110584518167456,0.0327102803738317,0.9926,180,73,106,41,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 5, 'DAT': 1, 'MET': 6, 'EXP': 10, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 17, 'SUB': 1, 'CLA': 2}",0,1,2,5,1,6,10,6,0,0,0,3,0,0,0,1,0,3,0,0,17,1,2,0.5741423285523283,0.5658223346982771,0.3599587275047276
ICLR2018-BkisuzWRW-R3,Accept,"One of the main problems with imitation learning in general is the expense of expert demonstration. The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task.  The proposed method has at its core a method for learning a parametric skill function (PSF) that takes as input a description of the initial state, goal state, parameters of the skill and outputs a sequence of actions (could be of varying length) which take the agent from initial state to goal state. The skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory (using an explicitly learnt forward model) and the action mismatch loss (using a model-free action prediction module) . This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches. So first they are separately learnt and then fine-tuned together. In order to decide when to stop, an independent goal detector is trained which was found to be better than adding a 'goal-reached' action to the PSF. Experiments on two domains are presented. 1. Visual navigation where images of start and goal states are given as input. 2. Robotic knot-tying with a loose rope where visual input of the initial and final rope states are given as input. Comments:  - In the visual navigation task no numbers are presented on the comparison to slam-based techniques used as baselines although it is mentioned that it will be revisited. - In the rope knot-tying task no slam-based or other classical baselines are mentioned. - My main concern is that I am really trying to place this paper with respect to doing reinforcement learning first (either in simulation or in the real world itself, on-policy or off-policy) and then just using the learnt policy on test tasks. Or in other words why should we call this zero-shot imitation instead of simply reinforcement learnt policy being learnt and then used. The nice part of doing RL is that it provides ways of actively controlling the exploration. See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability. Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al. I am genuinely curious and would love the authors' comments on this. It should help make it clearer in the paper as well. Update:  After evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion, I am revising my rating for this paper up. This will be an interesting paper to have at the conference and will spur more ideas and follow-on work.",21,471,19.625,4.954048140043764,231,0,471,0.0,0.0104602510460251,0.9565,137,52,77,26,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,0,4,0,0,10,3,0,0,0,0,3,0,1,1,0,0,3,0,1,4,2,0,0.3598992790735896,0.5577624294827859,0.2288636451967672
ICLR2018-BkktYCkZf-R1,,"This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used. Results are reported on about 50 UCI datasets with different topologies. Pros:  1. This paper proposes a simple and intuitive approach for training neural networks. 2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings. They found it improved the accuracy. Cons:  1. This paper proposed a rather ad hoc proposal for training neural networks. This is not supported by any strong theory or conceptual idea. Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder. For now, it looks like more like a trick than anything else. 2. The analysis of all facets of the proposal is missing. There is no indications on the extra computations required for handling this modification compared to standard training. It is very probably more costful. Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required. 3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured. There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense. 4. The Experiment section is not well structured, at least for me, I cannot understand it well. In particular, the strategy for choosing the hyperparameters (e.g., alpha, alpha_mu, local learning rate, alpha_mu) need to be developed . Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures. 5. For each weight w, we add K learning rates u_w^j. It means that we multiplied by K the number of parameters in our model (K is the number of classes). That's a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair. Space was clearly not an issue with the paper, it still have available space to add further explanations The paper is proposing a trick to train neural networks that is backed by strong arguments. The assessment of the method is incomplete and not convincing. I thus recommend a clear rejection. Comments on the details of the paper: The paper does not write mathematically rigorous. For example,  - Section Method, equation (4) partial L/ partial w (1 + alpha(mu_w)^T y (partial L / partial z partial w)   partial L/ partial w + O(alpha)  satisfied if and only if partial L / partial z partial w is bounded. i.e |partial L / partial z partial w|<  B. However, the author did not suppose this condition. Since L is NON Convex, it could not be automatically considered as bounded. - Section method, paragraph under equation (2) L(z(alpha),x,y)< L(w,x,y) is NOT necessary. It should be supposed L is at least locally convex. -   Equation (5)  should be - O(alpha^2) The choosing of alpha_mu is generally large (10^4-10^5). Does it have some support? ",37,517,12.30952380952381,5.278372591006424,234,3,514,0.0058365758754863,0.0334572490706319,0.9895,172,61,97,39,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 5, 'MET': 20, 'EXP': 8, 'RES': 2, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 2, 'REC': 1, 'EMP': 16, 'SUB': 3, 'CLA': 0}",0,1,0,1,5,20,8,2,1,2,0,2,0,0,0,0,0,2,2,1,16,3,0,0.6484843666019625,0.5652770015758364,0.4060424248950645
ICLR2018-BkktYCkZf-R2,,"In this paper, the authors propose to have a different learning rate depending on the class of the examples when learning a neural network. This paper's contribution are quite moderate, as the proposed method seems to be a very natural extension but it is backed up by lots of numerical results.  However, it would have been interesting to show the evolution of the learning rates (for every class) along the epochs and to correlate this evolution with the classes ratio or their separability or to analyse more in depths the properties of the obtained networks. It is rather unclear why changing the learning rate affects the performance of the model and it is would have been interesting to discuss. Moreover, it would be interesting to show if this class-based learning rates changes the convergence of the model or if the early stopping occurs earlier etc...  ",5,145,24.166666666666668,4.922535211267606,84,1,144,0.0069444444444444,0.0472972972972973,0.8518,33,9,32,7,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,0,0,0,1,0,2,0,2,0,0,0,0,0,0,0,0,0,0,3,1,0,0.2857947069876114,0.2234661370919081,0.1253917184728319
ICLR2018-BkktYCkZf-R3,,"Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights. The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label. The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop. Therefore, the authors derived the update formulate based on the analytical continuation technique. The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method. The idea of using class label to adjust each weight's learning rate is interesting and somewhat novel, but unfortunately the efficacy is not well justified in theory. The empirical study is not always convincing, and did not compare with many state-of-the-art baselines. Overall,  the study is interesting and contains some new idea. However, the work is not mature enough for publication. Originality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight's learning rate is somewhat novel. However, the paper falls short in lacking of theoretical justification and convincing empirical results. Clarity:  The paper is clearly presented and easy to follow. However, many related works are missing in the literature, for example, Highway Networks [1],  Deeply-Supervised Nets [2] and Deep Networks with Stochastic Depth [3], etc. Significance:  The paper lacks of theoretical justification as well as the experiments are not convincing. Specifically, the paper lacks of justification on why adjusting the learning rate based on the class labels are crucial to improve training FCNs, more specifically, how does it help resolving the exploding and vanishing gradient problems? The trick to reset $mu$ after half an epoch at the end of Section 3 is too heuristic. There lacks of explanation. The experimental results are not convincing. The proposed method doesn't seem always outperforming the baselines. There is no discussion on the failure cases. In addition, the authors should compare with more baselines such as [1], [2], [3] and try with deeper networks as many networks used in the experiments are not very deep, with only 8 layers or less. Besides, the idea of using adaptive learning rates are not completely new, and somewhat closely related to second order optimization methods. It will be interesting to compare with some existing second-order optimization algorithms for deep learning. Last but not least, it would be more convincing to show the convergence speed of the proposed method. other question: In Eqn.4-5 , the terms $O(alpha)$ and $O(alpha^2)$ are omitted, however, since $mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $alpha_{mu}$ and use the term $O(alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction? References  [1] Srivastava, Rupesh K., Klaus Greff, and Ju00fcrgen Schmidhuber. Training very deep networks. Advances in neural information processing systems. 2015.  [2] Lee, Chen-Yu, et al. Deeply-supervised nets. Artificial Intelligence and Statistics. 2015. [3] Huang, Gao, et al. Deep networks with stochastic depth. European Conference on Computer Vision. Springer International Publishing, 2016. ",31,521,14.47222222222222,5.421471172962226,238,0,521,0.0,0.0037735849056603,0.9906,166,61,99,42,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 7, 'DAT': 1, 'MET': 11, 'EXP': 6, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 5, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 5, 'CLA': 1}",0,1,5,7,1,11,6,2,0,0,0,3,5,0,0,4,0,4,0,1,7,5,1,0.6466639052623631,0.6713119277341347,0.46375900707434814
ICLR2018-Bkl1uWb0Z-R1,Reject,"This paper adds source side dependency syntax trees to an NMT model without explicit supervision. Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results. The paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads. Please define what semantic attention is. You just introduce this concept without any explanation.  I believe you mean standard attention, if so, please explain why standard attention is semantic. Clarity. What is shared attention exactly? Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more. Also the example in Figure 3 is not very clear and did not help me in understanding this concept. Results. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure. Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms. The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations). It looks as if the improvement comes from adding additional capacity to the model. Equation 3: please define H.",14,213,13.3125,5.435643564356436,126,1,212,0.0047169811320754,0.0560747663551401,0.9877,61,23,40,15,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,3,0,0,10,2,2,1,0,0,0,0,0,0,0,0,2,0,0,7,0,0,0.4311260310846766,0.22607112938847,0.18799151444899376
ICLR2018-Bkl1uWb0Z-R2,Reject,"This paper describes a method to induce source-side dependency structures in service to neural machine translation. The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited). This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word's soft head (called Shared Attention in the paper); and (3) they gate when attention is used. I feel that the first two ideas are particularly interesting.  Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU. However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared No in Table 2), there would be almost no gains at all. Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system's latent dependency decisions. This paper has some very good ideas, and asks questions that are very much worth asking. In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile. Unfortunately, this is mostly a negative result, with gains over ""flat attention"" being relatively small. I also like the ""Shared Attention"" - it makes a lot of sense to say that if the ""semantic"" attention mechanism has picked a particular word, one should also attend to that word's head; it is not something I would have thought of on my own. The paper is also marred by somewhat weak writing, with a number of disfluencies and awkward phrasings making it somewhat difficult to follow. In terms of specific criticisms:  I found the motivation section to be somewhat weak. We need a better reason than morphology to want to do source-side dependency parsing. All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems. The paper mentions ""significant improvements"" in only two places: the introduction and the conclusion. With BLEU score differences being so low, the authors should specify how statistical significance is measured; ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011). Equation (3): I couldn't find the definition for H anywhere. Sentence before Equation (5): I believe there is a typo here, ""f takes z_i"" should be ""f takes u_t"". First section of Section 3: please cite the previous work you are talking about in this sentence. My understanding was that the dependency marginals in p(z_{i,j} 1|x,phi) in Equation (11) are directly used as beta_{i,j}. If I'm correct, that's probably worth spelling out explicitly in Equation (11): beta_{i,j}   p(z_{i,j} 1|x,phi)   .... I don't don't feel like the clause between equations (17) and (18), ""when sharing attention weights from the decoder with the encoder"" is a good description of your clever ""shared attention"" idea. In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow. Section 4.4: It's very very good that you compared to ""flat attention"", but it's too bad for everyone cheering for linguistically-informed syntax that the results weren't better. Table 5: I had a hard time understanding Table 5 and the corresponding discussion. What are ""production percentages""? Finally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there.",30,685,25.37037037037037,5.347687400318979,318,2,683,0.0029282576866764,0.0173160173160173,0.9614,193,85,113,50,10,4,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 5, 'DAT': 0, 'MET': 15, 'EXP': 5, 'RES': 4, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 0, 'CLA': 4}",0,2,4,5,0,15,5,4,3,1,0,1,3,0,0,0,3,2,0,0,14,0,4,0.7188758567386409,0.452986532164051,0.4088544385202108
ICLR2018-Bkl1uWb0Z-R3,Reject,"This paper induces latent dependency syntax in the source side for NMT. Experiments are made in En-De and En-Ru. The idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017). In light of this, I see very little novelty in this paper. The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word. Seems thin for a ICLR paper. Caption of Fig 1: subject/object are syntactic functions, not semantic roles. I don't see how the German verb orders inflects with gender... Can you post the gold German sentence? Sec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing. Expressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper. Why is hard attention (sec 3.3) necessary? It's not differentiable and requires sampling for training. This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017). Experimentally, the gains are quite small compared to flat attention, which is disappiointing. In table 3, it would be very helpful to display the English source. Table 4 is confusing. The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset? Comparison with predicted parses by Spacy are by no means gold parses... Minor comments: - Sec 1: ... optimization techniques like Adam, Attention, ... -> Attention is not an optimization technique, but part of a model; - Sec 1: abilities not its representation -> comma before  ot.",21,265,13.25,5.053061224489796,148,2,263,0.0076045627376425,0.0073529411764705,0.6634,77,35,46,15,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 0, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 1, 'NOV': 3, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 4}",0,1,4,2,1,8,4,0,3,0,0,1,2,0,1,3,0,3,1,0,5,0,4,0.6452268275240198,0.6697224419020169,0.4603427985921566
ICLR2018-BkoCeqgR--R1,Reject,"The paper proposes and evaluates a method to make neural networks for image recognition color invariant. The contribution of the paper is:   - some proposed methods to extract a color-invariant representation - an experimental evaluation of the methods on the cifar 10 dataset - a new dataset crashed cars   - evaluation of the best method from the cifar10 experiments on the new dataset Pros:   - the crashed cars dataset is interesting. The authors have definitely found an interesting untapped source of interesting images. Cons:  - The authors name their method order network but the method they propose is not really parts of the network but simple preprocessing steps to the input of the network. - The paper is incomplete without the appendices. In fact the paper is referring to specific figures in the appendix in the main text. - the authors define color invariance as a being invariant to which specific color an object in an image does have, e.g. whether a car is red or green, but they don't think about color invariance in the broader context - color changes because of lighting, shades, ..... Also, the proposed methods aim to preserve the colorfullness of a color. This is also problematic, because while the proposed method works for a car that is green or a car that is red, it will fail for a car that is black (or white) - because in both cases the colorfulness is not relevant. Note that this is specifically interesting in the context of the task at hand (cars) and many cars being, white, grey (silver), or black. - the difference in the results in table 1 could well come from the fact that in all of the invariant methods except for ord the input is a WxHx1 matrix, but for ord and cifar the input is a WxHx3 matrix. This probably leads to more parameters in the convolutions. - the results in the  figure 4: it's very unlikely that the differences reported are actually significant. It appears that all methods perform approximately the same - and the authors pick a specific line (25k steps) as the relevant one in which the RGB-input space performs best. The proposed method does not lead to any relevant improvement. Figure 6/7: are very hard to read. I am still not sure what exactly they are trying to say. Minor comments:   - section 1: called for is network -> called for is a network - section 1.1: And and -> And - section 1.1: Appendix -> Appendix C  - section 2: Their exists many -> There exist many  - section 2: these transformation -> these transformations  - section 2: what does the wallpaper groups refer to?   - section 2: are a groups -> are groups  - section 3.2: reference to a non-existing figure  - section 3.2/Training: 2499999 iterations   steps?   - section 3.2/Training: longer as suggested -> longer than suggested",23,451,23.73684210526316,5.0095238095238095,186,3,448,0.0066964285714285,0.0178926441351888,0.9588,137,48,71,19,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 5, 'MET': 11, 'EXP': 3, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 3}",0,1,0,0,5,11,3,1,4,0,0,0,0,0,0,0,1,0,1,0,8,3,3,0.4314599981343864,0.5602848807578075,0.2730260027018061
ICLR2018-BkoCeqgR--R2,Reject,"The authors investigate a modified input layer that results in color invariant networks. The proposed methods are evaluated on two car datasets. It is shown that certain color invariant input layers can improve accuracy for test-images from a different color distribution than the training images. The proposed assumptions are not well motivated and seem arbitrary. Why is using a permutation of each pixels' color a good idea? The paper is very hard to read. The message is unclear and the experiments to prove it are of very limited scope,; i.e. one small dataset with the only experiment purportedly showing generalization to red cars. Some examples of specific issues: - the abstract is almost incomprehensible and it is not clear what the contributions are - Some references to Figures are missing the figure number, eg. 3.2 first paragraph, - It is not clear how many input channels the color invariant functions use, eg. p1 does it use only one channel and hence has fewer parameters? - are the training and testing sets all disjoint (sec 4.3)? - at random points figures are put in the appendix, even though they are described in the paper and seem to show key results (eg tested on nored-test) - Sec 4.6: The explanation for why the accuracy drops for all models is not clear. Is it because the total number of training images drops? If that's the case the whole experimental setup seems flawed. - Sec 4.6: the authors refer to the order net beating the baseline, however, from Fig 8 (right most) it appears as if all models beat the baseline. In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set.  Is that Fig 8 @0%? The baseline seems to be best performing on all cars and  on-red cars  In order to be at an appropriate level for any publication the experiments need to be much more general in scope. ",19,323,21.53333333333333,4.752411575562701,167,3,320,0.009375,0.0479041916167664,-0.8401,98,36,57,15,10,5,"{'ABS': 1, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 8, 'EXP': 4, 'RES': 1, 'TNF': 4, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 1}",1,1,0,0,2,8,4,1,4,2,0,1,1,0,0,0,1,0,2,0,10,1,1,0.7164263504599626,0.5612147413929456,0.4403571054589448
ICLR2018-BkoCeqgR--R3,Reject,"The authors test a CNN on images with color channels modified (such that the values of the three channels, after modification, are invariant to permutations). The main positive point is that the performance does not degrade too much. However, there are several important negative points which should prevent this work, as it is, from being published. 1. Why is this type of color channel modification relevant for real life vision? The invariance introduced here does not seem to be related to any real world phenomenon.  The nets, in principle, could learn to recognize objects based on shape only, and the shape remains stable when the color channels are changed. 2. Why is the crash car dataset used in this scenario? It is not clear to me why this types of theoretical invariance is tested on such as specific dataset. Is there a real reason for that? 3. The writing could be significantly improved, both at the grammatical level and the level of high level organization and presentation. I think the authors should spend time on better motivating the choice of invariance used, as well as on testing with different (potentially new) architectures, color change cases, and datasets. 4. There is no theoretical novelty and the empirical one seems to be very limited, with less convincing results.",12,216,16.615384615384617,4.990430622009569,126,2,214,0.0093457943925233,0.0506912442396313,0.9334,55,26,40,13,6,7,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 3, 'MET': 5, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,0,0,3,5,2,1,0,0,0,3,0,0,0,1,1,1,1,1,7,0,1,0.429821411914164,0.7815095223868356,0.32838545281204146
ICLR2018-BkoXnkWAb-R1,Reject,"The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs. The activation function keeps the activation roughly zero-centered. In general, this is an interesting direction to explore, the idea is interesting,; however, I would like to see more experiments. 1. The authors tested out this new activation function on RNNs. It would be interesting to see the results of the new activation function on LSTM. 2. The experimental results are fairly weak compared to the other methods that also uses many layers. For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers. 3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling. 4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm. 5. It would be nice to see the gradient flow with the new activation function compared to the ones without. 6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems.",12,215,12.647058823529411,5.15686274509804,104,3,212,0.0141509433962264,0.0136986301369863,0.936,62,26,34,13,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,2,1,0,6,3,3,0,0,0,0,0,0,0,0,1,3,0,0,3,1,0,0.4301866724115721,0.4459226844285103,0.2438231800493944
ICLR2018-BkoXnkWAb-R2,Reject,"This paper proposes a self-normalizing bipolar extension for the ReLU activation family. For every neuron out of two, authors propose to preserve the negative inputs. Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU. Combined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN. Authors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN. They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions. They also show that they can train deep residual network architecture on CIFAR without the use of BN. Question: - Which layer mean and variance are reported in Figure 2?  What is the difference between the left and right plots? - In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances. It would be nice to report the training loss to see if this is an optimization or a generalization problem. - How does bipolar activation compare to model train with BN on CIFAR10? - Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU? - As stated in the text, BELU-RNN outperforms BN-LSTM for PTB. However, BN-LSTM outperforms BELU-RNN on Text8. Do you know why the trend is not consistent across datasets? -Clarity/Quality The paper is well written and pleasant to read. - Originality: Self-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel. - Significance: Activation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost. In this paper, bipolar activations are used to train very deep stacked RNN.  However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures. It is not clear what are the advantage of deep stacked RNN in that context.",22,368,20.444444444444443,5.26183844011142,177,1,367,0.0027247956403269,0.0212201591511936,0.9267,114,47,64,16,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 15, 'EXP': 2, 'RES': 0, 'TNF': 3, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,4,0,0,15,2,0,3,4,0,1,0,0,0,1,2,3,0,0,8,0,1,0.503951103155108,0.5601916193622842,0.31780904215006334
ICLR2018-BkoXnkWAb-R3,Reject,"Summary: This paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks. The proposal is to replace the non-linearity in half of the units in each layer with its bipolar version -- one that is obtained by flipping the function on both axes. The technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained. Clarity: The paper is easy to read. The plots in Fig. 2 and the appendix are quite helpful in improving presentation. The experimental setups are explained in detail. Quality and significance: The main idea from this paper is simple and intuitive. However, the experiments to support the idea do not seem to match the motivation of the paper. As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent. However, the presented results focus on the performance on held-out data instead of improvements in training speed. This is especially the case for the RNN experiments. For the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning. However, it is unclear that overall training time can be reduced with the help of this technique. To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered. Nevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases. The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset),; but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.",17,310,18.23529411764705,5.098360655737705,165,1,309,0.0032362459546925,0.0160771704180064,0.9439,84,28,59,19,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 5, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,2,1,2,1,7,5,2,2,0,0,1,0,0,0,0,0,1,1,0,10,0,1,0.6448668973512728,0.4500420613580311,0.36613745600291936
ICLR2018-BkpXqwUTZ-R1,Reject,"- This paper is not well written and incomplete. There is no clear explanation of what exactly the authors want to achieve in the paper, what exactly is their approach/contribution, experimental setup, and analysis of their results.  - The paper is hard to read due to many abbreviations, e.g., the last paragraph in page 2. - The format is inconsistent. Section 1 is numbered, but not the other sections. - in page 2, what do the numbers mean at the end of each sentence? Probably the figures?  - in page 2, in this figure: which figure is this referring to? Comments on prior work:  p 1: authors write: vanilla backpropagation (VBP) was proposed around 1987 Rumelhart et al. (1985).    Not true. A main problem with the 1985 paper is that it does not cite the inventors of backpropagation. The VBP that everybody is using now is the one published by  Linnainmaa in 1970, extending Kelley's work of 1960. The first to publish the application of VBP to NNs was Werbos in 1982. Please correct.  p 1: authors write: Almost at the same time, biologically inspired convolutional networks was also introduced as well using VBP LeCun et al. (1989).   Here one must cite the person who really invented this biologically inspired convolutional architecture (but did not apply backprop to it): Fukushima (1979). He is cited later, but in a misleading way. Please correct. p 1: authors write: Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012).    Not true. Deep Learning was introduced by Ivakhnenko and Lapa in 1965: the first working method for learning in multilayer perceptrons of arbitrary depth. Please correct.(The term deep learning was introduced to ML in 1986 by Dechter for something else.) p1: authors write: Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015).    Highway networks were published half a year earlier than resnets, and reached many hundreds of layers before resnets. Please correct. General recommendation: Clear rejection for now. But perhaps the author want to resubmit this to another conference, taking into account the reviewer comments.  ",18,364,12.133333333333333,5.080597014925373,171,3,361,0.0083102493074792,0.020671834625323,0.8805,103,42,64,25,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 10, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 7, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 8, 'PNF': 2, 'REC': 1, 'EMP': 0, 'SUB': 0, 'CLA': 4}",0,0,10,0,0,1,0,0,2,0,0,5,7,1,0,0,0,8,2,1,0,0,4,0.4295751509148169,0.4455695564811398,0.24169930711743853
ICLR2018-BkpXqwUTZ-R2,Reject,"The paper falls far short of the standard expected of an ICLR submission. The paper has little to no content. There are large sections of blank page throughout. The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description. The experiments are only performed on MNIST. The subfigures are not labeled. The paper over-uses acronyms; sentences like ""In this figure, VBP, VBP with FBA, and ITD using FBA for VBP..."" are painful to read.",7,78,11.142857142857142,4.909090909090909,55,0,78,0.0,0.0126582278481012,-0.5141,24,8,15,4,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 3}",0,0,0,0,0,1,1,0,2,0,0,4,0,0,1,0,0,0,0,0,1,0,3,0.285873840099144,0.3334956034750414,0.14494987175166898
ICLR2018-BkpXqwUTZ-R3,Reject,"The paper is incomplete and nowhere near finished, it should have been withdrawn .   The theoretical results are presented in a bitmap figure and only referred to in the text (not explained), and  the results on datasets are not explained either (and pretty bad). A waste of my time.",3,48,16.0,4.804347826086956,35,0,48,0.0,0.0384615384615384,-0.4767,8,4,11,5,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 0, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 0, 'SUB': 0, 'CLA': 2}",0,0,0,0,1,0,0,2,1,1,0,1,0,0,0,0,0,0,0,1,0,0,2,0.3571985590355666,0.2223033572930763,0.1579271157331888
ICLR2018-BkpiPMbA--R1,Accept,"Summary of paper:  The authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations. They compare this optimization method with two baselines in MNIST and CIFAR, and provide an analysis of the decision boundaries by their adversarial examples, the baselines and non-altered examples. Review summary:  I think this paper is interesting. The novelty of the attack is a bit dim, since it seems it's just the straightforward attack against the region cls defense. The authors fail to include the most standard baseline attack, namely FSGM. The authors also miss the most standard defense, training with adversarial examples. As well, the considered attacks are in L2 norm, and the distortion is measured in L2, while the defenses measure distortion in L_infty (see detailed comments for the significance of this if considering white-box defenses). The provided analysis is insightful, though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks. If the authors add FSGM to the batch of experiments (especially section 4.1) and address some of the objections I will consider updating my score. A more detailed review follows. Detailed comments:  - I think the novelty of the attack is not very strong. The authors essentially develop an attack targeted to the region cls defense. Designing an attack for a specific defense is very well established in the literature, and the fact that the attack fools this specific defense is not surprising. - I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack (such as PGD or region based), or for defenses that know this is a likely attack (see the following comment as well). If the authors want to make the second claim, training the network with adversarial examples coming from OptMargin is missing. - The attacks are all based in L2, in the sense that the look for they measure perturbation in an L2 sense (as the paper evaluation does), while the defenses are all L_infty based (since the region classifier method samples from a hypercube, and PGD uses an L_infty perturbation limit). This is very problematic if the authors want to make claims about their attack being effective under defenses that know OptMargin is a possible attack. - The simplest most standard baseline of all (FSGM) is missing. This is important to compare properly with previous work. - The fact that the attack OptMargin is based in L2 perturbations makes it very susceptible to a defense that backprops through the attack. This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack. - I think the authors rush to conclude that a small ball around a given input distance can be misleading. Wether balls are in L2 or L_infty, or another norm makes a big difference in defense and attacks, given that they are only equivalent to a multiplicative factor of sqrt(d) where d is the dimension of the space, and we are dealing with very high dimensional problems. I find the analysis made by the authors to be very simplistic. - The analysis of section 4.1 is interesting, it was insightful and to the best of my knowledge novel. Again I would ask the authors to make these plots for FSGM. Since FSGM is known to be robust to small random perturbations, I would be surprised that for a majority of random directions, the adversarial examples are brought back to the original class. - I think a bit more analysis is needed in section 4.2. Do the authors think that this distinguishability can lead to a defense that uses these statistics? If so, how? - I think the analysis of section 5 is fairly trivial.  Distinguishability in high dimensions is an easy problem (as any GAN experiment confirms, see for example Arjovsky & Bottou, ICLR 2017), so it's not surprising or particularly insightful that one can train a classifier to easily recognize the boundaries. - Will the authors release code to reproduce all their experiments and methods? Minor comments: - The justification of why OptStrong is missing from Table2 (last three sentences of 3.3) should be summarized in the caption of table 2 (even just pointing to the text), otherwise a first reader will mistake this for the omission of a baseline. - I think it's important to state in table 1 what is the amount of distortion noticeable by a human.                                            After the rebuttal I've updated my score, due to the addition of FSGM added as a baseline and a few clarifications. I now understand more the claims of the paper, and their experiments towards them. I still think the novelty, significance of the claims and protocol are still perhaps borderline for publication (though I'm leaning towards acceptance), but I don't have a really high amount of experience in the field of adversarial examples in order to make my review with high confidence.",40,836,23.88571428571429,5.120051085568327,309,14,822,0.0170316301703163,0.0222965440356744,-0.9728,238,82,147,42,11,7,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 1, 'DAT': 1, 'MET': 15, 'EXP': 7, 'RES': 4, 'TNF': 2, 'ANA': 9, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 3, 'EMP': 21, 'SUB': 3, 'CLA': 0}",0,0,6,1,1,15,7,4,2,9,1,3,0,2,0,2,1,3,1,3,21,3,0,0.7906249139374664,0.7907493131345085,0.6272068761095148
ICLR2018-BkpiPMbA--R2,Accept,"Compared to previous studies, this paper mainly claims that the information from larger neighborhoods (more directions or larger distances) will better characterize the relationship between adversarial examples and the DNN model. The idea of employing ensemble of classifiers is smart and effective. I am curious about the efficiency of the method. The experimental study is extensive. Results are well discussed with reasonable observations. In addition to examining the effectiveness, authors also performed experiments to explain why OPTMARGIN is superior. Authors are suggested to involve more datasets to validate the effectiveness of the proposed method. Table 5 is not very clear. Authors are suggested to discuss in more detail. ",9,108,10.8,5.79245283018868,74,0,108,0.0,0.055045871559633,0.9429,29,16,23,6,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,1,2,1,3,2,1,2,0,0,0,0,0,0,0,0,0,1,0,6,2,0,0.5006691778418404,0.3365497970158697,0.244068636962436
ICLR2018-BkpiPMbA--R3,Accept,"The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks. I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is. My comments are the following:  1- I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks. 2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method. In section 3.2 they explain that Our attack uses existing optimization attack techniques to..., but one should be able to understand the method without reading further references. Specially a formal representation of the method should be included. 3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples (it succeeds on 28% of the samples on MNIST;73% on CIFAR-10). What is the meaning of success rate in here? Is it the % of times that the classifier is confused? 4- OPTSTRONG produces images that are notably more distorted than OPTBRITTLE (by RMS and also visually in the case of MNIST). So I actually cannot tell which method is better, at least in the MNIST experiment. One could do a method that completely distort the image and therefore will be classified with as a class. But adversarial images should be visually undistinguishable from original images. Generated CIFAR images seem similar than the originals, although CIFAR images are very low resolution, so judging this is hard. 4- As a side note, it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR-10 benign samples. As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. I would like to see more formal definitions of the methods presented. Also, just by looking at RMS it is expected that this method works better than OPTBRITTLE, since the images are more distorted. It would be needed to have a way of visually evaluate the similarity between original images and generated images.",19,372,21.88235294117647,5.315942028985507,176,1,371,0.0026954177897574,0.0294117647058823,-0.8645,92,46,69,31,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 4, 'MET': 10, 'EXP': 1, 'RES': 7, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 0}",0,1,0,0,4,10,1,7,0,3,0,0,0,0,0,0,0,4,0,0,5,3,0,0.4313150799326602,0.3363860037609181,0.21568372661155463
ICLR2018-BkrSv0lA--R1,Accept,"In this paper, the authors propose a method of compressing network by means of weight ternarization. The network weights ternatization is formulated in the form of loss-aware quantization, which originally proposed by Hou et al. (2017). To this reviewer's understanding, the proposed method can be regarded as the extension of the previous work of LAB and TWN, which can be the main contribution of the work. While the proposed method achieved promising results compared to the competing methods, it is still necessary to compare their computational complexity, which is one of the main concerns in network compression. It would be appreciated to have discussion on the results in Table 2, which tells that the performance of quantized networks is better than the full-precision network.",5,124,20.666666666666668,5.289256198347108,73,0,124,0.0,0.0080645161290322,0.836,36,11,21,2,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,3,1,1,1,1,0,0,1,0,0,0,2,2,0,0,1,0,0,0.5720094435585247,0.3334985325389712,0.29136871219978394
ICLR2018-BkrSv0lA--R2,Accept,"This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights). The paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose. The experiments are very clearly presented and solidly designed. In fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides. Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods. n Finally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm. However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t in Q^{n_l}$). This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis.",8,218,27.25,5.121359223300971,112,2,216,0.0092592592592592,0.0412844036697247,0.5195,59,20,29,23,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,3,1,0,2,3,1,0,3,0,0,0,0,0,3,2,1,0,0,1,0,1,0.5006329982632025,0.5556938073730397,0.3080563429668081
ICLR2018-BkrSv0lA--R3,Accept,"This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments. Review:  Pros This paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm.  They extend the scheme to allow the use of different scaling parameters and to m-bit quantization. Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods. The experiments are complete and the writing is good. Cons Although the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b) . Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary.  ",7,138,17.25,5.661538461538462,85,1,137,0.0072992700729927,0.0138888888888888,0.8658,47,19,18,0,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,2,0,4,3,0,0,0,0,0,1,0,0,1,0,1,0,0,3,0,1,0.4295536963580557,0.4456883593141303,0.24336106984899045
ICLR2018-BkrsAzWAb-R1,Accept,"SUMMARY:  The authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique. GENERAL IMPRESSION:  One central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value. Despite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods. CRITICISM:  The experimental evaluation is rather solid, but not perfect.  It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters alpha_0 and beta. The extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method. MINOR POINTS:  page 4, bottom: use citep for Duchi et al. (2011). None of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve. In figure 2, top row, please display the learning rate on a log scale. page 8, line 7 in section 4.3: the the (unintended repetition) End of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?",21,340,14.166666666666666,5.140065146579804,193,0,340,0.0,0.0202312138728323,0.914,95,49,46,22,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 7, 'EXP': 4, 'RES': 2, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 0, 'PNF': 5, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 0}",0,1,1,0,1,7,4,2,3,1,0,2,1,0,0,3,1,0,5,0,8,3,0,0.7161931709657215,0.5604591014803488,0.4522240803764134
ICLR2018-BkrsAzWAb-R2,Accept,"The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update. The basic observation (for SGD) is that if theta_{t+1}   theta_t - alpha  abla f(theta_t), then partial/partialalpha f(theta_{t+1})   -< abla f(theta_t),  abla f(theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate alpha. I have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization). The experiments are well-presented, and appear to convincingly show a benefit. Figure 3, which explores the robustness of the algorithms to the choice of alpha_0 and beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two). The authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one. This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable.",7,253,28.11111111111111,5.336134453781512,135,3,250,0.012,0.0306513409961685,0.9495,72,36,42,13,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,6,1,0,1,0,1,0,0,0,0,1,0,0,2,0,5,0,0,0.3583938177117804,0.3358827319965085,0.1789065893213422
ICLR2018-BkrsAzWAb-R3,Accept,"This paper revisits an interesting and important trick to automatically adapt the stepsize. They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize. Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice. Although the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance. But before that, there are several issues need to be addressed. 1, the derivation of the update of alpha relies on the expectation formulation. I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick. 2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing. 3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments. Moreover, the empirical comparisons are only conducted on MNIST. To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet. Minors:   In the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes. Could you please explain why such phenomenon happens?",13,248,15.5,5.317991631799163,143,0,248,0.0,0.02,0.9715,69,27,43,13,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 8, 'EXP': 5, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,1,0,1,8,5,2,1,0,0,1,1,0,0,0,0,3,0,1,8,0,0,0.6450089727131797,0.3379213604916141,0.32609396902494414
ICLR2018-BkwHObbRZ-R1,Accept,"[                             REVISION                                                                ] I am satisfied with the answers to my questions.  The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating. However I am fine accepting it. [                                END OF REVISION                                                      ]  This paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net).  Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization). First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD. Overall the paper is well written. The authors first introduce their suggested loss function and then go into details about what inspired its creation. I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful My issues with the paper are as follows: - The loss function designed seems overly complicated. On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used. I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework. - I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima. Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched. - It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem - No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better. Minor: fix margins in formula 2.7.",17,421,26.3125,4.98014888337469,209,4,417,0.0095923261390887,0.03125,-0.9712,105,56,84,24,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 11, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 2}",0,1,1,0,0,11,2,1,0,0,0,3,0,1,0,0,0,1,1,1,8,0,2,0.5026789545456565,0.5599903926703104,0.31377999819548724
ICLR2018-BkwHObbRZ-R2,Accept,"This paper studies the problem of learning one-hidden layer neural networks and is a theory paper. A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent. This paper establishes an interesting connection between least squares population loss and Hermite polynomials. Following from this connection authors propose a new loss function. Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings. Overall, pretty interesting result and solid contribution. The paper also raises good questions for future works. For instance, is designing alternative loss function useful in practice?  In summary, I recommend acceptance. The paper seems rushed to me so authors should polish up the paper and fix typos. Two questions: 1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows? If so they should clarify this otherwise it confuses the reader a bit. 2) What can be said about rate of convergence in terms of network parameters? Currently a generic bound is employed which is not very insightful in my opinion.",14,191,15.916666666666666,5.372222222222222,121,1,190,0.0052631578947368,0.0520833333333333,0.6159,57,30,33,10,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,9,0,1,0,0,1,2,0,0,0,2,0,0,0,1,8,0,1,0.359189812861185,0.4488432540728634,0.19648987359897763
ICLR2018-BkwHObbRZ-R3,Accept,"This paper proposes a tensor factorization-type method for learning one hidden-layer neural network. The most interesting part is the Hermite polynomial expansion of the activation function. Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem. They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015.  At last, they also establish the sample complexity for recovery. The organization and presentation of the paper need some improvement. For example, the authors defer many technical details. To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages. There are also some typos: For example, the dimension of a is inconsistent. In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector. On Page 8, P(B) should be a degree-4 polynomial of B. The paper does not contains any experimental results on real data.",12,172,13.23076923076923,5.462025316455696,105,0,172,0.0,0.0115606936416184,0.5256,52,26,21,4,7,3,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 3}",1,1,1,0,0,6,1,1,0,0,0,2,0,0,0,0,0,0,1,0,3,0,3,0.5012963793769739,0.3347395183447273,0.2547738595695487
ICLR2018-By-7dz-AZ-R1,Accept,"**** I acknowledge the author's comments and improve my score to 7. ****  Summary: The authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations. The basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c. The authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables. The paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could. Significance: The proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way. Clarity: The paper is lucidly written and very understandable. Quality: The authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant. A few comments: 1. How do the authors propose to deal with multimodal true latent factors? What if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data? 2. Scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation? 3. the actual sources of variation are interpretable and explicit measurable quantities here. However, oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z (for instance the sentiment of a scene) even when these factors are known. How do the authors propose to move past narrow definitions of factors of variation and handle more complex variables? Arguably, disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well-behaved and relatively easy to quantify since they relate to image formation physics. 4. For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation. I would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here, especially a variational autoencoder, beta-VAE and so on. Furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science. Novelty: Previous papers like beta-VAE (Higgins et al. 2017) and Bayesian Representation Learning With Oracle Constraints by Karaletsos et al (ICLR 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does. It would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework. ",19,548,24.90909090909091,5.627450980392157,257,2,546,0.0036630036630036,0.0144927536231884,0.9849,153,69,100,27,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 4, 'MET': 12, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 11, 'SUB': 0, 'CLA': 1}",0,1,2,1,4,12,5,2,0,1,0,2,0,0,0,1,1,0,0,1,11,0,1,0.6462439200984333,0.5617751299039853,0.4109450034920036
ICLR2018-By-7dz-AZ-R2,Accept,"The paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data. *Quality*  The problem addressed is surely relevant in general terms. However, the contributed framework did not account for previously proposed metrics (such as equivariance, invariance and equivalence). Within the experimental results, only two methods are considered: although Info-GAN is a reliable competitor, PCA seems a little too basic to compete against. The choice of using noise-free data only is a limiting constraint (in [Chen et al. 2016], Info-GAN is applied to real-world data). Finally, in order to corroborate the quantitative results, authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix. *Clarity* The explanation of the theoretical framework is not clear. In fact, Figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping. But, then, the authors missed to clarify how the definitions of D_i and C_j translate this requirement into math. Also, the criterion of informativeness of Section 2 is split into two sub-criteria in Section 3.3, namely test set NRMSE and Zero-Shot NRMSE: such shift needs to be smoothed and better explained, possibly introducing it in Section 2. *Originality* The paper does not allow to judge whether the three proposed criteria are original or not with respect to the previously proposed ones of [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. *Significance* The significance of the proposed evaluation framework is not fully clear. The initial assumption of considering factors of variations related to graphics-generated data undermines the relevance of the work. Actually, authors only consider synthetic (noise-free) data belonging to one class only, thus not including the factors of variations related to noise and/or different classes. PROS:  The problem faced by the authors is interesting. CONS: The criteria of disentanglement, informativeness & completeness are not fully clear as they are presented.. The proposed criteria are not compared with previously proposed ones - equivariance, invariance and equivalence [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. Thus, it is not possible to elicit from the paper to which extent they are novel or how they are related... The dataset considered is noise-free and considers one class only. Thus, several factors of variation are excluded a priori and this undermines the significance of the analysis. The experimental evaluation only considers two methods, comparing Info-GAN, a state-of-the-art method, with a very basic PCA. **FINAL EVALUATION** The reviewer rates this paper with a weak reject due to the following points. 1) The novel criteria are not compared with existing ones [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. 2) There are two flaws in the experimental validation:. t2.1) The number of methods in comparison (InfoGAN and PCA) is limited. t2.2) A synthetic dataset is only considered. The reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed. **EVALUATION AFTER AUTHORS' REBUTTAL** The reviewer has read the responses provided by the authors during the rebuttal period. In particular, with respect to the highlighted points 1 and 2, point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper. Despite the same level of clarification has not been reached for what concerns point 2, the proposed framework (although still limited in relevance due to the lack of more realistic settings) can be useful for the community as a benchmark to verify the level of disentanglement than newly proposed deep architectures can achieve. Finally, by also taking into account the positive evaluation provided by the fellow reviewers, the rating of the paper has been risen towards acceptance.    .",31,645,17.916666666666668,5.497520661157025,273,2,643,0.0031104199066874,0.0287009063444108,0.9827,199,70,116,49,11,9,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 4, 'DAT': 5, 'MET': 12, 'EXP': 4, 'RES': 2, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 2, 'IMP': 2, 'CMP': 8, 'PNF': 2, 'REC': 3, 'EMP': 7, 'SUB': 5, 'CLA': 3}",0,1,6,4,5,12,4,2,2,2,0,4,0,1,1,2,2,8,2,3,7,5,3,0.7896896640562883,1.0,0.7714518850608745
ICLR2018-By-7dz-AZ-R3,Accept,"The authors consider the metrics for evaluating disentangled representations. They define three criteria: Disentanglement, Informativeness, and Completeness. They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights. Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color. I think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at. Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me. A second limitation of the work is the reliance on a true set of disentangled factors. We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors. Could the authors perhaps comment on how well these metrics would work in the semi-supervised case? Overall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response. Minor Commments:  - Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption. - It is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. ",12,264,20.307692307692307,5.51394422310757,147,3,261,0.0114942528735632,0.037037037037037,0.8641,77,30,40,9,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,3,1,4,1,0,1,0,0,1,0,0,0,0,0,0,1,1,4,0,0,0.5008691760650655,0.3351992056378622,0.24826757847064443
ICLR2018-By-IifZRW-R1,Reject,"The paper addresses the problem of learning the form of the activation functions in neural networks. The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net. This  somehow allows to non-parametrically infer from the data the shape of the activation functions needed for a specific problem. The paper then proposes an inference framework (to approximately marginalize out all GP functions)  based on sparse GP methods that use inducing points and variational inference. The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values (denoted by U in the paper). Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations. In general I like the idea and I believe that it can lead to a very useful model. However, I have found the current paper quite preliminary and incomplete. The authors need to address the following:    First (very important): You need to show experimentally how your method compares against regular neural nets (with specific fixed forms for their activation functions such relus etc). At the moment in the last section you mention We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication. In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters.    >  Well all this needs to be included in the same paper.  Secondly: Discuss the connection with Deep GPs (Damianou and Lawrence 2013). Your method seems to be connected with Deep GPs although there appear to be important differences as well. E.g. you place GPs on the scalar activation functions in an otherwise  heavily parametrized neural network (having interconnection weights between layers) while deep GPs model the full hidden layer mapping as a single GP (which does not require interconnection weights). Thirdly:  You need to better explain the propagation of uncertainly in section 3.2.2  and the central limit of distribution in section 3.2.1. This is the technical part of your paper which is a non-standard approximation. I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix.   ",16,428,21.4,5.312807881773399,215,1,427,0.0023419203747072,0.0134831460674157,0.9625,120,53,68,25,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 9, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 3, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 0}",0,2,0,4,0,9,3,0,0,3,1,1,0,0,0,0,1,3,1,0,5,3,0,0.5024267857698099,0.5584910634259503,0.3187661770505373
ICLR2018-By-IifZRW-R2,Reject,"In Bayesian neural networks, a deterministic or parametric activation is typically used. In this work, activation functions are considered random functions with a GP prior and are inferred from data. - Unnecessary complexity The presentation of the paper is unnecessarily complex. It seems that authors spend extra space creating problems and then solving them. Although some of the derivations in Section 3.2.2 are a bit involved, most of the derivations up to that point (which is already in page 6) follow preexisting literature. For instance, eq. (3) proposes one model for p(F|X). Eq. (8) proposes a different model for p(F|X), which is an approximation to the previous one. Instead, the second model could have been proposed directly, with the appropriate citation from the literature, since it isn't new. Eq. (13) is introduced as a solution to a non-existent problem, because the virtual observations are drawn from the same prior as the real ones, so it is not that we are coming up with a convenient GP prior that turns out to produce a computationally tractable solution, we are just using the prior on the observations consistently.  In general, the authors seem to use approximately equal and equal interchangeably, which is incorrect. There should be a single definition for p(F|X). And there should be a single definition for L_pred. The expression for L_pred given in eq. (20) (exact) and eq. (41) (approximate) do not match and yet both are connected with an equality (or proportionality), which they shouldn't. Q(A) is sometimes taken to mean the true posterior (i.e., eq. (31)), sometimes a Gaussian approximation (i.e., eq (32) inside the integral), and both are used interchangeably. - Incorrect references to the literature Page 3: using virtual observations (originally proposed by Quiu00f1onero-Candela & Rasmussen (2005) for sparse approximations of GPs)  The authors are citing as the origin of virtual observations a survey paper on the topic. Of course, that survey paper correctly attributes the origin to [1]. Page 4: we apply the technique of variational inference Wainwright et al. (2008). How can variational inference be attributed to (again) a survey paper on the topic from 2008, when for instance [2] appeared in 2003? - Correctness of the approach Can the authors guarantee that the variational bound that they are introducing (as defined in eqs. (19) and (41)) is actually a variational bound? It seems to me that the approximations made to Q(A) to propagate the uncertainty are breaking the bounding guarantee. If it is no longer a lower bound, what is the rationale behind maximizing it? The mathematical basis for this paper is actually introduced in [3] and a single-layer version of the current model is developed in [4]. However, in [4] the authors manage to avoid the additional Q(A) approximation that breaks the variational bound. The authors should contrast their approach with [4] and discuss if and why that additional central limit theorem application is necessary. - No experiments The use of a non-parametric definition for the activation function should be contrasted with the use of a parametric one. With enough data, both might produce similar results. And the parameter sharing in the parametric one might actually be beneficial. With no experiments at all showing the benefit of this proposal, this paper cannot be considered complete. - Minor errors:  Eq. (4), for consistency, should use the identity matrix for the covariance matrix definition. Eq. (10) uses subscript d where it should be using subscript n Eq. (17) includes p(X^L|F^L) in the definition of Q(...), but it shouldn't. That was particularly misleading, since if we take eq. (17) to be correct (which I did at first), then p(X^L|F^L) cancels out and should not appear in eq. (20). Eq. (23) uses Q(F|A) to mean the same as P(F|A) as far as I understand. Then why use Q? - References  [1] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. [2] Beal, M.J. Variational Algorithms for Approximate Bayesian Inference. [3] M.K. Titsias and N.D. Lawrence. Bayesian Gaussian process latent variable model.  [4] M. Lu00e1zaro-Gredilla. Bayesian warped Gaussian processes. ",40,667,12.127272727272729,5.288998357963875,276,6,661,0.0090771558245083,0.0220588235294117,-0.7227,172,84,113,42,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 1, 'DAT': 1, 'MET': 22, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 5, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 0}",0,0,7,1,1,22,2,1,0,0,0,4,5,1,0,0,0,4,2,0,19,1,0,0.6488661288961343,0.4560527348669912,0.36841879974600317
ICLR2018-By-IifZRW-R3,Reject,"This paper investigates probabilistic activation functions that can be structured in a manner similar to traditional neural networks whilst deriving an efficient implementation and training regime that allows them to scale to arbitrarily sized datasets. The extension of Gaussian Processes to Gaussian Process Neurons is reasonably straight forward, with the crux of the paper being the path taken to extend GPNs from intractable to tractable. The first step, virtual observations, are used to provide stand ins for inputs and outputs of the GPN. These are temporary and are later made redundant. To avoid the intractable marginalization over latent variables, the paper applies variational inference to approximate the posterior within the context of given training data. Overall the process by which GPNs are made tractable to train leverages many recent and not so recent techniques. The resulting model is theoretically scalable to arbitrary datasets as the total model parameters are independent of the number of training samples. It is unfortunate but understandable that the GPN model experiments are confined to another paper.",8,171,21.375,5.594117647058823,112,0,171,0.0,0.0058479532163742,0.0644,48,25,33,7,4,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 6, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,0,0,2,6,2,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0.2870875328148112,0.1142208982853259,0.11770248260727874
ICLR2018-By03VlJGG-R1,,"The paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text. For this purpose, the method learns joint embeddings of symbolic data, images and text to predict the links in a knowledge graph. The multimodal embeddings are evaluated on newly created datasets, which extend the MovieLens-100k and YAGO-10 with multimodal information. The paper is written well, good to understand, and technically sound. I especially liked the general idea of using multiple modalities to improve embeddings of relational data. This direction is not only interesting because of the improvements it brings for link prediction tasks, but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embeddings. The technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods. With regard to related work: Recently, [1] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction tasks. This reference should be included in the related work. The authors mention also in the last sentence of Section 3 that previous approaches cannot handle missing data or uncertainty. This claim needs to be discussed clearer as it is not clear to me why this would be the case. With regard to the evaluation: Overall, I found the evaluation to be good, especially with regard to the different ablations. However, it would be nice to see results for more sophisticated models than DistMult (which, due to its symmetry, shouldn't be used on directed graphs anyway) as the improvements that can be gained might be less for these models. It would also be interesting to see how predictions using only the non-symbolic modalities would do (e.g. in Table 3). Furthermore, Section 5.3 would clearly benefit from a better analysis and discussion, as it isn't very informative in its current form and the analysis is quite hand-wavy (e.g. two of the predicted titles for Die Hard have something to do with dying and being buried). Further comments: - The proposed method to incorporate numerical data seems quite ad hoc. What are the motivations for this particular approach? - Are the image features fixed or learned? In the later case: how much do the results change with pretrained CNNs (e.g., on ImageNet). - p.3: We use an appropriate encoder is repeated twice. - Since the datasets are newly introduced, it would be good to provide a more detailed analysis of their characteristics. [1] Thoma et al: Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics, 2017.",22,431,19.59090909090909,5.3798076923076925,225,2,429,0.0046620046620046,0.0183908045977011,0.9919,115,61,78,28,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 3, 'MET': 10, 'EXP': 0, 'RES': 1, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 1}",0,1,4,2,3,10,0,1,2,3,0,1,1,0,0,1,0,1,0,0,11,0,1,0.7169831250805101,0.4506640187928741,0.40639496993745217
ICLR2018-By03VlJGG-R2,,"This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information such as text description or images. A model, based on DistMult, able to encode all sort of information when scoring triples is presented with experiments on 2 new datasets based on Yago and MovieLens. This paper reads well and the results appear sound. Unfortunately, the contribution seems rather small to be accepted for ICLR. This is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results. * Having only results on new datasets makes it hard to compare the objective quality of the DistMult baselines and hence of the improvements due to the multimodal info. Isn't there any existing benchmark where this could have an impact? * The much better performance of ConvE is worrying there. It is suggested that the proposed approach could be incorporated in ConvE to lead to similar improvements than on DistMult. The paper would be much stronger with those. * Are we sure that the textual description do not explicitly contain the information of the triple to be predicted? This would explain the massive gains in Yago. * For Table 8, the similarities are not striking. What were the nearest neighboring posters in the original VGG space? They should not be that bad too. * The work on multimodal embeddings like Multimodal Distributional Semantics by Bruni et al. or Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception. by Kiela et al. could be discussed/cited.",14,255,15.9375,5.254980079681275,153,1,254,0.0039370078740157,0.0269230769230769,0.9792,68,35,47,16,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 4, 'EXP': 3, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,3,0,2,4,3,4,1,1,0,2,1,0,1,1,1,3,0,0,7,0,1,0.715618273241326,0.6706327363901045,0.5011254887801913
ICLR2018-By03VlJGG-R3,,"The paper is well-written but is lacking detailed information in some areas (see list of questions). The approach of incorporating all the different facts around an entity is worthwhile but pretty straight-forward. The evaluation part of this paper is hard to assess due to the unavailability of the 2 datasets and appropriate baselines. Therefore, I am currently leaning towards rejecting this paper. ? p.3: What parts are pre-trained? Is e_o fixed for the non-structured knowledge? Or is it joint learning and you learn all LSTMs and CNNs yourself? (Besides the reuse of VGG, I could not find this information explicitly stated within the paper.). ? p.4: The word embeddings for the CNN are pre-trained word2vec/Glove/xyz embeddings? How do you deal with words (or even the whole string) for which you have no word embedding? ? p.6: Do you have one model for all the relations or does every relation has its own LSTM, CNN, feed-forward network? I.e. 1 or 3 feed-forward networks for age, zip code, and release dates? ? p.6: How does ""Ratings Only"" work as DistMult gets no information of the specific entities? Is it just choosing the most common class? ? p.7: What does ""find the mid-point of the bin"" mean and should it not be 1018 instead of 1000 bins? + Insights on how different modalities affect the prediction results. + The approach is capable of theoretically handling all linked information to an entity as additional information to the link structure . - As the evaluation data is not available, it is really hard to assess the quality of the models. No simple baseline like the Unstructured [1] + simple concatenation of an image vector is provided. - Training of CNNs, LSTMs and so on is not clear.(See question regarding whether the models are pre-trained or whether the models are also directly learned from the data.). Further comments: * In Figure 1, the feed-forward network looks like an encoder-decoder network and it does not show the projection from r to R^d which is mentioned in the text. * The found hyperparameter of the grid-search would also be interesting to know. [1] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2012. A semantic matching energy function for learning with multi- relational data. Machine Learning 1u201327.",22,366,21.529411764705884,5.031976744186046,189,1,365,0.0027397260273972,0.0395778364116095,0.8865,110,39,66,19,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 13, 'EXP': 1, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,0,3,0,2,13,1,1,2,0,0,2,1,0,0,0,0,0,2,1,10,0,1,0.574696660107406,0.4501036302818345,0.31873758536067426
ICLR2018-By0ANxbRW-R1,Reject,"1. Summary  This paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change. 2. High level paper  - I believe the writing is a bit sloppy. For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below). This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models. 3. High level technical  - I'm confused at the first and second lines of equation (19). In the first line, shouldn't the first term not contain Delta W ? In the second line, shouldn't the first term be tilde{mathcal{L}}(W_0 + Delta W) ? - For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data. My worry is if you're compressing these networks with your method are the weights not treated as binary anymore? Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright. But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply. - Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354). What is your compression ratio for 0 accuracy loss? I think this is a key experiment that should be run as this result would be much easier to compare with the other methods. - Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers? - The first paper to propose weight sharing was not Han et al., 2015, it was actually: Chen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. Compressing Neural Networks with the Hashing Trick ICML 2015 Although they did not learn the weight sharing function, but use random hash functions. 4. Low level technical  - The end of Section 2 has an extra 'p' character - Section 3.1: Here, X and y define a set of samples and ideal output distributions we use for training this sentence is a bit confusing. Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3. - Section 3.1: W is the learnt model...hat{W} is the final, trained model This is unclear: W and hat{W} seem to describe the same thing. I would just remove is the learnt model and 5. Review summary  While the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read. This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR.",23,561,21.57692307692308,5.061386138613861,249,5,556,0.0089928057553956,0.0276338514680483,-0.745,156,74,109,39,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 13, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 1, 'CLA': 6}",0,1,3,0,2,13,2,0,0,0,0,1,2,0,0,0,1,1,0,1,12,1,6,0.5032979607066234,0.6739138738042096,0.35335117210941414
ICLR2018-By0ANxbRW-R2,Reject,"The paper addresses an interesting problem of DNN model compression. The main idea is to combine the approaches in (Han et al., 2015) and (Ullrich et al., 2017) to get a loss value constrained k-means encoding method for network compression. An iterative algorithm is developed for model optimization. Experimental results on MNIST, CIFAR-10 and SVHN are reported to show the compression performance. The reviewer would expect papers submitted for review to be of publishable quality. However, this manuscript is not polished enough for publication: it has too many language errors and imprecisions which make the paper hard to follow. In particular, there is no clear definition of problem formulation, and the algorithms are poorly presented and elaborated in the context. Pros:   - The network compression problem is of general interest to ICLR audience. Cons:  - The proposed approach follows largely the existing work and thus its technical novelty is weak.  - Paper presentation quality is clearly below the standard. - Empirical results do not clearly show the advantage of the proposed method over state-of-the-arts.",11,170,15.454545454545457,5.395209580838324,109,0,170,0.0,0.0168539325842696,-0.6749,59,15,33,10,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,3,2,1,3,2,1,0,0,0,3,1,0,1,1,0,1,2,0,1,0,1,0.6437548361591457,0.66672823559047,0.45358934733934597
ICLR2018-By0ANxbRW-R3,Reject,"1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter. However, all compression methods such as pruning and quantization also have this concern. For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved. Therefore, the novelty of the proposed method is somewhat weak. 2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3]. However, the paper only did a very simple investigation on related works. [r1] CNNpack: packing convolutional neural networks in the frequency domain. [r2] LCNN: Lookup-based Convolutional Neural Network. [r3] Xnor-net: Imagenet classification using binary convolutional neural networks. 3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10. It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet.",11,158,11.285714285714286,5.655405405405405,95,0,158,0.0,0.0063291139240506,-0.3197,59,35,22,9,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 0, 'DAT': 2, 'MET': 2, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 3, 'CLA': 0}",0,1,6,0,2,2,2,0,0,0,0,0,0,0,0,1,0,2,0,0,1,3,0,0.3579177753001639,0.4447749600182774,0.20412838378608067
ICLR2018-By3VrbbAb-R1,Reject,"This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU. Regarding the latter methods: what is described in the paper sounds like competent engineering details that those performing such a task for launch in a real service would figure out how to accomplish, and the specific reported details may or may not represent the 'right' way to go about this versus other choices that might be made. The final threshold for 'successful' speedups feels somewhat arbitrary -- why 16ms in particular?  In any case, these methods are useful to document, but derive their value mainly from the fact that they allow the use of the completion/correction methods that are the primary contribution of the paper. While the idea of integrating the spelling error probability into the search for completions is a sound one, the specific details of the model being pursued feel very ad hoc, which diminishes the ultimate impact of these results. Specifically, estimating the log probability to be proportional to the number of edits in the Levenshtein distance is really not the right thing to do at all. Under such an approach, the unedited string receives probability one, which doesn't leave much additional probability mass for the other candidates -- not to mention that the number of possible misspellings would require some aggressive normalization.   Even under the assumption that a normalized edit probability is not particularly critical (an issue that was not raised at all in the paper, let alone assessed), the fact is that the assumptions of independent errors and a single substitution cost are grossly invalid in natural language. For example, the probability p_1 of 'pkoe' versus p_2 of 'zoze' as likely versions of 'poke' (as, say, the prefix of pokemon, as in your example) should be such that p_1 >>> p_2, not equal as they are in your model. Probabilistic models of string distance have been common since Ristad and Yianlios in the late 90s, and there are proper probabilistic models that would work with your same dynamic programming algorithm, as well as improved models with some modest state splitting. And even with very simple assumptions some unsupervised training could be used to yield at least a properly normalized model. It may very well end up that your very simple model does as well as a well estimated model, but that is something to establish in your paper, not assume. That such shortcomings are not noted in the paper is troublesome, particularly for a conference like ICLR that is focused on learned models, which this is not.  As the primary contribution of the paper is this method for combining correction with completion, this shortcoming in the paper is pretty serious. Some other comments:  Your presentation of completion cost versus edit cost separation in section 3.3 is not particularly clear, partly since the methods are discussed prior to this point as extension of (possibly corrected) prefixes. In fact, it seems that your completion model also includes extension of words with end point prior to the end of the prefix -- which doesn't match your prior notation, or, frankly, the way in which the experimental results are described. The notation that you use is a bit sloppy and not everything is introduced in a clear way. For example, the s_0:m notation is introduced before indicating that s_i would be the symbol in the i_th position (which you use in section 3.3). Also, you claim that s_0 is the empty string, but isn't it more correct to model this symbol as the beginning of string symbol? If not, what is the difference between s_0:m and s_1:m? If s_0 is start of string, the s_0:m is of length m+1 not length m. You spend too much time on common, well-known information, such as the LSTM equations. (you don't need them, but also why number if you never refer to them later? )  Also the dynamic programming for Levenshtein is foundational, not required to present that algorithm in detail, unless there is something specific that you need to point out there (which your section 3.3 modification really doesn't require to make that point). Is there a specific use scenario for the prefix splitting, other than for the evaluation of unseen prefixes? This doesn't strike me as the most effective way to try to assess the seen/unseen distinction, since, as I understand the procedure, you will end up with very common prefixes alongside less common prefixes in your validation set, which doesn't really correspond to true 'unseen' scenarios. I think another way of teasing apart such results would be recommended. You never explicitly mention what your training loss is in section 5.1. Overall, while this is an interesting and important problem, and the engineering details are interesting and reasonably well-motivated, the main contribution of the paper is based on a pretty flawed approach to modeling correction probability, which would limit the ultimate applicability of the methods.",29,823,34.291666666666664,5.047798742138365,331,8,815,0.0098159509202454,0.0311750599520383,0.9928,220,82,136,72,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 24, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 3}",0,1,4,2,0,24,2,2,0,0,0,0,0,0,1,1,0,0,2,0,19,1,3,0.4347685084419017,0.6780857395593516,0.30543115598070847
ICLR2018-By3VrbbAb-R2,Reject,"This paper focuses on solving query completion problem with error correction which is a very practical and important problem.  The idea is character based. And in order to achieve three important targets which are auto completion, auto error correction and real time, the authors first adopt the character-level RNN-based modeling which can be easily combined with error correction, and then carefully optimize the inference part to make it real time. Pros: (1) the paper is very well organized and easy to read . (2) the proposed method is nicely designed to solve the specific real problem. For example, the edit distance is modified to be more consistent with the task. (3) detailed information are provided about the experiments, such as data, model and inference. Cons: (1) No direct comparisons with other methods are provided. I am not familiar with the state-of-the-art methods in this field. If the performance (hit rate or coverage) of this paper is near stoa methods, then such experimental results will make this paper much more solid",9,169,16.9,5.134969325153374,101,0,169,0.0,0.0058479532163742,0.6757,47,23,31,13,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,1,2,1,1,4,1,1,0,0,0,1,0,1,0,0,0,2,0,0,2,1,1,0.6436882071008808,0.4451835644364774,0.3674363364958724
ICLR2018-By3VrbbAb-R3,Reject,"The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance. The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors. Details are also given on how the authors are able to achieve realtime completion .  Overall, it's nice a nice study of the query completion application. The paper is well explained, and it's also nice that the runtime is shown for each of the algorithm blocks. Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks. The final dataset is also a good size (36M search queries). My major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments. Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas. But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates. So the paper feels directed to an audience with less background in neural net LMs. Secondly, the experiments could have more thorough/stronger baselines. I don't really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data. The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers. More regularization required?",15,297,17.470588235294116,5.2631578947368425,171,2,295,0.0067796610169491,0.0167224080267558,0.9432,86,38,51,21,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 3, 'MET': 9, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 1}",0,1,3,0,3,9,1,0,0,1,0,1,0,1,1,0,0,0,0,0,6,2,1,0.5736632656234038,0.4476609081269808,0.32472578217494497
ICLR2018-By3v9k-RZ-R1,Reject,"The authors propose the N-Gram machine to answer questions over long documents. The model first encodes the document via tuple extraction. An autoencoder objective is used to produce meaningful tuples. Then, the model generates a program, based on the extracted tuple collection and the question, to find an answer. I am very disappointed in the authors' choice of evaluation, namely bAbI - a toy, synthetic task long abandoned by the NLP community because of its lack of practicality. If the authors would like to demonstrate question answering on long documents, they have the luxury of choosing amongst several large scale, realistic question answering datasets such as the Stanford Question answering dataset or TriviaQA. Beyond the problem of evaluation, the model the authors propose does not provide new ideas, and rather merges existing ones. This, in itself, is not a problem . However, the authors decline to cite many, many important prior work. For example, the tuple extraction described by the authors has significant prior work in the information retrieval community (e.g. knowledge base population, relation extraction).  The idea of generating programs to query over populated knowledge bases, again, has significant related work in semantic parsing and program synthesis. Question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact I believe Matt Gardner wrote his entire thesis on this topic). Finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years. Non of these areas, with the exception of semantic parsing, are addressed by the author. With sufficient knowledge of related works from these areas, I find that the authors' proposed method lacks proper evaluation and sufficient novelty.",13,277,17.3125,5.501845018450185,161,0,277,0.0,0.0142857142857142,0.2418,90,40,46,15,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 3, 'DAT': 1, 'MET': 6, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 0}",0,1,5,3,1,6,2,0,0,0,0,0,0,0,0,2,4,1,0,0,1,1,0,0.430349136265308,0.5557447730854174,0.2720690468626212
ICLR2018-By3v9k-RZ-R2,Reject,"This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations ( -grams) which can be queried efficiently. The authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate NGMs on five of the 20 bAbI tasks. I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA. However, I have some concerns about the specific implementation and model discussed here. How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose QA model for natural language? Addressing some of these issues would likely prevent scaling to millions of (real) sentences, as the scalability is reliant on programs being efficiently executed (by simple string matching) against a knowledge storage. The paper is missing a clear analysis of NGM's limitations... the examples of knowledge storage from bAbI in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simple. In its current form, I am borderline but leaning towards rejecting this paper. Other questions: - is  -gram really the most appropriate term to use for the symbolic representation? N-grams are by definition contiguous sequences... The authors may want to consider alternatives. - why focus only on extractive QA? The evaluations are only conducted on 5 of the 20 bAbI tasks, so  it is hard to draw any conclusions from the results as to the validity of this approach. Can the authors comment on how difficult it will be to add functions to the list in Table 2 to handle the other 15 tasks? Or is NGM strictly for extractive QA? - beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in Figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny RNN sizes used in the paper. How long does the encoding time take with 10 million sentences? - Need more detail on the programmer architecture, is it identical to the one used in Liang et al., 2017? ",17,405,28.928571428571427,5.077519379844961,216,4,401,0.0099750623441396,0.0145278450363196,0.8231,121,43,66,20,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 9, 'SUB': 3, 'CLA': 0}",0,1,1,1,0,12,2,1,1,2,0,1,1,0,0,0,0,0,2,1,9,3,0,0.7171487427096706,0.4496950258636344,0.4076669130485352
ICLR2018-By3v9k-RZ-R3,Reject,"The paper presents an interesting framework for bAbI QA. Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences). The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index. While the argument makes sense, it is not clear to me why one cannot simply index the original text. The additional encode/decode mechanism seems to introduce unnecessary noise. The framework does include several components and techniques from latest recent work, which look pretty sophisticated. However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven. Pros:   1. An interesting framework for bAbI QA by encoding sentence to n-grams Cons:   1. The overall justification is somewhat unclear 2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data ",10,167,15.181818181818182,5.453416149068323,109,1,166,0.0060240963855421,0.0406976744186046,0.9404,44,26,24,13,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 7, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,1,1,1,7,0,0,0,0,0,1,0,0,0,0,0,0,0,0,6,1,1,0.4300725812541365,0.3364431205075482,0.2126610182023221
ICLR2018-By4HsfWAZ-R1,Accept,"The paper 'Deep learning for Physical Process: incorporating prior physical knowledge' proposes to question the use of data-intensive strategies such as deep learning in solving physical  inverse problems that are traditionally solved through assimilation strategies. They notably show how physical priors on a given phenomenon can be incorporated in the learning process and propose  an application on the problem of estimating sea surface temperature directly from a given  collection of satellite images. All in all the paper is very clear and interesting. The results obtained on the considered problem are clearly of great interest, especially when compared to state-of-the-art assimilation strategies such as the one of Bu00e9ru00e9ziat. While the learning architecture is not original in itself, it is  shown that a proper physical regularization greatly improves the performance. For these reasons I  believe the paper has sufficient merits to be published at ICLR.  That being said, I believe that  some discussions could strengthen the paper:  - Most classical variational assimilation schemes are stochastic in nature, notably by incorporating uncertainties in the observation or physical evolution models. It is still unclear how those uncertainties  can be integrated in the model ;  - Assimilation methods are usually independent of the type of data at hand. It is not clear how the model learnt on one particular type of data transpose to other data sequences.  Notably, the question of transfer and generalization is of high relevance here. Does the learnt model performs well on other dataset (for instance, acquired on a different region or at a distant time).  I believe this type of issue has to be examinated  for this type of approach to be widely use in inverse physical problems.",14,275,22.916666666666668,5.49812734082397,146,1,274,0.0036496350364963,0.0309278350515463,0.9153,77,37,45,16,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,1,1,2,7,2,1,0,0,0,2,0,0,1,0,1,1,0,0,7,0,1,0.5730974294509321,0.5592873001646134,0.3576000565079904
ICLR2018-By4HsfWAZ-R2,Accept,"In this paper, the authors show how a Deep Learning model for sea surface temperature prediction can be designed to incorporate the classical advection diffusion model. The architecture includes a differentiable warping scheme which allows back propagation of the error and is inspired by the fundamental solution of the PDE model.  They evaluate the suggested model on synthetic data and outperform the current state of the art in terms of accuracy. pros - the paper is written in a clear and concise manner - it suggests an interesting connection between a traditional model and Deep Learning techniques n- in the experiments they trained the network on 64 x 64 patches and achieved convincing results cons - please provide the value of the diffusion coefficient for the sake of reproducibility - medium resolution of the resulting prediction I enjoyed reading this paper and would like it to be accepted. minor comments: - on page five in the last paragraph there is a left parenthesis missing in the inline formula nabla dot w_t(x))^2. - on page nine in the last paragraph there is the word 'flow' missing: '.. estimating the optical [!] between 2 [!] images.' - in the introduction (page two) the authors refer to SST prediction as a 'relatively complex physical modeling problem', whereas in the conclusion (page ten) it is referred to as 'a problem of intermediate complexity'. This seems to be inconsistent.",12,224,28.0,5.308056872037914,127,2,222,0.009009009009009,0.025531914893617,0.9214,75,27,34,3,9,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 2, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 3, 'REC': 1, 'EMP': 1, 'SUB': 0, 'CLA': 2}",0,2,2,0,1,6,1,2,2,1,0,2,0,0,0,0,0,2,3,1,1,0,2,0.6443385289192323,0.5558769910312062,0.4028520327573867
ICLR2018-By4HsfWAZ-R3,Accept,"The authors use deep learning to learn a surrogate model for the motion vector in the advection-diffusion equation that they use to forecast sea surface temperature. In particular, they use a CNN encoder-decoder to learn a motion field, and a warping function from the last component to provide forecasting. I like the idea of using deep learning for physical equations. I would like to see a description of the algorithm with the pseudo-code in order to understand the flow of the method. I got confused at several points because it was not clear what was exactly being estimated with the CNN. Having an algorithmic environment would make the description easier.  I know that authors are going to publish the code, but this is not enough at this point of the revision. Physical processes in Machine learning have been studied from the perspective of Gaussian processes. Just to mention a couple of references ""Linear latent force models using Gaussian processes"" and Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations   In Theorem 2, do you need to care about boundary conditions for your equation?  I didn't see any mention to those in the definition for I(x,t). You only mention initial conditions. How do you estimate the diffusion parameter D? Are you assuming isotropic diffusion? Is that realistic?  Can you provide more details about how you run the data assimilation model in the experiments? Did you use your own code?",13,239,21.727272727272727,5.238938053097345,132,0,239,0.0,0.0122950819672131,0.8135,69,26,46,7,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,1,1,1,10,1,0,0,0,0,0,0,0,0,0,0,1,0,1,8,0,0,0.4308231575954905,0.3376870353772341,0.21944237116785767
ICLR2018-By5SY2gA--R1,Reject,"This paper proposed to use affect lexica to improve word embeddings. They extended the training objective functions of Word2vec and Glove with the affect information. The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis.  Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove. In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups. Some detailed comments are: -tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new. Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016. -tAlthough the lexicons for valence, arousal, and dominance provide different information, their combination did not perform best. Do the authors have any intuition why? -tIn Figure 2, the authors picked four words to show that valence is helpful to improve Glove word beddings. It is not convincing enough for me. I would like to see to the top k nearest neighbors of each of those words. ",11,203,11.941176470588236,5.411167512690355,123,0,203,0.0,0.0195121951219512,0.9657,66,27,33,10,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 3, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,2,2,0,5,3,2,1,0,0,1,1,0,0,2,0,1,0,0,4,0,0,0.644225792552648,0.3352443132223804,0.32498576855499584
ICLR2018-By5SY2gA--R2,Reject,"This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm. The affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment. The justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me: - The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives). - Affect in language seems to me to be a very contextual phenomenon. Only a tiny subset of words have intrinsic and context-free affect. Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'... The model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements. However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful. It is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text. It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails. To understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines. For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?",13,357,25.5,5.348703170028818,185,5,352,0.0142045454545454,0.0583333333333333,0.972,94,47,61,26,4,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,12,1,0,0,0,0,0,0,0,0,1,0,1,0,0,11,0,0,0.2884663989659169,0.339552907681763,0.14058301740075468
ICLR2018-By5SY2gA--R3,Reject,"This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings. The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection. A considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning. The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work. The main originality seems to be captured in Algorithm 1, which computes the strength between two words. Unlike prior work, this is a real-valued instead of a binary quantity. Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques. In this light, the performance relative to the baselines is particularly important. From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods.  I therefore cannot recommend this paper for publication.",10,207,20.7,5.9109947643979055,127,1,206,0.0048543689320388,0.0384615384615384,-0.0417,64,20,35,12,5,6,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 1, 'SUB': 1, 'CLA': 0}",0,1,5,0,0,4,0,0,1,0,0,1,0,0,0,2,1,4,0,1,1,1,0,0.358215385093747,0.6670632619227548,0.2470859281293453
ICLR2018-By5ugjyCb-R1,Reject,"The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate . I do think that it is a solid contribution with thorough experiments. I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper. It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function.  _______________ ORIGINAL REVIEW:  This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations. It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded. The experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model. Related to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%. Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy . Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN. OTHER COMMENTS:  - the list of contributions is a bit strange . It seams that the true contribution is number 1 on the list, which is to introduce the parameter alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function. To provide an analysis of why it works and quantitative results, is part of the same contribution I would say.",13,374,28.76923076923077,5.402816901408451,159,1,373,0.002680965147453,0.0078534031413612,0.5149,116,33,68,13,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 0, 'DAT': 1, 'MET': 5, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,5,0,1,5,3,3,0,5,0,2,0,0,0,1,3,4,0,1,5,0,0,0.5017508117640265,0.5584909462633931,0.3201344771665605
ICLR2018-By5ugjyCb-R2,Reject,"The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter. Then,  PACT is combined with quantizing the activations. The proposed technique sounds. The performance improvement is expected and validated by experiments. But I am not sure if the novelty is strong enough for an ICLR paper.  ",5,53,8.833333333333334,5.596153846153846,39,0,53,0.0,0.0714285714285714,0.7841,15,6,14,4,3,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,2,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0.2145952062613547,0.3333333333333333,0.10258156660067651
ICLR2018-By5ugjyCb-R3,Reject,"This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. The experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too. Overall the paper is a descent one, but with limited novelty. I am a weak reject",7,96,13.714285714285714,5.043478260869565,57,0,96,0.0,0.0104166666666666,-0.1779,26,11,22,4,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 0, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 0, 'SUB': 1, 'CLA': 0}",0,1,0,2,0,0,1,2,0,1,0,2,0,0,0,2,0,0,0,1,0,1,0,0.4287318491340443,0.3333784409178515,0.21784709932271135
ICLR2018-By9iRkWA--R1,Reject,This paper introduces a fairly elaborate model for reading comprehension evaluated on the SQuAD dataset.  The model is shown to improve on the published results but not as-of-submission leaderboard numbers. The main weakness of the paper in my opinion is that the innovations seem to be incremental and not based on any overarching insight or general principle. A less significant issue is that the English is often disfluent. Specific comments: I would remove the significance daggers from table 2 as the standard deviations are already reported and the null hypothesis for which significance is measured seems unclear. I am also concerned to see test performance significantly better than development performance in table 3. Other systems seem to have development and test performance closer together.  Have the authors been evaluating many times on the test data?,8,135,16.875,5.5271317829457365,91,3,132,0.0227272727272727,0.0364963503649635,0.6869,37,16,28,10,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 1, 'EXP': 1, 'RES': 4, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,1,1,0,1,1,1,4,2,0,0,1,0,0,0,0,1,3,1,0,2,1,1,0.5716189750674064,0.6675229492158895,0.39524239731095634
ICLR2018-By9iRkWA--R2,Reject,"Summary: The paper introduces Phase Conductor, which consists of two phases, context-question attention phase and context-context (self) attention phase. Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector. The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention. Note: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard. Pros: - The model achieves SOTA on SQuAD among published papers. - The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel. Visual analysis of the attention map is convincing. - The paper is overall well-written and clear. Cons: - Using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models, and should cite relevant papers. For instance, Memory Networks [1] uses different embedding for key (computing attention weight) and value (computing attended vector). - While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt. For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer. While this may be statistically significant, it is a very small gain nonetheless. - Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art. I think this is a valuable engineering contribution,; but I feel that it is not well-suited / sufficient for ICLR audience. Questions: - page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor? Errors: - page 2 last para: gives an concrete -> gives a concrete - page 2 last para: matching -> matched Figure 1: I think passage embedding h and question embedding v boxes should be switched. - page 7 3.3 first para: evidence fully -> evidence to be fully. [1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015.",19,398,19.9,5.483695652173913,183,4,394,0.0101522842639593,0.0194174757281553,0.9673,126,55,68,24,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 0, 'RES': 4, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 2, 'EXT': 1}","{'APR': 1, 'NOV': 2, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,1,2,1,1,6,0,4,3,0,0,3,2,1,1,2,1,1,0,0,7,0,2,0.7159340949775016,0.6705246539310967,0.5050691218232018
ICLR2018-By9iRkWA--R3,Reject,"This paper proposes a new machine comprehension model, which integrates several contributions like different embeddings for gate function and passage representation function, self-attention layers and highway network based fusion layers. The proposed method was evaluated on the SQuAD dataset only, and marginal improvement was observed compared to the baselines. (1) One concern I have for this paper is about the evaluation. The paper only evaluates the proposed method on the SQuAD data with systems submitted in July 2017, and the improvement is not very large. As a result, the results are not suggesting significance or generalizability of the proposed method. (2) The paper gives some ablation tests like reducing the number of layers and removing the gate-specific question embedding, which help a lot for understanding how the proposed methods contribute to the improvement. However, the results show that the deeper self-attention layers are indeed useful (but still not improving a lot, about 0.7-0.8%). The other proposed components contribute less significant. As a result, I suggest the authors add more ablation tests regarding (1) replacing the outer-fusion with simple concatenation (it should work for two attention layers); (2) removing the inner-fusion layer and only use the final layer's output, and using residual connections (like many NLP papers did) instead of the more complicated GRU stuff. (3) Regarding the ablation in Table 2, my first concern is that the improvement seems small (~0.5%).  As a result, I am wondering whether this separated question embedding really brings new information, or the similar improvement can be achieved by increasing the size of LSTM layers. For example, if we use the single shared question embeddings, but increase the size from 128 to some larger number like 192, can we observe similar improvement. I suggest the authors try this experiment as well and I hope the answer is no, as separated input embeddings for gate functions was verified to be useful in some old works with syntactic features as gate values, like Semantic frame identification with distributed word representations and Learning composition models for phrase embeddings etc. (4) Please specify which version of the SQuAD leaderboard is used in Table 3. Is it a snapshot of the Jul 14 one? Because this paper is not comparing to the state-of-the-art, no specification of the leaderboard version may confuse the other reviewers and readers.  By the way, it will be better to compare to the snapshot of Oct 2017 as well, indicating the position of this work during the submission deadline. Minor issues:  (1) There are typos in Figure 1 regarding the notations of Question Features and Passage Features. (2) In Figure 1, I suggest adding an N times symbol to the left of the Q-P Attention Layer and remove the current list of such layers, in order to be consistent to the other parts of the figure. (3) What is the relation between the PhaseCond, QPAtt+b in Table 2 and the PhaseCond in Table 3? I was assuming that those are the same system but did not see the numbers match each other.",20,506,26.63157894736842,5.2601279317697225,236,2,504,0.0039682539682539,0.0216110019646365,0.995,151,52,86,18,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 4, 'MET': 5, 'EXP': 4, 'RES': 4, 'TNF': 5, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,1,4,0,4,5,4,4,5,2,0,0,0,0,0,0,2,4,2,0,8,1,0,0.5733226800872396,0.5603703508432775,0.3614804820586465
ICLR2018-ByBAl2eAZ-R1,Accept,"This paper explores the idea of adding parameter space noise in service of exploration. The paper is very well written and quite clear. It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies. However, the results are weak. Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains. The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well. E-greedy approaches will always struggle to choose the same random action repeatedly. Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns. Similarly for the continuous control with sparse rewards environments u2013 if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning. However, for complex domains with sparse reward (e.g. Montezuma's Revenge) parameter space noise is just not going to get you very far. Overall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that. However, I don't expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn't really gain you much more than policies that are biased towards particular actions. Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise. A non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO. ",14,308,19.25,5.260273972602739,158,1,307,0.003257328990228,0.0161290322580645,0.992,91,46,48,33,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,0,0,1,0,6,0,4,0,3,0,1,1,0,0,0,1,0,0,0,9,0,1,0.4300389335797128,0.3383089928120771,0.21834048635274814
ICLR2018-ByBAl2eAZ-R2,Accept,"In recent years there have been many notable successes in deep reinforcement learning. However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem. For off-policy algorithms it is common to explore by adding noise to the policy action in action space, while on-policy algorithms are often regularized in the action space to encourage exploration. This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets. This work argues this results in more consistent exploration and compares this approach empirically on a range of continuous and discrete tasks. By using layer norm and adaptive noise, they are able to generate robust parameter noise (it is often difficult to estimate the appropriate variance of parameter noise, as its less clear how this relates to the magnitude of variance in the action space). This work is well-written and cites previous work appropriately. Exploration is an important topic, as it often appears to be the limiting factor of Deep RL algorithms. The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards. One empirical baseline that would helpful to have would be a stochastic off-policy algorithm (both off-policy algorithms compared are deterministic), as this may better capture uncertainty about the value of actions (e.g. SVG(0) [3]). As with any empirical results with RL, it is a challenging problem to construct comparable benchmarks due to minor variations in implementation, environment or hyper-parameters all acting as confounding variables [1]. It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication. As one example, figure 4 of [1] seems to show DDPG performing much better than the DDPG baseline in this work on half-cheetah. Minor points: - The definition of a stochastic policy (section 2) is unusual (it is defined as an unnormalized distribution). Usually it would be defined as $mathcal{S} rightarrow mathcal{P}(mathcal{A})$ - This work extends DQN to learn an explicitly parametrized policy (instead of the greedy policy) in order to useful perturb the parameters of this policy. Instead of using a single greedy target, you could consider use the relationship between the advantage function and an entropy-regularized policy [2] to construct a target. [1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.  [2] O'Donoghue, B., Munos, R., Kavukcuoglu, K., & Mnih, V. (2016). Combining policy gradient and Q-learning. [3] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015). Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952).",23,464,16.0,5.699530516431925,230,7,457,0.0153172866520787,0.0148936170212765,0.9866,162,69,67,19,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 6, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,1,2,0,0,12,2,4,1,0,0,1,6,3,0,0,0,1,0,0,9,0,1,0.6460236666414957,0.3383089928120771,0.32840628464831034
ICLR2018-ByBAl2eAZ-R3,Accept,"This paper proposes a method for parameter space noise in exploration. Rather than the baseline epsilon-greedy (that sometimes takes a single action at random)... this paper presents an method for perturbations to the policy. In some domains this can be a much better approach and this is supported by experimentation. There are several things to like about the paper: - Efficient exploration is a big problem for deep reinforcement learning (epsilon-greedy or Boltzmann is the de-facto baseline) and there are clearly some examples where this approach does much better. - The noise-scaling approach is (to my knowledge) novel, good and in my view the most valuable part of the paper. - This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks. - The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up. - I like the algorithm, it's pretty simple/clean and there's something obviously *right* about it (in SOME circumstances). However, there are also a few things to be cautious of... and some of them serious: - At many points in the paper the claims are quite overstated. Parameter noise on the policy won't necessarily get you efficient exploration... and in some cases it can even be *worse* than epsilon-greedy... if you just read this paper you might think that this was a truly general statistically efficient method for exploration (in the style of UCRL or even E^3/Rmax etc). - For instance, the example in 4.2 only works because the optimal solution is to go right in every timestep... if you had the network parameterized in a different way (or the actions left/right were relabelled) then this parameter noise approach would *not* work... By contrast, methods such as UCRL/PSRL and RLSVI https://arxiv.org/abs/1402.0635 *are* able to learn polynomially in this type of environment. I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of deep exploration and you should be clear that your parameter noise does *not* address this issue. - That said I think that the example in 4.2 is *great* to include... you just need to be more upfront about how/why it works and  what you are banking on with the parameter-space exploration. Essentially you perform a local exploration rule in parameter space... and sometimes this is great - but you should be careful to distinguish this type of method from other approaches. This must be mentioned in section 4.2 does parameter space noise explore efficiently because the answer you seem to imply is yes ... when the answer is clearly NOT IN GENERAL... but it can still be good sometimes ;D - The demarcation of RL and evolutionary strategies suggests a pretty poor understanding of the literature and associated concepts. I can't really support the conclusion RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually. This sort of sentence is clearly wrong and for many separate reasons:     - Parameter noise exploration is not a separate/new thing from RL... it's even been around for ages! It feels like you are talking about DQN/A3C/(whatever algorithm got good scores in Atari last year) as RL and that's just really not a good way to think about it. - Parameter noise exploration can be *extremely* bad relative to efficient exploration methods (see section 2.4.3 https://searchworks.stanford.edu/view/11891201)    Overall, I like the paper, I like the algorithm and I think it is a valuable contribution.  I think the value in this paper comes from a practical/simple way to do policy randomization in deep RL. In some (maybe even many of the ones you actually care about) settings this can be a really great approach, especially when compared to epsilon-greedy. However, I hope that you address some of the concerns I have raised in this review. You shouldn't claim such a universal revolution to exploration / RL / evolution because I don't think that it's correct. Further, I don't think that clarifying that this method is *not* universal/general really hurts the paper... you could just add a section in 4.2 pointing out that the chain example wouldn't work if you needed to do different actions at each timestep (this algorithm does *not* perform deep exploration). I vote accept.",30,699,18.89189189189189,5.100903614457831,289,15,684,0.0219298245614035,0.0304287690179806,0.9983,184,87,116,65,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 23, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 7, 'PNF': 1, 'REC': 1, 'EMP': 16, 'SUB': 0, 'CLA': 1}",0,1,4,0,0,23,2,2,0,0,0,5,0,1,0,1,0,7,1,1,16,0,1,0.506069263127035,0.6766990035324512,0.35529334709022103
ICLR2018-ByCPHrgCW-R1,Reject,"This paper proposes a hybrid Homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm. The paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it. It is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear. The results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the paper. For example, reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext, whenever the number of 1s in the plain bit-string is greater than the number of 0s (3.4/Page 6). Result interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets used. Overall, fine work, well organized, decomposed, and its rationale clearly explained. The good results obtained support the design decisions made. Our main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims. Nota:       - In Figure 4/Page 4: AND Table A(1)/B(0), shouldn't  A And B be 0? - Unlike Figure 3/Page 3, in Figure 2/page 2, shouldn't  operations' precedence prevail (No brackets), therefore 1+2*2 5?",11,227,22.7,5.537735849056604,118,0,227,0.0,0.0168067226890756,0.9905,69,18,39,13,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 4, 'EXP': 0, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 2}",0,1,1,0,1,4,0,3,2,0,0,1,0,0,0,1,0,1,2,0,5,0,2,0.5008852780874795,0.5581860892895849,0.31663714194316356
ICLR2018-ByCPHrgCW-R2,Reject,"Summary: This paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping. The main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain. This hybrid argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time. This paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference. Comments: 1. I recommend the authors to tone down their claims. For example, the authors mentioned that there has been no complete implementation of established deep learning approaches in the abstract, however, the authors did not define what is complete. Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server. Also, the claim that we show efficient designs is very thin to me since there are no experimental comparisons between the proposed method and existing works. Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C].  For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE. 2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better. I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit. 3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper. Such optimizations are very common in deep learning field while less known in the field of security. [A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. [B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications. [C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations.",17,350,14.583333333333334,5.237237237237237,179,2,348,0.0057471264367816,0.017094017094017,0.9648,117,49,58,17,8,3,"{'ABS': 1, 'INT': 1, 'RWK': 7, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 3, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 1}",1,1,7,1,0,10,1,0,0,0,0,0,3,1,0,0,0,2,0,0,9,0,1,0.5741880138766119,0.3384261553692671,0.2912811731041433
ICLR2018-ByCPHrgCW-R3,Reject,"The paper presents a means of evaluating a neural network securely using homomorphic encryption . A neural network is already trained, and its weights are public. The network is to be evaluated over a private input, so that only the final outcome of the computation-and nothing but that-is finally learned. The authors take a binary-circuit approach: they represent numbers via a fixed point binary representation, and construct circuits of secure adders and multipliers, based on homomorphic encryption as a building block for secure gates. This allows them to perform the vector products needed per layer; two's complement representation also allows for an easy implementation of the ReLU activation function, by checking (multiplying by) the complement of the sign bit. The fact that multiplication often involves public weights is used to speed up computations, wherever appropriate. A rudimentary  experimental evaluation with small networks is provided. All of this is somewhat straightforward; a penalty is paid by representing numbers via fixed point arithmetic, which is used to deal with ReLU mostly. This is somewhat odd: it is not clear why, e.g., garbled circuits where not used for something like this, as it would have been considerably faster than FHE. There is also a work in this area that the authors do not cite or contrast to, bringing the novelty into question; please see the following papers and references therein:; GILAD-BACHRACH, R., DOWLIN, N., LAINE, K., LAUTER, K., NAEHRIG, M., AND WERNSING, J. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In Proceedings of The 33rd International Conference on Machine Learning (2016), pp. 201u2013210. SecureML: A System for Scalable Privacy-Preserving Machine Learning Payman Mohassel and Yupeng Zhang. SHOKRI, R., AND SHMATIKOV, V. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (2015), ACM, pp. 1310u20131321. The first paper is the most related, also using homomorphic encryption, and seems to cover a superset of the functionalities presented here (more activation functions, a more extensive analysis, and faster decryption times). The second paper uses arithmetic circuits rather than HE, but actually implements training an entire neural network securely. Minor details:  The problem scenario states that the model/weights is private, but later on it ceases to be so (weights are not encrypted). Both deep learning and FHE are relatively recent paradigms. Deep learning is certainly not recent, while Gentry's paper is now 7 years old. In theory, this system alone could be used to compute anything securely. This is informal and incorrect. Can it solve the halting problem? However in practice the operations were incredibly slow, taking up to 30 minutes in some cases. It is unclear what operations are referred to here.",21,447,16.555555555555557,5.488095238095238,247,2,445,0.0044943820224719,0.0155555555555555,0.7723,132,54,73,40,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 5, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,10,0,0,10,1,0,0,0,0,0,5,0,0,0,0,3,0,0,4,0,1,0.3601685493077241,0.3354335307522422,0.18244524299170034
ICLR2018-ByED-X-0W-R1,Reject,"This paper proposes a learning method (PIB) based on the information bottleneck framework. PIB pursues the very natural intuition outlined in the information bottleneck literature: hidden layers of deep nets compress the input X while maintaining sufficient information to predict the output Y. It should be noted that the limitations of the IB for deep learning are currently under heavy discussion on OpenReview. Optimizing the PIB objective is intractable and the authors propose an approximation that applies to binary valued stochastic networks. They use a variational bound to deal with the relevance term, I(Z_l,Y), and  Monte Carlo sampling to deal with the layer-by-layer compression term, I(Z_l,Z_{l+1}). They present results on MNIST aiming to demonstrate that using PIBs improves generalization and training speed. This is a timely and interesting topic. I enjoyed learning about the authors' proposed approach to a practical learning method based on the information bottleneck. However, the writing made it challenging and the experimental protocol raised some serious questions. In summary, I think the paper needs very careful editing for grammar and language and, more importantly, it needs solid experiments before it's ready for publication. When that is done it would make an exciting contribution to the community. More details follow.   Comments: 1. All architectures and objectives (both classic and PIB-based) are trained using a single, fixed learning rate (LR). In my opinion, this is a red flag. The PIB objective is new and different to the other objectives. Do all objectives happen to yield their best performance under the same LR? Maybe so, but we won't know unless the experimental protocol prescribes a sufficient range of LRs for each architecture. In light of this, the fact that SFNN is given extra epochs in Figure 4 does not mean much. 2. The batch size for MNIST classification is unusually low (8) . Common batch sizes range from 64 to 1K (typically >  128). Why did the authors make this choice? Is 8 good for architectures A through E? 3. On a related note, the authors only seem to report results from a single random seed (ie. deterministic architectures are trained exactly once). I would like to see results from a few different random seeds. As a result of comments 1,2,3, even though I do believe in the merit of the intuition pursued and the techniques proposed, I am not convinced about the main claim of the paper. In particular, the experiments are not rigorous enough to give serious evidence that PIBs improve generalization and training speed. 4. The paper needs some careful editing both for language (cf. following point) but also notation. The authors use notation p_D() in eqn (12) without defining it. My best guess is that it is the same as p_u(), the underlying data distribution, but makes parsing the paper hard. Finally there are a few steps that are not explained: for example, no justification is given for the inequality in eqn (13). 5. Language: the paper needs some careful editing to correct numerous language/grammar issues. At times it is detrimental to understanding. For example I had to read the text leading up to eqn (8) a number of times. 6. There is no discussion of computational complexity and wall-clock time comparisons. To be clear, I think that even if the proposed approach were to be slower than the state of the art it would still be very interesting. However, there should be some discussion and reporting of that aspect as well. Minor comments and questions: 7. Mutual information is typically typeset using a semicolon instead of a comma, eg. I(X;Z). 8. Why is the mutual information in Figure 3 so low? Are you perhaps using natural logarithms to estimate and plot I(Z;Y)? If this is base-2 logarithms I would expect a value close to 1. ",39,629,13.382978723404255,5.17039586919105,297,5,624,0.0080128205128205,0.0345911949685534,0.9905,171,75,121,37,11,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 5, 'MET': 13, 'EXP': 6, 'RES': 3, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 3, 'REC': 0, 'EMP': 15, 'SUB': 3, 'CLA': 7}",0,2,2,2,5,13,6,3,2,1,0,6,0,1,0,0,0,3,3,0,15,3,7,0.7898190354777526,0.5653205860471111,0.4939988757046466
ICLR2018-ByED-X-0W-R2,Reject,"This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck. A new training objective is defined as a sum of mutual informations (MI) between the successive stochastic hidden layers plus a sum of mutual informations between each layer and the relevance variable. The idea is interesting and to my knowledge novel. Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward. It would have been interesting to compare not only with SFNN but also to a model with the same architecture and same gradient estimator (Raiko et al. 2014) using maximum likelihood. This would allow to disentangle the impact of the learning mechanism from the impact of the learning objective. Why is it important to maximise I(X_l, Y) for every layer?  Does that impact the MI of the final layer and Y? To estimate the MI between a hidden layer and the relevance variable, a multilayer generalisation of the variational bound from Alemi et al. 2016. Computation of the bound requires integration over multiple layers (equation 15). How is this achieved in practice? With high-dimensional hidden layers a Monte-Carlo estimate on the minibatch can be very noisy and the resulting estimation of MI could be poor. Mutual information between the successive layers is decomposed as an entropy plus a conditional entropy term (eq 17). How is the conditional entropy term estimated? The entropy term is first bounded by conditioning on the previous layer and then estimated using Monte Carlo sampling with a plug-in estimator. Plug-in estimators are known to be inefficient in high dimensions even using a full dataset unless the number of samples is very large. It thus seems challenging to use mini batch MC, how does the mini batch estimation compare to an estimation using the full dataset?  What is the variance of the mini batch estimate? In the related work section, the IB problem can also be solved efficiently for meta-Gaussian distribution as explained in Rey et al. 2012 (Meta-gaussian information bottleneck). There is a small typo in (eq 5). ",19,354,19.666666666666668,5.308383233532934,168,1,353,0.0028328611898017,0.011204481792717,0.8648,105,47,56,15,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 3, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 1}",0,1,4,1,3,8,2,2,0,0,0,1,0,0,0,0,0,1,0,0,11,0,1,0.5736354590964211,0.339552907681763,0.2930497915974899
ICLR2018-ByED-X-0W-R3,Reject,"# Paper overview: This paper views the learning process for stochastic feedforward networks through the lens of an iterative information bottleneck process; at each layer an attempt is made to minimise the mutual information (MI) with the feed-in layer while maximising the MI between that layer and the presumed-endogenous variable, 'Y'. Two propositions are made, (although I would argue that their derivations are trivially the consequence of the model structure and inference scheme defined), and experiments are run which compare the approach to maximum likelihood estimation for 'Y' using an equivalent stochastic network architecture. # Paper discussion: In general I like the idea of looking further into the effect of adding network structure on the original information bottleneck results (empirical and theoretical). I would be interested to see if layerwise input skip connections (i.e. between each network layer L_i and the original input variable 'X') hastened the 'compression' stage of learning e.g. (i.e. the time during which the intermediate layers minimise MI with 'X'). I'm also interested that clear examples of the information bottleneck principle in practice (e.g. CCA) are rarely mentioned. On the other hand, I think this paper is not quite ready: it reads like work written in a hurry, and is at times hard to follow as a result. There are several places where I think the terminology does not quite reflect what the authors perhaps hoped to express, or was otherwise slightly clumsy e.g: * ...self-consistent equations are highly non-linear and still too abstract to be used for many..., presumably what was implied was that the original solution to the information bottleneck as expressed by Tishby et al is non-analytic for most practical cases of interest? * Furthermore, we exploit the existing network architecture as variational decoders rather than resort to variational decoders that are not part of the neural network architecture.  -> The existing network architecture is used to provide a variational inference framework for I(Z,Y). * On average, 2H(X|Z) elements of X are mapped to the same code in Z.  In an ideal world I would like the assumptions required for this to hold true to be a fleshed out a little here. * The generated bottleneck samples are then used to estimate mutual information -> an empirical estimation of I(Z,X) would seem a very high variance estimator; the dimensionality of X is typically large in modern deep-learning problems---do you have any thoughts on how the learning process fares as this varies? Further on you cite that L_PIB is intractable due to the high dimensionality of the bottleneck variables, I imagine that this still yields a high var MC estimator in your approximation (in practice)? Was the performance significantly worse without the Raiko estimator? * In this experiment, we compare PIBs with .... -> I find this whole section hard to read, the description of how the models relate to each other is a little difficult to follow at first sight. * Information dynamics of learning process (Figures 3, 6, 7, 8) -> I am curious as to why you did not run the PIB for the same number of epochs as the SFNN? I would also argue that you did not run either method as long as you should have (both approaches lack the longer term 'compression' stage whereby layers near the input reduce I(X,Z_i) as compared to their starting condition)? This property is visible in I(Z_2,X) for PIB in Figure 3, but otherwise absent. # Conclusion: In conclusion, while interesting, for me the paper is not yet ready for publication. I would recommend this work for a workshop presentation at this stage. ",21,586,29.3,5.209386281588448,281,4,582,0.0068728522336769,0.0315091210613598,0.9637,190,67,99,35,9,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 3, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 1, 'EMP': 10, 'SUB': 2, 'CLA': 1}",0,1,3,2,0,12,2,3,2,3,0,3,0,0,1,0,0,4,1,1,10,2,1,0.6461907056905657,0.7838335588712559,0.5044249704271568
ICLR2018-ByJ7obb0b-R1,Reject,"[Main comments]  * The authors made a really odd choice of notation, which made the equations hard to follow. Apparently, that notation is used in differential geometry, but I have never seen it used in an ML paper. If you talk about outer product structure, show some outer products!  * The function f that the authors differentiate is not even defined in the main manuscript! * The low-rank structure they describe only holds for a single sample at a time. I don't see how this would be understanding low rank structure of deep networks as the title claims... What is described is basically an implementation trick. * Introducing cubic regularization seems interesting. However, either some extensive empirical evidence or some some theoretical evidence that this is useful are needed. The present paper has neither (the empirical evidence shown is very limited). [Other minor comments]  * Strictly speaking Adagrad has not been designed for Deep Learning. It is an online algorithm that became popular in the DL community later on. * Second derivatives should suffice for now, but of course if a use arose for third derivatives, calculating them would be a real option   That sentence seems useless. * Missing citation:  Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex Newton Step.  Yair Carmon, John Duchi. ",13,206,14.714285714285714,5.353535353535354,141,2,204,0.0098039215686274,0.0271493212669683,0.5644,55,34,47,15,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,0,0,1,0,11,2,0,0,0,0,0,1,1,0,0,0,0,0,0,8,1,1,0.3597309740724124,0.3376870353772341,0.1805294950592036
ICLR2018-ByJ7obb0b-R2,Reject,"Summary:  This paper shows the feedforward network (with ReLU activation functions in the hidden layers, softmax at the output, and cross entropy-loss) exhibits a low-rank derivative structure, which is able to use second-order information without approximating Hessian. For numerical experiments, the author(s) implemented Cubic Regularization on this network structure with SGD (on MNIST and CIFAR10) and Adagrad and Adadelta (on MNIST). Comments:  The idea of showing low rank structure which makes it possible to use second-order information without approximations is interesting.  This feedforward network with ReLU activation, output softmax and cross-entropy-loss is well-known structure for neural networks. I have some comments and questions as follows.   Have you tried to apply this to another architecture of neural networks? Do you think whether your approach is able to apply to convolutional neural networks, which are widely used? There is no gain on using CR with Adam as you mention in Discussion part of the paper. Do you think that CR with SGD (or with Adagrad and Adadelta) can be better than Adam? If not, why do people should consider this approach, which is more complicated, since Adam is widely used? The author(s) should do more experiments to various dataset to be more convincing. I do like the idea of the paper, but at the current state, it is hard to evaluate the effective of this paper. I hope the author(s) could provide more experiments on different datasets. I would suggest to also try SVHN or CIFAR100. And if possible, please also consider CNN even if you are not able to provide any theory.  ",14,261,21.75,5.1015625,130,2,259,0.0077220077220077,0.0410447761194029,0.9176,72,34,48,10,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 4, 'MET': 5, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,0,4,4,5,3,0,0,0,0,1,0,0,0,0,1,2,0,0,6,2,0,0.3586021889866608,0.4477780706841707,0.19813467177587668
ICLR2018-ByJ7obb0b-R3,Reject,"This paper proposes to set a global step size gradient-based optimization algorithms such as SGD and Adam using second order information. Instead of using second-order information to compute the update directly (as is done in e.g. Newton method), it is used to estimate the change of the objective function in a pre-computed direction. This is computationally much cheaper than full Newton because (a) the Hessian does not need to be inverted (b) vector-Hessian multiplication is only O(#parameters) for a single sample.  There are many issues. ### runtime and computational issues ### Firstly, the paper does not clearly specify the algorithm it espouses. It states: once the step direction had been determined, we considered that fixed, took the average of gT Hg and gT u2207f over all of the sample points to produce m (u03b1) and then solved for a single u03b1j value You should present pseudo-code for this computation and not leave the reader to determine the detailed order of computation for himself. As it stands, it is not only difficult for the reader to infer these details, but also laborious to determine the computational cost per iteration on some network the reader might wish to apply your algorithm to. Since the paper discusses the computational cost of CR only in vague terms, you should at least provide pseudo-code. Specifically, consider equation (80) at the very end of the appendix and consider the very last term in that equation.  It contains d^2v/dwdw. This is a heavy term containing the second derivative of the last hidden layer with respect to weights. You do not specify how you compute this term or quantities involving this term. In a ReLU network, this term is zero due to local linearity, but since you claim that your algorithm is applicable to general networks, this term needs to be analyzed further. While the precise algorithm you suggest is unclear, it's purpose is also unclear. You only use the Hessian to compute the g^THg terms, i.e. for Hessian-vector multiplication. But it is well-known that Hessian-vector multiplication is relatively cheap in deep networks and this fact has been used for several algorithms, e.g. http://www.iro.umontreal.ca/~lisa/pointeurs/ECML2011_CAE.pdf and https://arxiv.org/pdf/1706.04859.pdf. How is your method for computing g^THg different and why is it superior? Also note that the low-rank structure of deep gradients is well-known and not a contribution of this paper.  See e.g. https://www.usenix.org/system/files/conference/atc17/atc17-zhang.pdf  ### Experiments ###  The experiments are very weak. In a network where weights are initialized to sensible values, your algorithm is shown not to improve upon straight SGD. You only demonstrate superior results when the weights are badly initialized. However, there are a very large number of techniques already that avoid the SGD on ReLU network with bad initial weights problem. The most well-known are batch normalization, He initialization and Adam but there are many others. I don't think it's a stretch to consider that problem solved. Your algorithm is not shown to address any other problems, but what's worse is that it doesn't even seem to address that problem well. While your learning curves are better than straight SGD, I suspect they are well below the respective curves for He init or batchnorm. In any case, you would need to compare your algorithm against these state-of-the-art methods if your goal is to overcome bad initializations. Also, in appendix A, you state that CR can't even address weights that were initialized to values that are too large. You claim that your algorithm helps with overcoming plateaus. While I have heard the claim that deep network optimization suffers from intermediate plateaus before, I have not seen a paper studying / demonstrating this behavior. I suggest you cite several papers that do this and then replicate the plateau situations that arose in those papers and show that CR overcomes them, instead of resorting to a platenau situation that is essentially artificially induced by intentionally bad hyperparameter choices. I do not understand why your initial learning rate for SGD in figures 2 and 3 (0.02 and 0.01 respectively) differ so much from the initial learning rate under CR. Aren't you trying to show that CR can find the correct learning rate? Wouldn't that suggest that initial learning rate for SGD should be comparable to the early learning rates chosen by CR? Wouldn't that suggest you should start SGD with a learning rate of around 2 and 0.35 respectively? Since you are annealing the learning rate for SGD, it's going to decline and get close to 0.02 / 0.01 anyway at some point. While this may not be as good as CR or indeed batchnorm or Adam, the blue constant curve you are showing does not seem to be a fair representation of what SGD can do. You say the minibatch size is 32. For MNIST, this means that 1 epoch is around 1500 iterations. That means your plots only show the first epoch of training. But MNIST does not converge in 1 epoch. You should show the error curve until convergence is reached. Same for CIFAR.   we are not interested in network performance measures such as accuracy and validation error I strongly suspect your readers may be interested in those things. You should show validation classification error or at least training classification error in addition to cross-entropy error. we will also focus on optimization iteration rather than wall clock time Again, your readers care more about the latter. You need to show either error curves by clock time or the total time to convergence or supplement your iteration-based graphs with a detailed discussion of how long an iteration takes. The scope of the experiments is limited because only a single network architecture is considered, and it is not a state-of-the art architecture (no convolution, no normalization mechanism, no skip connections). You state that you ran experiments on Adam, Adadelta and Adagrad, but you do not show the Adam results. You say in the text that they were the least favorable for CR. This suggests that you omitted the detailed results because they were unfavorable to you. This is, of course, unacceptable!  ### (Un)suitability of ReLU for second-order analysis ###  You claim to use second-order information over the network to set the step size. Unfortuantely, ReLU networks do not have second-order information! They are locally linear. All their nonlinearity is contained in non-differentiable region boundaries. While this may lead to the Hessian being cheaper to compute, it means it is not representative of the actual behavior of the network. In fact, the only second-order information that is brought to bear in your experiments is the second-order information of the error function.  I am not saying that this particular second-order information could not be useful, but you need to make a distinction in your paper between network second-order info and error function second-order info and make explicit that you only use the former in your experiments. As far as I know, most second-order papers use either tanh or a smoothed ReLU (such as the smoothed hinge used recently by Koh & Liang (https://arxiv.org/pdf/1703.04730.pdf)) for experiments to overcome the local linearity. ### The sigma hyperparameter ###  You claim that sigma is not as important / hard to set as alpha in SGD or Adam.  You state: We also found that this ap- proach requires less problem-specific information (e.g. an optimal initial learning rate) than other first-order methods in order to perform well.  You have not provided sufficient evidence for this claim. You say that sigma can be chosen by considering powers of 10. In many networks, choosing alpha by considering powers of 10 is sufficient! Even if powers of 2 are considered for alpha, this would reduce the search effort only by factor log_2(10). Also, what if the range of sigma values that need to be considered is larger than the range of alpha values? Then setting sigma would take more effort. You do not give precise protocols how you set sigma and how you set alpha for non-CR algorithms. This should be clearly specified in Appendix A as it is central to your argument of easing hyperparameter search. ### Minor points ###  - Your introduction could benefit from a few more citations n- The rank of the weighted sum of low rank components (as occurs with mini-batch sampling) is generally larger than the rank of the summed components, however. I don't understand this. Every sum can be viewed as a weighted sum and vice versa. - Equation (8) could be motivated a bit better. I know it derives from Taylor's theorem, but it might be good to discuss how Taylor's theorem (and its assumptions) relate to deep networks. - why the name cubic regularization? shouldn't it be something like quadratic step size tuning? . . .  The reason I am giving a 2 instead of a 1 is because the core idea behind the algorithm given seems to me to have potential, but the execution is sorely lacking. A final suggestion: You advertise as one of your algorithms upsides that it uses exact Hessian information. Howwever, since you only care about the scale of the second-order term and not its direction, I suspect exact calculation is far from necessary and you could get away with very cheap approximations, using for example techniques such as mean field analysis (e.g. http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos.pdf).",72,1526,19.31645569620253,5.085308056872038,539,10,1516,0.0065963060686015,0.0243433696348494,0.5619,385,171,293,109,11,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 5, 'DAT': 3, 'MET': 45, 'EXP': 11, 'RES': 5, 'TNF': 5, 'ANA': 11, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 47, 'SUB': 6, 'CLA': 1}",0,1,4,5,3,45,11,5,5,11,1,0,0,2,0,0,1,1,0,0,47,6,1,0.7987080625602437,0.5846989800999397,0.5145800396160317
ICLR2018-ByJDAIe0b-R1,Reject,"This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling. The authors propose a reservoir sampling algorithm for drawing samples from the memory. Some theoretical guarantees for the efficiency of reservoir sampling are provided. The whole algorithm is tested on a toy problem with 3 repeats. The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm. The paper is well written and easy to understand. Typos didn't influence reading. It is a novel setup to consider reservoir sampling for episodic memory. The theory part focuses on effectiveness of drawing samples from the reservoir. Physical meanings of Theorem 1 are not well represented. What are the theoretical advantages of using reservoir sampling? Four simple, shallow neural nets are built as query, write, value, and policy networks. The proposed architecture is only compared with a recurrent baseline with 10-unit GRU network. It is not clear the better performance comes from reservoir sampling or other differences. Moreover, the hyperparameters are not optimized on different architectures. It is hard to justify the empirically better performance without hyperparameter tuning. The authors mentioned that the experiments are done on a toy problem, only three repeats for each experiment.The technically soundness of this work is weakened by the experiments.",16,218,13.625,5.706161137440758,119,0,218,0.0,0.0137614678899082,0.3851,69,27,42,10,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 2}",0,1,1,1,0,9,2,0,0,0,0,2,0,0,0,1,0,2,0,0,6,3,2,0.4307045800814651,0.5590769933744574,0.2733458796045667
ICLR2018-ByJDAIe0b-R2,Reject,"The paper proposes a modified approach to RL, where an additional episodic memory is kept by the agent. What this means is that the agent has a reservoir of n states in which states encountered in the past can be stored. There are then of course two main questions to address (i) which states should be stored and how  (ii) how to make use of the episodic memory when deciding what action to take. For the latter question, the authors propose using a query network that based on the current state, pulls out one state from the memory according to certain probability distribution. This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory. Intuitively, one can see why this may be advantageous as one gets some information from the past. (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.) The first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights. The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose. However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past) . There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable. There is also a toy example created to show that this approach works well compared to the RNN based approaches. Positives:  - An interesting new idea that has potential to be useful in RL - An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks) Negatives: - The math is fudged around quite a bit with approximations that are not always justified - While overall the writing is clear, in some places I feel it could be improved . I had a very hard time understanding the set-up of the problem in Figure 2. [In general, I also recommend against using figure captions to describe the setup. ] - The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach.",20,435,25.58823529411765,4.8265060240963855,207,3,432,0.0069444444444444,0.0157303370786516,0.9948,114,37,82,21,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 13, 'EXP': 2, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,1,1,3,13,2,0,2,0,0,1,0,0,0,1,0,1,2,0,7,0,1,0.5746125520261897,0.5593488690884167,0.3640584832948902
ICLR2018-ByJDAIe0b-R3,Reject,"This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non-markov tasks. The essential problems here are how to identify which states should be stored and how to retrieve memory during action prediction. The proposed architecture could identify the 'key' states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory. The weight assigning (write) network is optimized for maximize the expected rewards. This article focuses on the calculation of gradient for write network, and provides some mathematical clues for that. This article compares their proposed architecture with RNN (GRU with 10 hidden unit) in few toy tasks. They demonstrate that proposed model could work better and rational of write network could be observed. However, it seems that hyper-parameters for RNN haven't been tuned enough. It is because the toy task author demonstrates is actually quite similar to copy tasks, that previous state should be remembered. To my knowledge, copy task could be solved easily for super long sequence through RNN model. Therefore, empirically, it is really hard to justify whether this proposed method could work better. Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps. According to that, the experiments they demonstrated in this article are not well designed so that the conclusion they made in this article is not robust enough. ",13,243,17.357142857142858,5.352697095435684,133,1,242,0.0041322314049586,0.0163934426229508,0.9593,62,25,50,20,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,1,1,0,11,1,0,0,1,0,0,0,0,0,0,0,1,0,0,8,0,0,0.4310733497092752,0.226575924266123,0.19567610018901255
ICLR2018-ByJHuTgA--R1,Accept,"The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs. This type of study is important to give perspective to non-standardized performance scores reported across separate publications, and indeed the results here are interesting as they favour relatively simpler structures. I have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.",5,79,26.33333333333333,6.157894736842105,65,0,79,0.0,0.0253164556962025,0.8176,27,9,12,4,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 0, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,0,0,0.3571428571428571,0.3333333333333333,0.17877941713853301
ICLR2018-ByJHuTgA--R2,Accept,"The authors did extensive tuning of the parameters for several recurrent neural architectures . The results are interesting. However the corpus the authors choose are quite small, the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn .  It would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens. This will use significant resources and is much more difficult, but it's also really valuable, because it's much more close to real world usage of language models. And less tuning is needed for these larger datasets. Finally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. ",9,132,16.5,5.0,88,0,132,0.0,0.0220588235294117,0.9332,31,20,21,11,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 3, 'CLA': 0}",0,1,0,0,2,3,2,1,0,2,0,1,0,0,0,0,1,0,0,1,4,3,0,0.5006473901397878,0.4465236697656163,0.2815660025551561
ICLR2018-ByJHuTgA--R3,Accept,"The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space. Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks. This is a significant result in language modeling and a milestone in deep learning reproducibility research. The paper is clearly motivated and authoritative in its conclusions but it's somewhat lacking in detailed model or experiment descriptions. Some further points:  - There are several hyperparameters set to the standard or default value, like Adam's beta parameter and the batch size/BPTT length. Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified. - The description of the model is ambiguous on at least two points. First, it wasn't completely clear to me what the down-projection is (if it's simply projecting down from the LSTM hidden size to the embedding size, it wouldn't represent a hyperparameter the tuner can set, so I'm assuming it's separate and prior to the conventional output projection). Second, the phrase additive skip connections combining outputs of all layers has a couple possible interpretations (e.g., skip connections that jump from each layer to the last layer or (my assumption) skip connections between every pair of layers?) .  - Fully evaluating the claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent differences result from trainability and regularisation would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization. - The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance. - The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table. The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces. For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters. - The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most standard or conventional LSTM implementation (e.g., as provided in optimized GPU libraries). In addition to further discussion on this point, this result also suggests evaluating other recently proposed minor changes to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016) - It would also have been nice to see a comparison between the variational/recurrent dropout parameterization in which there is further sharing of masks between gates and the one with independent noise for the gates, as described in the footnote. There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help.",18,566,31.444444444444443,5.571948998178506,286,3,563,0.0053285968028419,0.0260416666666666,0.9936,161,78,78,31,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 2, 'MET': 11, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 1}",0,1,4,0,2,11,2,2,1,0,1,1,0,1,0,0,0,3,0,0,10,2,1,0.7172070875550245,0.4503830629807326,0.4033102110296245
ICLR2018-ByJbJwxCW-R1,Reject,"The paper addresses the classification of medical time-series data by formulating the problem as a multi-instance learning (MIL) task, where there is an instance for each timestep of each time series, labels are observed at the time-series level (i.e. for each bag), and the goal is to perform instance-level and series-level (i.e. bag-level) prediction. The main difference from the typical MIL setup is that there is a temporal relationship between the instances in each bag.  The authors propose to model this using a recurrent neural network architecture. The aggregation function which maps instance-level labels to bag-level labels is modeled using a pooling layer (this is actually a nice way to describe multi-instance classification assumptions using neural network terminology). An attention mechanism is also used. The proposed time-series MIL problem formulation makes sense.  The RNN approach is novel to this setting, if somewhat incremental. One very positive aspect is that results are reported exploring the impact of the choice of recurrent neural network architecture, pooling function, and attention mechanism.  Results on a second dataset are reported in the appendix, which greatly increases confidence in the generalizability of the experiments.  One or more additional datasets would have helped further solidify the results, although I appreciate that medical datasets are not always easy to obtain. Overall, this is a reasonable paper with no obvious major flaws.  The novelty and impact may be greater on the application side than on the methodology side.  Minor suggestions:  -The term relational multi-instance learning seems to suggest a greater level of generality than the work actually accomplishes.  The proposed methods can only handle time-series / longitudinal dependencies, not arbitrary relational structure.  Moreover, multi-instance learning is typically viewed as an intermediary level of structure in between propositional learning (i.e. the standard supervised learning setting) and fully relational learning, so the relational multi-instance learning terminology sounds a little strange. Cf.: De Raedt, L. (2008). Logical and relational learning. Springer Science & Business Media.  -Pg 3, a capitalization typo: the Multi-instance learning framework   -The equation for the bag classifier on page 4 refers to the threshold-based MI assumption, which should be attributed to the following paper: Weidmann, N., Frank, E. & Pfahringer, B. 2003. A two-level learning method for generalized multi-instance problems.  In Proceedings of the 14th European Conference on Machine Learning, Springer, 468-479. (See also: J. R. Foulds and E. Frank. A review of multi-instance learning assumptions. Knowledge Engineering Review, 25(1):1-25, 2010. ) - Pg 5, Table 1 vs table 1 - be consistent. -A comparison to other deep learning MIL methods, i.e. those that do not exploit the time-series nature of the problem, would be valuable.  I wouldn't be surprised if other reviewers insist on this.",22,441,11.91891891891892,5.8288508557457215,220,2,439,0.0045558086560364,0.0238611713665943,0.8686,155,61,66,16,10,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 5, 'DAT': 2, 'MET': 10, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,0,1,5,2,10,1,2,1,0,0,1,3,1,0,2,1,0,0,0,9,1,1,0.7168912213994768,0.5605763226188175,0.45107754588282883
ICLR2018-ByJbJwxCW-R2,Reject,"This paper proposes a framework called 'multi-instance learning', in which a time series is treated as a 'set' of observations, and label is assigned to the full set, rather than individual observations. In this framework, authors propose to do set-level prediction (using pooling) and observation level predictions (using various attention mechanisms). They test their approach in a medical setting, where the goal is to annotate vital signs time series by clinical events. Their cohort is 2014 adults time series (average length 4 time steps), and their time series has dimension of 21 and their clinical events have dimension of 26. Their baselines are other 'multi-instance learning' prior work and results are achieved through cross-validation. A few of the relevant hyper-parameters are tuned and some important hyper-parameters (i.e. number of hidden states in the LSTMs, or optimization method and learning rate) are not tuned. Originality - I find the paper to be very incremental in terms of originality of the method. Quality and Significance - Due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments. Given that results are reported via cross-validation and without a true held-out dataset, and given that a number of hyperparameters are not even tuned, it is difficult to be confident that the differences of all the methods reported are significant. Clarity - The writing has good clarity. Major issues with the paper:  - Lack of reliable experiment section. Dataset is too small (2000 total samples), and model training is not described with enough details in terms of hyper-parameters tuned.  ",12,260,18.571428571428573,5.392712550607287,135,0,260,0.0,0.0074906367041198,0.8647,87,34,43,7,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 2, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 1}",0,0,1,2,3,2,5,2,0,0,0,2,0,0,0,1,1,0,0,0,3,2,1,0.5008275773230221,0.556906146933563,0.31915516495371365
ICLR2018-ByJbJwxCW-R3,Reject,"     Post Rebuttal      I went through the rebuttal, which unfortunately claimed a number statements without any experimental support as requested. The revision didn't address my concerns, and I've lowered my rating.      Original Review      This paper proposed a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. The paper also explored integrating RMIL with various attention mechanisms, and demonstrated its usage on medical concept prediction from time series data. The biggest technical innovation in this paper is it combines recurrent networks like Bi-LSTM with MIL to model the relations among instances. Other than that, the paper has limited technical innovations: the pooling functions were proposed earlier and their integration with MIL was widely studied before (as cited by the authors); the attention mechanisms are also proposed by others. However, I am doubtful whether it's appropriate to use LSTM to model the relations among instances. In general MIL, there exists no temporal order among instances, so modeling them with a LSTM is unjustified. It might be acceptable is the authors are focusing on time-series data; but in this case, it's unclear why the authors are applying MIL on it. It seems other learning paradigm could be more appropriate. The biggest concern I have with this paper is the unconvincing experiments. First, the baselines are very weak.  Both MISVM and DPMIL are MIL methods without using deep learning features. It them becomes very unclear how much of the gain on Table 3 is from the use of deep learning, and how much is from the proposed RMIL. Also, although the authors conducted a number of ablation studies, they don't really tell us much. Basically, all variants of the algorithm perform as well, so it's confusing why we need so many of them, or whether they can be integrated as a better model. This could also be due to the small dataset. As the authors are proposing a new MIL learning paradigm, I feel they should experiment on a number of MIL tasks, not limited to analyzing time series medical data. The current experiments are quite narrow in terms of scope. ",19,359,17.95,5.104046242774566,188,2,357,0.0056022408963585,0.0288713910761154,0.454,98,42,69,23,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 11, 'SUB': 3, 'CLA': 0}",0,0,1,4,1,8,4,0,1,1,0,1,0,0,0,0,0,1,0,1,11,3,0,0.5736164031857574,0.4508773718095171,0.3218117883801518
ICLR2018-ByKWUeWA--R1,Accept,"Summary: This paper proposes to estimate the individual treatment effects (ITE) through training two separate conditional generative adversarial networks (GANs). First, a counterfactual GAN is trained to estimate the conditional distribution  of the potential outcome vector, which consists of factual outcome and all  other counterfactual outcomes, given 1) the feature vector, 2) the treatment  variable, and 3) the factual outcome. After training the counterfactual GAN,  the complete dataset containing the observed potential outcome vector can be  generated by sampling from its generator. Second, a ITE GAN is trained to estimate the conditional distribution of the potential outcome vector given only the feature vector. In this way, for any test sample, its potential outcomes can be estimated using the generator of the trained ITE GAN. Given its pontential outcome vector, its ITE can be estimated as well. Experimental results on the synthetic data shows the proposed approach, called GANITE, is more robust to the existence of selection bias, which is defined as the mismatch between the treated and controlled distributions, compared to its competing alternatives. Experiments on three real world datasets show GANITE achieves the best performance on two datasets, including Twins and Jobs. It does not perform very well on the IHDP dataset. The authors also run experiments on the Twins dataset to show the proposed approach can estimate the multiple treatment effects with better performance. Comments 1) This paper is well written. The background and related works are well organized. 2) To the best of my knowledge, this is the first work that applies  GAN to ITE estimation. 3) Experiments on the synthetic data and the real-world data demonstrate the advantage of the proposed approach. 4) The authors directly present the formulation without providing sufficient  motivations. Could the authors provide more details or intuitions on why GAN  would improve the performance of ITE estimation compared to approaches that learn representations to minimize the distance between the distributions of different treatment groups, such as CFR_WASS? 5) As is pointed out by the authors, the proposed approach does not perform well when the dataset is small, such as the IHDP data. However, in practice, a lot of real-world datasets might have small sample size, such as the LaLonde dataset. Did the authors plan to extend the model to handle those small-sized  data sets without completely changing the model. 6) When training the ITE GAN, the objective is to learn the conditional distribution of the potential outcome vector given the feature vector. Did the authors try the option of replacing ITE GAN with multi-task regression? Will the performance become worse using multi-task regression?  I think this  comparison would be a sanity check on the utility of using GAN instead of  regression models for ITE estimation.",21,453,22.65,5.441913439635536,191,2,451,0.0044345898004434,0.0193548387096774,0.9484,130,56,85,18,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 5, 'DAT': 8, 'MET': 9, 'EXP': 7, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 0}",0,0,0,5,8,9,7,2,0,0,0,3,0,0,0,0,0,2,1,0,11,1,0,0.4317265143674459,0.4507811813500642,0.24450081978619692
ICLR2018-ByKWUeWA--R2,Accept,"This paper presents GANITE, a Generative Adversarial Network (GAN) approach for estimating Individualized Treatment Effects (ITE). This is achieved by utilising a GAN to impute the `missing` counterfactuals, i.e. the  outcomes of the treatments that were not observed in the training (i.e. factual) sample, and then using another GAN to estimate the ITE based on this `complete` dataset. The authors then proceed in combining the two GAN objectives with extra supervised losses to better account for the observed data; the GAN loss for the `G` network has an extra term for the `G` network to better predict the factual outcome `y_f` (which should be easy to do given the fact that y_f is an input to the network) and the GAN loss for the `I` network has an extra term w.r.t. the corresponding performance metric used for evaluation, i.e. PEHE for binary treatment and MSE for multiple treatments. This model is then evaluated on extensive experiments. The paper is reasonably well-written with clear background and diagrams for the overall architecture. The idea is novel and seems to be relatively effective in practice although I do believe that it has a lot of moving parts and introduces a considerable amount of hyperameters (which generally are problematic to tune in causal inference tasks). Other than that, I have the following questions and remarks: - I might have misunderstood the motivation but the GAN objective for the `G` network is a bit weird; why is it a good idea to push the counterfactual outcomes close to the factual outcomes (which is what the GAN objective is aiming for)? Intuitively, I would expect that different treatments should have different outcomes and the distribution of the factual and counterfactual `y` should differ. - According to which metric did you perform hyper-parameter optimization on all of the experiments? - From the first toy experiment that highlights the importance of each of the losses it seems that the addition of the supervised loss greatly boosts the performance, compared to just using the GAN objectives. What was the relative weighting on those losses in general? - From what I understand the `I` network is necessary for out-of-sample predictions where you don't have the treatment assignment, but for within sample prediction you can also use the `G` network . What is the performance gap between the `I` and `G` networks on the within-sample set? Furthermore, have you experimented with constructing `G` in a way that can represent `I` by just zeroing the contribution of `y_f` and `t`? In this way you can tie the parameters and avoid the two-step process (since `G` and `I` represent similar things). - For figure 2 what was the hyper parameters for CFR? CFR includes a specific knob to account for the larger mismatches between treated and control distributions. Did you do hyper-parameter tuning for all of the methods in this task? - I would also suggest to not use ""between"" when referring to the KL-divergence as it is not a symmetric quantity. Also it should be pointed out that for IHDP the standard evaluation protocol is 1000 replications (rather than 100) so there might be some discrepancy on the scores due to that.",20,523,30.764705882352946,5.116089613034624,232,4,519,0.0077071290944123,0.0244821092278719,0.5498,134,69,83,22,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 5, 'DAT': 2, 'MET': 13, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 1}",0,0,0,5,2,13,6,0,0,0,0,1,0,0,0,0,0,0,0,0,16,0,1,0.3608494316541329,0.2315515837448668,0.16329040788261606
ICLR2018-ByKWUeWA--R3,Accept,"This paper introduces a generative adversarial network (GAN) for estimating individualized treatment effects (ITEs) by (1) learning a generator that tries to fool a discriminator with feature, treatment, potential outcome- vectors, and (2) by learning a GAN for the treatment effect. In my view, the counterfactual component is the interesting and original component, and the results show that the ITE GAN component further improves performance (marginally but not significant). The analysis is conducted on semi-synthetic data sets created to match real data distributions with synthetically introduced selection bias and conducts extensive experimentation. While the results show worse performance compared to existing literature in the experiment with small data sizes,[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] the work does show improvements in larger data sets. However, Table 5 in the appendix suggests these results are not significant when considering average treatment effect estimation (eATE and eATE). Quality: good. Clarity: acceptable. Originality: original. Significance: marginal. The ITE GAN does not significantly outperform the counterfactual GAN alone (in the S and GAN loss regime), and in my understanding the counterfactual GAN is the particularly innovative component here, i.e., can the algorithm effectively enough generate indistinguishable counterfactual outcomes from x and noise. I wonder if the paper should focus on this in isolation to better understand and characterize this contribution. What is the significance of bold in the tables? I'd remove it if it's just to highlight which method is yours. Discussion section should be called Conclusion and a space permitting a Discussion section should be written. E.g. exploration of the form of the loss when k>2, or when k is exponential e.g. a {0,1}^c hypercube for c potentially related treatment options in an order set. E.g. implications of underperformance in settings with small data sets. We have lots of large data sets where ground truth is unknown, and relatively more small data sets where we can identify ground truth at some cost. E.g. discussion of Table 2 (ITEs) where GANITE is outperforming the methods (at least on large data sets) and Table 5 (ATEs) which does not show the same result is warranted. Why might we expect this to the case?",19,355,16.136363636363637,5.545994065281899,188,2,353,0.0056657223796034,0.0309859154929577,0.9847,124,54,50,17,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 3, 'MET': 4, 'EXP': 2, 'RES': 4, 'TNF': 5, 'ANA': 1, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 5, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,0,0,2,3,4,2,4,5,1,0,6,0,0,0,1,5,0,1,0,7,0,1,0.5728842164644845,0.559479446758405,0.35751494378412685
ICLR2018-ByL48G-AW-R1,Reject,"SUMMARY The paper deal with the problem of RL. It proposes a non-parametric approach that maps trajectories to the optimal policy. It avoids learning parameterized policies. The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it. COMMENTS  What happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different. Not certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem. How do you execute a trajectory? Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly.",8,133,16.625,5.150793650793651,83,0,133,0.0,0.0514705882352941,-0.8617,31,15,27,6,4,1,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,1,2,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0.2867743540312801,0.112355025980797,0.11212116550306649
ICLR2018-ByL48G-AW-R2,Reject,"This work shows that a simple non-parametric approach of storing state embeddings with the associated Monte Carlo returns is sufficient to solve several benchmark continuous control problems with sparse rewards (reacher, half-cheetah, double pendulum, cart pole) (due to the need to threshold a return the algorithms work less well with dense rewards, but with the introduction of a hyper-parameter is capable of solving several tasks there). The authors argue that the success of these simple approaches on these tasks suggest that more changing problems need to be used to assess new RL algorithms. This paper is clearly written and it is important to compare simple approaches on benchmark problems . There are a number of interesting and intriguing side-notes and pieces of future work mentioned. However, the originality and significance of this work is a significant drawback. The use non-parametric approaches to the action-value function go back to at least [1] (and probably much further). So the algorithms themselves are not particularly novel, and are limited to nearly-deterministic domains with either single sparse rewards (success or failure rewards) or introducing extra hyper-parameters per task. The significance of this work would still be quite strong if, as the author's suggest, these benchmarks were being widely used to assess more sophisticated algorithms and yet these tasks were mastered by such simple algorithms with no learnable parameters.  Yet, the results do not support the claim. Even if we ignore that for most tasks only the sparse reward (which favors this algorithm) version was examined, these author's only demonstrate success on 4, relatively simple tasks. While these simple tasks are useful for diagnostics, it is well-known that these tasks are simple and, as the author's suggest more challenging tasks  .... are necessary to properly assess advances made by sophisticated, optimization-based policy algorithms.  Lillicrap et al. (2015) benchmarked against 27 tasks, Houtfout et al. (2016) compared in the paper also used Walker2D and Swimmer (not used in this paper) as did [2], OpenAI Gym contains many more control environments than the 4 solved here and significant research is pursing complex manipulation and grasping tasks (e.g. [3]). This suggests the author's claim has already been widely heeded and this work will be of limited interest. [1] Juan, C., Sutton, R. S., & Ram, A. Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces.  [2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.  [3] Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2017). Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089.",13,431,15.392857142857142,5.5954773869346734,217,2,429,0.0046620046620046,0.0158730158730158,0.9953,155,57,64,28,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,3,0,0,7,0,1,0,0,0,1,1,0,0,1,2,2,0,0,5,0,1,0.4302335570589045,0.5582085845005653,0.27223448872513445
ICLR2018-ByL48G-AW-R3,Reject,"This paper presents a nearest-neighbor based continuous control policy. Two algorithms are presented: NN-1 runs open-loop trajectories from the beginning state, and NN-2 runs a state-condition policy that retrieves nearest state-action tuples for each state.  The overall algorithm is very simple to implement and can do reasonably well on some simple control tasks, but quickly gets overwhelmed by higher-dimensional and stochastic environments. It is very similar to Learning to Steer on Winding Tracks Using Semi-Parametric Control Policies and is effectively an indirect form of tile coding (each could be seen as a fixed voronoi cell). I am sure this idea has been tried before in the 90s but I am not familiar enough with all the literature to find it (A quick google search brings this up: Reinforcement Learning of Active Recognition Behaviors, with a chapter on nearest-neighbor lookup for policies: https://people.eecs.berkeley.edu/~trevor/papers/1997-045/node3.html). Although I believe there is work to be done in the current round of RL research using nearest neighbor policies, I don't believe this paper delves very far into pushing new ideas (even a simple adaptive distance metric could have provided some interesting results, nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domains....), and for that reason I don't think it has a place as a conference paper at ICLR. I would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area.",8,245,35.0,5.373913043478261,158,2,243,0.0082304526748971,0.048780487804878,0.9524,67,34,47,15,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,3,2,0,4,0,0,0,0,0,1,1,1,0,1,0,1,0,1,2,0,0,0.5009708520079778,0.4450664018792874,0.27391938118490483
ICLR2018-ByOExmWAb-R1,Accept,"Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention. This text generation task is traditionally done using recurrent neural networks. This paper proposes to generate text using GANs. GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature. This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator. The whole network is trained on the fill-in-the-blank task using the sequence-to-sequence architecture for both the generator and the discriminator. At training time, the generator's encoder computes a context representation using the masked sequence. This context is conditioned upon to generate missing words. The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real. With this approach, one can generate text at test time by setting all inputs to blanks. Pros and positive remarks:  --I liked the idea behind this paper. I find it nice how they benefited from context (left context and right context) by solving a fill-in-the-blank task at training time and translating this into text generation at test time. --The experiments were well carried through and very thorough. --I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence. I first thought that performance would be better when the generator's encoder uses the unmasked sequence. Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time. Cons and negative remarks: --There is a lot of pre-training required for the proposed architecture. There is too much pre-training. I find this less elegant.  --There were some unanswered questions:             (1) was pre-training done for the baseline as well? (2) how was the masking done? how did you decide on the words to mask? was this at random? (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence.  It needs to but                    that was not explicit in the paper. --Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well. Suggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well.",25,413,17.956521739130434,5.117794486215539,184,0,413,0.0,0.0335570469798657,0.9767,113,47,76,17,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 17, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,1,2,1,1,17,6,0,0,0,0,0,0,0,0,0,0,1,0,0,13,0,0,0.4330859692529106,0.2296857114403379,0.1938461718779522
ICLR2018-ByOExmWAb-R2,Accept,"Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words. Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues. As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed. But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging. Clarity: The mechanism of generating the text samples using the proposed methodology has been described clearly. However the description of the reinforcement learning step could have been made a bit more clear. Originality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings.  There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers. How this current work compares with the existing such literature? Significance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings. Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing. pros: (a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples. (b) Using a novel in-filling procedure to overcome the complexities in GAN training. (c) generation of high quality samples even with higher perplexity on ground truth set. cons: (a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive. (b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words? (c) Depending on the Mask quality GAN can produce low quality samples. Any practical way of choosing the mask?",18,323,20.1875,5.609677419354838,153,1,322,0.0031055900621118,0.0061728395061728,0.9843,107,40,58,13,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 15, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,1,2,3,15,6,0,0,0,0,0,0,0,0,3,2,1,0,0,10,0,0,0.3612080070360285,0.4501803131755153,0.207153789702409
ICLR2018-ByOExmWAb-R3,Accept,"This paper proposes MaskGAN, a GAN-based generative model of text based on the idea of recovery from masked text. For this purpose, authors employed a reinceforcement learning approach to optize a prediction from masked text. Moreover, authors argue that the  quality of generated texts is not appropriately measured by perplexities, thus using another criterion of a diversity of generated n-grams as well as qualitative evaluations by examples and by humans. While basically the approach seems plausible, the issue is that the result is not compared to ordinary LSTM-based baselines. While it is better than a  conterpart of MLE (MaskedMLE), whether the result is qualitatively better than ordinary LSTM is still in question. In fact, this is already appearent both from the model architectures and the generated examples: because the model aims to fill-in blanks from the text around (up to that time), generated texts are generally locally valid but not always valid globally. This issue is also pointed out by authors in Appendix A.2. While the idea of using mask is interesting and important, I think if this idea could be implemented in another way, because it resembles Gibbs sampling where each token is sampled from its sorrounding context, while its objective is still global, sentence-wise. As argued in Section 1, the ability of  obtaining signals token-wise looks beneficial at first, but it will actually break a global validity of syntax and other sentence-wise phenoma. Based on the arguments above, I think this paper is valuable at least conceptually, but doubt if it is actually usable in place of ordinary LSTM (or RNN)-based generation. More arguments are desirable for the advantage of this paper, i.e. quantitative evaluation of diversity of generated text as opposed to LSTM-based methods. *Based on the rebuttals and thorough experimental results, I modified the global rating.",11,300,23.07692307692308,5.321799307958478,160,4,296,0.0135135135135135,0.0363036303630363,0.8822,77,41,47,26,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,3,1,0,8,2,2,0,0,0,1,0,0,0,0,0,2,0,1,4,1,0,0.5020542182856788,0.4464274793061633,0.2783368211983896
ICLR2018-ByOfBggRZ-R1,Accept,"This paper presents a method to identify high-order interactions from the weights of feedforward neural networks. The main benefits of the method are: 1)tCan detect high order interactions and there's no need to specify the order (unlike, for example, in lasso-based methods). 2)tCan detect interactions appearing inside of non-linear function (e.g. sin(x1 * x2)) The method is interesting, in particular if benefit #2 holds experimentally. Unfortunately, there are too many gaps in the experimental evaluation of this paper to warrant this claim right now. Major:  1)tArguably, point 1 is not a particularly interesting setting. The order of the interactions tested is mainly driven by the sample size of the dataset considered, so in some sense the inability to restrict the order of the interaction found can actually be a problem in real settings. Because of this, it would be very helpful to separate the evaluation of benefit 1 and 2 at least in the simulation setting. For example, simulate a synthetic function with no interactions appearing in non-linearities (e.g. x1+x2x3x4+x4x6) and evaluate the different methods at different sample sizes (e.g. 100 samples to 1e5 samples). The proposed method might show high type-1 error under this setting. Do the same for the synthetic functions already in the paper. By the way, what is the sample size of the current set of synthetic experiments? 2)tThe authors claim that the proposed method identifies interactions ""without searching an exponential solution space of possible interactions"". This is misleading, because the search of the exponential space of interactions happens during training by moving around in the latent space identified by the intermediate layers. It could perhaps be rephrased as ""efficiently"". 3)tIt's not clear from the text whether ANOVA and HierLasso are only looking for second order interactions. If so, why not include a lasso with n-order interactions as a baseline? 4)tWhy aren't the baselines evaluated on the real datasets and heatmaps similar to figure 5 are produced? 5)tIs it possible to include the ROC curves coprresponding to table 2? Minor:  1)tHave the authors thought about statistical testing in this framework? The proposed method only gives a ranking of possible interactions, but does not give p-values or similar (e.g. FDRs). 2)t12 pages of text. Text is often repetitive and can be shortened without loss of understanding or reproducibility. ",22,380,17.272727272727273,5.327823691460055,195,4,376,0.0106382978723404,0.0260416666666666,-0.5023,105,51,57,25,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 2, 'MET': 14, 'EXP': 5, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,0,2,3,2,14,5,0,2,0,0,0,0,0,0,0,0,1,0,0,11,1,1,0.4324271855441595,0.4506640187928741,0.24323700742434465
ICLR2018-ByOfBggRZ-R2,Accept,"This paper develops a novel method to use a neural network to infer statistical interactions between input variables without assuming any explicit interaction form or order. First the paper describes that an 'interaction strength' would be captured through a simple multiplication of the aggregated weight and the weights of the first hidden layers. Then, two simple networks for the main and interaction effects are modeled separately, and learned jointly with posing L1-regularization only on the interaction part to cancel out the main effect as much as possible. The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks. A nice series of experimental validations demonstrate the various types of interactions can be detected, while it also fairly clarifies the limitations. In addition to the related work mentioned in the manuscript, interaction detection is also originated from so-called AID, literally intended for 'automatic interaction detector' (Morgan & Sonquist, 1963), which is also the origin of CHAID and CART, thus the tree-based methods like Additive Groves would be the one of main methods for this. But given the flexibility of function representations, the use of neural networks would be worth rethinking, and this work would give one clear example.  I liked the overall ideas which is clean and simple, but also found several points still confusing and unclear. 1) One of the keys behind this method is the architecture described in 4.1. But this part sounds quite heuristic, and it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1. Absorbing the main effect is not critical to these facts? In a standard sense of statistics, interaction would be something like residuals after removing the main (additive) effect. (like a standard test by a likelihood ratio test for models with vs without interactions) 2) the description about the neural network for the main effect is a bit unclear. For example, what does exactly mean the 'networks with univariate inputs for each input variable'? Is my guessing that it is a 1-10-10-10-1 network (in the experiments) correct...?  Also, do g_i and g_i' in the GAM model (sec 4.3) correspond to the two networks for the main and interaction effects respectively? 3) mu is finally fixed at min function, and I'm not sure why this is abstracted throughout the manuscript. Is it for considering the requirements for any possible criteria? Pros: - detecting (any order / any form of) statistical interactions by neural networks is provided. - nice experimental setup and evaluations with comparisons to relevant baselines by ANOVA, HierLasso, and Additive Groves. Cons: - some parts of explanations to support the idea has unclear relationship to what was actually done, in particular, for how to cancel out the main effect. - the neural network architecture with L1 regularization is a bit heuristic, and I'm not surely confident that this architecture can capture only the interaction effect by cancelling out the main effect.  ",19,487,27.05555555555556,5.2602150537634405,228,1,486,0.0020576131687242,0.0261569416498993,0.9559,131,62,72,30,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 10, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 0}",0,0,1,3,0,10,6,0,0,0,0,1,0,0,0,0,0,0,1,0,16,0,0,0.359944164849174,0.2315515837448668,0.16260818984164113
ICLR2018-ByOfBggRZ-R3,Accept,"Based on a hierarchical hereditary assumption, this paper identifies pairwise and high-order feature interactions by re-interpreting neural network weights, assuming higher-order interactions exist only if all its induced lower-order interactions exist. Using a multiplication of the absolute values of all neural network weight matrices on top of the first hidden layer, this paper defines the aggregated strength z_r of each hidden unit r contributing to the final target output y. Multiplying z_r by some statistics of weights connecting a subset of input features to r and summing over r results in final interaction strength of each feature interaction subsets, with feature interaction order equal to the size of each feature subset. Main issues:  1. Aggregating neural network weights to identify feature interactions is very interesting. However, completely ignoring  activation functions makes the method quite crude. 2. High-order interacting features must share some common hidden unit somewhere in a hidden layer within a deep neural network. Restricting to the first hidden layer in Algorithm 1 inevitably misses some important feature interactions. 3. The neural network weights heavily depends on the l1-regularized neural network training, but a group lasso penalty makes much more sense. See Group Sparse Regularization for Deep Neural Networks (https://arxiv.org/pdf/1607.00485.pdf). 4. The experiments are only conducted on some synthetic datasets with very small feature dimensionality p. Large-scale experiments are needed. 5. There are some important references missing. For example, RuleFit is a good baseline method for identifying feature interactions based on random forest and l1-logistic regression (Friedman and Popescu, 2005, Predictive learning via rule ensembles); Relaxing strict hierarchical hereditary constraints, high-order l1-logistic regression based on tree-structured feature expansion identifies pairwise and high-order multiplicative feature interactions (Min et al. 2014, Interpretable Sparse High-Order Boltzmann Machines); Without any hereditary constraint, feature interaction matrix factorization with l1 regularization identifies pairwise feature interactions on datasets with high-dimensional features (Purushotham et al. 2014, Factorized Sparse Learning Models with Interpretable High Order Feature Interactions).   6. At least, RuleFit (Random Forest regression for getting rules + l1-regularized regression) should be used as a baseline in the experiments. Minor issues:  Ranking of feature interactions in Algorithm 1 should be explained in more details. On page 3: b^{(l)} in R^{p_l}, l should be from 1, .., L. You have b^y. In summary, the idea of using neural networks for screening pairwise and high-order feature interactions is novel, significant, and interesting. However, I strongly encourage the authors to perform additional experiments with careful experiment design to address some common concerns in the reviews/comments for the acceptance of this paper.          The additional experimental results are convincing, so I updated my rating score.   ",17,430,14.827586206896552,6.256857855361596,206,0,430,0.0,0.0089086859688195,0.9859,153,86,56,14,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 11, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 2, 'EMP': 7, 'SUB': 4, 'CLA': 0}",0,0,2,3,0,11,4,1,0,0,0,0,0,0,0,1,0,0,0,2,7,4,0,0.3601024532819253,0.4485160190506317,0.20561303193970418
ICLR2018-ByOnmlWC--R1,Accept,"This is a highly interesting paper that proposes a set of methods that combine ideas from imitation learning, evolutionary computation and reinforcement learning in a novel way. It combines the following ingredients: a) a population-based setup for RL b) a pair-selection and crossover operator c) a policy-gradient based ""mutation"" operator d) filtering data by high-reward trajectories e) two-stage policy distillation; In its current shape it has a couple of major flaws (but those can be fixed during the revision/rebuttal period): (1) Related work. It is presented in a somewhat ahistoric fashion. In fact, ideas for evolutionary methods applied to RL tasks have been widely studied, and there is an entire research field called ""neuroevolution"" that specifically looks into which mutation and crossover operators work well for neural networks. I'm listing a small selection of relevant papers below, but I'd encourage the authors to read a bit more broadly, and relate their work to the myriad of related older methods. Ideally, a more reasonable form of parameter-crossover (see references) could be compared to -- the naive one is too much of a straw man in my opinion. To clarify: I think the proposed method is genuinely novel, but a bit of context would help the reader understand which aspects are and which aspects aren't. (2) Ablations. The proposed method has multiple ingredients, and some of these could be beneficial in isolation: for example a population of size 1 with an interleaved distillation phase where only the high-reward trajectories are preserved could be a good algorithm on its own. Or conversely, GPO without high-reward filtering during crossover. Or a simpler genetic algorithm that just preserves the kills off the worst members of the population, and replaces them by (mutated) clones of better ones, etc. (3) Reproducibility. There are a lot of details missing; the setup is quite complex, but only partially described. Examples of missing details are: how are the high-reward trajectories filtered? What is the total computation time of the different variants and baselines ? The x-axis on plots, does it include the data required for crossover/Dagger ? What are do the shaded regions on plots indicate? The loss on pi_S should be made explicit. An open-source release would be ideal. Minor points: - naively, the selection algorithm might not scale well with the population size (exhaustively comparing all pairs), maybe discuss that? - the filtering of high-reward trajectories is what estimation of distribution algorithms [2] do as well, and they have a known failure mode of premature convergence because diversity/variance shrinks too fast. Did you investigate this? n- for Figure 2a it would be clearer to normalize such that 1 is the best and 0 is the random policy, instead of 0 being score 0. - the language at the end of section 3 is very vague and noncommittal -- maybe just state what you did, and separately give future work suggestions? - there are multiple distinct metrics that could be used on the x-axis of plots, namely: wallclock time, sample complexity, number of updates. I suspect that the results will look different when plotted in different ways, and would enjoy some extra plots in the appendix. For example the ordering in Figure 6 would be inverted if plotting as a function of sample complexity? - the A2C results are much worse, presumably because batchsizes are different? So I'm not sure how to interpret them: should they have been run for longer? Maybe they could be relegated to the appendix? References: [1] Gomez, F. J., & Miikkulainen, R. (1999). Solving non-Markovian control tasks with neuroevolution. [2] Larranaga, P. (2002). A review on estimation of distribution algorithms. [3] Stanley, K. O., & Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies. [4] Igel, C. (2003). Neuroevolution for reinforcement learning using evolution strategies. [5] Hausknecht, M., Lehman, J., Miikkulainen, R., & Stone, P. (2014). A neuroevolution approach to general atari game playing. [6] Gomez, F., Schmidhuber, J., & Miikkulainen, R. (2006). Efficient nonlinear control through neuroevolution. Pros: - results - novelty of idea - crossover visualization, analysis - scalability; Cons: - missing background - missing ablations - missing details; [after rebuttal: revised the score from 7 to 8].",38,676,16.9,5.577996715927751,332,6,670,0.008955223880597,0.0186781609195402,0.8465,226,71,108,37,11,7,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 3, 'MET': 18, 'EXP': 2, 'RES': 2, 'TNF': 6, 'ANA': 2, 'FWK': 1, 'OAL': 2, 'BIB': 6, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 8, 'REC': 1, 'EMP': 12, 'SUB': 6, 'CLA': 1}",0,1,5,0,3,18,2,2,6,2,1,2,6,0,0,2,0,2,8,1,12,6,1,0.7907521754387524,0.7857459447109894,0.6264396225595635
ICLR2018-ByOnmlWC--R2,Accept,"The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a strong network). The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the crossover network is presumably time-consuming. It seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment. There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider evolutionary computing. Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning. I would perhaps even call this a distillation network rather than a crossover network. In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as expert trajectories can be generated in unlimited quantities.",7,227,32.42857142857143,5.551401869158879,127,4,223,0.0179372197309417,0.0262008733624454,0.9353,60,28,35,12,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,2,1,0,6,1,0,0,0,0,0,0,0,0,0,0,1,0,0,4,0,1,0.3584743056141644,0.3351992056378622,0.17962212124869525
ICLR2018-ByOnmlWC--R3,Accept,"This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks. The title and the motivation about the genetic algorithm are missing leading and improper. The genetic algorithm is a black-box optimization method, however, the proposed method has nothing to do with black-box optimization. The mutation is a method to sample individual independence of the objective function, which is very different with the gradient step. Mimicking the mutation by a gradient step is very unreasonable. The crossover operator is the policy mixing method employed in game context (e.g., Deep Reinforcement Learning from Self-Play in Imperfect-Information Games, https://arxiv.org/abs/1603.01121 ). It is straightforward if two policies are to be mixed. Although the mixing method is more reasonable than the genetic crossover operator, it is strange to compare with that operator in a method far away from the genetic algorithm. It is highly suggested that the method is called as population-based method as a set of networks is maintained, instead of as genetic method. Another drawback, perhaps resulted from the genetic algorithm motivation is that the proposed method has not been well explained. The only explanation is that this method mimics the genetic algorithm. However, this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high probability. The baseline methods result in rewards much lower than those in previous experimental papers. It is problemistic that if the baselines have bad parameters. 1. Benchmarking Deep Reinforcement Learning for Continuous Control 2. Deep Reinforcement Learning that Matters",13,265,15.588235294117649,5.670588235294118,129,1,264,0.0037878787878787,0.0187265917602996,0.8265,74,36,48,16,5,2,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,2,4,0,0,12,0,1,0,0,0,0,1,0,0,0,0,3,0,0,7,0,0,0.3601619530309558,0.22618829194566,0.16304324950608712
ICLR2018-ByQZjx-0--R1,Reject,"In the paper titled Faster Discovery of Neural Architectures by Searching for Paths in a Large Model, the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks. The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture. The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback); --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly. Overall, the paper is well-written. Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly.",6,115,23.0,5.491071428571429,72,1,114,0.0087719298245614,0.0172413793103448,0.9062,33,16,16,9,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,1,0,0,1,4,1,0,0,0,0,1,0,0,0,0,0,0,0,0,2,0,1,0.3578934334842111,0.2228441796570651,0.16171541139432427
ICLR2018-ByQZjx-0--R2,Reject,"In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources.  The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing. In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks. In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric. Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice. In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time. The authors present two ENAS models: one for CNNs, and another for RNNs. Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections. However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space. This is a limitation, as the model space is not as flexible as one would desire in a discovery task. Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections. Thus, they are essentially learning the skip connections while using a human-selected model. ENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections. Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections. Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. Overall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process. Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea. It is also impressive how much faster their model performs on tasks without sacrificing much performance. The main limitation is that the best architectures as currently described are less about discovery and more about human input; -- finding a more efficient search path would be an important next step.",20,484,25.473684210526315,5.245161290322581,218,3,481,0.0062370062370062,0.0205338809034907,0.9966,130,64,81,30,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 16, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 1}",0,1,4,2,0,16,1,1,0,0,1,0,0,0,0,0,0,3,0,0,13,0,1,0.5040536452757777,0.3410311476658289,0.25548915644345604
ICLR2018-ByQZjx-0--R3,Reject,"In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources.  The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing. In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks. In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric. Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice. In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time. The authors present two ENAS models: one for CNNs, and another for RNNs. Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections. However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space. This is a limitation, as the model space is not as flexible as one would desire in a discovery task. Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections. Thus, they are essentially learning the skip connections while using a human-selected model. ENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections. Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections. Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. Overall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process. Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea. It is also impressive how much faster their model performs on tasks without sacrificing much performance. The main limitation is that the best architectures as currently described are less about discovery and more about human input; -- finding a more efficient search path would be an important next step.",20,484,25.473684210526315,5.245161290322581,218,3,481,0.0062370062370062,0.0205338809034907,0.9966,130,64,81,30,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 16, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 1}",0,1,4,2,0,16,1,1,0,0,1,0,0,0,0,0,0,3,0,0,13,0,1,0.5040536452757777,0.3410311476658289,0.25548915644345604
ICLR2018-ByQpn1ZA--R1,Accept,"The submission describes an empirical study regarding the training performance of GANs; more specifically, it aims to present empirical evidence that the theory of divergence minimization is more a tool to understand the outcome of training (i.e. Nash equillibrium) than a necessary condition to be enforce during training itself .  The work focuses on studying  on-saturating GANs, using the modified generator objective function proposed by Goodfellow et al. in their seminal GAN paper, and aims to show increased capabilities of this variant, compared to the standard minimax formulation. Since most theory around divergence minimization is based on the unmodified loss function for generator G, the experiments carried out in the submission might yield somewhat surprising results compared the theory. If I may summarize the key takeaways from Sections 5.4 and 6, they are: - GAN training remains difficult and good results are not guaranteed (2nd bullet   point) ; - Gradient penalties work in all settings, but why is not completely clear; - NS-GANs + GPs seems to be best sample-generating combination, and faster than   WGAN-GP . - Some of the used metrics can detect mode collapse. The submission's (counter-)claims are served by example (cf. Figure 2, or Figure 3 description, last sentence), and mostly relate to statements made in the WGAN paper (Arjovsky et al., 2017). As a purely empirical study, it poses more new and open questions on GAN optimization than it is able to answer; providing theoretical answers is deferred to future studies. This is not necessarily a bad thing, since the extensive experiments (both toy and real) are well-designed, convincing and comprehensible . Novel combinations of GAN formulations (non-saturating with gradient penalties) are evaluated to disentangle the effects of formulation changes. Overall, this work is providing useful experimental insights, clearly motivating further study. ",12,287,20.5,5.630434782608695,178,4,283,0.0141342756183745,0.0297029702970297,0.9793,84,48,50,14,7,3,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,2,2,0,0,7,1,2,0,0,1,1,0,0,0,0,1,2,0,0,7,0,0,0.5016628614071169,0.3371822404995812,0.25098861347307333
ICLR2018-ByQpn1ZA--R2,Accept,"Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches. The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments .   Clarity: The paper is well-written and clear . The authors could be more concise when reporting results . I would suggest keeping the main results in the main body and move extended results to an appendix. Originality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs . More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails. They also show experimentally that penalized gradients stabilize the learning process. Significance: The problems the authors consider is worth exploring further . The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally . However, publishing this  work is in my opinion premature for the following reasons:  - The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap ; - The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory ; - The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.  ",13,223,18.58333333333333,5.780821917808219,125,0,223,0.0,0.0125,0.7876,65,28,38,15,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 3, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 2}",0,1,1,1,1,3,4,2,0,0,0,2,0,0,0,0,0,0,0,1,4,0,2,0.5722886637320268,0.3352803407087163,0.29073839066204393
ICLR2018-ByQpn1ZA--R3,Accept,"This paper answers recent critiques about ``standard GAN'' that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport. It makes main points 1) ``standard GAN'' is an ill-defined term that may refer to two different learning criteria, with different properties 2) though the non-saturating variant (see Eq. 3) of ``standard GAN'' may converge towards a minimum of the Jensen-Shannon divergence, it does not mean that the minimization process follows gradients of the Jensen-Shannon divergence (and conversely, following gradient paths of the Jensen-Shannon divergence may not converge towards a minimum, but this was rather the point of the previous critiques about ``standard GAN'').  3) the penalization strategies introduced for ``non-standard GAN'' with specific motivations, may also apply successfully to the ``standard GAN'', improving robustness, thereby helping to set hyperparameters. Note that item 2) is relevant in many other setups in the deep learning framework and is often overlooked. Overall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered. In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae). The answers to the critiques referenced in the   paper are convincing, though I must admit that I don't know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience. Details: - p. 4 please do not qualify KL as a distance metric  - Section 4.3: Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update is ambiguous: what is exactly meant by iteration (and sometimes step elsewhere)? - Section 4.3: the performance measure is not relevant regarding distributions. The l2 distance is somewhat OK for means, but it makes little sense for covariance matrices. ",11,325,25.0,5.524590163934426,188,7,318,0.0220125786163522,0.048048048048048,0.9587,84,42,62,21,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 2}",0,0,3,1,1,5,1,0,0,0,0,1,0,0,0,0,0,1,0,0,4,1,2,0.4297331728313352,0.4463914518198273,0.2354521931947634
ICLR2018-ByRWCqvT--R1,Accept,"The authors propose a method for performing transfer learning and domain adaptation via a clustering approach. The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class. Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide soft labels for whether or not pairs of target data belong to the same class. A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points. The proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest. The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task. The way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on. To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters. However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method.  Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model. Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear. Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO. Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works. The experimental evaluation is very thorough, spanning a variety of tasks and settings. Strong results in multiple settings indicate that the proposed method is effective and generalizable. Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results. It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works. I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup. At times, the experimental details are a little unclear. Consistent use of the A, T, and S' dataset abbreviations would help. Also, the results section seems to switch off between calling the method CCN and LCO interchangeably. Finally, a few of the experimental settings differ from their baselines in nontrivial ways. For the Office experiment, the LCO appears to be trained on ImageNet data. While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data. Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting. In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature. Overall, the paper seems to have both a novel contribution and strong technical merit. However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained. I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.                           Update after author rebuttal:                           I have read the author's response and have looked at the changes to the manuscript. I am satisfied with the improvements to the paper and have changed my review to 'accept'.",31,794,25.61290322580645,5.050599201065246,321,9,785,0.0114649681528662,0.0259740259740259,0.9952,215,72,149,49,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 8, 'MET': 19, 'EXP': 9, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 3, 'REC': 1, 'EMP': 14, 'SUB': 4, 'CLA': 1}",0,1,1,0,8,19,9,3,0,1,0,2,0,0,0,2,0,2,3,1,14,4,1,0.5770310535830887,0.7864686619450156,0.4541332785582008
ICLR2018-ByRWCqvT--R2,Accept,"(Summary) This paper tackles the cross-task and cross-domain transfer and adaptation problems. The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence). What's similar vs dissimilar is trained with a binary classifier. (Pros) 1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning. 2. Learning to output the k class membership probability and the loss in eqn 5 seems novel. (Cons) 1. The authors overclaim to be state of the art. For example, table 2 doesn't compare against two recent methods which report results exactly on the same dataset. I checked the numbers in table 2 and the numbers aren't on par with the recent methods. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2. 2. There are some typos in the related works section and the inferece procedure isn't clearly explained. Perhaps the authors can clear this up in the text after sec 4.3. (Assessment) Borderline. Refer to the Cons section above.",13,239,11.38095238095238,5.454954954954955,127,3,236,0.0127118644067796,0.0292887029288702,-0.0739,84,30,27,11,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 0, 'DAT': 3, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 0, 'SUB': 1, 'CLA': 2}",0,1,6,0,3,3,1,0,2,0,0,1,2,0,0,1,0,4,0,1,0,1,2,0.5724392676133915,0.5559881782979795,0.35659693930460856
ICLR2018-ByRWCqvT--R3,Accept,"pros: This is a great paper - I enjoyed reading it. The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion. The paper is clearly written and easy to understand . Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework. Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cases. cons/suggestions:  - the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail.",6,123,20.5,5.771186440677966,84,0,123,0.0,0.0232558139534883,0.92,35,18,23,5,4,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 2}",0,0,0,0,1,3,1,0,0,0,0,2,0,0,0,1,0,1,0,0,3,0,2,0.2862600887499056,0.4457694943849844,0.1606799469320556
ICLR2018-ByS1VpgRZ-R1,Accept,"I thank the authors for the thoughtful response and updated manuscript. After reading through both, my review score remains unchanged.                    The authors describe a new variant of a generative adversarial network (GAN) for generating images. This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models. Major comments: 1) Spatial resolution. What spatial resolution is the model generating images at? The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image. It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling. It would be great to see the authors address this issue in a serious manner. 2) FID in real data. The numbers in Table 1 appear favorable to the projection model. Please add error bars (based on Figure 4, I would imagine they are quite large). Additionally, would it be possible to compute this statistic for *real* images? I would be curious to know what the FID looks like as a 'gold standard'. 3) Conditional batch normalization.  I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator.  The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629 ).  If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful. Minor comments: - I believe you have the incorrect reference for conditional batch normalization on Page 5. A Learned Representation For Artistic Style Dumoulin, Shlens and Kudlur (2017) https://arxiv.org/abs/1610.07629 - Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images. - Please add citations for Figures 1a-1b. Do these correspond with some known models? Depending on how the authors respond to the reviews, I would consider upgrading the score of my review.",22,334,15.181818181818182,5.43217665615142,181,0,334,0.0,0.0333333333333333,0.9808,104,48,58,9,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 3, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 5, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 2, 'REC': 2, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,1,2,0,0,10,0,3,4,0,0,2,5,1,0,0,0,2,2,2,9,0,1,0.5740370768499558,0.5607297469874577,0.3578415897357363
ICLR2018-ByS1VpgRZ-R2,Accept,"The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset. Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) .  This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results. + Quality: - Simple method leading to great results on ImageNet! - While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work. One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes. - Appropriate comparison with existing conditional models: AC-GANs and PPGNs. - Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID) + Clarity: - Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2). - PPG should be PPGNs. n + Originality: This work proposes a simple method that is original compared existing GANs. + Significance: While the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger. Overall, I really enjoy reading this paper and recommend for acceptance!",12,254,23.09090909090909,5.634042553191489,143,2,252,0.0079365079365079,0.0150375939849624,0.9887,90,32,38,13,7,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,3,0,0,5,4,1,0,0,1,1,0,0,0,1,1,2,0,1,5,1,1,0.5014203316350326,0.7803827700743396,0.39654515188584744
ICLR2018-ByS1VpgRZ-R3,Accept,"This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network.  It motivates the method by examining the form of the log density ratio in the continuous and discrete cases. This paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). What bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1   0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward. The sentence containing assume that the network model can be shared had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.",8,203,22.55555555555556,5.147208121827411,135,2,201,0.0099502487562189,0.0242718446601941,0.9657,49,20,42,11,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 7, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,2,0,1,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,1,0.2872959262993776,0.2247100519615941,0.12914696170281595
ICLR2018-BySRH6CpW-R1,Accept,"This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization. The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution. As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick. The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting. The method is evaluated on multiple experiments. This paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice. My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]. While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward. I would thus suggest that the authors update the paper accordingly. Other than that, I have some other comments : - The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve? - For the binary setting you mentioned that you had to reduce the entropy thus added a ""beta density regulariser"". Did you add R(p) or log R(p) to the objective function? Also, with alpha, beta   2 the beta density is unimodal with a peak at p 0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid. To force the probability near the endpoints you have to use alpha, beta < 1 which results into a ""bowl"" shaped Beta distribution. I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization.  - I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful.  Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT). [1] Wang & Manning, Fast Dropout Training. Edit: After the authors rebuttal I have increased the rating of the paper: - I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1]. - The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p. The lambda term would then serve as an indicator to how much entropy is necessary. - There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved). - The authors showed benefits compared to a continuous relaxation baseline.",24,531,22.125,5.238476953907815,228,3,528,0.0056818181818181,0.016453382084095,0.9401,154,56,83,26,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 8, 'PDI': 1, 'DAT': 0, 'MET': 18, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,1,8,1,0,18,1,3,0,0,0,2,1,0,0,2,0,2,0,1,11,0,0,0.5764020752730682,0.4508262889345823,0.32739859429039
ICLR2018-BySRH6CpW-R2,Accept,"Summary of the paper: The paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights.  Results on MNIST, CIFAR-10 and ImageNet are very competitive .   Pros: - The proposed method leads to state of the art results . - The paper is easy to follow and clearly describes the implementation details needed to reach the results.  Cons: - The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward,; but its application for learning discrete networks is to my best knowledge novel and interesting. It could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets. Minor comments: - Some citations have a strange format: e.g. ""in Hubara et al. (2016); Restegari et al. (2016)"" would be better readable as   ""by Hubara et al. (2016) and Restegari et al. (2016)"". -  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and mathcal{W} is the joint distribution resulting from independently sampling from  mathcal{W}^l_{i,j}. - page 6: ""on the last full precision network"": should probably be ""on the last full precision layer""  "" distributions has"" ->  "" distributions have"".",11,228,15.2,5.087557603686636,131,2,226,0.0088495575221238,0.0160642570281124,0.98,73,32,34,10,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 3}",0,1,3,2,0,5,0,3,1,0,0,0,3,0,0,1,0,0,3,0,3,0,3,0.5013572339168558,0.4459737673034452,0.2835046142407736
ICLR2018-BySRH6CpW-R3,Accept,"This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE. Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel. From the experiments, we can see that the proposed method is effective.  It seems that there could be more things to show in the experiments part. For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs. Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram. Also, in the literature of low precision networks, people are concerning both training time and test time computation demand. Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order. Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly.",9,190,19.0,5.396739130434782,115,2,188,0.0106382978723404,0.0261780104712041,0.8999,58,17,30,9,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,3,1,0,4,5,0,1,0,0,0,0,0,0,1,0,2,1,0,3,1,0,0.4298277638843852,0.5569166329824315,0.2688418267067338
ICLR2018-ByUEelW0--R1,Reject,"The paper proposes to add a rotation operation in long short-term memory (LSTM) cells. It performs experiments on bAbI tasks and showed that the results are better than the simple baselines with original LSTM cells. There are a few problems with the paper.  Firstly, the title and abstract discuss modifying memories, but the content is only about a rotation operation. Perhaps the title should be Rotation Operation in Long Short-Term Memory? Secondly, the motivation of adding the rotation operation is not properly justified. What does it do that a usual LSTM cell could not learn? Does it reduce the excess representational power compared to the LSTM cell that could result in better models? Or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible before? This is not clear at all after reading the paper. Besides, the idea of using a rotation operation in recurrent networks has been explored before [3]. Finally, the task (bAbI) and baseline models (LSTM from a Keras tutorial) are too weak. There have been recent works that nearly solved the bAbI tasks to perfection (e.g., [1][2][4][5], and many others). The paper presented a solution that is weak compared to these recent results. In a summary, the main idea of adding rotation to LSTM cells is not properly justified in the paper, and the results presented are quite weak for publication in ICLR 2018. [1] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus. End-to-end memory networks, NIPS 2015 [2] Caiming Xiong, Stephen Merity, Richard Socher. Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016 [3] Mikael Henaff, Arthur Szlam, Yann LeCun, Recurrent Orthogonal Networks and Long-Memory Tasks, ICML 2016  [4] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio, Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes, ICLR 2017 [5] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun, Tracking the World State with Recurrent Entity Networks, ICLR 2017 ",15,326,23.285714285714285,5.245901639344262,170,1,325,0.003076923076923,0.0273556231003039,0.4625,123,48,48,17,8,4,"{'ABS': 2, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",2,1,4,1,0,8,1,1,0,0,0,0,1,0,1,1,0,1,0,0,10,0,0,0.5734228901908701,0.4500420613580311,0.3219222030826828
ICLR2018-ByUEelW0--R2,Reject,"The paper proposes an additional transform in the recurrent neural network units. The transform allows for explicit rotations and swaps of the hidden cell dimensions. The idea is illustrated for LSTM units, where the transform is applied after the cell values are computed via the typical LSTM updates. My first concern is the motivation. I think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via gates. In the proposed example, it's not clear to me why the gate is expected to be saturated at every time step such that it would require the memory swaps. Alternatively, experimentally showing that the network makes use of swaps in an interpretable way (e.g. at certain sentence positions) could strengthen the motivation. Secondly, the experimental analysis is not very extensive.  The method is only evaluated on the bAbI QA dataset, which is a synthetic dataset. I think a language modeling benchmark and/or a larger scale question answering dataset should be considered. Regarding the experimental setup, how are the hyper-parameters for the baseline tuned? Have you considered training jointly (across the tasks) as well? Also, is the setting the same as in Weston et al (2015)? While for many tasks the numbers reported by Weston et al (2015) and the ones reported here for the LSTM baseline are aligned in the order of magnitude, suggesting that some tasks are easier or more difficult for LSTMs, there are large differences in other cases, for task #5 (here 33.6, Weston 70), for task #16 (here 48, Weston 23), and so on. Finally, do you have an intuition (w.r.t. to swaps and rotations) regarding the accuracy improvements on tasks #5 and #18? Some minor issues: - The references are somewhat inconsistent in style: some have urls, others do not; some have missing authors, ending with et al. - Section 1, second paragraph: senstence - Section 3.1, first paragraph: thorugh - Section 5: architetures",17,321,21.4,5.096026490066225,173,2,319,0.006269592476489,0.0306748466257668,0.9474,92,33,60,21,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 2, 'MET': 4, 'EXP': 9, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 1}",0,1,2,2,2,4,9,0,0,1,0,1,1,0,0,0,0,0,1,0,6,3,1,0.6444731640362995,0.4477675846353022,0.35979194041998885
ICLR2018-ByUEelW0--R3,Reject,"Summary: This paper introduces a model that combines the rotation matrices with the LSTMs. They apply the rotations before the final tanh activation of the LSTM and before applying the output gate. The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations. The paper only provides results on the bAbI task. Questions: Have you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study) Have you tried on other tasks? Why did you just apply the rotations only on d_{t}. Pros: Uses a simple parametrization of the rotation matrices. Cons: Not clear justification and motivations The experiments are really lacking: No ablation study The results are only limited to single toy task. General Comments:  This paper proposes to use the rotation matrices with LSTMs.  However there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output gate. The experiments are seriously lacking, an ablation study should have been made and the results are not good enough.  The experiments are only limited to bAbI task which doesn't tell you much. This paper is not ready for publication, and really feels like it is rushed. Minor Comment: This paper needs more proper proof-reading. There are some typos in it, e.g.: 1st page, senstence --> sentence 4th page, the the ...",18,246,17.571428571428573,5.024793388429752,125,0,246,0.0,0.0358565737051792,-0.9193,76,20,46,16,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 8, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 1, 'CLA': 3}",0,1,0,1,0,7,8,3,0,0,0,3,0,0,0,1,0,1,0,1,5,1,3,0.4308781931976173,0.6693167665477467,0.2986539748890199
ICLR2018-ByW5yxgA--R1,Reject,"The paper focuses on a very particular HMM structure which involves multiple, independent HMMs. Each HMM emits an unobserved output with an explicit duration period . This explicit duration modelling captures multiple scales of temporal resolution. The actual observations are a weighted linear combination of the emissions from each latent HMM. The structure allows for fast inference using a spectral approach. I found the paper unclear and lacking in detail in several key aspects: 1. It is unclear to me from Algorithm 2 how the weight vectors w are estimated. This is not adequately explained in the section on estimation. 2. The authors make the assumption that each HMM injects noise into the unobserved output which then gets propagated into the overall observation. What are reasons for his choice of model over a simpler model where the output of each HMM is uncorrupted? 3. The simulation example does not really demonstrate the ability of the MSHMM to do anything other than recover structure from data simulated under an MSHMM. It would be more interesting to apply to data simulated under non-Markovian or other setups that would enable richer frequency structures to be included and the ability of MSHMM to capture these. 4. The real data experiments shows some improvements in predictive accuracy with fast inference. However, the authors do not give a sufficiently broad exploration of the representations learnt by the model which allows us to understand the regimes in which the model would be advantageous. Overall, the paper presents an interesting approach but the work lacks maturity. Furthermore, simulation and real data examples to explore the properties and utility of the method are required. ",16,274,14.421052631578949,5.377862595419847,150,0,274,0.0,0.0072463768115942,0.8089,76,32,45,12,6,3,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 0, 'DAT': 4, 'MET': 9, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",0,2,0,0,4,9,5,2,0,0,0,2,0,0,0,0,0,0,0,0,7,1,1,0.4311526604982965,0.3370650779423911,0.22058451936309686
ICLR2018-ByW5yxgA--R2,Reject,"This paper proposes a variant of hierarchical hidden Markov Models (HMMs) where the chains operate at different time-scales with an associate d spectral estimation procedure that is computationally efficient. The model is applied to artificially generated data and to high-frequency equity data showing promising results. The proposed model and method are reasonably original and novel. The paper is well written and the method reasonably well explained (I would add an explanation of the spectral estimation in the Appendix, rather than just citing Rodu et al. 2013). Additional experimental results would make it a stronger paper. It would be great if the authors could include the code that implements the model.",7,110,15.714285714285714,5.561904761904762,73,0,110,0.0,0.009090909090909,0.9584,32,14,18,8,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 2, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 0, 'SUB': 0, 'CLA': 1}",0,1,0,0,1,2,1,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,1,0.571678763542356,0.3333333333333333,0.29085348594806826
ICLR2018-ByW5yxgA--R3,Reject,"The paper presents an interesting spectral algorithm for multiscale hmm . The derivation and analysis seems correct. However, it is well-known that spectral algorithm is not robust to model mis-specification. It is not clear whether the proposed algorithm will be useful in practice. How will the method compare to EM algorithms and neural network based approaches? ",5,55,11.0,5.490909090909091,42,1,54,0.0185185185185185,0.087719298245614,0.3356,14,10,10,3,3,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,4,0,0,0,1,0,0,0,0,0,0,0,1,0,0,2,0,0,0.2150362906270682,0.2228441796570651,0.09055267167912524
ICLR2018-ByYPLJA6W-R1,Reject,"The paper considers distribution to distribution regression with MLPs. The authors use an energy function based approach. They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters. This seems to be a nice treatment of distribution to distribution regression with neural networks. The approach is methodological similar to using expected likelihood kernels. While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters. That's what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation. In the discussion, it says.  ""For future work, a possible study is to investigate what classes of problems DRN can solve. ""  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve. Its practical utility is questionable.  It's not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments. In the introduction, it would also improve the paper to outline clear points of methodological novelty.",11,215,15.357142857142858,5.571428571428571,122,2,213,0.0093896713615023,0.05,0.9708,62,27,39,11,4,2,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,2,0,0,0,8,2,0,0,0,1,0,0,0,0,0,0,0,0,0,6,0,1,0.2875773452318025,0.225332009396437,0.12606830463497998
ICLR2018-ByYPLJA6W-R2,Reject,"Summary:  This paper presents a new network architecture for learning a regression of probability distributions. The distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes. The conditional probability function is an unnormalized distribution with the same form as the Boltzman distribution, and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bins. By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level. Under these dynamics and discretization, the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribution. These gradients are in turn used to calculate updates for the network parameters with respect to the Jensen Shannon divergence between the predicted distribution and a target distribution. The approach is evaluated on three tasks, two synthetic and one real world. The baselines are the state of the art triple basis estimator (3BE) or a standard MLP that represents the output distribution using a softmax over quantiles. On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance. On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines. However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing. Notes to authors:  I'm not familiar with 3BE but the fact that it is used outside of its intended use case for the stock data is worrying. How does 3BE perform at predicting the FTSE distribution at time t + k from the FTSE distribution at time t only? Do the multiple input distributions actually help? You use a kernel density estimate with a Gaussian kernel function to estimate the stock market pdf, but then you apply your network directly to this estimate. What would happen if you built more complex networks using the kernel values themselves as inputs? Could you also run experiments on the real-world datasets used by the 3BE paper? What is the structure of the DRN that uses > 10^3 parameters (from Fig. 4)?  The width of the network is bounded by the two input distributions, so is this network just incredibly deep? Also, is it reasonable to assume that both the DRN and MLP are overfitting the toy task when they have access to an order of magnitude more parameters than datapoints. It would be nice if section 2.4 was expanded to actually define the cost gradients for the network parameters, either in line or in an appendix.",20,511,31.9375,5.338085539714868,231,0,511,0.0,0.0134615384615384,0.9633,163,39,86,22,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 3, 'MET': 12, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 0}",0,1,5,0,3,12,6,0,0,0,0,1,0,0,0,0,0,2,0,0,16,0,0,0.4321486538709261,0.2316687463020567,0.19774470757458015
ICLR2018-ByYPLJA6W-R3,Reject,"This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions.  A well-written manuscript,; though the introduction could have motivated the problem a little better (i.e. why would we want to do this). The novelty in the paper is implementing such a regression in a layered network. The paper shows how the densities at each nodes are computed (and normalised). Optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow. The paper uses three problems to illustrate the idea -- a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices.  My only two reservations of this paper is the illustration on the stock index data -- it seems to me, returns on individual constituent stocks of an index are used as samples of the return on the index itself. But this cannot be true when the index is a weighted sum of the constituent assets.   Secondly, it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into 100 bins for numerical reasons; -- does the smoothing that results from this give any advantage over a histogram of the returns in 100 bins?",11,220,20.0,5.077669902912621,130,1,219,0.0045662100456621,0.026431718061674,0.4077,66,20,30,6,7,3,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,2,0,1,2,7,1,1,0,0,0,1,0,0,0,1,0,0,0,0,5,0,1,0.5015627623519169,0.3358211630727052,0.2514183271453568
ICLR2018-ByZmGjkA--R1,Reject,"This paper presents an analysis of an agent trained to follow linguistic commands in a 3D environment. The behaviour of the agent is analyzed by means of a set of psycholinguistic experiments probing what it learned, and by inspection of its visual component through an attentional mechanism. On the positive side, it is nice to read a paper that focuses on understanding what an agent is learning. On the negative side, I did not get many new insights from the analyses presented in the study. 3 A situated language learning agent  I can't make up the chair from the refrigerator in the figure. 4.1 Word learning biases  This experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. Conversely, when it is exposed to colors only, it will have a color bias. When the training set is balanced, the agent shows a mild bias for the simpler color property. How is this interesting or surprising? The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. This, however, is not addressed in the paper. Minor comments about this section:  - Was there noise also in shape generation, or were all object instances identical? - propensity to select o_2: rather o_1? - I did not follow the paragraph starting with This effect provides. 4.2 The problem of learning negation  I found this experiment very interesting. Perhaps, the authors could be more explicit about the usage of negation here. The meaning of commands containing negation are, I think, conjunctions of the form pick something and do not pick X (as opposed to the more natural do not pick X). modifiation: modification  4.3 Curriculum learning  Perhaps the difference in curriculum effectiveness in language modeling vs grounded language learning simulations is due to the fact that the former operates on large amounts of natural data, where it's hard to define the curriculum, while the latter are typically grounded in toy worlds with a controlled language, where it's easier to construct the curriculum. 4.4 Processing and representation differences  There is virtually no discussion of what makes the naturalistic setup naturalistic, and thus it's not clear which conclusions we should derive from the corresponding experiments. Also, I don't see what we should learn from Figure 5 (besides the fact that in the controlled condition shapes are easier than categories). For the naturalistic condition, the current figure is misleading, since different classes contain different numbers of instances. It would be better to report proportions. Concerning the attention analysis, it seems to me that all it's saying is that lower layers of a CNN detect lower-level properties such as colors, higher layers detect more complex properties, such as shapes characterizing objects. What is novel here?  Also, since introducing attention changes the architecture, shouldn't the paper report the learning behaviour of the attention-augmented network? The explanation of the attention mechanism is dense, and perhaps could be aided by a diagram (in the supplementary materials?). I think the description uses length when dimensional(ity) is meant. 6. Supplementary material  It would be good to have an explicit description of the architecture, including number of layers of the various components, structure of the CNN, non-linearities, dimensionality of the layers, etc. (some of this information is inconsistently provided in the paper). It's interesting that the encoder is actually a BOW model. This should be discussed in the paper, as it raises concerns about the linguistic interest of the controlled language that was used. Table 3: indicates is: indicates if ",29,614,21.928571428571427,5.341924398625429,273,6,608,0.0098684210526315,0.0334928229665071,0.9665,170,67,112,32,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 8, 'EXP': 8, 'RES': 0, 'TNF': 5, 'ANA': 5, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 20, 'SUB': 2, 'CLA': 0}",0,0,0,3,0,8,8,0,5,5,0,0,0,0,0,0,0,0,2,0,20,2,0,0.3598082415703136,0.3453187700274746,0.18080103532404304
ICLR2018-ByZmGjkA--R2,Reject,"In this manuscript, the authors connect psychological experimental methods to understand how the black box of the mind solves problems with current issues in understanding how the black box of deep learning methods solves problems. The authors used situated versions of human language learning tasks as simulation environments to test a CNN + LSTM deep learning network. They examined a few key phenomena: shape/color bias, learning negation concepts, incremental learning, and how learning affects the representation of objects via attention-like processes. They illustrated conditions in which their deep learning network acts similarly to people in simulations. Developing methods that enable humans to understand how deep learning models solve problems is an important problem for many reasons (e.g., usability of models for science, ethical concerns) that has captured the interest of a wide range of researchers. By adapting experimental methodology from psychology to test that have been used to understand and explain the internal workings of the mind, the authors approach the problem in a novel and innovative manner. I was impressed by the range of phenomena they tackled and their analyses were informative in understanding the behavior of deep learning models  I found the analogy persuasive in theory, but I was not convinced that the current manuscript really demonstrates its value. In particular, I did not see the value of situating their model in a grounded environment. One analysis that would have helped convince me is a comparison to an equivalent non-grounded deep learning model (e.g., a CNN trained to make equivalent classifications), and show how this would not help us understand human behavior. However, the more I thought about the logic of this type of analysis, the more concerned I became about the logic of their approach. What would it mean if the equivalent non-situated model does not show the phenomena?  If it does not, it could illustrate the efficacy of using situated environments. But, it also could mean that their technique acts differently for equivalent situated and non-situated models. In this case though, what would we learn about the more general non-situated case then? It does not seem like we would learn much, which would defeat the purpose of the technique. Alternatively, if the equivalent non-situated model does show the phenomena, then using the situated version would not be useful because the model acts equivalently in both cases. I am not fully convinced by the argument I just sketched, but leaves me very concerned about the usefulness of their approach.  (note that the ""controlled"" vs. ""naturalistic"" analyses in the word learning section did not convince me. This argues for the importance of using naturalistic statistics u2013 not necessarily cross-modal, situated environments as the authors argue for). Additionally, I was unconvinced that simpler models could not be used to examine the phenomena that they analyzed. Although combining LSTM with CNN via a ""mixing"" module was interesting, it added another layer of complexity that made it more difficult to assess what the results meant. This left me less convinced of the usefulness of their paradigm. If we need to create a novel deep learning method to illustrate its efficacy, how will it be useful for solving the problem that motivated everything: understanding how pre-existing deep learning methods solve problems.  ",24,538,23.39130434782609,5.40655105973025,230,0,538,0.0,0.0183823529411764,-0.7366,137,62,102,31,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 7, 'DAT': 0, 'MET': 13, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 4}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,0,0,7,0,13,0,0,0,4,0,0,0,4,0,2,0,0,0,0,13,0,0,0.2891745325971562,0.229730819024856,0.1314174490918516
ICLR2018-ByZmGjkA--R3,Reject,"This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information. The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Nonetheless the paper takes on an ambitious goal of relating questions in machine learning in cognitive science and does a reasonably good job of analyzing the results. Comments:  1. The results on word learning biases are not particularly surprising given previous work in this area, much of which has used similar neural network models. Linda Smith and Eliana Colunga have published a series of papers that explore these questions in detail:  http://www.iub.edu/~cogdev/labwork/kinds.pdf http://www.iub.edu/~cogdev/labwork/Ontology2003.pdf  2. In figure 2 and the associated analyses, why were 20 shape terms used rather than 8 to parallel the other cases? It seems like there is a strong basic color bias. This seems like one of the most novel findings in the paper and is worth highlighting. This figure and the corresponding analysis could be made more systematic by mapping out the degree of shape versus color bias as a function of the number of shape and color terms in a 2D plot. The resulting plot would show the degree of bias towards color. 3. The section on curriculum learning does not mention relevant work on ""starting small""  and the ""less is more hypothesis in language development by Jeff Elman and Elissa Newport:  https://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf http://www.sciencedirect.com/science/article/pii/0388000188900101 4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later. I found these human results hard to reconcile with the results from the models. I also found it hard to understand why colors were hard to learn given the bias towards colors shown earlier in the paper. 5. The section on layerwise attention claims to give a ""computational level"" explanation, but this is a misleading term to use u2014 it is not a computational level explanation in the sense introduced by David Marr which is the standard use of this term in cognitive science.  The explanation of layerwise attention could be clearer. Minor:  ""analagous"" -> ""analogous""  The paper runs longer than eight pages, and it is not obvious that the extra space is warranted. ",19,471,21.40909090909091,5.36980306345733,232,4,467,0.0085653104925053,0.0228690228690228,0.4913,146,56,72,19,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 3, 'TNF': 5, 'ANA': 7, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 0}",0,0,2,1,0,4,0,3,5,7,0,1,0,1,0,1,0,1,2,0,7,3,0,0.5726125475842524,0.5595622221050597,0.3602875652729106
ICLR2018-Bya8fGWAZ-R1,Reject,"The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training. This paper extends that work by - training from reinforcement signals only, rather than near-expert trajectories - making the transition model more state-depdendent - scaling to larger problem domains by propagating reward values for navigational goals in a special way The paper is fairly clear and these extensions are reasonable .  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact .  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value. So, making improvements to help solve grid-worlds better is not so motivating .  It may be possible to motivate and demonstrate the methods of this paper in other domains, however. The work on dynamic environments was an interesting step:  it would have been interesting to see how the models learned for the dynamic environments differed from those for static environments.",8,202,28.857142857142858,5.575129533678757,125,4,198,0.0202020202020202,0.0328638497652582,0.9888,52,27,37,16,4,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,2,0,0,5,0,0,0,1,0,0,0,0,0,0,2,2,0,0,1,0,1,0.2867955420718083,0.4446096436500822,0.15925638037627177
ICLR2018-Bya8fGWAZ-R2,Reject,"ORIGINALITY & SIGNIFICANCE  The authors build upon value iteration networks: the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network.  The authors point out that the original value iteration network"" (Tamar 2016) did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network. It seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update steps.  Extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problems. The work is therefore original and significant. The algorithm is evaluated on the original obstacle grids from Tamar 2016 and larger grids generated to test scalability. The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster. The M in MVProp in particular seems to be very useful in scaling up to the large grids. The authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over time. QUALITY  The symbol d_{rew} is never defined u2014 what does ""new"" stand for? It appears to be the number of latent convolutional filters or channels generated by the state embedding network.  Section 2.2 Sentence 2: The final layer representing the encoding is given as ( R^{d_rew  x d_x x d_y }. Based on the description  in the first paragraph of section 2, it sounds like d_rew might be the number of channels or filters in the last convolutional layer. In equation 1, it wasn't obvious to me that the expression max_a q_{ij}^{k-1} q^{k} corresponds to an actual operation? The h( Phi( x ), v^{k-1} ) sort of makes sense ...  value is only calculated with respect to only the observation of the maze obstacles but the policy pi is calculated with respect to the joint  observation and agent state. The expression      h_{aid} ( phi(0), v )       <  Wa,   [ phi(o) ; v ]   >   +   b  makes sense and reminds me of the Value Iteration network work where we take the previous value function, combine it with the reward function and use convolution to compute the expectation (the weights Wa encode the effect of transitions). I gather the tensor Wa   R^{|A| x (d_{rew} x d_x x d_y } both converts the feature embedding phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same time?  I didn't understand the r^in, r&out representation in section 4.1. These are given by the domain? I did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decision. A bit more detail defining terms, explaining their intuitive role and how the output of one module feeds into the next would be helpful. POST REVISION COMMENTS:  - I didn't reread the whole thing -  just used the diff tool.  - It looks like the typos in the equations got fixed - The new phrase enables to learn to plan seems pretty awkward.",22,533,26.65,5.01984126984127,266,5,528,0.0094696969696969,0.0271646859083191,0.9967,180,67,84,21,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 17, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 5}",0,1,2,2,0,17,2,0,0,0,0,1,0,1,0,1,1,0,0,0,13,0,5,0.504229057376503,0.4522324739459763,0.2837513798769895
ICLR2018-Bya8fGWAZ-R3,Reject,"The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al. VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment. The authors propose two new updates value propagation (VProp) and max propagation (MVProp), which are roughly speaking additive and multiplicative versions of the update used in the Bellman-Ford algorithm for shortest path. The approaches are evaluated in grid worlds with and without other agents. I had some difficulty to understand the paper because of its presentation and writing (see below). In Tamar's work, a mapping from observation to reward is learned. It seems this is not the case for VProp and MVProp, given the gradient updates provided in p.5. As a consequence, those two methods need to take as input a new reward function for every new map. Is that correct? I think this could explain the better experimental results  In the experimental part, the results for VIN are worse than those reported in Tamar et al.'s paper. Why did you use your own implementation of VIN and not Tamar et al.'s, which is publicly shared as far as I know? I think the writing needs to be improved on the following points: - The abstract doesn't fit well the content of the paper. For instance, its variants is confusing because there is only other variant to VProp. Adversarial agents is also misleading because those agents act like automata. - The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self-contained, e.g., (1) is hardly understandable. - The writing should be careful, e.g.,  value iteration is presented as a learning algorithm (which in my opinion is not) pi^* is defined as a distribution over state-action space and then pi is defined as a function;  ...  - The mathematical writing should be more rigorous; e.g.,  p.2: T: s to a to s', pi : s to a A denotes a set and its cardinal In (1), shouldn't it be Phi(o)? all the new terms should be explained p. 3: definition of T and R  shouldn't V_{ij}^k depend on Q_{aij}^k? T_{::aij} should be defined In the definition of h_{aij}, should Phi and b be indexed by a? - The typos and other issues should be fixed: p. 3: K iteration with capable p.4: close 0 p.5: our our s^{t+1} should be defined like the other terms The state is represented by the coordinates of the agent and 2D environment observation should appear much earlier in the paper. pi_theta described in the previous sections, notation pi_theta appears the first time here... 3x3 -> 3 times 3 ofB V_{theta^t w^t} p.6: the the Fig.2's caption: What does both cases refer to? They are three models. References: et al. YI WU ",23,473,22.52380952380953,4.758620689655173,224,5,468,0.0106837606837606,0.0307377049180327,0.9354,150,40,89,24,8,4,"{'ABS': 1, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 11}",1,0,4,1,0,11,3,0,1,0,0,4,1,0,0,0,0,3,2,0,4,0,11,0.5744806042811391,0.4474175614956971,0.32058459620649493
ICLR2018-ByaQIGg0--R1,Reject,"This paper proposes to use neural network and gradient descent to automatically design for engineering tasks. It uses two networks, parameterization network and prediction network to model the mapping from design parameters to fitness. It uses back propagation (gradient descent) to improve the design.  The method is evaluated on heat sink design and airfoil design. This paper targets at a potentially very useful application of neural networks that can have real world impacts. However, I have three main concerns: 1) Presentation. The organization of the paper could be improved.  It mixes the method, the heat sink example and the airfoil example throughout the entire paper. Sometimes I am very confused about what is being described. My suggestion would be to completely separate these three parts: present a general method first, then use heat sink as the first experiment and airfoil as the second experiment. This organization would make the writing much clearer. 2) In the paragraph above Section 4.1, the paper made two arguments. I might be wrong, but I do not agree with either of them in general.  First of all,  eural networks are good at generalizing to examples outside their train set. This depends entirely on whether the sample distribution of training and testing are similar and whether you have enough training examples that cover important sample space. This is especially critical if a deep neural network is used since overfitting is a real issue. Second, it is easy to imagine a hybrid system where a network is trained on a simulation and fine tuned .... Implementing such a hybrid system is nontrivial due to the reality gap. There is an entire research field about closing the reality gap and transfer learning. So I am not convinced by these two arguments made by this paper. They might be true for a narrow field of application. But in general, I think they are not quite correct. 3) The key of this paper is to approximate the dynamics using neural network (which is a continuous mapping) and take advantage of its gradient computation. However, many of dynamic systems are inherently discontinuous (collision/contact dynamics) or chaotic (turbulent flow). In those scenarios, the proposed method might not work well and we may have to resort to the gradient free methods.  It seems that the proposed method works well for heat sink problem and the steady flow around airfoil, both of which do not fall into the more complex physics regime. It would be great that the paper could be more explicit about its limitations. In summary, I like the idea, the application and the result of this paper. The writing could be improved. But more importantly, I think that the proposed method has its limitation about what kind of physical systems it can model. These limitation should be discussed more explicitly and more thoroughly.",26,471,15.193548387096774,5.10913140311804,223,9,462,0.0194805194805194,0.0314465408805031,0.9759,121,62,77,30,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 9, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 14, 'SUB': 0, 'CLA': 2}",0,1,0,1,0,14,9,1,0,0,0,3,0,1,0,0,1,0,3,0,14,0,2,0.5040329014289712,0.4527341640158638,0.28257080876912477
ICLR2018-ByaQIGg0--R2,Reject,"This paper introduces an appealing application of deep learning: use a deep network to approximate the behavior of a complex physical system, and then design optimal devices (eg airfoil shapes) by optimizing this network with respect to its inputs. Overall, this research direction seems fruitful, both in terms of different applications and in terms of extra machine learning that could be done to improve performance, such as ensuring that the optimization doesn't leave the manifold of reasonable designs. On one hand, I would suggest that this work would be better placed in an engineering venue focused on fluid dynamics. On the other hand, I think the ICLR community would benefit from about the opportunities to work on problems of this nature.  Quality  The authors seem to be experts in their field. They could have done a better job explaining the quality of their final results, though.  It is unclear if they are comparing to strong baselines.  Clarity  The overall setup and motivation is clear.  Originality  This is an interesting problem that will be novel to most member of the ICLR community.  I think that this general approach deserves further attention from the community.  Major Comments  * It's hard for me to understand if the performance of your method is actually good. You show that it outperforms simulated annealing. Is this the state of the art? How would an experienced engineer perform if he or she just sat down and drew the shape of an airfoil, without relying on any computational simulation at all? * You can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the inputs. Why not do lots of random initializations for the optimization? Isn't that a good way to help avoid local optima? * I'd like to see more analysis of the reliability of your deep-network-based approximation to the physics simulator. For example, you could evaluate the deep-net-predicted drag ratio vs. the simulator-predicted drag ratio at the value of the parameters corresponding to the final optimized airfoil shape.  If there's a gap, it suggests that your NN approximation might have not been that accurate.  Minor Comments  * We also found that adding a small amount of noise too the parameters when computing gradients helped jump out of local optima Generally, people add noise to the gradients, not the values of the parameters. See, for example, uses of Langevin dynamics as a non-convex optimization method. * You have a complicated method for constraining the parameters to be in [-0.5,0.5]. Why not just enforce this constraint by doing projected gradient descent? For the constraint structure you have, projection is trivial (just clip the values) .    * The gradient decent approach required roughly 150 iterations to converge where as the simulated annealing approach needed at least 800. This is of course confounded by the necessary cost to construct the training set, which is necessary for the gradient descent approach. I'd point out that this construction can be done in parallel, so it's less of a computational burden. * I'd like to hear more about the effects of different parametrizations of the airfoil surface. You optimize the coefficients of a polynomial. Did you try anything else? * Fig 6: What does 'clean gradients' mean? Can you make this more precise? * The caption for Fig 5 should explain what each of the sub figures is.",31,557,20.62962962962963,5.074211502782932,283,6,551,0.0108892921960072,0.0240137221269296,0.9969,153,61,100,25,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 3, 'DAT': 2, 'MET': 18, 'EXP': 4, 'RES': 1, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 21, 'SUB': 0, 'CLA': 0}",0,1,2,3,2,18,4,1,2,3,0,0,0,2,1,1,0,3,1,0,21,0,0,0.7191148107512648,0.568229029366795,0.45588306187443506
ICLR2018-ByaQIGg0--R3,Reject,"1. This is a good application paper, can be quite interesting in a workshop related to Deep Learning applications to physical sciences and engineering. 2. Lacks in sufficient machine learning related novelty required to be relevant in the main conference 3. Design, solving inverse problem using Deep Learning are not quite novel, see Stoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017). 4. However, this paper introduces two different types of networks for parametrization and physical behavior mapping, which is interesting, can be very useful as surrogate models for CFD simulations. 5. It will be interesting to see the impacts of physics based knowledge on choice of network architecture, hyper-parameters and other training considerations. 6. Just claiming the generalization capability of deep networks is not enough, need to show how much the model can interpolate or extrapolate? what are the effects of regulariazations in this regard? ",7,160,12.307692307692308,5.758389261744966,102,0,160,0.0,0.0186335403726708,0.9379,50,24,25,8,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 2, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,1,0,0,5,1,0,0,0,0,1,1,0,1,2,2,0,0,0,4,0,0,0.3581436255979958,0.4464034609819393,0.20208815546151102
ICLR2018-BydLzGb0Z-R1,Accept,"1) Summary This paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably. The authors propose to train a forward and backward RNN in parallel. The forward RNN predicts forward in time and the backward RNN predicts backwards in time. While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step. In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition. 2) Pros: + Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned. + Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected. + Improved performance in speech recognition task. + The idea is clearly explained and well motivated. 3) Cons: Image captioning experiment: In the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines. This experiment shows improvement over such baselines, however, the performance is still worse compared against baselines such as Lu et al, 2017 and Yao et al, 2016. It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done. Unconditioned generation experiments: In these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help. Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation. The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case. For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given.  Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it's hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is. If the authors believe that the 50% case is not necessary, please feel free to explain why. Additional comments: The method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN). It would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn). Experiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment). 4) Conclusion The paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse. Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past. The proposed method presents a possible way of better modeling the future, however, some the results do not clearly back up the claim yet. The given score will improve if the authors are able to address the stated issues. POST REBUTTAL RESPONSE: The authors have addressed the comments on the MNIST experiments and show better results, however, as far as I can see, they did not address my concern about the comparisons on the image captioning experiment. In the image captioning experiment the authors choose two networks (Show & Tell and Soft attention) that they improve using the proposed method that end up performing similar to the second best baseline (Yao et al. 2016) based on Table 3 and their response. I requested for the authors to use their method on the best performing baselines (i.e. Yao et al. 2016 or Liu et al. 2017) or explain why this cannot be done (maybe my request was not clearly stated). Applying the proposed method on the strong baselines would highlight the author's claims more strongly than just applying on the average performing chosen baselines. This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines. Given, that the authors were able to improve the results in the sequential MNIST and improve the average baselines, my rating improves one point. However, I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result. My rating changes to marginally above threshold for acceptance.",36,823,22.24324324324324,5.157560355781449,300,3,820,0.0036585365853658,0.0156815440289505,0.9984,225,72,174,48,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 5, 'DAT': 0, 'MET': 15, 'EXP': 10, 'RES': 7, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 3, 'EMP': 20, 'SUB': 0, 'CLA': 0}",0,1,10,5,0,15,10,7,3,2,0,2,0,0,0,2,0,4,0,3,20,0,0,0.6485481306107219,0.4566978319068792,0.3708332524451979
ICLR2018-BydLzGb0Z-R2,Accept,"** post-rebuttal revision **  I thank the authors for running the baseline experiments, especially for running the TwinNet to learn an agreement between two RNNs going forward in time. This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns. I am raising the score. ** original review **   The paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward. Applied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders.  A penalty term is added that forces an agreement between the hidden states of the two decoders. During model evaluation only the forward decoder is used, with the backward operating decoder discarded. The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states. Experiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network. Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy. The proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks. I have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id rkr1UDeC-)? Also, would using the backward RNN, e.g. for rescoring, bring another advantage? In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty? Quality: The proposed approach is well motivated and the experiments show the limits of applicability range of the technique. Clarity: The paper is clearly written .  Originality: The presented idea seems novel. Significance: The method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods. Also, as the authors note the method seems to be limited to conditional sequence generators. Pros and cons: Pros: the method is simple to implement, the paper lists for what kind of datasets it can be used. Cons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary ",21,436,22.94736842105263,5.3947990543735225,210,4,432,0.0092592592592592,0.0244988864142538,0.984,147,37,87,19,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 2, 'DAT': 2, 'MET': 15, 'EXP': 6, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 1}",0,0,2,2,2,15,6,2,0,0,0,2,0,0,0,1,0,2,0,0,11,0,1,0.504210667756422,0.4507811813500642,0.28408584603406767
ICLR2018-BydLzGb0Z-R3,Accept,"Twin Networks: Using the Future as a Regularizer ** PAPER SUMMARY **  The authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN. Both RNNs are trained jointly and only the forward model is used at test time. Experiments on conditional generation (speech recognition, image captioning), and unconditional generation (MNIST pixel RNN, language models) show the effectiveness of the regularizer. ** REVIEW SUMMARY **  The paper reads well, has sufficient reference. The idea is simple and well explained. Positive empirial results support the proposed regularizer. ** DETAILED REVIEW **  Overall, this is a good paper. I have a few suggestions along the text but nothing major. In related work, I would cite co-training approaches. In effect, you have two view of a point in time, its past and its future and you force these two views to agree, see  (Blum and Mitchell, 1998) or Xu, Chang, Dacheng Tao, and Chao Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634 (2013). I would also relate your work to distillation/model compression which tries to get one network to behave like another. On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained? In section 2, it is not obvious to me that the regularizer (4) would not be ignored in absence of regularization on the output matrix. I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings. Could you comment why this would not happen? In Section 4.2, you need to refer to Table 2 in the text. You also need to define the evaluation metrics used. In this section, why are you not reporting the results from the original Show&Tell paper? How does your implementation compare to the original work? On unconditional generation, your hypothesis on uncertainty is interesting and could be tested. You could inject uncertainty in the captioning task for instance, e.g. consider that multiple version of each word e.g. dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates. Would your regularizer still be helpful there? At which point would it break?",24,366,16.636363636363637,5.08,194,0,366,0.0,0.0132978723404255,0.9718,125,33,60,16,11,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 11, 'EXP': 3, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 2}",0,1,5,2,0,11,3,2,1,1,0,3,1,1,0,0,0,4,1,0,12,0,2,0.7889163894157519,0.4517185989701411,0.4499250761861807
ICLR2018-BydjJte0--R1,Accept,"The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach). It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy. As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable, yet Table 2 shows that batch size can be predicted with much higher accuracy than chance . It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately . That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization .  The selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries? One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy. In table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io. In the ImageNet classifier family prediction, how different are the various families from each other? Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader. Overall the results seem interesting, but without more insights it's difficult to judge how generally useful they are.",13,272,27.2,5.163498098859316,132,5,267,0.0187265917602996,0.0471014492753623,0.8048,58,37,45,13,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 2, 'RES': 3, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,0,1,1,2,4,2,3,2,2,0,1,0,0,0,0,0,1,1,0,8,1,0,0.6438894268568408,0.4487981464883452,0.3539740874456072
ICLR2018-BydjJte0--R2,Accept,"-----UPDATE------  Having read the responses from the authors, and the other reviews, I am happy with my rating and maintain that this paper should be accepted. ----------------------    In this paper, the authors trains a large number of MNIST classifier networks with differing attributes (batch-size, activation function, no. layers etc.) and then utilises the inputs and outputs of these networks to predict said attributes successfully. They then show that they are able to use the methods developed to predict the family of Imagenet-trained networks and use this information to improve adversarial attack .  I enjoyed reading this paper. It is a very interesting set up, and a novel idea. A few comments:  The paper is easy to read, and largely written well . The article is missing from the nouns quite often though so this is something that should be amended. There are a few spelling slip ups (to a certain extend --> to a certain extent, as will see --> as we will see)] It appears that the output for kennen-o is a discrete probability vector for each attribute, where each entry corresponds to a possibility (for example, for batch-size it is a length 3 vector where the first entry corresponds to 64, the second 128, and the third 256) . What happens if you instead treat it as a regression task, would it then be able to hint at intermediates (a batch size of 96) or extremes (say, 512). n A flaw of this paper is that kennen-i and io appear to require gradients from the network being probed (you do mention this in passing), which realistically you would never have access to. (Please do correct me if I have misunderstood this) It would be helpful if Section 4 had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict . Also, the caption for Table 2 could contain more information regarding the network outputs. You have jumped from predicting 12 attributes on MNIST to 1 attribute on Imagenet . It could be beneficial to do an intermediate experiment (a handful of attributes on a middling task) .  I think this paper should be accepted as it is interesting and novel .  Pros ------ - Interesting idea - Reads well - Fairly good experimental results Cons ------ - kennen-i seems like it couldn't be realistically deployed - lack of an intermediate difficulty task ",19,381,22.41176470588235,5.022792022792022,192,5,376,0.0132978723404255,0.0171990171990172,0.9911,92,32,81,19,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 2, 'EMP': 4, 'SUB': 2, 'CLA': 6}",0,1,0,1,1,6,2,2,1,0,0,5,0,1,0,2,0,0,0,2,4,2,6,0.6444316763426866,0.5579986877793595,0.4071446293594806
ICLR2018-BydjJte0--R3,Accept,"The paper attempts to study model meta parameter inference e.g. model architecture, optimization, etc using a supervised learning approach . They take three approaches one whereby the target models are evaluated on a fixed set of inputs, one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combined. The authors also show that these inferred quantities can be used to generate more effective attacks against the targets. The paper is generally well written and most details for reproducibility are seem enough . I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing . It is of course not entirely surprising that the system can be trained but that there is some form of generalization happening. Aside that I think most system in practical use will be much more different than any a priori enumeration/brute force search for model parameters . I suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing, ensembling, non-standard data and a number of optimization and architectural tricks that are developer dependent. It is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that case .   I have found it hard to understand what table 3 in section 4.2 actually means . It seems to say for instance that a model is trained on 2 and 3 layers then queried with 4 and the accuracy only slightly drops . Accuracy of what ? Is it the other attributes ? Is it somehow that attribute ? if so how can that possibly ?  My main main concern is extrapolation out of the training set which is particularly important here. I don't find enough evidence in 4.2 for that point. One experiment that i would find compelling is to train for instance a meta model on S,V,B,R but not D on imagenet, predict all the attributes except architecture and see how that changes when D is added . If these are better than random and the perturbations are more successful it would be a much more compelling story. ",15,374,22.0,5.037037037037037,193,3,371,0.0080862533692722,0.0256410256410256,0.9591,96,33,73,26,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 11, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,0,1,2,11,1,0,1,1,0,0,0,0,0,0,0,0,0,0,8,0,1,0.5025380118777402,0.226575924266123,0.22601608769392645
ICLR2018-ByeqORgAW-R1,Accept,"Summary:  Using a penalty formulation of backpropagation introduced in a paper of Carreira-Perpinan and Wang (2014), the current submission proposes to minimize this formulation using explicit step for the update of the variables corresponding to the backward pass, but implicit steps for the update of the parameters of the network. The implicit steps have the advantage that the choice of step-size is replaced by a choice of a proximity coefficient, which the advantage that while too large step-size can increase the objective, any value of the proximity coefficient yields a proximal mapping guaranteed to decrease the objective. The implicit are potentially one order of magnitude more costly than an explicit step since they require to solve a linear system, but can be solved (exactly or partially) using conjugate gradient steps. The experiments demonstrate that the proposed algorithm are competitive with standard backpropagation and potentially faster if code is optimized further. The experiments show also that in on of the considered case the generalization accuracy is better for the proposed method. Summary of the review:   The paper is well written, clear, tackles an interesting problem. But, given that the method is solving a formulation that leverages second order information, it would seem reasonable to compare with existing techniques that leverage second order information to learn neural networks, namely BFGS, which has been studied for deep learning (see the references to Li and Fukushima (2001) and Ngiam et al (2011) below). Review:  Using an implicit step leads to a descent step in a direction which is different than the gradient step. Based on the experiment, the step in the implicit direction seems to decrease faster the objective, but the paper does not make an attempt to explain why.  The authors must nonetheless have some intuition about this. Is it because the method can be understood as some form of block-coordinate Newton with momentum? It would be nice to have an even informal explanation. Since a sequence of similar linear systems have to be solved could a preconditioner be gradually be solved and updated from previous iterations, using for example a BFGS approximation of the Hessian or other similar technique. This could be a way to decrease the number of CG iterations that must done at each step. Or can this replaced by a single BFGS style step? The proposed scheme is applicable to the batch setting when most deep network are learned using stochastic gradient type methods. What is the relevance/applicability of the method given this context? In fact given that the proposed scheme applies in the batch case, it seems that other contenders that are very natural are applicable, including BFGS variants for the non-convex case (  see e.g. Li, D. H., & Fukushima, M. (2001). On the global convergence of the BFGS method for nonconvex unconstrained optimization problems. SIAM Journal on Optimization, 11(4), 1054-1064. and  J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, Q. V. Le, and A. Y. Ng, ""On optimization methods for deep learning,"" in Proceedings of the 28th International Conference on Machine Learning, 2011, pp. 265u2013272.  )   or even a variant of BFGS which makes a block-diagonal approximation to the Hessian with one block per layer. To apply BFGS, one might have to replace the RELU function by a smooth counterpart.. How should one choose tau_theta? In the experiments the authors compare with classical backpropagation, but they do not compare with  the explicit step of Carreira-Perpinan and Wang? This might be a relevant comparison to add to establish more clearly that it is the implicit step that yields the improvement. Typos or question related to notations, details etc: In the description of algorithm 2: the pseudo-code does not specify that the implicit step is done with regularization coefficient tau_theta In equation (10) is z_l z_l^k or z_l^(k+1/2) (I assume the former). 6th line of 5.1 theta_l is initialised uniformly in an interval -> could you explain why and/or provide a reference motivating this ? 8th line of 5.1 you mention Nesterov momentum method -> a precise reference and precise equation to lift ambiguities might be helpful. In section 5.2 the reference to Table 5.2 should be Table 1. ",31,685,18.513513513513512,5.278996865203761,288,5,680,0.0073529411764705,0.0199146514935988,0.9963,193,77,118,24,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 4, 'DAT': 0, 'MET': 16, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 5, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 14, 'SUB': 0, 'CLA': 1}",0,0,2,4,0,16,5,0,1,3,0,2,5,0,0,0,0,4,1,0,14,0,1,0.5759290533787595,0.4528813787689729,0.32667023590206773
ICLR2018-ByeqORgAW-R2,Accept,"This work proposes to replace the gradient step for updating the network parameters to a proximal step (implicit gradient) so that a large stepsize can be taken. Then to make it fast, the implicit step is approximated using conjugate gradient method because the step is solving a quadratic problem. The theoretical result of the ProxProp considers the full batch, and it can not be easily extended to the stochastic variant (mini-batch). The reason is that the gradient of proximal is evaluated at the future point, and different functions will have different future points. While for the explicit gradient, it is assessed at the current point, and it is an unbiased one. In the numerical experiment, the parameter tau_theta is sensitive to the final solution. Therefore, how to choose this parameter is essential. Given a new dataset, how to determine it for a good performance. In Fig 3. The full batch loss of Adam+ProxProp is higher than Adam+BackProp regarding time, which is different from Fig. 2. Also, the figure shows that the performance of Adam+BackProp is worst than Adam+ProxProp even though the training loss of Adam+BackProp is smaller that of Adam+ProxProp. Does it happen on this dataset only or it is the case for many datasets? ",11,205,15.76923076923077,5.05050505050505,109,0,205,0.0,0.0097087378640776,-0.7128,51,28,36,9,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 2, 'EXP': 1, 'RES': 6, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,0,1,2,2,1,6,2,0,0,0,0,0,0,0,0,1,0,0,7,0,0,0.500588090277932,0.22595396683128,0.22861258830299255
ICLR2018-ByeqORgAW-R3,Accept,"The paper uses a lesser-known interpretation of the gradient step of a composite function (i.e., via reverse mode automatic differentiation or backpropagation), and then replaces one of the steps with a proximal step. The proximal step requries the solution of a positive-definite linear system, so it is approximated using a few iterations of CG. The paper provides theory to show that their proximal variant (even with the CG approximations) can lead to convergent algorithms (and since practical algorithms are not necessarily globally convergent, most of the theory shows that the proximal variant has similar guarantees to a standard gradient step). On reading the abstract and knowing quite a bit about proximal methods, I was initially skeptical, but I think the authors have done a good job of making their case.  It is a well-written, very clear paper, and it has a good understanding of the literature, and does not overstate the results. The experiments are serious, and done using standard state-of-the-art tools and architectures. Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements. The main drawback of this paper is that there is no theory to suggest the ProxProp algorithm has better worst-case convergence guarantees, and that the experiments do not show a consistent benefit (in terms of time) of the method. On the one hand, I somewhat agree with the authors that while the running time is higher... we expect that it can be improved through further engineering efforts, but on the other hand, the idea of nested algorithms (matrix-free or truncated Newton) always has this issue. A very similar type of ideas comes up in constrained or proximal quasi-Newton methods, and I have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 SIAM Review paper on FWI by Metivier et al. at https://doi.org/10.1137/16M1093239). In every case, the answer seems to be that it can work on *some problems* and for a few well chosen parameters, so I don't yet buy that ProxProp is going to make a huge savings on a wide-range of problems. In brief: quality is high, clarity is high, originality is high, and significance is medium. Pros: interesting idea, relevant theory provided, high-quality experiments Cons: no evidence that this is a break-through idea Minor comments:  - Theorems seemed reasonable and I have no reason to doubt their accuracy - No typos at all, which I find very unusual. Nice job! - In Algo 1, it would help to be more explicit about the updates (a), (b), (c), e.g., for (a), give a reference to eq (8), and for (b), reference equations (9,10). It's nice to have it very clear, since gradient step doesn't make it clear what the stepsize is, and if this is done in a Jacob-like or Gauss-Seidel-like fashion. (c) has no reference equation, does it? - Similarly, for Algo 2, add references. In particular, tie in the stepsizes tau and tau_theta here. - Motivation in section 4.1 was a bit iffy. A larger stepsize is not always better, and smaller is not worse.  Minimizing a quadratic f(x)   .5||x||^2 will converge in one step with a step-size of 1 because this is well-conditioned; on the flip side, slow convergence comes from lack of strong convexity, or with strong convexity, ill-conditioning of the Hessian (like a stiff ODE). - The form of equation (6) was very nice, and you could also point out the connection with backward Euler for finite-difference methods. This was the initial setting of analysis for most of original results that rely on the proximal operator (e.g., Lions and Mercier 1970s). - Eq (9), this is done component-wise, i.e., Hadamard product, right? - About eq (12), even if softmax cross-entropy doesn't have a closed-form prox (and check the tables of Combettes and Pesquet), because it is separable (if I understand correctly) then it ought to be amenable to solving with a handful of Newton iterations which would be quite cheap. Prox tables (see also the new edition of Bauschke and Combettes' book): P. L. Combettes and J.-C. Pesquet, Proximal splitting methods in signal processing, in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering (2011) http://www4.ncsu.edu/~pcombet/prox.pdf - Below prop 4, discussing why not to make step (b) proximal, this was a bit vague to me. It would be nice to expand this .  - Page 6 near the top, to apply the operator, in the fully-connected case, this is just a matrix multiply, right? and in a conv net, just a convolution? It would help the reader to be more explicit here. - Section 5.1, 2nd paragraph, did you swap tau_theta and tau, or am I just confused? The wording here was confusing. - Fig 2 was not that convincing since the figure with time showed that either usual BackProp or the exact ProxProp were best, so why care about the approximate ProxProp with a few CG iterations?  The argument of better generalization is based on very limited experiments and without any explanation, so I find that a weak argument (and it just seems weird that inexact CG gives better generalization). The right figure would be nice to see with time on the x-axis as well. - Section 5.2, this was nice and contributed to my favorable opinion about the work. However, any kind of standard convergence theory for usual SGD requires the stepsize to change per iteration and decrease toward zero. I've heard of heuristics saying that a fixed stepsize is best and then you just make sure to stop the algorithm a bit early before it diverges or behaves wildly -- is that true here? - Final section of 5.3, about the validation accuracy, and the accuracy on the test set after 50 epochs. I am confused why these are different numbers. Is it just because 50 epochs wasn't enough to reach convergence, while 300 seconds was? And why limit to 50 epochs then? Basically, what's the difference between the bottom two plots in Fig 3 (other than scaling the x-axis by time/epoch), and why does ProxProp achieve better accuracy only in the right figure?",46,1015,26.025641025641026,4.926829268292683,430,3,1012,0.0029644268774703,0.0202312138728323,0.9987,264,138,158,66,12,6,"{'ABS': 1, 'INT': 2, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 25, 'EXP': 5, 'RES': 4, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 25, 'SUB': 1, 'CLA': 3}",1,2,1,4,0,25,5,4,3,2,0,3,1,2,0,0,1,3,2,0,25,1,3,0.8640342562198225,0.6820518092827894,0.61611461641277
ICLR2018-BygpQlbA--R1,Reject,"This paper proposes a new algorithm to generate the optimal control inputs for unknown linear dynamical systems (LDS) with known system dimensions. The idea is exciting LDS by wave filter inputs and record the output and directly estimate the operator that maps the input to the output instead of estimating the hidden states. After obtaining this operator, this paper substitutes this operator to the optimal control problem and solve the optimal control problem to estimate the optimal control input, and show that the gap between the true optimal cost and the cost from applying estimated optimal control input is small with high probability. I think estimating the operator from the input to the output is interesting, instead of constructing (A, B, C, D) matrices, but this idea and all the techniques are from Hazan et. el., 2017. After estimating this operator, it is straightforward to use this to generate the estimated optimal control input. So I think the idea is OK, but not a breakthrough. Also I found the symmetric matrix assumption on A is quite limited. This limitation is from Hazan et. el., 2017, where the authors wants to predict the output. For prediction purposes, this restriction might be OK, but for control purposes, many interesting plants does not satisfy this assumption, even simple RL circuit. I agree with authors that this is an attempt to combine system identification with generating control inputs together, but I am not sure how to remove the restriction on A. Dean et. el., 2017 also pursued this direction by combining system identification with robust controller synthesis to handle estimation errors in the system matrices (A, B) in the state-feedback case (LQR), and I can see that Dean et. el. could be extended to handle observer-feedback case (LQG) without any restriction. Despite of this limitation I think the paper's idea is OK and the result is worth to be published but not in the current form. The paper is not clearly written and there are several areas need to be improved. 1. System identification. Subspace identification (N4SID) won't take exponential time. I recommend the authors to perform either proper literature review or cite one or two papers on the time complexity and their weakness. Also note that subspace identification can estimate (A, B, C, D) matrices which is great for control purposes especially for the infinite horizon LQR. 2. Clarification on the unit ball constraints. Optimal control inputs are restricted to be inside the unit ball and overall norm is bounded by L. Where is this restriction coming from? The standard LQG setup does not have this restriction. 3. Clarification on the assumption (3). Where is this assumption coming from? I can see that this makes the analysis go through but is this a reasonable assumption?  Does most of system satisfy this constraint? Is there any? It's ok not to provide the answer if it's hard to analyze, but if that's the case the paper should provide some numerical case studies to show this bound either holds or the gap is negligible in the toy example. 4. Proof of theorem 3.3. Theorem 3.3 is one of the key results in this paper, yet its proof is just  oted. The setup is slightly different from the original theorem in Hazan et. el., 2017 including the noise model, so I strongly recommend to include the original theorem in the appendix, and include the full proof in the appendix. 5. Proof of lemma 3.1. I found it's hard to keep track of which one is inside the expectation. I recommend to follow the notation E[variable] the authors been using throughout the paper in the proof instead of dropping these brackets. 6. Minor typos In theorem 2.4, ||Q||_op is used for defining rho, but in the text ||Q||_F is used. I think ||Q||_op is right.",28,637,15.166666666666666,4.936454849498328,250,5,632,0.0079113924050632,0.0266040688575899,0.9836,188,65,123,25,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 3, 'DAT': 0, 'MET': 15, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 13, 'SUB': 2, 'CLA': 2}",0,1,5,3,0,15,0,2,0,1,0,4,1,1,0,0,0,1,1,1,13,2,2,0.6469923421002368,0.6743179674639579,0.45488669093486706
ICLR2018-BygpQlbA--R2,Reject,"This paper studies the control of symmetric linear dynamical systems with unknown dynamics. Typically this problem is split into a (non-convex) system ID step followed by a derivation of an optimal controller, but there are few guarantees about this combined process. This manuscript formulates a convex program of optimal control without the separate system ID step, resulting in provably optimality guarantees and efficient algorithms (in terms of the sample complexity). The paper is generally pretty well written. This paper leans heavily on Hazan 2017 paper (https://arxiv.org/pdf/1711.00946.pdf).  Where the Hazan paper concerns itself with the system id portion of the control problem, this paper seems to be the controls extension of that same approach. From what I can tell, Hazan's paper introduces the idea of wave filtering (convolution of the input with eigenvectors of the Hankel matrix); the filtered output is then passed through another matrix that is being learned online (M). That matrix is then mapped back to system id (A,B,C,D). The most novel contribution of this ICLR paper seems to be equation (4), where the authors set up an optimization problem to solve for optimal inputs; much of that optimization set-up relies on Hazan's work, though. However, the authors do prove their work, which increases the novelty. The novelty would be improved with clearer differentiation from the Hazan 2017 paper. My biggest concerns that dampen my enthusiasm are some assumptions that may not be realistic in most controls settings:  - First, the most concerning assumption is that of a symmetric LDS matrix A (and Lyapunov stability). As far as I know, symmetric LDS models are not common in the controls community. From a couple of quick searches it seems like there are a few physics / chemistry applications where a symmetric A makes sense, but the authors don't do a good enough job setting up the context here to make the results compelling. Without that context it's hard to tell how broadly useful these results are. In Hazan's paper they mention that the system id portion, at least, seems to work with non-symmetric, and even non-linear dynamical systems (bottom of page 3, Hazan 2017). Is there any way to extend the current results to non-symmetric systems? - Second, it appears that the proposed methods may rely on running the dynamical system several times before attempting to control it. Am I misunderstanding something? If so this seems like it may be a significant constraint that would shrink the application space and impact even further. ",20,411,21.63157894736842,5.180661577608142,212,9,402,0.0223880597014925,0.0311750599520383,0.994,122,49,65,29,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,6,2,0,4,0,4,0,0,0,2,0,2,0,3,0,3,0,0,6,0,1,0.5014341682694657,0.4478787719020755,0.27982825530933747
ICLR2018-BygpQlbA--R3,Reject,"The paper presents a provable algorithm for controlling an unknown linear dynamical system (LDS). Given the recent interest in (deep) reinforcement learning (combined with the lack of theoretical guarantees in this space), this is a very timely problem to study. The authors provide a rigorous end-to-end analysis for the LDS setting, which is a mathematically clean yet highly non-trivial setup that has a long history in the controls field. The proposed approach leverages recent work that gives a novel parametrization of control problems in the LDS setting. After estimating the values of this parametrization, the authors formulate the problem of finding optimal control inputs as a large convex problem. The time and sample complexities of this approach are polynomial in all relevant parameters. The authors also highlight that their sample complexity depends only logarithmically on the time horizon T. The paper focuses on the theoretical results and does not present experiments (the polynomials are also not elaborated further). Overall, I think it is important to study control problems from a statistical perspective, and the LDS setting is a very natural target. Moreover, I find the proposed algorithmic approach interesting. However, I am not sure if the paper is a good fit for ICLR since it is purely theoretical in nature and has no experiments. I also have the following questions regarding the theoretical contributions:  (A) The authors emphasize the logarithmic dependence on T. However, the bounds also depend polynomially on L, and as far as I can tell, L can be polynomial in T for certain systems if we want to achieve a good overall cost. It would be helpful if the authors could comment on the dependence between T and L. (B) Why does the bound in Theorem 2.4 become worse when there are some directions that do not contribute to the cost (the lambda dependence)? (C) Do the authors expect that it will be straightforward to remove the assumption that A is symmetric, or is this an inherent limitation of the approach? Moreover, I have the following comments:  (1) Theorem 3.3 is currently not self-contained. It would enhance readability of the paper if the results were more self-contained. (It is obviously good to cite results from prior work, but then it would be more clear if the results are invoked as is without modifications.) (2) In Theorem 1.1, the notation is slightly unclear because B^T is only defined later. (3) In Section 1.2 (Tracking a known system): given instead of give (4) In Section 1.2 (Optimal control): symmetric instead of symmetrics (5) In Section 1.2 (Optimal control): the paper says rather than solving a recursive system of equations, we provide a formulation of control as a one-shot convex program. Is this meant as a contrast to the work of Dean et al. (2017)? Their abstract also claims to utilize a convex programming formulation. (6) Below Definition 2.3: What is capital X? (7) In Definition 2.3: What does the parenthesis in phi_j(1) denote? (8) Below Theorem 2.4: Why is Phi now nk x T instead of nk x nT as in Definition 2.3? (9) Lemma 3.2: Is hat{D} defined in the paper? I assume that it involves hat{M}, but it would be good to formally define this notation.",29,539,25.666666666666668,5.202083333333333,236,1,538,0.0018587360594795,0.022181146025878,0.9776,140,66,86,39,8,6,"{'ABS': 1, 'INT': 1, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 16, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 9}",1,1,2,3,0,16,2,4,0,0,0,3,0,0,1,0,0,1,1,0,14,1,9,0.5757246798473751,0.6754011938864577,0.40701482421358204
ICLR2018-Byht0GbRZ-R1,Reject,"Summary: This paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences. The span representations are weighted by the spans marginal scores given by the inside-outside algorithm. Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016). Strengths: The idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting. Weaknesses: The paper is 8.5 pages long .  The method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017) Aside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive. Am I missing something about the algorithm? It would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax. The paper claimed ""the model is able to recover tree structures that very closely mimic syntax"", but it's hard to draw this conclusion from the two examples in Figure 2",10,180,22.5,5.4678362573099415,109,0,180,0.0,0.0109890109890109,0.5937,55,27,25,9,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,1,2,6,1,0,1,0,0,1,1,0,0,0,0,4,1,0,2,0,0,0.6442246820683436,0.3343067784397462,0.3274598060444335
ICLR2018-Byht0GbRZ-R2,Reject,"This paper proposes a model of structured alignments between sentences as a means of comparing two sentences by matching their latent structures. Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention .  In section 3.1, the formula for p(c|x) looks wrong: c_{ijk} are indicator variables. but where are the scores for each span? I think it should be c_{ijk} * delta_{ijk} under the summations instead. In the same section, the expression for alpha_{ij} seems to assume that delta_{ijk}   dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions). This seems rather limiting, can you comment on that? In the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing. The plots in Fig 2 with the marginals on CKY charts are not very enlightening. How do this marginals help solving the NLI task? Minor comments: - Sec. 3: Language is inherently tree structured -- this is debatable... - page 8: (laf, 2008): bad formatted reference",12,180,12.857142857142858,5.230769230769231,115,4,176,0.0227272727272727,0.0478723404255319,-0.8425,53,29,30,10,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,9,0,0,1,0,0,0,2,0,0,0,1,1,3,0,2,0,0,0.3592372749603557,0.445189539726894,0.19662538167473284
ICLR2018-Byht0GbRZ-R3,Reject,"This paper describes the use of latent context-free derivations, using a CRF-style neural model, as a latent level of representation in neural attention models that consider pairs of sentences. The model implicitly learns a distribution over derivations, and uses marginals under this distribution to bias attention distributions over spans in one sentence given a span in another sentence. This is an intriguing idea . I had a couple of reservations however:  * The empirical improvements from the method seem pretty marginal, to the point that it's difficult to know what is really helping the model. I would liked to have seen more explanation of what the model has learned, and more comparisons to other baselines that make use of attention over spans. For example, what happens if every span is considered as an independent random variable, with no use of a tree structure or the CKY chart? * The use of the alpha^0 vs. alpha^1 variables is not entirely clear. Once they have been calculated in Algorithm 1, how are they used? Do the rho values somewhere treat these two quantities differently? * I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately. I think something much more extensive would be interesting here. As one example, the PP attachment example with at a large venue is highly suspect; there's a 50/50 chance that any attachment like this will be correct, there's absolutely no way of knowing if the model is doing something interesting/correct or performing at a chance level, given a single example. ",11,251,22.818181818181817,5.225531914893617,147,1,250,0.004,0.0583657587548638,0.9177,70,27,45,15,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,1,1,0,9,1,0,0,1,0,0,0,0,0,0,0,1,1,0,4,1,0,0.4305729654817058,0.4463103167489733,0.23347181369258885
ICLR2018-ByhthReRb-R1,Reject,"The paper proposes to generate embedding of named-entities on the fly during dialogue sessions. If the text is from the user, a named entity recognizer is used. If it is from the bot response, then it is known which words are named entities therefore embedding can be constructed directly. The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way. One thing I hope the author could provide more clarification is the use of NER. For example, the experimental result on structured QA task (section 3.1), where it states that the performance different between models of With-NE-Table and W/O-NE-Table is positioned on the OOV NEs not present in the training subset. To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER. This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful. You could use an NER for W/O-NE-Table and update the NE embeddings, and it should be as good as With-NE-Table model (and fairer to compare with too). That said, overall the paper is a nice contribution to dialogue and QA system research by pointing out a simple way of handling named entities by dynamically updating their embeddings. It would be better if the paper could point out the importance of NER for user utterances, and the fact that using the knowledge of which words are NEs in dialogue models could help in tasks where DB queries are necessary.",11,301,27.363636363636363,4.76530612244898,147,1,300,0.0033333333333333,0.0166112956810631,0.9789,82,25,54,11,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,8,1,1,0,0,0,1,0,0,0,1,0,1,0,0,7,0,0,0.4303227733679212,0.3370650779423911,0.21810778017897858
ICLR2018-ByhthReRb-R2,Reject,"The paper addresses the task of dealing with named entities in goal oriented dialog systems. Named entities, and rare words in general, are indeed troublesome since adding them to the dictionary is expensive, replacing them with coarse labels (ne_loc, unk) looses information, and so on. The proposed solution is to extend neural dialog models by introducing a named entity table, instantiated on the fly, where the keys are distributed representations of the dialog context and the values are the named entities themselves. The approach is applied to settings involving interacting to a database and a mechanism for handling the interaction is proposed. The resulting model is illustrated on a few goal-oriented dialog tasks. I found the paper difficult to read. The concrete mappings used to create the NE keys and attention keys are missing. Providing more structure to the text would also be useful vs. long, wordy paragraphs. Here are some specific questions:  1. How are the keys generated?  That are the functions used? Does the knowledge of the current user utterance include the word itself? The authors should include the exact model specification, including for the HRED model. 2. According to the description, referring to an existing named entity must be done by generating a key to match the keys in the NE table and then retrieve the corresponding value and use it. Is there a guarantee that a same named entity, appearing later in the dialog, will be given the same key? Or are the keys for already found entities retrieved directly, by value? 3. In the decoding phase, how does the system decide whether to query the DB? 4. How is the model trained? In its current form, it's not clear how the proposed approach tackles the shortcomings mentioned in the introduction. Furthermore, while the highlighted contribution is the named entity table, it is always used in conjunction to the database approach. This raises the question whether the named entity table can only work in this context. For the structured QA task, there are 400 training examples, and 100 named entities. This means that the number of training examples per named entity is very small. Is that correct? If yes, then it's not very surprising that adding the named entities to the vocabulary leads to overfitting. Have you compared with using random embeddings for the named entities? Typos: page 2, second-to-last paragraph: firs -> first, page 7, second to last paragraph: and and -> and.",25,406,18.454545454545453,5.061538461538461,200,0,406,0.0,0.0268292682926829,0.8246,108,36,91,20,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 4, 'MET': 16, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 2}",0,1,0,1,4,16,3,0,0,0,0,3,0,0,0,0,0,1,1,0,11,1,2,0.4326958116973974,0.5618562649748394,0.2717987667615332
ICLR2018-ByhthReRb-R3,Reject,"Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation. Mots successful approaches have argued for separate mechanism for NE captures, that rely on various hacks and tricks. This paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments. The proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process. All we have a besides text is a small figure (figure 1). Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies? The initial description (section 2)  leaves way too many unanswered questions: - What embeddings are used for words detected as NE? Is it the same as the generated representation? - What is the exact mechanism of generating a representation for NE EECS545? (end of page 2) - Is it correct that the same representation stored in the NE table is used twice? (a) To retrieve the key (a vector) given the value (a string)  as the encoder input. (b) To find the value that best matches a key at the decoder stage? - Exact description of the column attention mechanism: some similarity between a key embedding and embeddings representing each column? Multiplicative? Additive? - How is the system supervised? Do we need to give the name of the column the Attention-Column-Query attention should focus on? Because of this unknown, I could not understand the experiment setup and data formatting! The list goes on...  For such a complex architecture, the authors must try to analyze separate modules as much as possible. As neither the QA and the Babi tasks use the RNN dialog manager, while not start with something that only works at the sentence level. The Q&A task could be used to describe a simpler system with only a decoder accessing the DB table. Complexity for solving the Babi tasks could be added later.",20,360,32.72727272727273,5.115044247787611,196,1,359,0.0027855153203342,0.0135869565217391,0.9711,110,36,62,16,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 12, 'EXP': 5, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,1,1,1,2,12,5,0,2,0,0,0,0,0,0,0,0,0,1,0,10,1,0,0.5031562851190661,0.33893095024692,0.25627339441613256
ICLR2018-Byk4My-RZ-R1,Reject,"Summary:  The paper proposes to learn new priors for latent codes z  for GAN training. for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator . To fix this the paper proposes to learn a second GAN to learn the prior distributions of real latent code of the first GAN. The first GAN then uses the second GAN as prior to generate the z codes.  Quality/clarity:  The paper is well written and easy to follow. Originality:  pros: -The paper while simple sheds some light on important problem with the prior distribution used in GAN. - the second GAN solution trained on reverse codes from real data is interesting  - In general the topic is interesting, the solution presented is simple but needs more study cons:  - It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement.  - The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder) General Review:  More experimentation with the latent codes will be interesting: - Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator?  Is this data low rank?  how does this change depending on the dimensionality of the latent codes? Maybe adding plots to the paper can help. - the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate. Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior? - Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc? Maybe also rotating the codes with the singular vector matrix V or Sigma^{0.5} V? - What architecture did you use for the prior generator GAN? - Have you thought of an end to end way to learn the prior generator GAN? ****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful.",22,374,34.0,4.818681318681318,161,4,370,0.0108108108108108,0.0149625935162094,0.9894,114,57,66,11,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 15, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 1}",0,1,1,2,1,15,1,0,1,0,0,1,0,1,0,2,0,1,1,0,10,2,1,0.646419132311984,0.672416067673093,0.4589317834958049
ICLR2018-Byk4My-RZ-R2,Reject,"The paper demonstrates the need and usage for flexible priors in the latent space alongside current priors used for the generator network. These priors are indirectly induced from the data - the example discussed is via an empirical diagonal covariance assumption for a multivariate Gaussian.  The experimental results show the benefits of this approach. The paper provides for a good read. Comments:  1. How do the PAG scores differ when using a full covariance structure? Diagonal covariances are still very restrictive. 2. The results are depicted with a latent space of 20 dimensions. It will be informative to see how the model holds in high-dimensional settings. And when data can be sparse.  3. You could consider giving the Discriminator, real data etc in Fig 1 for completeness as a graphical summary.",10,130,10.833333333333334,5.408333333333333,85,0,130,0.0,0.0074626865671641,0.8316,37,17,20,3,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 3, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,0,0,3,4,2,2,1,0,0,1,0,0,0,0,0,0,1,0,7,0,1,0.5009646555055591,0.3370650779423911,0.2562460055102345
ICLR2018-Byk4My-RZ-R3,Reject,"The paper proposes, under the GAN setting, mapping real data points back to the latent space via the generator reversal procedure on a sample-by-sample basis (hence without the need of a shared recognition network) and then using this induced empirical distribution as the ideal prior targeting which yet another GAN network might be trained to produce a better prior for the original GAN. I find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below. 1. Actually I find the entire notion of an ideal prior under the GAN setting a bit strange. To start with, GAN is already training the generator G to match the induced P_G(x) (from P(z)) with P_d(x), and hence by definition, under the generator G, there should be no better prior than P(z) itself (because any change of P(z) would then induce a different P_G(x) and hence only move away from the learning target). I get it that maybe under different P(z) the difficulty of learning a good generator G can be different, and therefore one may wish to iterate between updating G (under the current P(z)) and updating P(z) (under the current G), and hopefully this process might converge to a better solution. But I feel this sounds like a new angle and not the one that is adopted by the authors in this paper. 2. I think the discussions around Eq. (1) are not well grounded. Just as you said right before presenting Eq. (1), typically the goal of learning a DGM is just to match Q_x with the true data distrubution P_x. It is **not** however to match Q(x,z) with P(x,z). And btw, don't you need to put E_z[ ... ] around the 2nd term on the r.h.s. ? 3. I find the paper mingles notions from GAN and VAE sometimes and misrepresents some of the key differences between the two. E.g. in the beginning of the 2nd paragraph in Introduction, the authors write Generative models like GANs, VAEs and others typically define a generative model via a deterministic generative mechanism or generator .... While I think the use of a **deterministic** generator is probably one of the unique features of GAN, and that is certainly not the case with VAE, where typically people still need to specify an explicit probabilistic generative model. And for this same reason, I find the multiple references of a generative model P(x|z) in this paper inaccurate and a bit misleading. 4. I'm not sure whether it makes good sense to apply an SVD decomposition to the hat{z} vectors. It seems to me the variances  u^2_i shall be directly estimated from hat{z} as is. Otherwise, the reference ideal distribution would be modeling a **rotated** version of the hat{z} samples, which imo only introduces unnecessary discrepancies. 5. I don't quite agree with the asserted multi-modal structure in Figure 2. Let's assume a 2d latent space, where each quadrant represents one MNIST digit (e.g. 1,2,3,4). You may observe a similar structure in this latent space yet still learn a good generator under even a standard 2d Gaussian prior. I guess my point is, a seemingly well-partitioned latent space doesn't bear an obvious correlation with a multi-modal distribution in it. 6. The generator reversal procedure needs to be carried out once for each data point separately, and also when the generator has been updated, which seems to be introducing a potentially significant bottleneck into the training process.",18,581,17.606060606060606,4.98499061913696,271,15,566,0.0265017667844522,0.0392491467576791,0.9926,165,71,90,53,9,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 13, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 0, 'CLA': 3}",0,1,2,1,1,13,3,0,1,0,0,1,0,1,0,0,0,0,0,0,15,0,3,0.6461123277083602,0.2310918964517319,0.28795842735721194
ICLR2018-BykJlIAbM-R1,,"This paper proposed a NMT system that expands each sentence pair to two groups of similar sentences. The idea of using similar sentence pairs as cluster-to-cluster translation is interesting. The experimental results seem promising, but the presentation can be improved. Some parts of the paper are hard to read. Major 1. What is the model/baseline in Tables 3, 4, 5? 2. What is the intuition in adding target cluster entropy in Eq. 3? 3. In the adaptive cluster, I am a bit confused on the target of the parametric models. Where are X, Y of P(X|X*), P(Y|Y*) from? Is it from pretrained models? It wasn't clear until I read the algorithm. Also, why are p(X|X*) called target cluster and P(Y|Y*) called source cluster? 4. In section 4.2, the name cluster is a bit confusing with the one in section 3.1. What's the relationship? The symbols C(Y*) and C(X*) are not used afterward. 5. In the conclusion, it claims the system is efficient in helping current model. What do you mean by efficient? 6. The improvements of WMT are relatively small. Does it mean the proposed methods are not beneficial when there are large amounts of sentence pairs? 7. What's the reward used in the experiments ? 8. In the Monte-Carlo sampling, how many pairs are sampled?  Minor  1. In Table 1, where is sigma defined? 2. The notation D for a dataset in Section 3.3 is confusing with D in system D. 3. There is some redundancy between Systems A, B, C, D and in the algorithm 1. I wonder whether it can be simplified. 4. In section 4.3, backward NMT (X|Y) -> backward NMT P(X|Y). 5. It will be great to show detailed derivation, for example from Eq. 9 to Eq. 10. 6. Some recent results on WMT DE-EN are missing, such ashttps://arxiv.org/abs/1706.03762. ",19,302,8.882352941176471,4.7727272727272725,144,0,302,0.0,0.022801302931596,0.9627,96,29,58,7,11,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 3, 'MET': 9, 'EXP': 4, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,2,1,3,9,4,1,1,1,0,1,1,0,0,0,0,0,4,0,6,0,1,0.7881270793818601,0.3366278272789582,0.396821227004131
ICLR2018-BykJlIAbM-R2,,"This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples. The paper claims superior results using the described method. Overall, there are a few problems with the paper. 1) The arguments for using clusters instead of single sentences are questionable.   The paper claims several times that MLE training for NMT faces over-training (or data sparsity) -- while that can be true depending on the corpus and model used, there are well-known remedies for that, for example regularization via dropout (almost everybody uses that). It is not clear why that is not used or at least compared to the method presented. 2) The writing of the paper is often unclear (and sometimes grammatically wrong, typos etc. but that aside), there are some made up words/concepts (What is 'Golden Centroid Augmentation or Model Centroid Augmentation? The reason for attention is not to better memorize input information, it is to be able to attend to certain regions in the input. The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood but not for improving on the train/test loss discrepancy. There are lots more examples of unclear statements in this paper -- it should be heavily improved. 3) Section 3 and 4 are very hard/impossible to understand, it is not clear how the formulas help the reader to better understand the concept in any way. 5) The results presented in this paper given the complexity of the method are just not great -- for example, WMT en-de is 21.3 BLEU reported by you while much older papers report for example 24.67 BLEU (Google's Neural Machine Translation System) -- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that? .  6) Finally, what is missing most is simply why a much simpler method (just generate some data using a trained system and use that as additional training data, with details on how much etc.) -- is not directly compared to this very complicated looking method.",13,357,27.46153846153846,4.895348837209302,173,2,355,0.0056338028169014,0.0218579234972677,0.2613,88,38,63,33,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 3, 'MET': 9, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 4}",0,1,1,0,3,9,1,2,0,0,0,3,0,0,0,0,0,0,0,0,6,0,4,0.5022202578988748,0.2255754146089992,0.22650256635189406
ICLR2018-BykJlIAbM-R3,,"This work tries to generalize the framework of reward augmented maximum likelihood criterion by introducing the notion of cluster, which represents a set of similar data point, e.g., sentence, according to a metric. By employing the cluster, this work propose a joint source/target modeling by varying how sampling is performed, e.g., draw independently or conditionally, and how the cluster are constructed, e.g., model-wise or non-model. Experiments on German/English and Chinese/English show gains over other reinforcement learning methods. If my understanding is correct, the motivation is investigate alternative combination of how a cluster is constructed, e.g., sampling and model-based scoring. However one of the problems of this paper is clarity. - The notion of cluster is still unclear and it took me long to understand it probably because it might be easily confused with other terminology, e.g., clustering. Also, cluster-to-cluster might not fit well. - It is hard to map System-{ABCD} to the underlying proposed methods described in Table 2. Also, I feel algorithm 1 is spurious given that it merely switch by systems. Probably better to introduce branch for key methods, parallel sampling/ translation broadcasting and inadaptive or adaptive model.",10,188,18.8,5.591160220994476,120,4,184,0.0217391304347826,0.0368421052631578,0.7562,45,26,38,13,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,0,1,1,5,1,0,1,0,0,1,0,0,0,0,0,0,2,0,3,0,1,0.5010007684551386,0.3346388171268226,0.24824206701615717
ICLR2018-Bym0cU1CZ-R1,Reject,"The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues. In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement. While this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue. If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems. Some points: 1. The introduction uses scalability throughout to mean something closer to ability to generalize. Consider revising the wording here. 2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000). It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora. 3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?) 3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space. A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity. 4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices. If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited. Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable. 5. The comparison against previous work is missing some assurances I'd like to see. While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to. Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison. 6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?) 7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections. It's fine if that pushes the paper somewhat over the 8th page. 8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research. 9. The examples provided in the appendix are great. It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked).",22,589,16.82857142857143,5.32620320855615,286,9,580,0.0155172413793103,0.0237691001697792,0.9963,162,65,101,34,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 0, 'DAT': 1, 'MET': 11, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 11, 'SUB': 3, 'CLA': 2}",0,2,5,0,1,11,3,1,1,0,0,2,2,0,0,1,0,3,2,0,11,3,2,0.645936737930146,0.6734766231407767,0.45505081455578456
ICLR2018-Bym0cU1CZ-R2,Reject,"The topic discussed in this paper is interesting. Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling,; however this paper is difficult for me to follow. For example:  1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning. 2) the formulation in equation 4 seems to be problematic 3) simplify pr(ri|si,ai) as pr(ri|ai,uiu22121,uiu22122) since decoding natural language responses from long conversation history is challenging  to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something? 4) about section 3.2, again I didn't get whether the model needs RL for training. 5) We train m(u00b7, u00b7) with the 30 million crawled data through negative sampling. not sure I understand the connection between training $m(cdot, cdot)$ and the entire model. 6) the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.",14,227,18.916666666666668,5.30622009569378,126,1,226,0.0044247787610619,0.0393013100436681,-0.5547,67,29,39,12,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 2}",0,0,0,1,0,10,3,0,1,1,0,1,0,0,0,0,0,0,0,0,9,1,2,0.4309955491789079,0.3383901278829311,0.21383112781874736
ICLR2018-Bym0cU1CZ-R3,Reject,"The paper describes a technique to incorporate dialog acts into neural conversational agents. This is very interesting work.  Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act. A very important type of dialog act is switching topic, often done to ensure that the conversation will continue.  The paper describes a classifier that predicts the dialog act of the next utterance. The next utterance is then generated based on this dialog act. The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning. This is also very interesting. The empirical evaluation demonstrates the effectiveness of the approach. The paper is also well written. I do not have any suggestion for improvement.  This is good work that should be published.",10,144,12.0,5.4071428571428575,81,1,143,0.0069930069930069,0.0136054421768707,0.9582,41,14,29,13,4,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,6,1,0,0,0,0,3,0,0,0,0,0,0,0,1,6,0,1,0.2870560838993102,0.3364431205075482,0.14584075237771316
ICLR2018-Byni8NLHf-R1,,"The authors propose first applying dependency parsing to documents, then using pairs of words connected via dependency as features in a similarity metric. While intriguing, a lot more work would be required to publish this at ICLR. Namely, the following questions need to be answered:  1. Does using linked-word-pairs truly raise the state of the art? Unlike what is stated in the abstract, the experimental results only compare RBMs with and without this feature. RBMs are not state-of-the-art in topic modeling, therefore it's difficult to assess whether this is helpful. 2. If linked words does improve topic modeling, why does it do so? There needs to be some sort of error analysis to show why this idea improves, rather than simply stating metrics. 3. Are words that are linked via a dependency better than commonly co-occuring words? Experiments need to be done to show that a full dependency parse is actually required, rather than simply looking for co-occuring words. 4. How is this work related to the extensive work in NLP in applying parsing to various tasks? A quick search reveals [1] (probabilistic modeling of dependency parses to create Bayesian topic models directly) and [2] (creating a semantic vector space from a dependency parse) I suspect there are others . Citations in [2] could be a good place to start. 5. Can the selection of word pairs be done automatically, from data, rather than pre-computed with a known dependency parser? After all, this is submitted to the International Conference on Learning Representations --- feature engineering papers can easily be published at EMNLP, ICML, etc. An excellent ICLR paper would show some way to either (a) use dependency parsing only at training time (to provide a hint), or (b) not require dependency parsing at all. A few suggestions for experiments: A. I would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation, just using log(tf)-idf as a distance metric. By stripping away more advanced modeling, that could reveal whether the dependency bi-gram has utility . B. The authors may wish to consider applying LSA to both bag of words and dependency-bigrams, using log(tf)-idf weighting for both . From what I've seen, log(tf)-idf LSA seems to perform about as well as LDA . Plain LSA takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on LSA at all . C. Reiterating point (3) above, to really show whether the power of the dependency parse is being used, I would strongly suggest doing a null experiment with co-occuring nearby words. References: [1] Boyd-Graber, J. L., & Blei, D. M. (2009). Syntactic topic models. In Advances in neural information processing systems(pp. 185-192). [2] Padu00f3, S. and Lapata, M., 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2), pp.161-199.",21,457,13.057142857142855,5.450602409638554,233,2,455,0.0043956043956043,0.0299785867237687,0.9874,143,43,95,27,11,5,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 5, 'MET': 8, 'EXP': 8, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 2, 'CLA': 0}",1,1,1,1,5,8,8,2,0,2,0,1,1,0,0,0,1,4,0,1,10,2,0,0.7882937852856388,0.5616113366490336,0.493490493008794
ICLR2018-Byni8NLHf-R2,,"This paper applies the word pairs, instead to bag of words, to current RBM models . The word pairs are extracted using Stanford parser . The word pairs are further filtered and clustered to improve the representation . The experiments show improvement over baselines. However, I think this paper has limited contribution and novelty, and the experiments also need to be improved . The detailed comments are as follows:  - The main contribution of this paper is to apply word pairs instead of words to RBM models . However, the main techniques such as RBM, parser to extract word pairs, tf-idf for filtering, and k-means for clustering, are all existing standard techniques. It is more like an application of these methods, and has limited contribution and novelty. - For experiments, they apply k-means clustering in the process so k is one parameter to tune. K needs to be tuned on validation set instead of testing set.  This paper simply presents the results of different parameter k on testing set directly. - The structure of Section 3 needs to be improved. Instead of listing each step in each subsection, a general introduction picture should be introduced first. More intuition is also needed for each step. - Some figures and tables are overlapping in the experiments. Just keep one is enough. - The format of reference should be fixed in this paper.",15,220,12.941176470588236,5.065420560747664,107,1,219,0.0045662100456621,0.0172413793103448,0.902,67,11,53,14,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 4, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,2,0,1,1,5,4,1,1,0,0,1,1,0,0,2,0,1,1,0,5,1,1,0.644142017616723,0.6691996039905567,0.45480895457524767
ICLR2018-Byni8NLHf-R3,,"This submission does not fit ICLR. - The center topic does not fit ICLR . The main novelty is about using word pair embedding to improve the Topic model . The word-pair was generated by the Standford dependency parser .   - Many citation errors exist - No clear novelty - The experimental setup is problematic . The authors filtered the number of words and word-pairs to very small . It is hard to justify any of the results after these strategies .   - The baselines are not thorough and lack proper justifications .   - The experimental results are not properly presented, with many overlapping figures . No insights can be derived from the presented results.",12,101,10.1,5.1683168316831685,69,0,101,0.0,0.0165289256198347,-0.7988,30,12,22,6,10,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 1, 'MET': 2, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,0,1,3,1,2,2,3,1,1,0,1,1,0,1,2,0,0,1,1,3,1,0,0.714852105700338,0.6679556891208708,0.504467006693945
ICLR2018-BypdvewVM-R1,,"I liked this paper mostly because it surprised me and because it might spur the development of novel variants of Difference Target-Propagation (DTP). The paper does a good job of highlighting the relevant background and issues and introduces a slight variation to DTP which actually works as well while being more biologically plausible. There was a concern or assumption in the original DTP paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the DTP propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments. In call cases, the variant using the DTP target update everywhere works about as well as using the true gradient for the output layer. Another quirk that the proposed variant (SDTP) removes from the orignal DTP paper is the way noise is handled, and I agree that denoising makes a lot of sense (than noise preservation) while being more biologically plausible.  Finally, the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop. The paper is very clear and I have just outlined the original contributions and significance (DTP may have been a bit forgotten and is worth another look, apparently). In the negatives, the paper should mention in the discussion and intro that all the TP variants ignore the issue of dynamics. We know that there are of course lateral connections and that feedback connections do not operate independently of the feedforward one (or there would be a need for a precise 'clockwork' mechanism to sweep layers forward and backward, which seems not very plausible). In the experimental results section, it would be good to report the CNN results as well (with shared weights, same architecture) . Also, training errors should be shown, since I suspect that underfitting may be happening especially in the case of ImageNet. If that was the case, future work should first explore higher capacity (which may require larger-memory GPUs...). Finally, in the description of architectures, please define the structure notation, e.g. (3 x 3, 32, 2, SAME).",13,367,26.214285714285715,5.142857142857143,196,6,361,0.0166204986149584,0.040650406504065,0.9824,93,35,66,29,7,4,"{'ABS': 0, 'INT': 3, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 2}",0,3,2,0,0,4,2,2,0,0,1,2,0,0,0,0,0,1,1,0,5,0,2,0.5010694185948376,0.4470134092546703,0.2767480856487446
ICLR2018-BypdvewVM-R2,,"Summary of the paper: The paper analysis how well difference target probation (DTP) - an optimisation algorithm designed to be biologically more plausible than backpropagation - scales to bigger datasets like CIFAR-10 and ImageNet.  The DTP algorithm is slightly adapted to make it more biologically plausible, by replacing the gradient computation the original paper applied between the highest hidden layers by target propagation (leading to the variant SDTP), and by making the optimisation of both involved losses parallel. Furthermore, only feedforward and locally connected networks (CNN) are considered since their architecture is considered more biologically plausible than convolutional neural networks. While on MNIST and CIFAR, DTP and SDTP performed as well as backprop, they perform worse on ImageNet . Furthermore, it becomes clear, that without CNN structure no really good performance is achieved neither on CIFAR nor on ImageNet  Pros: - The paper is nicely written and good to follow. - Suggested modifications from DTP to STDP increase its biological plausibility without making its performance worse. - The worse performance compared to backprop and CNNs underlines the open question how to yield biologically plausible AND efficient algorithms and network architectures. Cons: - The title of the paper seems to general to me, since target propagation is the only algorithm compared against backpropagation. - Since the adaptions to DTP are rather small, the work does not contain much novelty. It can rather be seen as an interesting empirical study, with  egative result .   Minor comments: - page 1: The reference list could also include  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00934 and  https://arxiv.org/abs/1510.02777 -  page 5: ""the the degree"" , ""specified as (....) followed by"" -> , ""as (....) followed by"" ?, - This notation probably stems from the code, but SAME and VALID could be nicer described as ""0 padding"" and ""no padding"" for example. - page 8:  ""applying BP to the brain"" sounds strange to me. ",14,292,22.46153846153846,5.510273972602739,168,2,290,0.0068965517241379,0.0155763239875389,0.8571,85,31,49,27,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 4}",0,2,1,0,0,7,1,3,0,1,0,2,1,0,0,1,0,0,0,0,5,0,4,0.5731120656340644,0.3360645682852674,0.29049010649976137
ICLR2018-BypdvewVM-R3,,"This is a high-quality and clear paper looking at biologically-plausible learning algorithms for deep neural networks. The contributions here are: 1) experiments testing the DTP algorithm on more difficult datasets, 2) proposing a minor modification of the DTP algorithm at the output layer, and 3) testing the DTP algorithm on locally-connected architectures. These are all novel contributions, but each one seems incremental in the context of previous work on this and similar algorithms (E.G. Nokland, Direct Feedback Alignment Provides Learning in Deep Neural Networks, 2016; Baldi et al, Learning in the Machine: The Symmetries of the Deep Learning Channel, 2017).   ",5,100,20.0,5.731182795698925,60,1,99,0.0101010101010101,0.0194174757281553,0.1415,30,16,13,2,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 1, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,1,1,1,2,0,0,0,0,0,0,0,0,1,0,0,0,0,4,0,0,0.3572290529345658,0.2240880945267511,0.16126481052845373
ICLR2018-ByqFhGZCW-R1,Reject,"The game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study. Lemma 1 summarizes properties of the solutions that are expected to have after reaching equilibria. Important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave w.r.t. to the target variable. In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1. u3000Nonetheless, this type of analysis can be useful under appropriate solutions if non-trivial claims are derived; however, Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims. As long as the analysis is experimental, the state of the art should be considered. As long as the reviewer knows, the CW attack gives the most powerful attack and this should be considered for comparison. The results with MNIST and CIFAR-10 are different. In some cases, MNIST is too easy to consider the complex structure of deep architectures. I prefer to have discussions on experimental results with both datasets. The main takeaway from the entire paper is not clear very much. It contains a game-theoretic framework of adversarial examples/training, novel attack method, and many experimental results. Minor: Definition of g in the beginning of Sec 3.1 seems to be a typo. What is u? This is revealed in the latter sections but should be specified here. In Section 3.1,  >This is in stark contrast with the near-perfect misclassification of the undefended classifier in Table 1. The results shown in the table seems to indicate the ""perfect"" misclassification. Sentence after eq. 15 seems to contain a grammatical error. The paragraph after eq. 17 is duplicated with a paragraph introduced before.",17,304,14.476190476190476,5.381944444444445,156,3,301,0.0099667774086378,0.0392156862745098,0.7154,78,48,50,16,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 10, 'EXP': 2, 'RES': 1, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 3}",0,1,0,0,1,10,2,1,2,1,0,2,0,0,0,0,1,1,4,0,4,0,3,0.5738352130130993,0.5577684047732026,0.35456617086539194
ICLR2018-ByqFhGZCW-R2,Reject,"This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses. It has a lemma which claims that the minimax and the maximin solutions provide the best worst-case defense and attack models, respectively, without proof, although that statement is supported experimentally. + Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain. - The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5. - There is no exact (alternating optimization could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm. - The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis. - It is unclear (from Figures 3 and 7) that alternative optimization and minimax converged fully, and/or that the sets of hyperparameters were optimal. + This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims. Nitpicks: the gradient descent -> gradient descent or the gradient descent algorithm seeming -> seemingly arbitrary flexible -> arbitrarily flexible can name gradient descent that maximizes: gradient ascent. The mini- max or the maximin solution is defined -> are defined is the follow -> is the follower ",9,256,18.285714285714285,5.6016260162601625,146,2,254,0.0078740157480314,0.0261194029850746,0.8888,76,34,42,11,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 4, 'EXP': 3, 'RES': 0, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,1,0,1,4,3,0,2,1,0,0,0,0,0,0,0,1,1,0,5,1,0,0.500946265885478,0.4469322741838163,0.28070708199372807
ICLR2018-ByqFhGZCW-R3,Reject,"The authors describe a mechanism for defending against adversarial learning attacks on classifiers. They first consider the dynamics generated by the following procedure. They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating. They next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent. They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach. They then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples). In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps. They conclude with empirical observations about the performance of this algorithm. The paper is well-written and easy to follow. However, I found the empirical results to be a little underwhelming. Sens-FGSM outperforms the adversarial training defenses tuned for the ""wrong"" iteration, but it does not appear to perform particularly well with error rates well above 20%. How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)? Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM?  It is my understanding that Sens-FGSM is not trained to a particular iteration of the ""cat-and-mouse"" game. Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81? With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.),; thus I would liked to see more thorough experiments here as well. For example, it's unlikely that the defender would ever know the attack network utilized by an attacker. How robust is the defense against samples generated by a different attack network? The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying. Any defense can be *evaluated* against samples generated by any attacker strategy. Is it the case that the defenses fall flat against samples generated by different architectures?  Minor Comments: Section 3.1, First Line. ""f(ul(g(x),y))"" appears to be a mistake.",23,404,18.363636363636363,5.4755784061696655,197,1,403,0.0024813895781637,0.0147783251231527,-0.9385,114,51,72,26,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 14, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 2}",0,1,1,0,2,14,6,1,0,3,0,2,0,0,0,1,1,0,0,0,14,1,2,0.575242996175492,0.5637221372793683,0.3664039325654947
ICLR2018-ByquB-WC--R1,Reject,"The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer. The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task .  Pros: - The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model. - The paper is clearly written. Cons: - I am not sure what is novel in the proposed model. While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network. Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N. Please tell me if I am missing something, but I am not sure of the contribution of the paper. Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful. Question: - What is the exact contribution of the paper with respect to MemN2N and GMemN2N?",10,194,16.166666666666668,5.005376344086022,110,1,193,0.005181347150259,0.065,0.919,62,19,38,8,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,0,2,0,5,1,0,0,1,0,2,0,0,0,2,1,0,1,0,3,0,1,0.4296769156964735,0.5568445780097596,0.2584954637504703
ICLR2018-ByquB-WC--R2,Reject,"This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN). RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017). RMN reduces the complexity to linear time for the bAbi dataset. RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning. RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time. Comments for the author:  The paper addresses an important problem since understanding object interactions are crucial for reasoning. However, how widespread is this problem across other models or are you simply addressing a point problem for RN? For example, Entnet is able to reason as the input is fed in and the decoding costs are low.  Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply. The relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP). It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks. For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion. More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights. Showing results with multiple hops (1,2,..) would be useful here. More details are needed about Figure 3. Is this on bAbi as well? How did you generate these stories with so many sentences? Another clarification is the bAbi performance over Entnet which claims to solve all tasks. Your results show 4 failed tasks, is this your reproduction of Entnet? Finally, what are the savings from reducing this time complexity? Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops. Overall, this paper feels like a small improvement over RN. Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement. One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.",24,381,19.05,5.190860215053763,189,1,380,0.0026315789473684,0.010443864229765,0.9437,120,46,64,20,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 1, 'DAT': 4, 'MET': 16, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 6, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 0}",0,1,6,1,4,16,2,2,1,0,0,0,1,0,0,0,1,6,0,0,14,1,0,0.6472626339799313,0.453115703883353,0.3707114427264686
ICLR2018-ByquB-WC--R3,Reject,"This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input. The model achieves good results on bAbI compared to memory networks and the relation network model. From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question). This average is then fed to a softmax layer for answer prediction. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)? The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments. I'd encourage the authors to do a more detailed experimental study with more tasks,; but I can't recommend this paper's acceptance in its current form. other questions / comments: - we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question. isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation? - writing could be cleaned up for spelling / grammar (e.g., last 70 stories instead of last 70 sentences), currently the paper is very hard to read and it took me a while to understand the model",11,306,34.0,5.153061224489796,163,1,305,0.0032786885245901,0.0290322580645161,0.8812,94,25,48,22,5,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 1}",0,1,2,0,0,7,2,0,0,0,0,2,0,0,0,0,0,2,0,1,4,2,1,0.3588561123277083,0.5576452669255959,0.2239750172926234
ICLR2018-ByrZyglCb-R1,Accept,"The paper is written well and clear. The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist.  Pros: the intuition and geometry is rather clearly presented. Cons:  References to CaffeNet  and LeNet (even though the latter is well-known) are missing. In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries, there is no description of how these networks were trained. It is not clear why the authors have decided to use out-dated 5-layer LeNet  and NiN (Network in network) architectures instead of more recent and much better performing architectures (and less complex than NiN architectures). It would be nice to see how the behavior and boundaries look in these cases.  The conclusion is speculative: Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it is key to suppress this subspace of shared positive directions, which can possibly be done through regularization of the objective function. This will be the subject of future works.    It is clear that regularization should play a significant role in shaping the decision boundaries. Unfortunately, the paper does not provide details at the basic level, which algorithms,  architectures, hyper-parameters or regularization terms are used. All these factors should play a very significant role in the experimental validation of their hypothesis. Notes: I did not check the proofs of the theorems in detail.  ",13,247,17.642857142857142,5.44672131147541,141,1,246,0.0040650406504065,0.0465116279069767,0.9825,66,33,47,12,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,0,1,0,0,7,5,0,0,1,1,1,1,0,0,0,1,0,1,0,8,0,1,0.5018459358495427,0.4487981464883452,0.276049360045418
ICLR2018-ByrZyglCb-R2,Accept,"This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points. The main results are two-fold: if the decision boundary are flat (such as linear classifiers), then the classifiers tend to be vulnerable to universal perturbations when the decision boundaries are correlated. If the decision boundary are curved, then vulnerability to universal perturbations is directly resulted from existence of shared direction along with the decision boundary positively curved. The authors also conducted experiments to show that deep nets produces decision boundary that satisfies the curved model. The main issue I am having is what are the applicable insight from the analysis: 1. Why is universal perturbation an important topic (as opposed to adversarial perturbation). 2. Does the result implies that we should make the decision boundary more flat, or curved but on different directions? And how to achieve that? It might be my mis-understanding but from my reading a prescriptive procedure for universal perturbation seems not attained from the results presented.",8,170,21.25,5.66060606060606,100,2,168,0.0119047619047619,0.0292397660818713,0.4423,43,23,35,7,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,6,1,4,0,0,0,1,0,0,0,0,1,1,0,0,2,0,0,0.3585609233899088,0.3339552907681763,0.17889769569231578
ICLR2018-ByrZyglCb-R3,Accept,"The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks u2014 i.e., the existence of a single perturbation which causes a network to misclassify most inputs. The paper develops two models for the decision boundary:  (a) A locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low-dimensional linear subspace. (b) A locally positively curved model, in which there is a positively curved outer bound for the collection of points which are assigned a given label. The paper works out a probabilistic analysis arguing that when either of these conditions obtains, there exists a fooling perturbation which affects most of the data.  The theoretical analysis in the paper is straightforward, in some sense following from the definition. The contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations, argue experimentally that they occur in (some) neural networks of practical interest. One challenge in assessing the experimental claims is that practical neural networks are nonsmooth; the quadratic model developed from the hessian is only valid very locally. This can be seen in some of the illustrative examples in Figure 5: there *is* a coarse-scale positive curvature, but this would not necessarily come through in a quadratic model fit using the hessian. The best experimental evidence for the authors' perspective seems to be the fact that random perturbations from S_c misclassify more points than random perturbations constructed with the previous method. I find the topic of universal perturbations interesting, because it potentially tells us something structural (class-independent) about the decision boundaries constructed by artificial neural networks.  To my knowledge, the explanation of universal perturbations in terms of positive curvature is novel. The paper would be much stronger if it provided an explanation of *why* there exists this common subspace of universal fooling perturbations, or even what it means geometrically that positive curvature obtains at every data point. Visually, these perturbations seem to have strong, oriented local high-frequency content u2014 perhaps they cause very large responses in specific filters in the lower layers of a network, and conventional architectures are not robust to this?  It would also be nice to see some visual representations of images perturbed with the new perturbations, to confirm that they remain visually similar to the original images.",14,400,30.76923076923077,5.624020887728459,196,2,398,0.0050251256281407,0.0123762376237623,0.9835,100,60,60,23,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,12,4,0,1,2,0,0,0,0,0,1,0,1,1,0,8,0,0,0.3601782771502307,0.4487981464883452,0.20500226585792192
ICLR2018-Bys4ob-Rb-R1,Accept,"This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied. While the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive. However the new bound is an upper bound of the worst-case performance which is very different from the conventional sampling based lower bounds.  Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier. This paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound). In conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier, and the paper is clearly written and easy to follow. There are possible future directions to be developed. 1. Apply the sum-of-squares (SOS) method. The paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy. One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound. The paper already mentions about this direction and it would be interesting to see the experimental results. 2. Develop a similar relaxation for deep neural networks. The author already mentioned that they are pursuing this direction. While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure.",13,284,17.75,5.398523985239852,147,2,282,0.0070921985815602,0.0209790209790209,0.9783,73,51,43,14,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,9,1,1,0,0,1,2,0,0,0,0,0,0,0,1,6,0,1,0.4306183842897564,0.3364431205075482,0.21717128590269372
ICLR2018-Bys4ob-Rb-R2,Accept,"The authors propose a new defense against security attacks on neural networks. The attack model involves a standard l_inf norm constraint. Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice. Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks. The approach is evaluated for several attacks on MNIST data. First of all, the paper is very well written and structured. As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic). The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. An innovative training criterion based on that certificate is proposed. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks. In summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation. For me, it is a clear accept. The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al. ",14,228,15.2,5.422727272727273,120,0,228,0.0,0.0043668122270742,0.954,70,32,35,7,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,1,0,0,10,2,0,0,0,0,2,0,0,0,0,0,1,0,1,8,0,1,0.3595262007666783,0.4487981464883452,0.20627668319178652
ICLR2018-Bys4ob-Rb-R3,Accept,"This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer. The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time. The main idea of  using upper bound (as opposed to lower bound) is reasonable. However, I find there are some limitations/weakness of the proposed method: 1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer) 2. SDP while tractable, would still require very expensive computation to solve exactly. 3. The relaxation seems a bit loose - in particular, in above step 2 and 3, the authors replace the gradient value by a global upper bound on that, which to me seems can be pretty loose.",6,188,23.5,4.923976608187134,97,3,185,0.0162162162162162,0.0210526315789473,0.9393,43,30,26,11,3,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0.2150362906270682,0.11297698341564,0.08672404312315055
ICLR2018-Bys_NzbC--R1,Reject,"The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks. Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16. The results seem to show that a delayed application of the regularization parameter leads to improved classification performance. The proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning. In the latter case, a stronger parameter is applied, followed by reduced regularization parameter. One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization.  It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on. One limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported.",8,173,21.625,5.547058823529412,89,4,169,0.0236686390532544,0.0459770114942528,0.9584,52,20,31,2,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,3,0,0,4,2,3,0,1,0,0,0,0,0,0,0,3,0,0,3,1,0,0.4296805802946781,0.3348115733173992,0.21100390869956792
ICLR2018-Bys_NzbC--R2,Reject,"The paper is well motivated and written. However, there are several issues. 1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems. Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks. The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel . Several of the key aspects here not addressed are:  1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)?  1b. Why should we keep increasing the regularization constant beyond a limit? Is this for compressing the networks (for which there are alternate procedures), or anything else. In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers? Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)?  2. The proposed experiments are not very conclusive. Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks. Secondly, more datasets including imagenet needs to be tested. Unless these two are done, we cannot assertively say that the proposal seems to do interesting things. Thirdly, it is not clear what Figure 5 means in terms of goodness of learning. And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other).",16,275,17.1875,5.458333333333333,166,1,274,0.0036496350364963,0.0213523131672597,0.9101,64,29,49,31,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 9, 'EXP': 3, 'RES': 0, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,0,2,2,1,9,3,0,2,1,0,1,0,0,0,1,0,1,1,0,11,1,1,0.5737655856472125,0.6728862410150964,0.40494645698054244
ICLR2018-Bys_NzbC--R3,Reject,"The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization. When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning. The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the strong penalty parameter. In their experimental results, the phase transition is not observed anymore with their protocol. This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol. The problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked. The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem. I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions. I think that the problem would be better handled that way than with the proposed strategy,; which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases. To summarize, I believe that the paper addresses an important point,; but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere. Details : - there is a typo in the definition of the proximal operator in Eq. (9)  - there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper  - the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion",15,337,25.923076923076923,5.346625766871166,179,4,333,0.012012012012012,0.017391304347826,0.5062,86,29,65,18,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 1}",0,1,1,2,1,8,4,1,1,0,0,0,0,0,0,0,0,1,0,0,10,1,1,0.5734978034620459,0.4500420613580311,0.3251884510006291
ICLR2018-Byt3oJ-0W-R1,Accept,"Quality: The paper is built on solid theoretical grounds and supplemented by experimental demonstrations. Specifically, the justification for using the Sinkhorn operator is given by theorem 1 with proof given in the appendix. Because the theoretical limit is unachievable, the authors propose to truncate the Sinkhorn operator at level $L$. The effect of approximation for the truncation level $L$ as well as the effect of temperature $tau$ are demonstrated nicely through figures 1 and 2(a). The paper also presents a nice probabilistic approach to permutation learning, where the doubly stochastic matrix arises from Gumbel matching distribution. Clarity: The paper has a good flow, starting out with the theoretical foundation, description of how to construct the network, followed by the probabilistic formulation.  However, I found some of the notation used to be a bit confusing. 1. The notation $l$ appears in Section 2 to denote the number of iterations of Sinkhorn operator. In Section 3, the notation $l$ appears as $g_l$, where in this case, it refers to the layers in the neural network. This led me to believe that there is one Sinkhorn operator for each layer of neural network. But after reading the paper a few times, it seemed to me that the Sinkhorn operator is used only at the end, just before the final output step (the part where it says the truncation level was set to $L 20$ for all of the experiments confirmed this). If I'm correct in my understanding, perhaps different notation need to be used for the layers in the NN and the Sinkhorn operator. Additionally, it would have been nice to see a figure of the entire network architecture, at least for one of the applications considered in the paper. 2. The distinction between $g$ and $g_l$ was also a bit unclear. Because the input to $M$ (and $S$) is a square matrix, the function $g$ seems to be carrying out the task of preparing the final output of the neural network into the input formate accepted by the Sinkhorn operator.  However, $g$ is stated as the output of the computations involving $g_l$. I found this statement to be a bit unclear and did not really describe what $g$ does; of course my understanding may be incorrect so a clarification on this statement would be helpful. Originality: I think there is enough novelty to warrant publication. The paper does build on a set of previous works, in particular Sinkhorn operator, which achieves continuous relaxation for permutation valued variables. However, the paper proposes how this operator can be used with standard neural network architectures for learning permutation valued latent variable. The probabilistic approach also seems novel. The applications are interesting, in particular, it is always nice to see a machine learning method applied to a unique application; in this case from computational neuroscience. Other comments:  1. What are the differences between this paper and the paper by Adams and Zemel (2011)? Adams and Zemel also seems to propose Sinkhorn operator for neural network. Although they focus only on the document ranking problem, it would be good to hear the authors' view on what differentiates their work from Adams and Zemel. 2. As pointed out in the paper, there is a concurrent work: DeepPermNet. Few comments regarding the difference between their work and this work would also be helpful as well. Significance: The Sinkhorn network proposed in the paper is useful as demonstrated in the experiments. The methodology appears to be straight forward  to implement using the existing software libraries, which should help increase its usability. The significance of the paper can greatly improve if the methodology is applied to other popular machine learning applications such as document ranking, image matching, DNA sequence alignment, and etc. I wonder how difficult it is to extend this methodology to bipartite matching problem with uneven number of objects in each partition, which is the case for document ranking. And for problems such as image matching (e.g., matching landmark points), where each point is associated with a feature (e.g., SIFT), how would one formulate such problem in this setting?  ",31,680,19.428571428571427,5.1984375,268,9,671,0.0134128166915052,0.0276967930029154,0.995,185,75,116,29,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 3, 'DAT': 0, 'MET': 17, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 5, 'PNF': 3, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 3}",0,0,5,3,0,17,3,0,1,0,0,4,0,0,0,1,1,5,3,0,13,0,3,0.4333237017327997,0.674884214102857,0.3045004146422038
ICLR2018-Byt3oJ-0W-R2,Accept,"Learning latent permutations or matchings is inherently difficult because the marginalization and partition function computation problems at its core are intractable.  The authors propose a new method that approximates the discrete max-weight matching by a continuous Sinkhorn operator, which looks like an analog of softmax operator on matrices. They extend the Gumbel softmax method (Jang et al., Maddison et al. 2016) to define a Gumbel-Sinkhorn method for distributions over latent matchings. Their empirical study shows that this method outperforms competitive baselines for tasks such as sorting numbers, solving jigsaw puzzles etc. In Theorem 1, the authors show that Sinkhorn operator solves a certain entropy-regularized problem over the Birkhoff polytope (doubly stochastic matrices). As the regularization parameter or temperature tau tends to zero, the continuous solution approaches the desired best matching or permutation. An immediate question is, can one show a convergence bound to determine a reasonable choice of tau? The authors use the Gumbel trick that recasts a difficult sampling problem as an easier optimization problem. To get around non-differentiable re-parametrization under the Gumbel trick, they extend the Gumbel softmax distribution idea (Jang et al., Maddison et al. 2016) and consider Gumbel-Sinkhorn distributions. They illustrate that at low temperature tau, Gumbel-matching and Gumbel-Sinkhorn distributions are indistinguishable. This is still not sufficient as Gumbel-matching and Gumbel-Sinkhorn distributions have intractable densities. The authors address this with variational inference (Blei et al., 2017) as discussed in detail in Section 5.4. The empirical results do well against competitive baselines. They significantly outperform Vinyals et al. 2015 by sorting up to N   120 uniform random numbers in [0, 1] with great accuracy < 0.01, as opposed to Vinyals et al. who used a more complex recurrent neural network even for N   15 and accuracy 0.9. The empirical study on jigsaw puzzles over MNIST, Celeba, Imagenet gives good results on Kendall tau, l1 and l2 losses, is slightly better than Cruz et al. (arxiv 2017) for Kendall tau on Imagenet 3x3 but does not have a significant literature to compare against. I hope the other reviewers point out references that could make this comparison more complete and meaningful. The third empirical study on the C. elegans neural inference problem shows significant improvement over Linderman et al. (arxiv 2017). Overall, I feel the main idea and the experiments (especially, the sorting and C. elegance neural inference) merit acceptance. I am not an expert in this line of research, so I hope other reviewers can more thoroughly examine the heuristics discussed by the authors in Section 5.4 and Appendix C.3 to get around the intractable sub-problems in their approach.    ",20,430,15.925925925925926,5.799492385786802,207,2,428,0.0046728971962616,0.0113636363636363,0.9862,140,74,56,16,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 8, 'DAT': 1, 'MET': 10, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,0,8,8,1,10,2,3,0,2,0,0,0,0,0,0,0,3,0,1,8,0,0,0.503452562331484,0.3379213604916141,0.2567357166792748
ICLR2018-Byt3oJ-0W-R3,Accept,"The idea on which the paper is based - that the limit of the entropic regularisation over Birkhoff polytope is on the vertices   permutation matrices -, and the link with optimal transport, is very interesting. The core of the paper, Section 3, is interesting and represents a valuable contribution. I am wondering whether the paper's approach and its Theorem 1 can be extended to other regularised versions of the optimal transport cost, such as this family (Tsallis) that generalises the entropic one:  https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14584/14420  Also, it would be good to keep in mind the actual proportion of errors that would make a random choice of a permutation matrix for your Jigsaws. When you look at your numbers, the expected proportion of parts wrong for a random assignment could be competitive with your results on the smallest puzzles (typically, 2x2). Perhaps you can put the *difference* between your result and the expected result of a random permutation; this will give a better understanding of what you gain from the non-informative baseline. (also, it would be good to define Prop. wrong and Prop. any wrong. I think I got it but it is better to be written down)  There should also be better metrics for bigger jigsaws -- for example, I would accept bigger errors if pieces that are close in the solution tend also to be put close in the err'ed solution. Typos:  * Rewrite definition 2 in appendix. Some notations do not really make sense.",7,240,21.818181818181817,5.065789473684211,132,2,238,0.0084033613445378,0.028,0.9676,64,31,34,13,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,0,1,1,0,2,0,3,0,1,0,1,0,0,0,0,0,0,0,0,6,0,1,0.4289330244706321,0.225332009396437,0.19221850965227066
ICLR2018-ByuI-mW0W-R1,Reject,"`The papers aims to provide a quality measure/test for GANs.  The objective is ambitious an deserve attention. As GANs are minimizing some f-divergence measure, the papers remarks that computing a  Wasserstein distance between two distributions made of a sum of Diracs is not a degenerate case and is tractable. So they propose evaluate the current approximation of a distribution learnt by a GAN by using this distance as a baseline performance (in terms of W distance and computed on a hold out dataset). A first remark is that the papers does not clearly develop the interest of puting things a trying to reach a treshold of performance in W distance rather than just trying to minimize the desired f-divergence. More specifically as they assess the performance in terms of W distance I would would be tempted to just minimize the given criterion. This would be very interesting to have arguments on why being better than the Dirac estimation in terms of W2 distance would lead to better performance for others tasks (as other f-divergences or image generation). According to the authors the core claims are: 1/ We suggest a formalisation of the goal of GAN training (/generative modelling more broadly) in terms of divergence minimisation. This leads to a natural, testable notion of generalisation.  Formalization in terms of divergence minimization is not new (see O. Bousquet & all https://arxiv.org/pdf/1701.02386.pdf ). and I do not feel like this paper actually performs any test (in a statistical sense). In my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of W2 distance. 2/ We use this test to evaluate the success of GAN algorithms empirically, with the Wasserstein distance as our divergence.  Here the distance does not seems so good because the performance in generation does not seems to only be related to W2 distance. Nevertheless, there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictures. I would enjoyed more digging in this direction. The authors proposes to solve this issue by relying to an embedded space where the L2 distance makes more sense for pictures (DenseNet) . This is of course very reasonable but I would expect anyone working on distribution over picture to work with such embeddings. Here I'm not sure if this papers opens a new way to improve the embedding making use on non labelled data. One could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work. 3/ We find that whether our proposed test matches our intuitive sense of GAN quality depends heavily on the ground metric used for the Wasserstein distance.  This claim is highly biased by who is giving the intuitive sense. It would be much better evaluated thought a mechanical turk test. 4/ We discuss how to use these insights to improve the design of WGANs more generally.  As our understanding of the GANs dynamics are very coarse, I feel this is not a good thing to claim that doing xxx should improve things without actually trying it.",25,528,20.307692307692307,5.111111111111111,246,3,525,0.0057142857142857,0.0335195530726257,0.9792,143,41,101,38,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 1, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 0}",0,1,5,1,0,14,4,2,0,0,1,0,1,2,0,1,0,3,0,0,12,0,0,0.6467552092818719,0.340409190230986,0.323389510885669
ICLR2018-ByuI-mW0W-R2,Reject,"The quality of the paper is good, and clarity is mostly good. The proposed metric is interesting,; but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice. Pros:  - clear definitions of terms  - overall outline of paper is good  - novel metric. Cons  - text is a bit over-wordy, and flow/meaning sometimes get lost. A strict editor would be helpful, because the underlying content is good  - odd that your definition of generalization in GANs appears immediately preceding the section titled Generalisation in GANs  - the paragraph at the end of the Generalisation in GANs section is confusing. I think this section and the previous (The objective of unsupervised learning) could be combined, removing some repetition, adding some subtitles to improve clarity. This would cut down the text a bit to make space for more experiments. - why is your definition of generalization that the test set distance is strictly less than training set ? I would think this should be less-than-or-equal - there is a sentence that doesn't end at the top of p.3: ... the original GAN paper showed that [ends here].   - should state in the abstract what your  otion of generalization for gans is, instead of being vague about it. - more experiments showing a comparison of the proposed metric to others (e.g. inception score, Mturk assessments of sample quality, etc.) would be necessary to find the metric convincing - what is a pushforward measure? (p.2)  - the related work section is well-written and interesting, but it's a bit odd to have it at the end. Earlier in the work (e.g. before experiments and discussion) would allow the comparison with MMD to inform the context of the introduction. - there are some errors in figures that I think were all mentioned by previous commentators.",17,292,19.466666666666665,5.114285714285714,148,5,287,0.0174216027874564,0.0220820189274447,0.9844,79,41,55,9,7,6,"{'ABS': 1, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 5}",1,1,2,0,0,6,2,0,1,0,0,5,0,0,0,1,0,1,3,0,4,2,5,0.5015993194952183,0.6690868936105399,0.35241395999307085
ICLR2018-ByuI-mW0W-R3,Reject,"This paper proposed a procedure for assessing the performance of GANs by re-considering the key of observation. And using the procedure to test and improve current version of GANs. It demonstrated some interesting stuff. It is not easy to follow the main idea of the paper. The paper just told difference stories section by section. Based on my understanding, the claims are 1) the new formalization of the goal of GAN training and 2) using this test to evaluate the success of GAN algorithms empirically?  I suggested that the author should reform the structure, ignore some unrelated content and make the clear claims about the contributions on the introduction part.  Regarding the experimental part, it can not make strong support for all the claims. Figure 2 showed almost similar plots for all the varieties. Meanwhile, the results are performed on some specific model configurations (like ResNet) and settings. It is difficult to justify whether it can generalize to other cases. Some of the figures do not have the notations of curvey, making people hard to compare. Therefore, I think the current version is not ready to be published. The author can make it stronger and consider next venue.",11,198,15.23076923076923,5.052083333333333,112,1,197,0.0050761421319796,0.025,0.4809,52,21,36,9,6,5,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,2,0,2,0,3,2,0,2,0,0,2,0,0,0,0,0,0,1,1,5,1,1,0.429311544150635,0.5580433852949275,0.2691385360136947
ICLR2018-ByuP8yZRb-R1,Reject,"The below review addresses the first revision of the paper . The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out. ---  The authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized. The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model. The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods. There are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good? In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.",19,405,17.608695652173914,5.552631578947368,189,3,402,0.0074626865671641,0.0314769975786924,-0.9484,112,53,72,15,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 0, 'DAT': 0, 'MET': 11, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,0,5,0,0,11,3,0,0,0,0,2,0,1,0,0,0,2,0,1,8,0,0,0.3601845402817076,0.3378041979344241,0.17999763507990457
ICLR2018-ByuP8yZRb-R2,Reject,"- The authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring representations. They show that their idea is effective in reducing private information leakage, but this idea alone might not be signifcant enough as a contribution. - The idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduction. Indeed judging from the large variance in the accuracy of predicting S in Table 1a-c for single adversaries, I suspect one of the main advantage of the current MARS method comes from variance reduction. The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks. - The definition of S, the private information set, is not clear. There is no statement about it in the experiments section, and I assume S is the subject identity. But this makes the train-test split described in 4.1 rather odd, since there is no overlap of subjects in the train-test split. We need clarifications on these experimental details. - Judging from Figure 2 and Table 1, all the methods tested are not effective in hiding the private information S in the learned representation. Even though the proposed method works better, the prediction accuracies of S are still high.",10,234,21.27272727272728,5.260089686098655,129,1,233,0.0042918454935622,0.0336134453781512,-0.9397,64,33,35,15,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,2,1,4,2,0,1,0,0,0,0,1,0,1,1,0,0,0,4,0,0,0.5008960719949184,0.3351992056378622,0.24913237923284803
ICLR2018-ByuP8yZRb-R3,Reject,"MARS is suggested to combine multiple adversaries with different roles. Experiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wearables. Experiments a are satisfying and show good performance when compared to other methods. It could be made clearer how significance is tested given the frequent usage of the term. The idea is slightly novel, and the framework otherwise state-of-the-art. The paper is well written, but can use some proof-reading. Referencing is okay.",7,82,11.714285714285714,5.469135802469136,63,0,82,0.0,0.024390243902439,0.8178,20,11,21,3,4,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,0,0,0,0,3,2,0,0,0,0,1,1,0,0,1,1,1,0,0,1,0,1,0.2863008657335637,0.5555555555555556,0.17914733137822134
ICLR2018-BywyFQlAW-R1,Accept,"Overview: This paper proposes an approach to curriculum learning, where subsets of examples to train on are chosen during the training process. The proposed method is based on a submodular set function over the examples, which is intended to capture diversity of the included examples and is added to the training objective (eq. 2). The set is optimized to be as hard as possible (maximize loss), which results in a min-max problem. This is in turn optimized (approximately) by alternating between gradient-based loss minimization and submodular maximization. The theoretical analysis shows that if the loss is strongly convex, then the algorithm returns a solution which is close to the optimal solution. Empirical results are presented for several benchmarks. The paper is mostly clear and the idea seems nice. On the downside, there are some limitations to the theoretical analysis and optimization scheme (see comments below). Comments: - The theoretical result (thm. 1) studies the case of full optimization, which is different than the proposed algorithm (running a fixed number of weight updates). It would be interesting to show results on sensitivity to the number of updates (p). - The algorithm requires tuning of quite a few hyperparameters (sec. 3). - Approximating a cluster with a single sample (sec. 2.3) seems rather crude. There should be some theoretical and/or empirical study of its effect on quality of the solution. Minor/typos: - what is G(j|Gj) in eq. (9)? - why cite Anonymous (2018) instead of Appendix...? - define V in Thm. 1. - in eq. (4) it may be clearer to denote g_k(w). Likewise in eq. (6) hat{g}_hat{A}(w), and in eq. (14) tilde{g}_{cal{A}}(w). - figures readability can be improved.",17,269,10.346153846153848,5.26,145,3,266,0.0112781954887218,0.0216606498194945,0.9528,86,30,46,9,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 12, 'EXP': 1, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,1,0,1,0,12,1,4,1,1,0,0,1,0,0,0,0,0,2,0,10,0,1,0.5743477903583311,0.3389925191707234,0.29019902733279496
ICLR2018-BywyFQlAW-R2,Accept,"This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data. The authors formulate the learning problem as a minimax problem which tries to choose diverse example and hard examples, where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function. The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration. The authors argue the convergence results on the minimax objective subproblem, but do not seem to give results on the general problem. The ideas for this paper are built on existing work in Curriculum learning, which attempts to provide the learner easy examples followed by harder examples later on. The belief is that this learning style mimics human learners .  Pros: - The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas. - This is a very interesting application of joint convex and submodular optimization, and uses properties of both to show the final convergence results. - Even through the submodular objective is only approximately solvable, it still translates into a convergence result. - The experimental results seem to be complete for the most part. They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried. Cons: - The main algorithm MCL is only a hueristic. Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner. - It seems somewhat hand wavy in the way the authors describe the hyper parameters of MCL, and it seems unclear when the algorithm converge and how to increase/decrease it over iterations. - The objective function also seems somewhat non-intuitive. Though the experimental results seem to indicate that the idea works, I think the paper does not motivate the loss function and the algorithm well. - It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups). This being mainly an empirical paper, I would have expected results on a few larger datasets (e.g. ImageNet, CelebFaces etc.), particularly to see if the idea also scales to these more real world larger datasets. Overall, I would like to see if the paper could have been stronger empirically. Nevertheless, I do think there are some interesting ideas theoretically and algorithmically. For this reason, I vote for a borderline accept.",21,397,18.045454545454547,5.420365535248042,193,7,390,0.0179487179487179,0.0442260442260442,0.9839,117,43,67,26,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 6, 'DAT': 2, 'MET': 10, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 13, 'SUB': 2, 'CLA': 0}",0,1,0,6,2,10,5,3,0,1,0,2,0,0,0,1,0,0,0,1,13,2,0,0.5745144962621099,0.4520146101708815,0.31776556558041424
ICLR2018-BywyFQlAW-R3,Accept,"The main strength of this paper, I think, is the theoretical result in Theorem 1 . This result is quite nice. I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further. The authors ended the discussion on thm 1 on page 7 (just above Sec 2.3) by saying what is sufficiently close to w*. If one goes back to (10), it is easy to see that what converges to w* when one of three things happen (assuming beta is fixed once loss L is selected). 1) k goes to infinity 2) alpha goes to 1 3) g(w*) goes to 0 The authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w*. In fact this proof shows the situation is much better than that. If we are really concerned about making what converge to w*, and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k, we can schedule k to increase over time which guarantees that both alpha goes to 1 and g(w*) goes to zero. There is also a remark that G(A) tends to be modular when lambda is small which is useful. From the algorithm, it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly. It would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion.  Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta.",13,285,23.75,4.783783783783784,143,2,283,0.0070671378091872,0.0174216027874564,0.9923,65,29,64,17,4,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,10,2,3,0,2,0,0,0,0,0,0,0,0,0,0,10,0,0,0.2881883336960915,0.1167087280246978,0.11708513597565555
ICLR2018-ByxLBMZCb-R1,Reject,"Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc. The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks. As the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y). Originality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work. In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results. Thus, I would say original. Importance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points. While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference.  Clarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version. Comments: 1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5). 2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector? 3. In section 5, the authors make a connection with the work of Ge et al. 2016. They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016. However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation). It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work. If that is the case, could the authors describe this a bit further?",16,384,18.285714285714285,5.131506849315069,189,2,382,0.0052356020942408,0.0155038759689922,0.9386,115,41,51,15,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 6, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,3,3,0,4,1,6,0,2,0,1,0,0,0,3,1,3,0,0,4,0,1,0.5727619521425684,0.5577459681435006,0.36238684769671664
ICLR2018-ByxLBMZCb-R2,Reject,"Summary:  This paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems. The paper studies locally open maps, which preserve the local minima geometry. Hence a local minima of l(F(W)) is a local minima of l(s) when s F(W) is a locally open map. Theorem 3 provides conditions under which the multiplication X*Y is a locally open map. For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima. Comments:  The locally open maps (Behrends 2017) is an interesting concept. However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks. Further the claims all over the paper, comparing with the existing works. are over the top and not justified. I believe the paper needs a significant rewriting .  The results are not a strict improvement over existing works. For neural networks, Nguyen and Hein (2017) assume the link function is differentiable. This paper assumes the link function is invertible. Both papers can handle sigmoid/tanh, but cannot handle ReLU. Results for linear networks are not an improvement over existing works. Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point. Results are also written in a confusing way as stating each critical point is a saddle or a global minima. Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly. Proof of Lemma 7 seems to have typos/mistakes. What is bar{W_i}? Why are the first two equations just showing d_i leq d_i ? How do you use this to conclude locally openness of mathcal{M}? Authors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W.",20,370,16.08695652173913,5.063768115942029,169,1,369,0.002710027100271,0.013262599469496,-0.9514,115,53,60,23,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 3}",0,1,4,1,0,10,0,6,0,0,0,2,2,0,0,0,0,5,1,0,5,0,3,0.502829514007649,0.4475631945542843,0.285588444547024
ICLR2018-ByxLBMZCb-R3,Reject,"The paper studies the local optima of certain types of deep networks. It uses the notion of a locally open map to draw equivalences between local optima and global optima. The basic idea is that for fitting nonlinear models with a convex loss, if the mapping from the weights to the outputs is open, then every local optimum in weight space corresponds to a local optimum in output space; by convexity, in output space every local optimum is global. This is mostly a ""theory building"" work. With an appropriate fix, lemma 4 gives a cleaner set of assumptions than previous work in the same space (Nguyen + Hein '17), but yields essentially the same conclusions. The notion of local openness seems very well adapted to deriving these type of results in a clean manner. The result in Section 3 on local openness of matrix multiplication on its range (which is substantially motivated by Behrends 2017) may be of independent interest. I did not check the proof of this result in detail, but it appears to be correct. For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi). For deep nonlinear networks, the results require the ""pyramidal"" assumption that the dimensionality is nonincreasing with respect to layer and (more restrictively) the feature dimension in the first layer is larger than the number of input points. This seems to differ from typical practice, in the sense that it does not allow for wide intermediate layers. This seems to be a limitation of the methodology: unless I'm missing something, this situation cannot be addressed using locally open maps. There are some imprecisions in the writing. For example, Lemma 4 is not correct as written u2014 an invertible mapping sigma is not necessarily locally open.  Take $sigma_k(t)   t for t rational and -t for t irrational$ as an example. This is easy to fix, but not correct as written. Despite mentioning matrix completion in the introduction and comparing to work of Ge et. al., the paper does not seem to have strong implications for matrix completion. It extends results of Ge and collaborators for the fully observed symmetric case to non-symmetric problems. But the main interest in matrix completion is in the undersampled case u2014 in the full observed case, there is nothing to complete.",19,385,19.25,4.98087431693989,184,5,380,0.0131578947368421,0.0179487179487179,0.9816,110,60,54,20,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 13, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 4}",0,2,5,1,0,13,1,3,0,0,0,2,0,0,0,0,1,3,0,0,8,0,4,0.5035065984977368,0.4492758768152873,0.2851239437830314
ICLR2018-ByzvHagA--R1,Reject,"The authors propose a penalization term that enforces decorrelation between the dimensions of the representation  They show that it can be included as additional term in cost functions to train generic models. The idea is simple and it seems to work for the presented examples. However, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the  proposed term depending on the parameters of the model (and this depends on the model!). On the other hand,  given the expression of the proposed regulatization, it seems to lead to non-convex optimization problems which are hard to solve. Any comment on that? .  Moreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the related work section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis). Minor points: Fig.1 conveys not that much information.",7,175,21.875,5.293103448275862,114,2,173,0.0115606936416184,0.0222222222222222,0.8516,49,21,32,5,8,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,1,1,0,3,1,1,1,0,0,0,1,0,0,0,0,1,0,0,4,0,0,0.5719289556561408,0.2240880945267511,0.25746419260046827
ICLR2018-ByzvHagA--R2,Reject,"I think the first intuition is interesting. However I think the benefits are not clear enough. Maybe finding better examples where the benefits of the proposed regularization are stressed could help. There is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal. I do not see a clear novelty in the proposal. For instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA. As shown in [Szegedy et al 2014, Intriguing properties of neural networks] adding an extra linear transformation does not change the expressive power of the representation.  - Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate...   The first paragraph is confusing since jumps from total correlation to correlation without making clear the differences. Although correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that. - Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance. I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal. - All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting. - The first experiment (dim red) is not clear to me. The original dimensionality of the data is 4, and only a linear relation is introduced. I do not understand the dimensionality reduction if the dimensionality of the transformed space is 10. Also the data problem is extremely simple, and it is not clear the didactic benefit of using it. I think a much more complicated data would be more interesting. Besides L_1 is not well defined. If it is L_1 norm on the output coefficients the comparison is misleading. - Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits .  - Sec 3.4. Here the comparison makes clear that not a real benefit is obtained with the proposal. The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples. However the MSE obtained when not using regularization is the same (or even smaller) than when using it.",23,418,13.933333333333334,5.1367088607594935,200,5,413,0.0121065375302663,0.0396270396270396,0.9605,101,49,88,30,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 2, 'MET': 19, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 1, 'CLA': 0}",0,0,3,1,2,19,3,0,0,0,0,0,1,0,0,1,0,4,0,0,16,1,0,0.4334443447476314,0.4541252936386589,0.24135325681454622
ICLR2018-ByzvHagA--R3,Reject,"This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network. This penalty is meant to disentangle the latent representation by removing shared covariance between each dimension. While the proposed penalty is described as a novel contribution, there are multiple instances of previous work which use the same type of penalty (Cheung et. al. 2014, Cogswell et. al. 2016) . Like this work, Cheung et. al. 2014 propose the XCov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder models. Cogswell et. al. 2016 also proposes a similar penalty (DeCov) to this work for reducing overfitting in supervised learning. The novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covariance. Intuitively, this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruction. But given the similarity to previous work, the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work. Cogswell et. al. 2016 explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily rescaled. It would be helpful to understand how this approach avoids this issues; - i.e.,  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities, so what prevents this trivial solution? There doesn't appear to be a definition of the L1 penalty this paper compares against and it's unclear why this is a reasonable baseline. The evaluation metrics this work uses (MAPC, CVR, TdV, UD) need to be justified more in the absence of their use in previous work. While they evaluate their method on non-toy dataset such as CIFAR, they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at all. Again, the utility of the evaluation metrics proposed in this work is unclear. The toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets. > Our method has no penalty on the performance on tasks evaluated in the experiments, while it does disentangle the data This needs to be expanded in the results as all the results presented appear to show Mean Squared Error increasing when increasing the weight of the regularization penalty.",18,406,16.24,5.404580152671755,195,0,406,0.0,0.0146341463414634,0.7935,115,30,76,19,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 0, 'DAT': 2, 'MET': 9, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 4, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 0}",0,1,6,0,2,9,3,1,0,1,0,0,4,0,0,2,0,5,0,0,9,1,0,0.5740782091885913,0.4499338617364662,0.32638098197184084
ICLR2018-H1-IBSgMz-R1,,"The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix. However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words. However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution. Does the argument hold for general noise distributions ? With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof. In experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising. The paper is interesting but lacks strong empirical results. It could be stronger if they could exploit some of their findings to improve language modeling over a strong baseline. ",8,149,18.625,5.397163120567376,89,1,148,0.0067567567567567,0.0198675496688741,0.9628,41,14,25,4,6,1,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,3,1,1,0,0,0,0,0,1,0,0,0,0,0,0,4,0,0,0.4291523007013819,0.11297698341564,0.17328216942888222
ICLR2018-H1-IBSgMz-R2,,"This paper considers the problem of self-normalizing models. This kind of approaches, such as NCE (Noise Contrastive Estimation) is very promising and important to provide efficient and large vocabulary language models. By interpreting the NCE in terms of matrix factorization allows the authors to better explain this learning criterion and more specifically the self-normalizing mechanism. However, the first (theoritical) contribution is to make the link between matrix decomposition and sampling based objective. This was already shown for negative sampling in the paper of Melamud et al. in EMNLP 2017. Therefore, nothing new here, the difference is slight. Moreover, this paper is only cited in the experimental part, while the contribution should be far more emphasized by the authors. The second part makes the link with the self-normalization. This is not really surprising. This was already explained in the same way in papers from Pihlaja,Gutmann and Hyvarinen published in 2010/12.",6,149,13.545454545454543,5.503401360544218,93,0,149,0.0,0.0134228187919463,0.3239,38,25,21,13,4,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,4,1,0,0,0,0,0,0,0,0,1,0,1,0,0,2,0,0,0.2865453499580237,0.3339552907681763,0.14524606224298864
ICLR2018-H1-IBSgMz-R3,,It is hard to interpret this work as the authors do not mention the original work by Gutmann and his colleague on the NCE in the required details. Their paper provides a proof that in the non-parametric case the optimum on NCE objective function is at the data distribution with normalisation constant either learned or held fixed (0 or any value you like). What exactly is the purpose of this paper?  There are a number of minor issues as well. In language modelling we do not compute normalisation term during NCE training or testing as explicitly stated by the authors you are referring to (Chen 2016) - that is the whole point of using NCE. What is p(c) in equation 4 and where it comes from?,5,125,25.0,4.566666666666666,79,0,125,0.0,0.0078740157480314,0.6483,31,10,22,5,3,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,0,2,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0.2143662021880983,0.3333333333333333,0.10919607236468061
ICLR2018-H1-oTz-Cb-R1,Reject,"The paper proposes an approach to learning a distribution over filters of a CNN. The method is based on a adversarial training: the generator produces filters, and the discriminator aims to distinguish the activation maps produced by real filters from those produced by the generated ones. Pros: 1) The general task of learning distributions over network weights is interesting 2) To my knowledge, the proposed approach is new Cons: 1) Experimental evaluation is very substandard. The experiments on invariances seem to be the highlight of the paper, but they basically do not tell me anything.  - Figures 3 and 4 take 2 pages, but what should one see there? - There are no quantitative results. Could there be a way to measure the invariances? - Can the results be applied to some practical task? Why are the results interesting and/or useful? 2) The experiments are restricted to a single dataset - MNIST.  The authors mention that ""the test accuracy obtained by following the above procedure is of 0.982, against a test accuracy of 0.971 for the real CNN"" - these are very poor accuracies for MNIST. So even the MNIST results do not seem convincing. 3) Presentation is suboptimal, and many details are missing. For instance, architectures of networks are not provided. To conclude, while the general direction is interesting and the proposed method might work, the experimental evaluation is very poor, and the paper absolutely cannot be accepted for publication.",12,236,21.454545454545453,5.2681818181818185,123,1,235,0.0042553191489361,0.0205761316872428,-0.9168,59,21,50,14,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 4, 'EXP': 3, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,0,2,1,4,3,3,1,1,0,0,0,0,0,1,1,0,1,1,2,1,0,0.5725222430006174,0.6672886241015097,0.4021254398974697
ICLR2018-H1-oTz-Cb-R2,Reject,"Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane. This paper proposes a method to uncover this structure from the filters of a trained ConvNet . The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters. The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter. The idea of empirically studying the manifold / topological / group structure in the space of filters is interesting. A priori, using a GAN to model a relatively small number of filters seems problematic due to overfitting, but the authors show that their InfoGAN approach seems to work well. My main concerns are:  Controls To generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced. To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (x-space of the GAN), or varied the magnitude of filters or filter planes. Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines. Lack of new Insights The visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than something non-trivial is going on in these networks. In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images).  In any case, no attempt is made to analyze the results, or provide new insights into the computations performed by a trained ConvNet. Interpretation This is a minor point, but I would not say (as the paper does) that the method captures the invariances learned by the model, but rather that it aims to show the variability captured by the model. A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers. The presented method does not consider this and hence does not analyze invariances. Minor issues: - In the last equation on page 2, the right-hand side is missing a min max.",15,468,26.0,4.804054054054054,214,3,465,0.0064516129032258,0.0295358649789029,0.2842,125,43,86,19,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 7, 'EXP': 6, 'RES': 3, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,1,3,1,1,7,6,3,1,2,0,0,0,0,0,0,0,0,1,0,10,0,1,0.6450863734691973,0.33893095024692,0.32336422045706126
ICLR2018-H1-oTz-Cb-R3,Reject,"This paper wants to probe the non-linear invariances learnt by CNNs. This is attempted by selecting a particular layer, and modelling the space of filters that result in activations that are indistinguishable from activations generated by the real filters (using a GAN).  For a GAN noise vector a plausible filter set is created, and for a data sample a set of plausible activations are computed.  If the noise vector is perturbed and a new plausible filter set is created, the input data can be optimised to find the input that produces the same set of activations. The claim is that the found input represents the non-linear transformations that the layer is invariant to .  This is a really interesting perspective on probing invariances and should be explored more. I am not convinced that this particular method is showing much information or highlighting anything particularly interesting, but could be refined in the future to do so. It seems that the generated images are not actually plausible images at all and so not many conclusions can be drawn from this method. Instead of performing the optimisation to find x' have you tried visualising the real data sample that gives the closest activations? I think you may want to consider minimising ||a(x'|z) - a(x|z_k)|| instead to show that moving from x -> x' is the same as is invariant under the transformation z -> z_k  (and thus the corresponding movement in filter space). This (the space between x and x') I think is more interpretable as the invariance corresponding to the space between z and z_k. Have you tried that? There is no notion of class invariance, so the GAN can find the space of filters that transform layer inputs into other classes, which may not be desirable. Have you tried conditioning the GAN on class? Overall I think this method is inventive and shows promise for probing invariances. I'm not convinced the current incarnation is showing anything insightful or useful. It also should be shown on more than a single dataset and for a single network, at the moment this is more of a workshop level paper in terms of breadth and depth of results.",14,359,25.642857142857142,4.994082840236686,154,6,353,0.0169971671388102,0.0408719346049046,0.8212,89,41,84,15,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 10, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 0}",0,1,0,0,2,10,1,1,1,1,0,0,0,0,0,1,0,0,0,0,10,2,0,0.5022878197639554,0.3390376267552415,0.24897163982310996
ICLR2018-H113pWZRb-R1,Reject,"The authors propose a new CNN approach to graph classification that generalizes previous work. Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed. This incorporates information from more distant vertices in one propagation step. The proposed idea is not exceptional original, but the paper has several strong points:  * The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one. * The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level. * The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach). * The experimental evaluation appears to be sound. A moderate improvement compared to other approaches is observed for all data sets. In summary, I think the paper can be accepted for ICLR. ----------- EDIT ----------- After reading the publications mentioned by the other reviewers as well as the following related contributions  * Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018) * Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916) I agree that the relation to previous work is not adequately outlined. Therefore I have modified my rating accordingly.",15,226,18.83333333333333,5.529953917050691,135,2,224,0.0089285714285714,0.0296610169491525,0.9871,63,28,54,11,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 4, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 2, 'EMP': 3, 'SUB': 0, 'CLA': 2}",0,0,2,4,1,4,1,0,0,0,0,4,0,2,0,1,0,3,0,2,3,0,2,0.501154548321614,0.5571347310826407,0.3124324379781064
ICLR2018-H113pWZRb-R2,Reject,"In this paper a new neural network architecture for semi-supervised graph classification is proposed. The new construction builds upon graph polynomial filters and utilizes them on each successive layer of the neural network with ReLU activation functions. In my opinion writing of this paper requires major revision. The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora, Pubmed and Citeseer benchmarks. The one layer analysis in sections 2.1, 2.2 and 2.3 is simply an explanation of graph polynomial filters, which were previously proposed and analyzed in cited work of Sandryhaila and Moura (2013). Together with the summary of other methods and introduction, it composes the first 8 pages of the paper. I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader. I also recommend stating which ideas came from the Sandryhaila and Moura (2013) work in a more pronounced manner. Next, I disagree with the statement that it is not clear how to keep the vertex local property when filtering in the spectrum domain. Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain. See Eq. 18 and 19 in [1]. Finally, I should say that TAGCN idea is interesting. I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e. first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate). With more experiments and interpretation of the model, including some sort of multilayer analysis, this can be a good acceptance candidate. [1] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83u201398, 2013.",15,346,18.21052631578948,5.4,188,2,344,0.005813953488372,0.0231213872832369,0.9385,111,51,43,11,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,1,4,3,0,4,2,0,0,2,0,2,2,0,0,0,0,1,3,1,3,1,1,0.5727079381860017,0.6680337193839593,0.4015323939262277
ICLR2018-H113pWZRb-R3,Reject,"The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional networks to graph-structured data. I find the paper interesting but not very clearly written in some sections, for instance I would better explain what is the main contribution and devote some more text to the motivation. Why is the proposed approach better than the previously published ones, and when is that there is an advantage in using it? The main contribution seems to be the use of the graph shift operator from Sandryhaila and Moura (2013), which closely resembles the one from Shuman et al. (2013). It is actually not very well explained what is the main difference. Equation (2) shows that the learnable filters g are operating on the k-th power of the normalized adjacency matrix A, so when K 1 this equals classical GCN from T. Kipf et al. By using K > 1 the method is able to leverage information at a farther distance from the reference node. Section 2.2 requires some polishing as I found hard to follow the main story the authors wanted to tell. The definition of the weight of a path seems disconnected from the main text, ins't A^k kind of a a diffusion operator or random walk? This makes me wonder what would be the performance of GCN when the k-th power of the adjacency is used. I liked Section 3, however while it is true that all methods differ in the way they do the filtering, they also differ in the way the input graph is represented (use of the adjacency or not). Experiments are performed on the usual reference benchmarks for the task and show sensible improvements with respect to the state-of-the-art. TAGCN with K 2 has twice the number of parameters of GCN, which makes the comparison not entirely fair. Did the author experiment with a comparable architecture? Also, how about using A^2 in GCN or making two GCN and concatenate them in feature space to make the representational power comparable? It is also known that these benchmarks, while being widely used, are small and result in high variance results. The authors should report statistics over multiple runs. Given the systematic parameter search, with reference to the actual validation (or test?) set I am afraid there could be some overfitting. It is quite easy to probe the test set to get best performance on these benchmarks. As a minor remark, please make figures readable also in BW. Overall I found the paper interesting but also not very clear at pointing out the major contribution and the motivation behind it. At risk of being too reductionist: it looks as learning a set of filters on different coordinate systems given by the various powers of A. GCN looks at the nearest neighbors and the paper shows that using also the 2-ring improves performance. ",25,471,21.40909090909091,4.891647855530474,232,2,469,0.0042643923240938,0.0211416490486257,0.9926,132,44,81,31,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 3, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 8, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 3}",0,0,5,2,1,8,3,4,1,1,0,5,0,0,0,0,0,8,2,0,6,1,3,0.6455109116187752,0.5597093196956117,0.4069172244080213
ICLR2018-H135uzZ0--R1,Accept,"This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation. This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations. These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10. However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported. In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets. Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16. The reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs. However, I was hoping to see a direct comparison between FP16 and INT16.  The paper is written clearly and the English is fine.",9,165,18.33333333333333,5.352564102564102,101,0,165,0.0,0.0120481927710843,0.9295,52,24,27,7,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 4, 'DAT': 3, 'MET': 3, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,4,4,3,3,5,3,0,2,0,1,0,0,0,0,2,1,0,0,4,0,1,0.6443299781900882,0.4463583533974213,0.36595894098177134
ICLR2018-H135uzZ0--R2,Accept,"This paper is about low-precision training for ConvNets. It proposed a dynamic fixed point scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format. The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K. According to the paper, this is the first time such kind of performance are demonstrated for limited precision training. Potential improvements: t   - Please define the terms like FPROP and WTGRAD at the first occurance. - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training).",6,121,20.166666666666668,5.635593220338983,80,0,121,0.0,0.016,0.9136,40,16,21,4,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 5, 'DAT': 0, 'MET': 4, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,2,1,5,0,4,5,1,0,2,0,0,1,0,0,0,1,0,0,0,6,2,0,0.5728113686941149,0.3365497970158697,0.2903555655482806
ICLR2018-H135uzZ0--R3,Accept,"This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training. The work is clearly presented and the evaluations seem convincing. The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation. I'm not an expert in this area but the contribution seems relevant to me, and enough for being published.",4,63,15.75,5.344262295081967,48,1,62,0.0161290322580645,0.0476190476190476,0.4678,16,5,14,3,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,0,2,1,0,1,2,0,0,3,0,0,0,1,0,0,0,1,1,0,2,0,0,0.4287875510267538,0.3339552907681763,0.2100399646280443
ICLR2018-H139Q_gAW-R1,Reject,"The paper presents a Depthwise Separable Graph Convolution network that aims at generalizing Depthwise convolutions, that exhibit a nice performance in image related tasks, to the graph domain. In particular it targets Graph Convolutional Networks. In the abstract the authors mention that the Depthwise Separable Graph Convolution that they propose is the key to understand the connections between geometric convolution methods and traditional 2D ones. I am afraid I have to disagree as the proposed approach is not giving any better understanding of what needs to be done and why. It is an efficient way to mimic what has worked so far for the planar domain but I would not consider it as fundamental in closing the gap. I feel that the text is often redundant and that it could be simplified a lot. For example the authors state in various parts that DSC does not work on non-Euclidean data. Section 2 should be clearer and used to better explain related approaches to motivate the proposed one. In fact, the entire motivation, at least for me, never went beyond the simple fact that this happens to be a good way to improve performance. The intuition given is not sufficient to substantiate some of the claims on generality and understanding of graph based DL. In 3.1, at point (2), the authors mention that DSC filters are learned from the data whereas GC uses a constant matrix. This is not correct, as also reported in equation 2.  The matrix U is learned from the data as well. Equation (4) shows that the proposed approach would weight Q different GC layers. In practical terms this is a linear combination of these graph convolutional layers. What is not clear is the Delta_{ij} definition. It is first introduced in 2.3 and described as the relative position of pixel i and pixel j on the image, but then used in the context of a graph in (4). What is the coordinate system used by the authors in this case? This is a very important point that should be made clearer. Why is the Related Work section at the end? I would put it at the front. The experiments compare with the recent relevant literature. I think that having less number of parameters is a good thing in this setting as the data is scarce, however I would like to see a more in-depth comparison with respect to the number of features produced by the model itself. For example GCN has a representation space (latent) much smaller than DSCG. No statistics over multiple runs are reported, and given the high variance of results on these datasets I would like them to be reported. I think the separability of the filters in this case brings the right level of simplification to the learning task, however as it also holds for the planar case it is not clear whether this is necessarily the best way forward. What are the underlying mathematical insights that lead towards selecting separable convolutions? Overall I found the paper interesting but not ground-breaking. A nice application of the separable principle to GCN. Results are also interesting but should be further verified by multiple runs. ",31,529,18.892857142857142,4.943434343434343,237,3,526,0.0057034220532319,0.0263653483992467,0.9919,136,55,103,29,10,5,"{'ABS': 1, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 3, 'MET': 16, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 10, 'SUB': 3, 'CLA': 3}",1,0,3,2,3,16,2,3,0,2,0,3,0,1,0,0,0,1,4,0,10,3,3,0.7186442097127399,0.5617135023989034,0.45156284230742094
ICLR2018-H139Q_gAW-R2,Reject,"The paper presents an extension of the Xception network of (Chollet et al. 2016) 2D grids to generic graphs. The Xception network decouples the spatial correlations from depth channels correlations by having separate weights for each depth channel. The weights within a depth channel is shared thus maintaining the stationary requirement. The proposed filter relaxes this requirement by forming the weights as the output of a two-layer perception. The paper includes a detailed comparison of the existing formulations from the traditional label propagation scheme to more recent more graph convolutions (Kipf & Welling, 2016 ) and geometric convolutions  (Monti et al. 2016). The paper provides quantitative evaluations under three different settings i) image classification, ii) Time series forcasting iii) Document classification. The proposed method out-performs all other graph convolutions on all the tasks (except image classification) though having comparable or less number of parameters. For image classification, the performance of proposed method is below its predecessor Xception network. Pros: i) Detailed review of the existing work and comparison with the proposed work. ii) The three experiments performed showed variety in terms of underlying graph structure hence provides a thorough evaluation of different methods under different settings. iii) Superior performance with fewer number of parameters compared to other methods. Cons: i) The architecture of the 2 layer MLP used to learn weights for a particular depth channel is not provided. ii) The performance difference between Xception and proposed method for image classification experiments using CIFAR is incoherent with the intuitions provided Sec 3.1 as the proposed method have more parameters and is a generalized version of DSC.",13,264,17.6,5.927710843373494,133,0,264,0.0,0.0112359550561797,0.8836,107,28,37,5,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 1, 'MET': 7, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,2,3,1,7,2,0,0,0,0,0,0,0,0,0,0,4,0,0,5,0,0,0.3589292932433693,0.225061539633164,0.16352256595928397
ICLR2018-H139Q_gAW-R3,Reject,"Paper Summary: This work proposes a new geometric CNN model to process spatially sparse data. Like several existing geometric CNNs, convolutions are performed on each point using nearest neighbors. Instead of using a fixed or Gaussian parametric filters, this work proposes to predict filter weights using a multi-layer perception. Experiments on 3 different tasks showcase the potential of the proposed method. Paper Strengths: - An incremental yet interesting advance in geometric CNNs. - Experiments on three different tasks indicating the potential of the proposed technique. Major Weaknesses: - Some important technical details about the proposed technique and networks is missing in the paper. It is not clear whether a different MLP is used for different channels and for different layers, to predict the filter weights. Also, it is not clear how the graph nodes and connectivity changes after the max-pooling operation. - Since filter weight prediction forms the central contribution of this work, I would expect some ablation studies on the MLP (network architecture, placement, weight sharing etc.) that predicts filter weights. But, this is clearly missing in the paper. - If one needs to run an MLP for each edge in a graph, for each channel and for each layer, the computation complexity seems quite high for the proposed network. Also, finding nearest neighbors takes time on large graphs. How does the proposed technique compare to existing methods in terms of runtime? Minor Weaknesses: - Since this paper is closely related to Monti et al., it would be good if authors used one or two same benchmarks as in Monti et al. for the comparisons. Why authors choose different set of benchmarks? Because of different benchmarks, it is not clear whether the performance improvements are due to technical improvements or sub-optimal parameters/training for the baseline methods. - I am not an expert in this area. But, the chosen benchmarks and datasets seem to be not very standard for evaluating geometric CNNs. - The technical novelty seems incremental (but interesting) with respect to existing methods. Clarifications: - See the above mentioned clarification issues in 'major weaknesses'.  Those clarification issues are important to address. - 'Non-parametric filter' may not be right word as this work also uses a parametric neural network to estimate filter weights? Suggestions: - It would be great if authors can add more details of the multi-layer perceptron, used for predicting weights, in the paper. It seems some of the details are in Appendix-A. It would be better if authors move the important details of the technique and also some important experimental details to the main paper. Review Summary: The proposed technique is interesting and the experiments indicate its superior performance over existing techniques.  Some incomplete technical details and non-standard benchmarks makes this not completely ready for publication.",28,449,17.26923076923077,5.459090909090909,202,4,445,0.0089887640449438,0.0346320346320346,0.9867,138,60,74,19,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 13, 'EXP': 8, 'RES': 1, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 6, 'PNF': 2, 'REC': 1, 'EMP': 10, 'SUB': 5, 'CLA': 0}",0,0,4,3,0,13,8,1,0,4,0,3,0,3,0,1,0,6,2,1,10,5,0,0.5755779626610759,0.6733383713232926,0.4010479615892474
ICLR2018-H13WofbAb-R1,Reject,"This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers. Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters. This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method. The method is evaluated with Resnet -50 using synthetic delays. Comments for the author:  The paper is well-written and easy to follow. The problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large blocks. n 1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers. Unfortunately, in your Figure 2, this is not as obvious and not real since it is using simulated delays. More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them.   2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)?  The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients. I feel that, with such a design, you may actually end up hurting the performance by sending a large number of small packets in the no failure case. For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem). You need to show the evidence that you do not hurt the failure-free case for a large number of workers. 3) The evaluation is on fairly small workloads (CIFAR-10). Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful. Furthermore, a clear description of your ""pull"" configuration (such as in Figure 1) i.e. how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%). 4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc). These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline. Overall, I feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown. To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays) . Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution.",22,465,17.22222222222222,5.22196261682243,212,4,461,0.0086767895878524,0.0340425531914893,0.945,118,59,85,26,11,6,"{'ABS': 0, 'INT': 2, 'RWK': 6, 'PDI': 12, 'DAT': 8, 'MET': 9, 'EXP': 7, 'RES': 1, 'TNF': 1, 'ANA': 7, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 1, 'PNF': 0, 'REC': 2, 'EMP': 12, 'SUB': 1, 'CLA': 3}",0,2,6,12,8,9,7,1,1,7,0,1,3,0,0,0,5,1,0,2,12,1,3,0.7897389917690903,0.6738824156576041,0.5508939724663684
ICLR2018-H13WofbAb-R2,Reject,"This paper considers distributed synchronous SGD, and proposes to use partial pulling to alleviate the problem with slow servers. The motivation is that the server may be a straggler. The authors suggested one possibility, namely that the server and some workers are located on the same machine and the workers take most of the computational resource . However, if this is the case, a simple solution would be to move the server to a different node. A more convincing argument for a slow server should be provided .  Though the authors claimed that they used 3 techniques to accelerate synchronous SGD, only partial pulling is proposed by them (the other 2 are borrowed straightforwardly from existing papers).  The mechanism of partial pulling is very simple (just let SGD proceed after pulling a partial parameter block instead of the whole block). As mentioned by the authors in section 1, any relaxation in synchrony brings more noise and higher variance to the updates, and also may cause slow convergence or convergence to a poor solution. However, the authors provided no theoretical study on any of these aspects. Experimental results are not convincing. Only one relatively small dataset (cifar10) is used Moreover, the slow server problem is only simulated by artificially adding delays to the server.",11,211,19.181818181818183,5.268656716417911,117,2,209,0.0095693779904306,0.0325581395348837,-0.7988,51,25,38,18,9,3,"{'ABS': 0, 'INT': 4, 'RWK': 7, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 0}",0,4,7,2,1,8,4,1,0,2,0,0,1,0,0,0,4,0,0,0,10,2,0,0.6455105784734839,0.3391817367005852,0.3226813150138772
ICLR2018-H13WofbAb-R3,Reject,"Paper proposes a weak synchronization approach to synchronous SGD with the goal of improving even with slow parameter servers. This is an improvement on earlier proposals (e.g. Revisiting Synchronous SGD) that allow for slow workers. Empirical results on ResNet50 on CIFAR show promising results for simulations with slow workers and servers, with the proposed approach. Issues with the paper: - Since the paper is focused on empirical results, having results only for ResNet50 on CIFAR is very limiting - Empirical results are based on simulations and not real workloads. The choice of simulation constants (% delayed, and delay time) seems somewhat arbitrary as well. - For the simulated results, the comparisons seem unfair since the validation error is different . It will be useful to also provide time to a certain accuracy that all of them get to e.g. the validation error of 0.1609 (reached by the 3 important cases). Overall, the paper proposes an interesting improvement to this area of synchronous training, however it is unable to validate the impact of this proposal.",8,169,16.9,5.230303030303031,93,1,168,0.0059523809523809,0.0229885057471264,0.8942,51,24,25,9,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 0, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,6,2,0,0,3,3,0,5,0,0,0,0,0,0,3,3,0,0,3,0,0,0.3579872694079342,0.334907646614295,0.1799027750447582
ICLR2018-H15RufWAW-R1,Reject,"I am overall positive about the work but I would like to see some questions addressed. Quality: The paper is good but does not address some important issues. The paper proposes a GAN model to generate graphs with non-trivial properties. This is possibly one of the best papers on graph generation using GANs currently in the literature. However, there are a number of statistical issues that should be addressed. I fear the paper is not ready yet, but I am not opposed to publication as long as there are warnings in the paper about the shortcomings. Originality: This is an original approach. Random walks sometimes are overused in the graph literature, but they seem justified in this work. But it also requires extra work to ensure they are generating meaningful graphs. Significance: The problem is important. Learn to generate graphs is a key task in drug discovery, relational learning, and knowledge discovery. Evaluation: The link prediction task is too easy, as links are missing at random. It would be more useful to predict links that are removed with an unknown bias. The graph (wedge, claw, etc) characteristics are good (but simple) metrics; however, it is unclear how a random graph with the same size and degree distribution (configuration model) would generate for the same metrics (it is not shown for comparison). Issues that I wish were addressed in the paper:  a)tHow is the method learning a generator from a single graph? What are the conditions under which the method is likely to perform well? It seems to rely on some mixing RW conditions to model the distinct graph communities. What are these mixing conditions? These are important questions that should have at least an empirical exploration. b)tWhat is the spatial independence assumption needed for such a generator? c)tWould this approach be able to generate a lattice? Would it be able to generate an expander graph? What about a graph with poorly connect communities? Is there any difficulties with power law graphs? d)tHow is the RW statistically addressing the generation of high-order (subgraph) features? e)tCan this approach be used with multiple i.i.d. graphs? f)tIsn't learning the random walk sample path a much harder / higher-dimensional task than it is necessary? Again, the short walk may be capturing the communities but the high-dimensional random walk sample path seems like a high price to pay to learn community structure. g)tClearly, with a large T (number of RW steps), the RW is not modeling just a single community. Is there a way to choose T? How larger values of T to better model inter-community links? Would different communities have different choices of T? h)tAnd a related question, how well can the method generate the inter-community links? i)tThe RW model is actually similar to an HMM. Would learning a mixture of HMMs (one per community) have similar performance? ",35,472,22.476190476190474,5.118303571428571,216,6,466,0.0128755364806866,0.0294736842105263,0.9928,141,61,93,25,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 20, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 21, 'SUB': 4, 'CLA': 0}",0,0,0,4,0,20,4,0,0,4,0,4,0,2,0,1,1,2,0,1,21,4,0,0.4339813083281881,0.6795430074456805,0.3058426604072077
ICLR2018-H15RufWAW-R2,Reject,"The authors proposed a generative model of random walks on graphs. Using GAN, the architecture allows for model-agnostic learning, controllable fitting, ensemble graph generation. It also produces meaningful node embeddings with semi-interpretable latent spaces. The overall framework could be relevant to multiple areas in graph analytics, including graph comparison, graph sampling, graph embedding and relational feature selection. The draft is well written with convincing experiments. I support the acceptances of this paper. I do have a few questions that might help further improve the draft. More baseline besides DC-SBM could better illustrate the power of GAN in learning longer random walk trajectories. DC-SBM, while a generative model, inherently can only capture first order random walks with target degree biases, and generally over-fits into degree sequences. Are there existing generative models based on walk paths? The choice of early stopping is a very interesting problem especially for the EO-creitenrion. In Fig3 (b), it seems assortativity is over-fitted beyond 40k iterations. It might be helpful to discuss more about the over-fitting of different graph properties. The node classification experiment could use a bit more refinement. The curves in Fig. 5(a) are not well explained. What is the combined? The claim of competitive performance needs better justification according to the presentation of the F1 scores. The Latent variable interpolation experiment could also use more explanations. How is the 2d subspace chosen? What is the intuition behind the random walks and graphs of Fig 6? Can you provide visualizations of the communities of the interpolated graphs in Fig 7? ",21,255,14.166666666666666,5.755186721991701,150,3,252,0.0119047619047619,0.015625,0.9749,88,34,37,14,10,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 6, 'RES': 1, 'TNF': 5, 'ANA': 1, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 8, 'SUB': 3, 'CLA': 1}",0,0,2,1,0,5,6,1,5,1,1,2,0,1,0,0,1,0,2,1,8,3,1,0.7159365602526574,0.6712952906510137,0.5073808224294459
ICLR2018-H15RufWAW-R3,Reject,"This paper proposes a WGAN formulation for generating graphs based on random walks. The proposed generator model combines node embeddings, with an LSTM architecture for modeling the sequence of nodes visited in a random walk; the discriminator distinguishes real from fake walks. The model is learned from a single large input graph (for three real-world networks) and evaluated against one baseline generative graph model: degree-corrected stochastic block models. The primary claims of the paper are as follows: i) The proposed approach is a generative model of graphs, specifically producing sibling graphs ii) The learned latent representation provides an interpretation of generated graph properties iii) The model generalizes well in terms of link and node classification The proposed method is novel and the incorporated ideas are quite interesting (e.g., discriminating real from fake random walks, generating random walks from node embeddings and LSTMs). However, from a graph generation perspective, the problem formulation and evaluation do not sufficiently demonstrate the utility of proposed method. First, wrt claim (i) the problem of generating sibling graphs is ill-posed. Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution. The notion of a sibling graph used in this paper is not clearly defined, but it seems to only be useful if the sibling graphs are likely under the distribution. Unfortunately, the likelihood of the sampled graphs is not explicitly evaluated. On the other hand, since many of the edges are shared the siblings may be nearly isomorphic to the input graph, which is not useful from a graph modeling perspective. For claim (i), the comparison to related work is far from sufficient to demonstrate its utility as a graph generation model. There are many graph models that are superior to DC-SBM, including KPGMs, BETR, ERGMs, hierarchical random graph models and latent space models. Moreover, a very simple baseline to assess the LSTM component of the model, would be to produce a graph by sampling links repeatedly from the latent space of node embeddings. Next, the evaluation wrt to claim (ii) is novel and may help developers understand the model characteristics. However, since the properties are measured based on a set of random walks it is still difficult to interpret the impact on the generated graphs (since an arbitrary node in the final graph will have some structure determined from each of the regions). Do the various regions generate different parts of the final graph structure (i.e., focusing on only a subset of the nodes)? Lastly, the authors evaluate the learned model on link and node prediction tasks and state that the model's so-so performance supports the claim that the model can generalize. This is the weakest claim of the paper. The learned node embeddings appear to do significantly worse than node2vec, and the full model is worse than DC-SBM. Given that the proposed model is transductive (when there is significant edge overlap) it should do far better than DC-SBM which is inductive. Overall, while the paper includes a wide range of experimental evaluation, they are aimed too broadly (and the results are too weak) to support any specific claim of the work. If the goal is to generate transductively (with many similar edges), then it would be better to compare more extensively to alternative node embedding and matrix factorization approaches, and assess the utility of the various modeling choices (e.g., LSTM, in/out embedding). If the goal is to generate inductively, over the full distribution of graphs, then it would be better to (i) assess whether the sampled graphs are isomorphic, and (ii) compare more extensively to alternative graph models (many of which have been published since 2010).   ",26,622,27.043478260869566,5.25249169435216,251,4,618,0.0064724919093851,0.016,0.9473,172,89,107,38,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 20, 'EXP': 5, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 7, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,0,3,3,0,20,5,3,1,0,0,0,0,0,0,2,0,7,0,0,9,1,1,0.4340608412140702,0.5612792979619573,0.2754659450080105
ICLR2018-H15odZ-C--R1,Accept,"The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs. The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution. The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines. To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution). This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations. While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue.  The paper is clearly written and presents the theory and experimental results nicely. I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score.",8,200,22.22222222222222,5.438144329896907,107,0,200,0.0,0.0099502487562189,0.6908,63,20,32,5,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 3, 'DAT': 2, 'MET': 3, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 1}",0,1,5,3,2,3,5,1,0,3,0,1,0,1,0,0,3,1,0,0,7,3,1,0.7156569625144918,0.5595967264781521,0.4561617343594202
ICLR2018-H15odZ-C--R2,Accept,"The authors discuss a direct Gamma sampling method for the interpolated samples in GANs, and show the improvements over usual normal sampling for CelebA, MNIST, CIFAR and SVHN datasets. The method involves a nice, albeit minor, trick, where the chi-squared distribution of the sum of the z_{i}^{2} has its dependence on the dimensionality removed . However I am not convinced by the distribution of |z^prime|^{2} in the first place (eqn (2)): the samples from the gaussian will be approximately orthogonal in high dimensions, but the inner product will be at least O(1) . Thus although the |z_{0}|^{2} and |z_{1}|^{2} are chi-squared/gamma, I don't think |z^prime|^{2} is exactly gamma in general. The experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome. Figures 2 and 3 do not add anything to the story, since 2 is just a plot of gamma pdfs and 3 shows the difference between the constant KL and the normal case that is linear in d.  Overall I think the trick needs to be motivated better, and the experiments improved to really show the import of the d-independence of the KL. Thus I think this paper is below the acceptance threshold.",8,203,25.375,5.096774193548387,111,3,200,0.015,0.0388349514563106,0.9785,56,28,32,13,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 8, 'PDI': 2, 'DAT': 3, 'MET': 3, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 6, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 2, 'CLA': 0}",0,1,8,2,3,3,5,0,0,6,0,0,0,0,0,0,4,1,0,1,7,2,0,0.501663660955816,0.5595380866182785,0.31063948644580175
ICLR2018-H15odZ-C--R3,Accept,"The authors propose the use of a gamma prior as the distribution over  the latent representation space in GANs.  The motivation behind it is that  in GANs interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions. The use of the proposed gamma distribution, as a simple alternative, overcomes this problem. In general, the proposed work is very interesting and the idea is neat. The paper is well presented and I want to underline the importance of this. The authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow. The work itself is interesting and can provide useful alternatives for the distribution over the latent space.",7,135,19.285714285714285,5.046511627906977,72,0,135,0.0,0.0144927536231884,0.9803,37,16,21,4,9,8,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 5, 'SUB': 3, 'CLA': 1}",0,1,5,2,1,3,3,1,0,3,0,1,0,0,0,1,1,1,1,1,5,3,1,0.6439606089007538,0.8915900716449037,0.5694863851632171
ICLR2018-H18WqugAb-R1,Reject,"This paper focuses on the zero-shot learning compositional capabilities of modern sequence-to-sequence RNNs. Through a series of experiments and a newly defined dataset, it exposes the short-comings of current seq2seq RNN architectures. The proposed dataset, called the SCAN dataset, is a selected subset of the CommonAI navigation tasks data set. This subset is chosen such that each command sequence corresponds to exactly one target action sequence, making it possible to apply standard seq2seq methods. Existing methods are then compared based on how accurately they can produce the target action sequence based on the command input sequence. The introduction covers relevant literature and nicely describes the motivation for later experiments. Description of the model architecture is largely done in the appendix, this puts the focus of the paper on the experimental section. This choice seems to be appropriate, since standard methods are used. Figure 2 is sufficient to illustrate the model to readers familiar with the literature. The experimental part establishes a baseline using standard seq2seq models on the new dataset, by exploring large variations of model architectures and a large part of the hyper-parameter space. This papers experimentation sections sets a positive example by exploring a comparatively large space of standard model architectures on the problem it proposes. This search enables the authors to come to convincing conclusions regarding the shortcomings of current models. The paper explores in particular: 1.) Model generalization to unknown data similar to the training set. 2.) Model generalization to data-sequences longer than the training set. 3.) Generalization to composite commands, where  a part of the command is never observed in sequence in the training set. 4.) A recreation of a similar problem in the machine translation context. These experiments show that modern sequence to sequence models do not solve the systematicity problem, while making clear by application to machine translation, why such a solution would be desirable.  The SCAN data-set has the potential to become an interesting test-case for future research in this direction. The experimental results shown in this paper are clearly compelling in exposing the weaknesses of current seq2seq RNN models. However, where the paper falls a bit short is in the discussion / outlook in terms of suggestions of how one can go about tackling these shortcomings.  ",20,374,17.80952380952381,5.6722689075630255,183,1,373,0.002680965147453,0.0131926121372031,0.8836,123,42,59,11,11,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 6, 'DAT': 5, 'MET': 4, 'EXP': 13, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,2,6,5,4,13,1,1,1,1,1,0,0,0,0,1,0,3,0,4,1,0,0.7880205617273806,0.44643345459658,0.4470922841487104
ICLR2018-H18WqugAb-R2,Reject,"The paper analyzed the composition abilities of Recurrent Neural Networks. The authors analyzed the the generalization for the following scenarios  - the generalization ability of RNNs on random subset of SCAN commands - the generalization ability of RNNs on longer SCAN commands - The generalization ability of composition over primitive commands. The experiments supported the hypothesis that the RNNs are able to   - generalize zero-shot to new commands. - difficulty generalizing to longer sequence (compared to training sequences) of commands. - the ability of the model generalizing to composition of primitive commands seem to depend heavily on the whether the action is seen during training. The model does not seem to generalize to completely new action and commands (like Jump), however, seems to generalize much better for Turn Left, since it has seen the action during training (even though not the particular commands) Overall, the paper is well written and easy to follow. The experiments are complete. The results and analysis are informative. As for future work, I think an interesting direction would also be to investigate the composition abilities for RNNs with latent (stochastic) variables. For example, analyzing whether the latent stochastic variables may shown to actually help with generalization of composition of primitive commands.    ",14,200,18.181818181818183,5.623115577889447,97,3,197,0.015228426395939,0.0422535211267605,0.9747,58,27,33,9,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 5, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 2, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,0,0,4,0,5,5,1,0,1,2,1,0,0,0,0,2,0,0,0,6,0,1,0.5015275822091531,0.3364911571559961,0.24762022987722093
ICLR2018-H18WqugAb-R3,Reject,"This paper argues about limitations of RNNs to learn models than exhibit a human-like compositional operation that facilitates generalization to unseen data, ex. zero-shot or one-shot applications. The paper does not present a new method, it only focuses on analyzing learning situations that illustrate their main ideas. To do this, they introduce a new dataset that facilitates the analysis of a Seq2Seq learning case. They conduct a complete experimentation, testing different popular RNN architectures, as well as parameter and hyperparameters values. The main idea in the paper is that RNNs applied to Seq2Seq case are learning a representation based only on memorizing a mixture of constructions that have been observed during training, therefore, they can not show the compositional learning abilities exhibit by humans (that authors refer as systematic compositionality). Authors present a set of experiments designed to support this observation. While the experiments are compelling, as I explain below, I believe there is an underlying assumption that is not considered. Performance on training set by the best model is close to perfect (99.5%), so the model is really learning the task. Authors are then testing the model using test sets that do not follow the same distribution than training data, example,  longer sequences. By doing so, they are breaking one of the most fundamental assumptions of inductive machine learning, i.e., the distribution of train and test data should be equal. Accordingly, my main point is the following: the model is indeed learning the task, as measured by performance on training set, so authors are only showing that the solution selected by the RNN does not follow the one that seems to be used by humans. Importantly, this does not entail that using a better regularization a similar RNN model can indeed learn such a representation. In this sense, the paper would really produce a more significant contribution is the authors can include some ideas about the ingredients of a RNN model, a variant of it, or a different type of model, must have to learn the compositional representation suggested by the authors, that I agree present convenient generalization capabilities. Anyway, I believe the paper is interesting and the authors are exposing interesting facts that might be worth to spread in our community, so I rate the paper as slightly over the acceptance threshold.",15,384,25.6,5.3994490358126725,181,2,382,0.0052356020942408,0.0207792207792207,0.9878,102,40,83,24,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 4, 'MET': 4, 'EXP': 7, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,0,0,3,4,4,7,1,0,2,0,2,0,0,0,2,2,1,0,1,4,2,0,0.5015647612236649,0.668732359712483,0.35272399704235813
ICLR2018-H18uzzWAZ-R1,Reject,The paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal. A loss function based on the Wasserstein distance is used.  The paper is interesting but could certainly do with more explanations. Comments: 1. It is difficult for the reader to understand a) why Wasserstein is used  and b) how exactly the nuisance variation is reduced. A dedicated section on motivation is missing.` 2. Does the Deep Metric network always return a '64-dim' vector? Have you checked your model using different length vectors? 3. Label the y-axis in Fig 2. 4. The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation. ,10,114,10.363636363636363,5.53921568627451,80,0,114,0.0,0.0085470085470085,0.6199,30,15,24,5,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,1,0,1,0,6,2,0,1,1,0,3,0,0,0,0,0,0,1,0,5,2,0,0.5014279939767331,0.3359278395810267,0.25591384550084434
ICLR2018-H18uzzWAZ-R2,Reject,"The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or batches. A network is trained to learn the transformations that minimize the Wasserstein distance between distributions. The wasserstein distance is also called the earth mover distance and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another. In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions. Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network. Results are examined mainly by looking at the first two PCA components of the data. The paper presents an interesting idea and is fairly well written. However I have a few concerns: 1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017). Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of lambda and the choice of points for gradient computation. 2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection. This section could be improved by demonstrating the approach on more datasets. 3. There is a lack comparison to other methods such as Shaham et al. (2017). Why is using earth mover distance better than MMD based distance? They only compare it to a method named CORAL and to Typical Variation Normalization (TVN). What about comparison to other batch normalization methods in biology such as SEURAT?  4. Why is the affine transform assumption valid in biology?   There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences. 5. Only early stopping seems to constrain their model to be near identity. Doesn't this also prevent optimal results ? How does this compare to the near-identity constraints in resnets in Shaham et al. ?  ",20,368,13.62962962962963,5.360230547550432,192,2,366,0.0054644808743169,0.0053333333333333,0.9102,109,43,61,19,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 13, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 1}",0,1,3,0,2,13,3,3,0,0,0,2,0,2,0,0,0,4,0,0,5,3,1,0.5749664855836928,0.4474971148720292,0.3291374796295508
ICLR2018-H18uzzWAZ-R3,Reject,"This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation. The authors define a Wasserstein Distance Network to find  a suitable affine transformation that reduces the nuisance factor. The evaluation on a real dataset yields correct results, this approach is quite general and could be applied to different problems. The contribution of this approach could be better highlighted. The early stopping criteria tend to favor suboptimal solution, indeed relying on the Cramer distance is possible improvement. As a side note, the k-NN MOA is central to for the evaluation of the proposed approach. A possible improvement is to try other means for the embedding instead of the Euclidean one.  ",7,124,15.5,5.720338983050848,79,0,124,0.0,0.0236220472440944,0.8625,42,15,21,4,5,1,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,2,0,1,1,6,0,3,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0.3585307404265148,0.1142208982853259,0.14440375853548904
ICLR2018-H196sainb-R1,Accept,"This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages. The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping). The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks. n The paper presents an interesting approach which achieves good performance. The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed. n My one concern is that the supervised approach that the paper compares to is limited: it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words . I think the paper's comparisons are valid , but the abstract and introduction make very strong claims about outperforming state-of-the-art supervised approaches . I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened.  The same holds for statements like ... our method is a first step ..., which is very hard to justify.  I also do not think it is necessary to over-sell, given the solid work in the paper. Further comments, questions and suggestions: - It might be useful to add more details of your actual approach in the Abstract, not just what it achieves . - Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way.  The actual word embedding method, therefore, has a big influence on performance (as you show). Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains /data with similar co-occurrence statistics, in order for your method to be appropriate? - Would it be possible to add weights to the terms in eq. (6), or is this done implicitly ? - How were the 5k source words for Procrustes supervised baseline selected ? - Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces ? - Do you think your approach would benefit from having a few parallel training points? Some minor grammatical mistakes/typos (nitpicking): - gives a good performance ->  gives good performance - Recent works, several works, most works, etc.->  recent studies, several studies, etc. - i.e, the improvements -> i.e., the improvements  nThe paper is well-written, relevant and interesting . I therefore recommend that the paper be accepted.  ",26,417,21.94736842105263,5.546599496221663,203,5,412,0.012135922330097,0.0336322869955156,0.9929,113,66,86,14,10,8,"{'ABS': 2, 'INT': 7, 'RWK': 13, 'PDI': 10, 'DAT': 7, 'MET': 11, 'EXP': 4, 'RES': 5, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 6, 'CMP': 4, 'PNF': 1, 'REC': 1, 'EMP': 10, 'SUB': 1, 'CLA': 2}",2,7,13,10,7,11,4,5,0,3,0,1,0,0,0,2,6,4,1,1,10,1,2,0.7191891909899745,0.8952044193716573,0.6262167681793164
ICLR2018-H196sainb-R2,Accept,"The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique. The task is interesting and relevant, especially for in low-resource language pair settings. n The paper, however, misses comparison against important work from the literature that is very relevant to their task u2014 decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA. The former set of works, while focused on machine translation also learns a translation table in the process . Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions . Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction.  The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods. While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods. Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach .  The proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works. For the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach. The paper mentions that the approach is ""unsupervised"".  However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages . How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?",14,312,22.285714285714285,5.501683501683502,167,0,312,0.0,0.00625,0.2354,98,37,49,14,8,4,"{'ABS': 0, 'INT': 3, 'RWK': 11, 'PDI': 10, 'DAT': 2, 'MET': 8, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 5, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,3,11,10,2,8,0,1,1,0,0,0,2,0,0,1,5,4,0,0,6,0,0,0.5746180156089674,0.4480978658840208,0.32883409900554084
ICLR2018-H196sainb-R3,Accept,"An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning. The paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times .  There are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016). n The evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings. Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest. In my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.",7,189,23.625,4.917127071823204,114,0,189,0.0,0.005181347150259,0.9752,51,30,26,11,7,6,"{'ABS': 0, 'INT': 3, 'RWK': 3, 'PDI': 3, 'DAT': 1, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 2}",0,3,3,3,1,3,1,0,0,2,0,0,0,0,1,1,1,0,0,0,1,1,2,0.500855716995296,0.6667478017375207,0.35705310498120135
ICLR2018-H1A5ztj3b-R1,Reject,"The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates. Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper. The main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum. In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well. However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge. In the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior. Personally i would not use the term convergence in this setting at all as the runs are very short and thus we might not be close to any region of convergence. Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all. The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping. Pros: - Many experiments which try to study the effect Cons: -The described phenomenon seems to depend strongly on the problem surface and might never  be encountered on any problem aside of Cifar-10 - Only single runs are shown, considering the noise on those the results might not be reproducible. -Experiments are not described in detail -Experiment design feels ad-hoc and unstructured -The role and value of the many LR-plots remains unclear to me. Form: - The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas - Figures are not properly described, e.g. axes in Figures 3 a) and b) - Explicit references to code are made which require familiarity with the used framework(if at all published).",15,410,29.285714285714285,4.879093198992443,199,6,404,0.0148514851485148,0.03125,-0.9589,109,42,65,44,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 8, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 3}",0,1,0,0,2,8,2,1,1,0,0,2,0,0,0,0,1,0,0,0,8,1,3,0.5019190501361454,0.4489604166300533,0.28038096525735845
ICLR2018-H1A5ztj3b-R2,Reject,"In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR). The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets u2013 this is called ""super-convergence"". However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting.  Some specific comments by sections:  2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates. The authors mention Loshchilov & Hutter in next section but do not compare it to their work. The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf  3. Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence. Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units. It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence. 4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods. 5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot). Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time. 6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work.  However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work. Overall, the work is presented as a positive result in very specific conditions but it seems more like a negative result. It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing.",16,403,18.318181818181817,5.71578947368421,201,2,401,0.0049875311720698,0.0098039215686274,0.9537,116,43,68,22,11,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 3, 'MET': 2, 'EXP': 3, 'RES': 4, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 4, 'CLA': 1}",0,1,3,1,3,2,3,4,2,3,0,1,2,0,1,1,0,1,0,1,6,4,1,0.7866222621009474,0.7812075944769571,0.6258757282277403
ICLR2018-H1A5ztj3b-R3,Reject,"This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR. The paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The super-convergence occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen. The explanation of the cause of super-convergence from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.",8,178,19.77777777777778,5.408536585365853,105,1,177,0.0056497175141242,0.0167597765363128,0.6892,51,16,31,9,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,3,1,1,0,2,0,1,0,0,1,0,1,0,0,0,2,0,0,0.5005251036081856,0.3339552907681763,0.2528643727217353
ICLR2018-H1AdTAxC--R1,Reject,"Summary of paper: This paper learns parameters of a generative model by minimizing the Wasserstein distance between the real generative model and the generative model that is being learned, as has been proposed by Arjovsky et al. (2017). The difference is that this paper uses the Hamming distance instead of the Euclidean distance. This makes the optimization difficult because the condition for the critic function to be K-Lipschitz can no longer be enforced by weight clipping. The paper proposes some sort of approximation to enforce this condition. ---  It is a straightforward idea: just use Hamming instead of Euclidean distance. I'm a bit concerned with the many approximations that have to made to the algorithm to work in practice. In particular: - It is not clear how h(x, x') relates to f(x) - f(x') in the practical algorithm. - The thing about switching arguments to ensure that h(x, x')   -h(x', x) is OK but but whereas h(x, x')   f(x) - f(x') implies h(x, x')   -h(x', x), the opposite is not true. - The rounding of of the generator output is also a bit weird: Does this not make the generator not differentiable? How is that dealt with? - Does having to convert to one-hot preclude this from working in high-dimensional spaces? It seems that the practical algorithm goes too far from the proposed theory. This reduces the value of the theory as an explanation of why the practical algorithm works. The practical algorithm has only been tested on a toy experiment which makes judgement of the practical algorithm alone also difficult.",13,254,21.166666666666668,4.950413223140496,126,1,253,0.0039525691699604,0.0186567164179104,-0.8407,75,28,48,15,4,1,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,0,1,4,0,8,1,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0.2876435300963456,0.1154648131550119,0.1165744433348902
ICLR2018-H1AdTAxC--R2,Reject,"This paper introduces an algorithm for learning Wasserstein GANs for discrete distributions.  Taking the dual form of the discrete Wasserstein distance introduced by [Evans 1997] (which produces a constrained optimization problem) and using this as a basis of a GAN training algorithm. The key algorithmic distinction from conventional GAN approaches is that the critic takes pairs of real and simulated datapoints as inputs and returns a measure of which it thinks is the real datapoint, namely f(x_r)u2013f(g(z)) where x_r is the data point and g(z) the generator output, rather than the critic corresponding to f directly. An architecture is proposed for this framework that guarantees that the constraints from the formulation of [Evans 1997] as satisfied. The topic of this paper is timely and of clear interest to the ICLR community. The underlying idea is interesting and the architecture seems appropriate for the chosen target. Further, the paper is relatively clear and easy to follow. However, the experimental evaluation is not sufficiently strenuous and produces very underwhelming results. Relatedly, I think the motivation behind using the discrete Wasserstein distance, though seemingly reasonable, needs more careful consideration. Unfortunately, the serious shortfalls in the experiments mean that the paper, in my opinion, falls noticeably below the acceptance threshold for ICLR. Having said that, my stance might change substantially if more impressive experimental results can be obtained; without this, I am unconvinced the method actually behaves as intended.  Regretfully, this is probably beyond the scope of a revision during the rebuttal period though as it will most likely require significant algorithmic changes. %%%% Shortcomings with Experiments %%%%%  Put simply, I do not think that the experiments demonstrate that the approach works and actually suggest the opposite.  The so-called ""mode collapse"" issue is effectively a sugar-coated way of saying that the method has learned to return a single output rather than learning a generative model. This is a huge issue and needs sorting before the paper can be seriously considered for publication. The attempted at a fix at the end of the paper might be a step in the right direction but is not evaluated sufficiently and the preliminary results provided are not particularly promising. Going past this issue, there are still a lot of problems with the experimental evaluation. More numerous and more difficult problems need to be considered. For example, an NLP problem would fit well with the motivation for the approach given in the intro. It is also necessary to demonstrate that the GAN is doing something different to just memorizing previous examples. For example, this is a problem where one can actually reasonably compare to just sampling from the data by checking that more unseen correct samples are generated then incorrect samples. It is worrying that the convergence plots show a single line that ostensibly comes from the ""best result"". This is not a reasonable scientific comparison method and should be replaced by mean (or median) performance with uncertainty estimates (i.e. +/- some numbers of standard deviations or quantiles). The small number of samples from the MNIST problem really doesn't convey anything meaningful u2013 even if this looked exceptional, a small number of unqualified samples is hardly a serious evaluation metric. More iterations for Figure 4a or needed to see if the WGAN keeps improving beyond on the DWGAN, noting that this has not converged at 100%. All four methods should be added to Figure 5. %%%% Is the Discrete Metric Always Better for Training the Generative Model? %%%%%  Though I think it is probably true that the discrete metric should more beneficial from the point of the view of the critic, I am not completely convinced it is always beneficial from the point of view of training the generator, which is at the end of the day what really matters. Note that I am not trying to argue that discrete metric is worse, just that you haven't done enough to convince me that it is always better. If indeed it is always better, please tear apart my following argument, which will hopefully provide stimulation for improving the motivation of the metric in the paper. If it is not always better, the paper should be updated to outline some of the potential pathologies and the cases were you expect the approach to work well and when you do not. To demonstrate my argument, consider the training sample [0, 0, 1; 0, 1, 0] and the following four example generated outputs  (1) [0.333, 0.333, 0.334;        0.333, 0.334, 0.333] (2) [0, 0, 1;         0, 1, 0] (3) [0, 0, 1;        0.334, 0.333, 0.333] (4) [0.333, 0.334, 0.333;         0.334, 0.333, 0.333]  giving respective discrete distances to of 0, 0, 1, and 2; and continuous distances of 1.330668, 0, 0.667334, and 1.334668. Now imagine we are training a GAN with the one example datapoint [0, 0, 1; 0, 1, 0]. Even though (1) will lead to the target sample [0, 0, 1; 0, 1, 0] after passing through the argmax function and (3) will not, (1) is also very close to (4) which has the maximum possible discrete distance. Consequently, the generator for (1) is most likely not at a stable optimum and very close in the space of neural net weights to some very poor generators. During training, we would thus like to guide our network towards generating (2), as this is a stable solution, particularly given we are using stochastic optimization methods. From this perspective, then (3) is perhaps a better generated output than (1), a fact conveyed by the continuous metric but not the discrete metric. In other words, even though (1) is arguably a better final solution than (3), from the perspective of effective training, it may be favorable to use a target function that prefers (3) to (1) to better guide the training to a stable solution. Once we consider the fact that our aim is not to replicate a single datapoint but learn a generator that in some way interpolates between datapoints, it becomes even less clear if the discrete metric is better. For example, small changes to the input z for (1) are likely to lead to generating samples that are very different to anything seen in the training data (which is, in this case, a single point). Though I do not think this argument undermines the suggested approach, I do think it highlights why the supplied motivation for the approach is insufficient and needs more explicitly linking back to the training procedure. At the very least, I think the above argument shows why it is not immediately clear cut that the suggested approach will perform better and so needs backing up with strong empirical evidence, which unfortunately the paper does not currently have, and/or a more convincing argument for why the discrete metric is better. %%%% Other points %%%%   - Tables 1, 2, and 3 are not worth spending more than a few lines on, let alone nearly a page. They should be substantially compressed or preferably just cut (particularly Tables 2 and 3). The point they convey is obvious and actually somewhat tangential to the key questions. - It worries me that the method learns an h that explicitly takes y as input, not x.  This is a notational issue in the exposition, but also, more importantly, raises questions about whether the original linear programming problem is actually being solved because the inputs from the generative method are not discrete variables. - It should be made clear that the Appendices are directly following the derivation of Evans 1997, rather than being a new derivation. - At times the paper is little sloppy at making distinguishing between y u2208 [0, 1] and y u2208 {0, 1}. This makes it a bit confusing at times what is output from the generator, particularly when you talk about it being one-hot encoded. As I understand it, the output is always the former, while data lives in the latter but this is not always clear. I think you should also make a bigger deal of this in terms of the problem with previous methods that ignore that the critic might be able to distinguish based on the fact that the training and generated data points have an explicitly different type (discrete and continuous respectively). - On the fourth line of the abstract, both instances of GAN should be GANs. - What do you mean ""explained in the sequel""? - Does k need to be the same for each variable?  %%%% References %%%%  Lawrence C Evans. Partial differential equations and Monge-Kantorovich mass transfer. Current developments in mathematics, 1997(1):65u2013126, 1997.",54,1412,25.214285714285715,5.181603773584905,496,19,1393,0.0136396267049533,0.0387755102040816,0.9957,338,173,239,136,10,6,"{'ABS': 1, 'INT': 1, 'RWK': 0, 'PDI': 9, 'DAT': 4, 'MET': 17, 'EXP': 17, 'RES': 6, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 3, 'REC': 2, 'EMP': 34, 'SUB': 0, 'CLA': 1}",1,1,0,9,4,17,17,6,4,0,0,3,1,0,1,0,0,2,3,2,34,0,1,0.7206898328499024,0.6874513628934465,0.5071895031333764
ICLR2018-H1AdTAxC--R3,Reject,Generative adversarial networks (GANs) are state-of-art when it comes to image generation. Many existing works extend GANs to divergences other than the original Jensen-Shannon divergence but also propose ideas for stabilizing training. One such extension is the Wasserstein GAN. This paper extends the Wasserstein GAN to discrete data. This is an important line of work since many observed data are discrete (text is an obvious example). Pros: --The paper is very well written. Each choice leading to the final architecture and learning algorithm is well motivated --I liked the justification (empirical) of why existing extensions of GANs to discrete data are not optimal. --I appreciated the transparency regarding the shortcomings of the proposed approach. We need more of this! --I found the synthetic experiment very insightful. Cons: --The experiments are very very limited. Given this is work extending GANs to discrete data I was expecting an application to text data. --I would have liked to see a theoretical analysis of convergence. What is the optimal discriminator like? I will improve my rating if I see results on a real experiment such as text generation.,15,184,14.153846153846152,5.382857142857143,110,0,184,0.0,0.0163043478260869,0.9704,55,25,42,8,7,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 4, 'MET': 3, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,0,0,2,4,3,4,1,0,1,0,1,0,0,0,0,0,0,0,0,9,0,1,0.5009265436842315,0.227197881700966,0.22754613403904536
ICLR2018-H1BHbmWCZ-R1,Reject,"The paper is motivated with building robots that learn in an open-ended way, which is really interesting. What it actually investigates is the performance of existing image classifiers and object detectors. I could not find any technical contribution or something sufficiently mature and interesting for presenting in ICLR. Some issues: - submission is supposed to be double blind but authors reveal their identity at the start of section 2.1. - implementation details all over the place (section 3. is called Implementation, but at that point no concrete idea has been proposed, so it seems too early for talking about tensorflow and keras).",5,100,16.666666666666668,5.371134020618556,78,1,99,0.0101010101010101,0.0294117647058823,0.2724,28,9,20,5,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,2,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0.2859644778280704,0.2222222222222222,0.12757163592655055
ICLR2018-H1BHbmWCZ-R2,Reject,"This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image. These three proposals are presented in a framework of a robot vision module, although neither the experiments nor the dataset correspond to this domain. From my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges. The proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other technique. This important flaw in the evaluation prevents any fair comparison with the state of the art. The text is also difficult to follow. The three contributions seem disconnected and could have been presented in separate works with a more deeper discussion. In particular, I have serious problems understanding:  1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ? The reference to a GAN architecture seems very forced and out of the scope. 2. What is the interest of the localization network ? The task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the 3x3 grid. 3. The authors talk about an autoencoder architecture, but also on a classification network where the labels correspond to the object count. I could not undertstand what is exactly assessed in this section. Finally, the authors violate the double-bind review policy by clearly referring to their previous work on Experiental Robot Learning. I would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it, with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vision.",12,354,22.125,5.116766467065868,175,4,350,0.0114285714285714,0.0168067226890756,0.8739,98,32,46,14,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 6, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,3,0,2,6,2,1,0,1,0,0,0,0,0,1,0,1,0,1,5,0,1,0.5015342229052935,0.5580433852949275,0.3170021349671829
ICLR2018-H1BHbmWCZ-R3,Reject,"The authors disclosed their identity and violated the terms of double blind reviews. Page 2 In our previous work (Aly & Dugan, 2017)  Also the page 1 is full of typos and hard to read.",1,34,17.0,4.419354838709677,26,0,34,0.0,0.0277777777777777,-0.7579,9,6,4,1,1,0,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 0, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0714285714285714,0.0,0.02555722796588608
ICLR2018-H1BLjgZCb-R1,Accept,Quality: Although the research problem is an interesting direction the quality of the work is not of a high standard. My main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way. How different it will be compared to a perturbation in an input space?  Clarity: The use of the term adversarial is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label (e.g. from church to tower or vice-versa) Originality: The generation of adversarial examples in black-box classifiers has been looked in GAN literature as well and gradient based perturbations are studied too . What is the main benefit of the proposed mechanism compared to the existing ones? Significance: The research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail. pros: (a) An interesting problem to evaluate the robustness of black-box classifier systems (b) generating adversarial examples for image classification as well as text analysis. (c) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples. cons: (a) The proposed search algorithm in the semantic latent space could be computationally intensive. any remedy for this problem? (b) Searching in the latent space z could be strongly dependent on the matching inverter $I_gamma(.)$. any comment on this? (c) The application of the search algorithm in case of imbalanced classes could be something that require further investigation.,16,262,26.2,5.440476190476191,142,1,261,0.003831417624521,0.0227272727272727,-0.9422,85,30,37,13,5,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 4, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 1}",0,0,3,4,0,10,3,0,0,0,0,1,0,0,0,1,2,1,0,0,6,2,1,0.3599058531406717,0.6699311669976509,0.2529132064765112
ICLR2018-H1BLjgZCb-R2,Accept,"Summary:  A method for creation of semantical adversary examples in suggested. The 'semantic' property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter). The generator is trained with a WGAN optimization. Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space. The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences. It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected. Page 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated. This should be explicitly said and the parameter value should be stated. Page 4:  -tthe implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers. This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described. -tthe visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier. In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier). In the churce-vs. tower case, a  relatively weak MLP classifier was used. It would be more instructive to see the results for a better, convolutional classifier. Page 5: -tthe description of the various networks used for text generation is insufficient for understanding: otThe AREA is described in two sentences. It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task ot 'inverter' here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces. ot It is not clear what the 'four-layers strided CNN' is: its structure, its role in the system. How is it optimized? otIn general: a block diagram showing the relation between all the system's components may be useful, plus the details about the structure and optimization of the various modules. It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough. Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear. otSNLI data should be described: content, size, the task it is used for Pro: -tA novel idea of producing natural adversary examples with a GAN -tThe generated examples are in some cases useful for interpretation and network understandin g  -tThe method enables creation of adversarial examples for block box classifiers Cons -tThe idea implementation is basic . Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested -tThe generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier). Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers. ",26,618,23.76923076923077,5.105351170568562,252,8,610,0.0131147540983606,0.0528,0.7853,159,87,109,34,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 3, 'MET': 15, 'EXP': 14, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 7, 'CLA': 1}",0,1,1,2,3,15,14,1,0,0,0,1,0,0,0,1,0,2,0,0,12,7,1,0.5761832876554124,0.5631543089459471,0.35311017783937537
ICLR2018-H1BLjgZCb-R3,Accept,"The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space) . The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{gamma}, which can be regarded as the inverse function of the original GAN G_{theta}, is trained to learn a map from the original input data space to the latent z-space. The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input). Generally, I think that the paper is written well (except some issues listed at the end). The intuition of the proposed approach is clearly explained and it seems very reasonable to me. My main concern, however, is in the current sampling-based search algorithm in the latent z-space, which the authors have already admitted in the paper. The efficiency of such a search method decreases very fast when the dimensions of the z-space increases. Furthermore, such an approximation solution based on the sampling may be not close to the original optimal solution z* in Equation (3). This makes me feel that there is large room to further advance the paper. Another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method (such as FGSM) in generating the adversaries. The example in Table 1 is very good; but more examples (especially involving the quantitative comparison) are needed to demonstrate the claimed advantages. For example, could the authors add such a comparison in Human Evaluation in Section 4 to support the claim that the adversaries generated by their method are more natural? Other issues are listed as follows: (1). Could you explicitly specify the dimension of the latent z-space in each example in image and text domain in Section 3? (2). In Tables 7 and 8, the human beings agree with the LeNet in >  58% of cases. Could you still say that your generated ""adversaries"" leading to the wrong decision from LeNet?  Are these really ""adversaries""? (3). How do you choose the parameter lambda in Equation (2)? ",15,382,23.875,5.154696132596685,178,3,379,0.0079155672823219,0.0155038759689922,0.9306,106,44,56,21,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 4, 'CLA': 1}",0,1,0,1,1,10,5,0,1,0,0,1,0,0,0,0,1,0,0,0,4,4,1,0.5025965121908967,0.4466303462739378,0.284551471357388
ICLR2018-H1BO9M-0Z-R1,Reject,"This paper presents a lifelong learning method for learning word embeddings. Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings. The general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR. However, this submission needs to be improved in terms of clarity and its experiments. In terms of clarity, the paper has a large number of typos (I list a few at the end of this review) and more significantly, at several points in the paper is hard to tell what exactly was done and why. When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful. What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain? The paper states the purpose of the meta learning is to learn a general word context similarity from the first m domains, but I was never sure what this meant. Further, some of the paper's pseudocode includes unexplained steps like invert by domain index and scanco-occurrence. In terms of the experiments, the paper is missing some important baselines that would help us understand how well the approach works. First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered. Also, the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuracy,; see : Think Globally, Embed Locallyu2014Locally Linear Meta-embedding of Words, Bollegala et al., 2017 https://arxiv.org/pdf/1709.06671.pdf  That paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission. Beyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach. What if we just selected the one past domain that was most similar to the new domain, by some measure? And how does the performance of the technique depend on the setting of m? Investigating some of these questions would help us understand how well the approach works and in which settings. Minor:  Second paragraph, GloVec should be GloVe  given many domains with uncertain noise for the new domain -- not clear what uncertain noise means, perhaps uncertain relevance would be more clear. The text refers to a Figure 3 which does not exist, probably means Figure 2. I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2 When m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary. from the first m domain corpus -> from the first m domains? may not helpful -> may not be helpful  vocabularie -> vocabulary  system first retrieval -> system first retrieves. COMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines. The concatenation does fairly comparably to LL in Tables 3&4. LL wins by a bit more in Table 2. Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation.",27,600,28.571428571428573,5.202443280977312,264,7,593,0.0118043844856661,0.0341463414634146,0.9913,172,70,100,38,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 0, 'DAT': 0, 'MET': 15, 'EXP': 3, 'RES': 0, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 7}",0,1,6,0,0,15,3,0,4,0,0,2,1,0,1,0,0,3,2,0,12,0,7,0.504192833378493,0.5631797918021358,0.31481723092413477
ICLR2018-H1BO9M-0Z-R2,Reject,"Summary: This paper proposes an approach to learn embeddings in new domains by leveraging the embeddings from other domains in an incremental fashion . The proposed approach will be useful when the new domain does not have enough data available. The baselines chosen are 1). no embeddings 2). generic embeddings from english wiki, common crawl and combining data from previous and new domains. Empirical performance is shown on 3 downstream tasks: Product-type classification, Sentiment Classification and Aspect Extraction. The proposed embeddings just barely beat the baseline on product classification and sentiment classification, but significantly beat them on aspect extraction task. Comments:  The paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be addressed. There is reasonable novelty in the proposed method compared to the existing literature. But, I was a little disappointed by the paper as several details of the model were unclear to me and the paper's writing could definitely be improved to make things clearer. 1). In the Meta-learner section 4.1, the authors talk about word features (u{_w_{i,j,k}},u{_w_{i,j',k}}). It is unclear what these word features are. Are they one-hot encodings or embeddings or something else? It would really help if the paper gave some expository examples. 2). In Algorithm 1, how do you deal with vocabulary items in the new domain that do not exist in the previous domains i.e. when the intersection of V_i and V_{n+1} is the null set. This is very important because the main appeal of this work is its applicability to new domains with scarce data which have far fewer words and hence the above scenario is more likely to happen. 3). The results in Table 3 are a little confusing. Why do the lifelong word embeddings relatively perform far worse on precision but significantly better on recall compared to the baselines? What is driving those difference in results? 4). Typos: In Section 3, ...is depicted in Figure 1 and Figure 3. I think you mean Figure 1 and Figure 2 as there is no Figure 3.",17,358,15.565217391304348,5.112426035502959,181,2,356,0.0056179775280898,0.0166666666666666,0.7182,100,51,60,18,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 2, 'MET': 9, 'EXP': 3, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,1,4,0,2,9,3,2,2,0,0,0,0,0,0,1,0,2,1,0,8,1,1,0.5025304827941561,0.6711375312677574,0.35571404545352975
ICLR2018-H1BO9M-0Z-R3,Reject,"In this paper, the authors proposed to learn word embedding for the target domain in the lifelong learning manner. The basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus. Overall, the descriptions of the proposed model (Section 3 - Section 5) are hard to follow. This is not because the proposed model is technically difficult to understand. On the contrary, the model is heuristic, and simple, but the descriptions are unclear. Section 3 is supposed to give an overview and high-level introduction of the whole model using the Figure 1, and Figure 2 (not Figure 3 mentioned in text). However, after reading Section 3, I do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is used. Section 4 and  Section 5 are supposed to give details of different components of the proposed model and explain the motivations. However, descriptions in these two sections are very confusing, e.g, many symbols in Algorithm 1 are presented with any descriptions. Moreover, the motivations behind the proposed methods for different components are missing. Also, a lot of types make the descriptions more difficult to follow, e.g., may not helpful or even harmful, 'Figure 3, we show this Section 6, large size a vocabulary, etc. Another major concern is that the technical contributions of the proposed model is quite limited. The only technical contributions are (4) and the way to construct the co-occurrence information A. However, such contributions are quite minor, and technically heuristic. Moreover, regarding the aggregation layer in the pairwise network, it is similar to feature engineering. In this case, why not just train a flat classifier, like logistic regression, with rich feature engineering, in stead of using a neural network. Regarding experiments, one straight-forward baseline is missing. As n domains are supposed to be given in advance before the n+1 domain (target domain) comes, one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain. For instance, one can learn n pairwise (1 out of n sources + the target) cross-domain word embedding, and combine them using the similarity between each source and the target as the weight.",19,388,20.42105263157895,5.3434903047091415,173,1,387,0.0025839793281653,0.0127877237851662,-0.925,107,45,73,18,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 2, 'MET': 14, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 4}",0,1,2,0,2,14,4,0,1,0,0,2,0,0,0,0,0,1,1,0,12,0,4,0.5036730823046547,0.4515293814402793,0.28734082910956293
ICLR2018-H1DGha1CZ-R1,Reject,The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations. Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance. The approach of using BN after non-linearity is termed standardization layer (https://arxiv.org/pdf/1301.4083.pdf). I encourage the authors to validate their claims against simple approach of using BN after non-linearity.  ,4,78,15.6,6.320987654320987,55,1,77,0.0129870129870129,0.025,0.836,26,12,16,4,3,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0.2143662021880983,0.2222222222222222,0.09605375665907057
ICLR2018-H1DGha1CZ-R2,Reject,"This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero. As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU.  The DReLU is supposed to remedy the problem of covariate shift better. The presentation of the paper is clear. The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed). Statistical tests are performed for many of the experimental results, which is solid. However, I have some concerns.  1) As DReLU(x)   max{-delta, x}, what is the optimal strategy to determine delta?  If it is done by hyperparameter tuning with cross-validation, the training cost may be too high. 2) I believe the control experiments are encouraging, but I do not agree that other techniques like Dropouts are not useful. Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important. The arguments for skipping this experiments are respectful, but not convincing enough. 3) Batch normalization is popular, especially for the convolutional neural networks. However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway.",16,224,14.933333333333334,5.30622009569378,129,1,223,0.0044843049327354,0.0218340611353711,0.8533,61,26,45,13,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,12,2,1,0,0,0,1,0,0,0,0,0,0,1,0,9,0,0,0.3599811661861971,0.227197881700966,0.16261989327546741
ICLR2018-H1DGha1CZ-R3,Reject,"This paper describes DReLU, a shift version of ReLU.DReLU shifts ReLU from (0, 0) to (-sigma, -sigma). The author runs a few CIFAR-10/100 experiments with DReLU. Comments:  1. Using expectation to explain why DReLU works well is not sufficient and convincing. Although DReLU's expectation is smaller than expectation of ReLU, but it doesn't explain why DReLU is better than very leaky ReLU, ELU etc. 2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc. 3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious. I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU. Overall, I don't think this paper meet ICLR's novelty standard, although the authors present some good numbers, but they are not convincing. ",7,139,11.583333333333334,5.108527131782946,82,1,138,0.0072463768115942,0.0354609929078014,-0.1786,40,19,27,8,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,0,1,3,1,0,0,0,0,1,0,1,1,1,0,0,0,0,4,0,0,0.4290718127989979,0.3351992056378622,0.21301498459597798
ICLR2018-H1DJFybC--R1,Reject,"I think the idea of inferring programmatic descriptions of handwritten diagrams is really cool, and that the combination of SMC-based inference with constraint-based synthesis is nice. I also think the application is clearly useful u2013 one could imagine that this type of technology would eventually become part of drawing / note-taking applications. That said, based on the current state of the manuscript, I find it difficult to recommend acceptance. I understand that the ICLR does not strictly have a page limit, but I think submitting a manuscript of over 11 pages is taking things a bit too far. The manuscript would greatly benefit from a thorough editing pass and some judicious reconsideration of space allocated to figures. Moreover, despite its relative verbosity, or perhaps because of it, I found it surprisingly difficult to extract simple implementation details from the text (for example I had to dig up the size of the synthetic training corpus from the 44-page appendix). Presentation issues aside, I think this is great work. There is a lot here, and I am sympathetic to the challenges of explaining everything clearly in a single (short) paper. That said, I do think that the authors need to take another stab at this to get the manuscript to a point where it can be impactful.  Minor Comments   - I don't understand what the hypothesis is in the trace hypothesis. Breaking down the problem into an AIR-style sequential detection task and a program induction is certainly a reasonable thing to do. However, the word hypothesis is generally used to refer to a testable explanation of a phenomenon, which is not really applicable here. - How is the edit distance defined? In particular, are we treating the drawing commands as a set or a sequence when we calculate the number of drawing commands by which two trace sets differ? - I took me a while to understand that the authors first consider the case of SMC for synthetic images with a pixel-based likelihood, and then move on to SMC with and edit-distance based surrogate likelihood for hand-drawn pictures. The text seems to suggest that only 100 of such hand drawn images were actually used, is that correct? - What does the (+) operator do in Figure 3? - I am not sure that correcting errors made by the neural network is the most accurate way to describe a reranking of the top-k samples returned by the SMC sweep. - Table 3 is very nice, but does not need to be a full page.  - I would recommend that the authors consolidate wrap-around figures into full-width figures.  ",20,425,25.0,5.135897435897436,232,7,418,0.0167464114832535,0.0386363636363636,0.9826,106,50,83,32,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 0, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 1, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,0,1,1,6,0,1,4,0,0,6,0,1,1,0,0,0,3,1,10,0,0,0.501547948491296,0.4501651992056378,0.2784470870153969
ICLR2018-H1DJFybC--R2,Reject,"Summary of paper:  This paper tackles the problem of inferring graphics programs from hand-drawn images by splitting it into two separate tasks: (1) inferring trace sets (functions to use in the program) and (2) program synthesis, using the results from (1). The usefulness of this split is referred to as the trace hypothesis. (1) is done by training a neural network on data [input   rendered image; output   trace sets] which is generated synthetically. During test time, a trace set is generated using a population-based method which samples and assigns weights to the guesses made by the neural network based on a similarity metric. Generalization to hand-drawn images is ensured by by learning the similarity metric. (2) is done by feeding the trace set into a program synthesis tool of Solar Lezama. Since this is too slow, the authors design a search policy which proposes a restriction on the program search space, making it faster. The final loss for (2) in equation 3 takes into consideration the time taken to synthesize images in a search space. ---  Quality: The experiments are thorough and it seems to work. The potential limitation is generalization to non-synthetic data. Clarity: The high level idea is clear however some of the details are not clear. Originality: This work is one of the first that tackles the problem described. Significance: There are many ad-hoc choices made in the paper, making it hard to extract an underlying insight that makes things work. Is it the trace hypothesis? Or is it just that trying enough things made this work? ---  Some questions/comments: - Regarding the trace set inference, the loss function during training and the subsequent use of SMC during test time is pretty unconventional. The use of the likelihood P_{theta}[T | I] as a proposal, as the paper also acknowledges, is also unconventional. One way to look at this which could make it less unconventional is to pose the training phase as learning the proposal distribution in an amortized way (instead of maximizing likelihood) as, for example, in [1, 2]. - In section 2.1., the paper talks about learning the surrogate likelihood function L_{learned} in order to work well for actual hand drawings. This presumably stems from the problem of mismatch between the distribution of the synthetic data used for training and the actual hand drawings. But then L_{learned} is also learned from synthetic data. What makes this translate to non-synthetic data? Does this translate to non-synthetic data? - What does Intersection over Union in Figure 8 mean? - The details for 3.1 are not clear.",24,421,21.05,5.12781954887218,183,2,419,0.0047732696897374,0.0206896551724137,-0.3565,138,39,75,14,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 3, 'MET': 8, 'EXP': 4, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,0,4,3,8,4,0,2,0,0,3,0,1,0,1,1,0,1,0,10,0,0,0.5023741488137806,0.4500420613580311,0.2839214355857441
ICLR2018-H1DJFybC--R3,Reject,"This paper proposes a method to infer lines of code that produces a given image. The method consists of two components. One is to generate traces, which are primitive commands of a graphic program, given an image. The other is to infer lines of code given traces. The first component uses a deep neural network for the conversion and a novel architecture is used for the network. The second component uses a learnt search polity to speed up the inference. Experimental results on a small dataset show that the proposed method can generate lines of code of a graphics program for the images reasonably well. It also discusses possible applications of the method. Overall, the paper is interesting and the proposed method seems reasonable. Also, it is well contrasted with related work. However, the paper contains too many contents and it is hard to understand the important details without reading supplement and the references. It might be even worth considering to split the paper into two ones and each paper proposes one idea (component) at a time with more details. That said, I understood the basic ideas of the paper and I liked them. My concern is only around how to write.",14,202,14.428571428571429,4.958115183246074,111,3,199,0.0150753768844221,0.0198019801980198,0.9062,52,24,38,11,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 10, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,0,1,2,1,10,0,1,0,0,0,0,0,0,0,1,0,1,0,0,6,0,1,0.3594538860287748,0.4475542316186593,0.20282390668537081
ICLR2018-H1DkN7ZCZ-R1,Reject,"In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies. The tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets. The paper is well written (up to a few misprints), the introduction and the biological background very accurate (although a bit technical for the broader audience) and the bibliography reasonably complete. Maybe the manuscript part with the definition of the accuracy measures may be skipped. Moreover, the authors themselves suggest how to proceed along this line of research with further improvements. I would only suggest to expand the experimental section with further (real) examples to strengthen the claim. Overall, I rate this manuscript in the top 50% of the accepted papers.",7,150,21.428571428571427,5.440559440559441,97,3,147,0.0204081632653061,0.0466666666666666,0.9422,41,24,20,7,7,2,"{'ABS': 0, 'INT': 3, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,3,1,0,0,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,2,0,1,0.5000510378586309,0.2228441796570651,0.2193980548256324
ICLR2018-H1DkN7ZCZ-R2,Reject,"Summary:  In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads. This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard. The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations. To this, they add channels based on low base quality, low mapping quality. The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. One more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate performance. Pros:  The paper tackles what seems to be both an important and challenging problem. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. Cons:  While we liked both the challenge posed and the idea to solve it we found several major issues with the work.  First, the writing is far from clear. There are typos and errors all over at an unacceptable level. Many terms are not defined or defined after being introduced (e.g. CIGAR, MF, BQMQ). A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.  Yet Sec. 2 ""Results"" p. 3 is not really results but part of the methods. The ""pipeline"" is never well defined, only implicitly in p.7 top, and then it is hard to relate the various figures/tables to bottom line results (having the labels wrong does not help that). The filters by themselves seem trivial and as such do not offer much novelty. Moreover, the authors filter the ""normal"" samples using those (p.7 top), which makes the entire exercise a possible circular argument. If the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that. The entire dataset is based on 4 patients. It is not clear what is the source of the other cancer control case. The authors claim the reduced performance show they are learning lung cancer-specific context. What evidence do they have for that? Can they show a context they learned and make sense of it? How does this relate to the original papers they cite to motivate this direction (Alexandrov 2013)? Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients. As such, this may have very little relevance for the actual problem of cfDNA. Finally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited. Albeit the above caveats, we iterate the paper offers a nice construction for an important problem. We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB. ",29,692,20.352941176470587,4.886397608370703,299,4,688,0.005813953488372,0.0228898426323319,-0.7648,180,72,135,39,10,8,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 5, 'DAT': 7, 'MET': 12, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 2, 'PNF': 3, 'REC': 1, 'EMP': 4, 'SUB': 3, 'CLA': 1}",0,1,1,5,7,12,2,1,1,1,0,4,0,0,0,2,1,2,3,1,4,3,1,0.7177140236399898,0.8912535221993755,0.629191003966753
ICLR2018-H1DkN7ZCZ-R3,Reject,"his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA. They key innovation is a convolutional architecture that represents the invariance around the target base. The method is validated on simulations as well as in cfDNA and is s hown to provide increased precision over competing methods. While the method is of interest, there are more recent mutation callers that should be compared. For example, Snooper which uses a RandomForest  (https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-016-3281-2) and hence would be of interest as another machine learning framework. They also should compare to Strelka whic h interestingly they included only to make final calls of mutations but not in the comparison. Further, I  would also have liked to see the use of standard benchmark datasets for mutation calling ( https://www.nature.com/articles/ncomms10001) .  It appears that the proposed method (Kittyhawk) has a steep decrease in PPV and enrichment for low tumor fraction which are presumably the parameter of greatest interest. The authors should explore this behavior in greater detail.",8,172,19.11111111111111,5.627218934911243,118,3,169,0.0177514792899408,0.0282485875706214,0.9545,51,20,31,10,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,3,1,0,3,0,0,0,1,0,0,0,0,0,0,0,3,0,0,2,0,0,0.3578042171751944,0.2230785047714451,0.1599427397486114
ICLR2018-H1Dy---0Z-R1,Accept,"This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner. The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers. Using this system, the authors are able to harness much more compute to learn very high quality policies in little time. The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow. It's hard to take issue with a paper that has such overwhelmingly convincing experimental results. However, there are a couple additional experiments that would be quite nice: u2022tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X). u2022tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time. For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames? Pros: u2022tWell written and clear . u2022tVery impressive results . u2022tIt's remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm. Cons: u2022tHard to replicate experiments without the deep computational pockets of DeepMind. ",12,234,16.714285714285715,5.273542600896861,142,0,234,0.0,0.0042194092827004,0.9893,62,39,40,14,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,0,0,0,3,5,2,0,0,0,1,0,0,0,0,0,3,0,0,5,1,1,0.3580437264299707,0.4471665992981963,0.205121463760238
ICLR2018-H1Dy---0Z-R2,Accept,"A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory. Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines. The core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial).  I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA). Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so! There are essentially two more things I would have really liked to see in this paper (maybe for future work?): - Using all Rainbow components - Using multiple learners (with actors cycling between them for instance) Sharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus! Minor points: - Figure 1 does not seem to be referenced in the text  - u00ab In principle, Q-learning variants are off-policy methods u00bb  > not with multi-step unless you do some kind of correction! I think it is important to mention it even if it works well in practice (just saying u00ab furthermore we are using a multi-step return u00bb is too vague) - When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics. - In caption of Fig. 5 25K should be 250K - In appendix A why duplicate memory data instead of just using a smaller memory size? - In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme? - Why store rewards and gamma's at each time step in memory instead of just the total discounted reward? - It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix - Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?",16,410,34.166666666666664,4.950520833333333,232,2,408,0.0049019607843137,0.0258823529411764,0.9979,108,52,73,29,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 5, 'EXP': 2, 'RES': 0, 'TNF': 4, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 2}",0,1,0,1,3,5,2,0,4,1,1,1,1,0,0,0,0,1,2,0,7,2,2,0.7155147538944685,0.5595366806675923,0.4488355430908566
ICLR2018-H1Dy---0Z-R3,Accept,"This paper proposes a distributed architecture for deep reinforcement learning at scale, specifically, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework. It has a very nice introduction and literature review of Prioritized experience replay and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel, so that the experience replay can obtain more data for the learner to sample and learn. Not surprisingly, as this framework is able to learn from way more data (e.g. in Atari), it outperforms the baselines, and Figure 4 clearly shows the more actors we have the better performance we will have.  While the strength of this paper is clearly the good writing as well as rigorous experimentation, the main concern I have with this paper is novelty. It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature; hence the challenge of the work is not quite clear. Hence, I feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper, for example. ",8,183,22.875,5.254237288135593,104,1,182,0.0054945054945054,0.0432432432432432,0.9287,53,22,28,15,5,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 0, 'DAT': 2, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 1}",0,2,2,0,2,4,2,0,0,0,0,0,0,0,0,2,0,1,0,0,1,1,1,0.3581217268475127,0.5556006631400737,0.2199126380316631
ICLR2018-H1K6Tb-AZ-R1,Reject,"An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated. The approach is based on `profile coefficients' which are learned for every channel in a convolution layer, or for every column in the fully connected layer. Based on the magnitude of this profile coefficient, which determines the importance of this `filter,' individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner. Different from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn't fully utilize the deep net performance. To address this issue a `loss aggregation' is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used. The method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime. Summary: u2014u2014 In summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method. The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing. In addition, the writing should be improved as it is often ambiguous. See below for details.  Review: u2014u2014u2014u2014u2014 1. Experiments are only provided on very small datasets. According to my opinion, this isn't sufficient to illustrate the effectiveness of the proposed approach. As a reader I wouldn't want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.  2.  Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent. More specifically, while the profile coefficient is applied for every input element in Eq. (1), it's applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layer' is more confusing than clarifying.  3. The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where leq x geq 100, means the x% of terms used in dot products'. While sentences like those can be deciphered, they aren't that appealing. 4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc. 5. Algorithm 1 and Algorithm 2 call functions that aren't described/defined. 6. Baseline numbers for training on datasets without incomplete dot products should be provided. ",17,439,11.864864864864863,5.315403422982885,203,4,435,0.0091954022988505,0.0337837837837837,0.9132,117,49,84,30,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 5, 'MET': 10, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 5, 'CLA': 4}",0,1,5,0,5,10,3,3,0,0,0,1,0,0,0,0,0,3,0,0,3,5,4,0.5030018389620081,0.4465927956743584,0.2796119748567583
ICLR2018-H1K6Tb-AZ-R2,Reject,"This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints. The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized. These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity.  The original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities. This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting.  They also present results for a 'harmonic' combination which was not explained in the paper at all. Overall the paper has very good motivation and significance. However the writing is not very clear and the paper is not self-contained at all. I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting. Additionally, there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in Figure  3. As is, while results are promising, I can't fully assess that the paper has major contributions.  ",12,244,20.33333333333333,5.329059829059829,127,0,244,0.0,0.0281124497991967,0.96,65,27,47,15,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 3}",0,1,2,0,1,5,0,2,1,0,0,3,0,0,0,0,2,2,0,0,1,1,3,0.5012277958663333,0.5558830249029015,0.3125708786753867
ICLR2018-H1K6Tb-AZ-R3,Reject,"The authors propose a method for reducing the computational burden when performing inference in deep neural networks. The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients. The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product. Unfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not. While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure. Positive points: - The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal. - Application to two well-known benchmarking datasets. Negative points: - The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches. - It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task. Minor point: - Figure 1 is unclear and requires a better caption. ",9,238,19.83333333333333,5.431034482758621,143,2,236,0.0084745762711864,0.0532786885245901,0.4125,65,32,38,8,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,6,0,0,1,0,0,0,0,1,0,0,0,0,1,0,4,0,0,0.3583938177117804,0.2240880945267511,0.1564039114807275
ICLR2018-H1LAqMbRW-R1,Reject,"The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning. The idea of re-using a pretrained agent has both pros and cons. On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment. On the other hand, the usefulness of the learned representation for planning is unclear. A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions. A reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation. u2028In addition to that, one could fine-tune the representation during forward model training. It would be interesting to see if this can improve the results. I personally miss a more technical and detailed exposition of the ideas. For example, it is not described anywhere what loss is used for learning the model.  MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained. Again, due to lack of equations, I don't completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way. I also could find the details on how figure 1 was produced . As I understand, MCTS was not used in this experiment. If so, how would one play with just a forward model? It is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability. Is mini-RTS a deterministic environment?  Would it be possible to include a non-deterministic baseline in the experimental comparison? Experimentally, the results are rather weak compared to pure model-free agents. Somewhat unsatisfying, longer-term prediction results into weaker game play. Doesn't this support the argument about need in stochastic prediction? To me, the paper in it's current form is not written well and does not contain strong enough empirical results, so that I can't recommend acceptance. Minor comments: * MatchA and PredictPi models are not introduced under such names * Figure 1 that introduces them contains typos. * Formatting of figure 8 needs to be fixed. This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there.",26,424,20.19047619047619,5.3291457286432165,217,0,424,0.0,0.0255813953488372,-0.9731,100,56,80,37,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 14, 'EXP': 2, 'RES': 4, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 17, 'SUB': 0, 'CLA': 4}",0,1,1,1,0,14,2,4,4,0,0,1,0,0,0,0,0,1,2,1,17,0,4,0.575004264259729,0.5658118486494086,0.36307445018883894
ICLR2018-H1LAqMbRW-R2,Reject,"Summary:  This paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games. The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS. Forecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning. The paper: 1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space. 2. Evaluates how informative the latent states are via state reconstruction. 3. trains variatns of a forward model f on the hidden states of the various learned agents. 4. evaluates different f within MCTS for MiniRTS. Pro: - This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods. - The experimental setting is very non-trivial and novel. Con: - The manuscript is unclear in many parts -- this should be greatly improved. 1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?). Which forward model is trained from which model-free agent? 2. How is the forward model / value function used in MCTS? I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together. - The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X. Yet it is unclear how this informs us about the quality of the learned forward models f. It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix. - As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models. Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8. - The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent. Is this because of the MCTS approach? Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation.   Overall: I think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis .  Detailed: - What are the right prediction tasks that ensure the latent space captures enough of the forward model? - What is the error of the raw h-predictions? Only the state-reconstruction error is shown now. - Figure 6 / sect 4.2: which model-free agent is used? Also fig 6 misses captions. - Figure 8: scrambled caption. - Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?",29,468,15.096774193548388,5.070454545454545,211,3,465,0.0064516129032258,0.0346938775510204,-0.9065,129,69,77,30,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 17, 'EXP': 3, 'RES': 2, 'TNF': 4, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 17, 'SUB': 0, 'CLA': 1}",0,1,1,2,0,17,3,2,4,4,0,1,0,0,0,1,0,0,3,0,17,0,1,0.6472916620396487,0.4545189012495387,0.3623541353336078
ICLR2018-H1LAqMbRW-R3,Reject,"Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS). The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information. They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%). Pros: - Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory. - Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available. Cons: - The model-based approach is disappointing compared to the model-free approach. Quality and Clarity:  The paper in general is well-written and easy to follow and seems technically correct,; though I found some of the figures and definitions confusing, specifically:  - The terms for different forward models are not defined (e.g. MatchPi, MatchA, etc.). I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly. - In Figure 3b, it is not clear to me what the difference between the red and blue curves is. - In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI. - The caption in Figure 8 is malformatted. - In Figure 7, the baseline of hat{h_t} h_{t-2} seems strange---I would find it more useful for Figure 7 to compare to the performance if the model were not used (i.e. if hat{h_t} h_t) to see how much performance suffers as a result of model error. Originality:  I am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain. However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features. The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below). Significance:  I feel the paper overstates the results in saying that the learned forward model is usable in MCTS. The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms. The baseline used in the paper is a random baseline, which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice, such as the model-free approach.",19,486,23.142857142857142,5.061002178649238,219,6,480,0.0125,0.0240963855421686,0.9439,139,59,78,21,9,6,"{'ABS': 1, 'INT': 2, 'RWK': 6, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 6, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 2, 'CMP': 4, 'PNF': 4, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",1,2,6,1,0,10,0,2,6,1,0,1,0,0,0,3,2,4,4,0,6,0,1,0.6457090220186829,0.6704509001013457,0.45336769571284785
ICLR2018-H1MczcgR--R1,Accept,"The paper discusses the problems of meta optimization with small look-ahead: do small runs bias the results of tuning? The result is yes and the authors show how differently the tuning can be compared to tuning the full run. The Greedy schedules are far inferior to hand-tuned schedules as they focus on optimizing the large eigenvalues while the small eigenvalues can not be seen with a small lookahead. The authors show that this effect is caused by the noise in the obective function. pro: - Thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work - Easy to read and follow cons: -Small issues in presentation:  * Figure 2 optimal learning rate -> optimal greedy learning rate, also reference to Theorem 2 for increased clarity. * The optimized learning rate in 2.3 is not described. This reduces reproducibility. * Figure 4 misses the red trajectories, also it would be easier to have colors on the same (log?)-scale.  The text unfortunately does not explain why the loss function looks so vastly different   with different look-ahead. I would assume from the description that the colors are based   on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective LR. Typos and notation: page 7 last paragraph: We train the all -> We train all notation page 5: i find  abla_{theta_i} confusing when theta_i is a scalar, i would propose frac{partial}{partial theta_i} page 2: But this would come at the expense of long-term optimization process: at this point of the paper it is not clear how or why this should happen. Maybe add a sentence regarding the large/Small eigenvalues?",12,271,24.63636363636364,5.1400778210116735,151,1,270,0.0037037037037037,0.0456140350877193,0.8183,74,39,51,15,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 3}",0,1,0,0,0,5,0,3,2,1,0,1,0,0,0,0,0,0,2,0,2,2,3,0.4297068987726927,0.4453969174531204,0.23577694926514145
ICLR2018-H1MczcgR--R2,Accept,"This paper proposes a simple problem to demonstrate the short-horizon bias of the learning rate meta-optimization. - The idealized case of quadratic function the analytical solution offers a good way to understand how T-step look ahead can benefit the meta-algorithm. - The second part of the paper seems to be a bit disconnected to the quadratic function analysis. It would be helpful to understand if there is gap between gradient based meta-optimization and the best effort(given by the analytical solution) - Unfortunately, no guideline or solution is offered in the paper. In summary, the idealized model gives a good demonstration of the problem itself. I think it might be of interest to some audiences in ICLR.",7,113,18.83333333333333,5.37037037037037,69,3,110,0.0272727272727272,0.0517241379310344,0.9519,30,15,20,2,4,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,0,2,0,3,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,0,0.2862739698037108,0.3333333333333333,0.139568913668106
ICLR2018-H1MczcgR--R3,Accept,"This paper studies the issue of truncated backpropagation for meta-optimization. Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like. This paper highlights this problem as a fundamental issue limiting meta-optimization approaches. The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST. (side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2) .  The paper is generally clear and well written. Major comments ------------------------- I was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature?  The authors suggest this is needed because the dynamics of training are different at the very start compared to later stages, which is a bit vague. Perhaps the authors can expand upon  this point? The conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective --but Fig 2. and earlier discussion talked about the noise in the objective as introducing the bias (e.g. from earlier in the paper, The noise in the problem adds uncertainty to the objective, resulting in failures of greedy schedule). Which is the real issue, noise or curvature? Would running the problem on quadratics with different condition numbers be insightful? Minor comments ------------------------- The stochastic gradient equation in Sec 2.2.2 is missing a subscript: h_i instead of h n It would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves. n It looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct?  I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)? Figure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process. Figure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case?  I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly. Figure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls . I would add a note in the caption to make it clear that the SMD trajectories are the red curves, e.g.: SMD trajectories (red) during meta-optimization of initial effective .... I would also change the caption to use meta-training losses instead of training losses (I believe those numbers are for the meta-loss, correct?) . Finally, I would add a colorbar to indicate numerical values for the different grayscale values. Some recent references that warrant a mention in the text: - both of these learn optimizers using longer numbers of unrolled steps: Learning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017 Learned optimizers that scale and generalize, Wichrowska et al, ICML 2017 - another application of unrolled optimization: Unrolled generative adversarial networks, Metz et al, ICLR 2017 In the text discussing Figure 4 (middle of pg. 8) , which is obtained by using... should be which are obtained by using...   In the conclusion, optimal for deterministic objective should be deterministic objectives",26,618,24.72,5.305555555555555,269,3,615,0.0048780487804878,0.0157728706624605,0.9934,173,76,96,32,12,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 8, 'EXP': 2, 'RES': 5, 'TNF': 10, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 7, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 6}",0,2,2,1,1,8,2,5,10,3,0,1,1,1,0,0,0,1,7,0,6,0,6,0.8595683325411878,0.4483293205157495,0.4873520582776182
ICLR2018-H1Nyf7W0Z-R1,Reject,"The paper proposes another training objective for training neural sequence-to-sequence models. The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p. The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively. The paper has significant writing issues. In Paragraph ""Maximum Likelihood"", page 2, the formalization of the studied problem is unclear. Do X and Y denote the complete input/output spaces, or do they stand for the training set examples only? In the former case, the statement ""x is uniformly sampled from X"" does not make sense because X is practically infinite. Same applies to the dirac distribution q(y|x), the true conditional distribution of outputs given inputs is multimodal even for machine translation. If X and Y were meant to refer to the training set, it would be worth mentioning the existence of the test set. Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used. The proposed method is evaluated on just one dataset. Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al. The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison. To sum up, I can not recommend the paper to acceptance, because (a) an important baseline is missing (b) there are serious writing issues.",16,285,21.923076923076923,5.350746268656716,155,1,284,0.0035211267605633,0.0314685314685314,0.5756,87,31,52,19,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 5, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 1, 'SUB': 4, 'CLA': 3}",0,1,3,1,5,4,0,0,0,0,0,3,0,0,0,0,0,2,0,1,1,4,3,0.4297181812932256,0.556155017779418,0.2671515466399977
ICLR2018-H1Nyf7W0Z-R2,Reject,"This paper considers a dichitomy between ML and RL based methods for sequence generation. It is argued that the ML approach has some discrepancy between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. An alpha-divergence formulation is considered to combine both methods. Unfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper.  I therefore have no option but to vote for reject of this paper, based on my educated guess. Below are the points that I'm particularly confused about:  1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me . For example,   1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators. From the context, I guess the authors mean empirical training distribution? 1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a discrepancy to me. The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand.  In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator. In addition, I don't see at all why this discrepancy is a discrepancy between training and testing data. As long as both of them are identically distributed, then no discrepancy exists. 1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong. First, the model is *not* trained on the true distribution which is unknown. The model is trained on an empirical distribution whose points are sampled from the true distribution. I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution. 2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a reward function, but I don't know what it means and the authors should perhaps explain further. I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples.",20,483,19.32,4.930957683741648,199,5,478,0.0104602510460251,0.0368098159509202,0.1128,124,48,97,40,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 3, 'MET': 14, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 13, 'SUB': 2, 'CLA': 0}",0,1,1,0,3,14,0,0,0,0,0,2,0,0,0,0,0,1,0,1,13,2,0,0.3605129549098952,0.4520146101708815,0.20088501631741043
ICLR2018-H1Nyf7W0Z-R3,Reject,"Summary of the paper:   This paper presents a method, called alpha-DM (the authors used this name because they are using alpha-Divergence to measure the distance between two distributions),  that addresses three important problems simultaneously:  (a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT). (b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution  (c) Sample inefficiency: The RL model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective function's optimization Then the authors present the results for machine translation task and also analysis of their proposed method. My comments / feedback:   The paper is well written and the problem addressed by the paper is an important one. My main concerns about this work are have two aspects:  (a)tNovelty 1.tThe idea is a good one and is great incremental research building on the top of previous ideas. I do not agree with statements like ""We demonstrate that the proposed objective function generalizes ML and RL objective functions ..."" that authors have made in the abstract. There is not enough evidence in the paper to validate this statement. (b)tExperimental Results 2.tThe performance of the proposed method is not significantly better than other models in MT task. I am also wondering why authors have not tried their method on at least one more task? E.g., in CNN+LSTM based image captioning, the perplexity is minimized as cost function but the performance is measured by BLEU etc. Some minor comments:   1.tIn page 2, 6th line after eq (1), ""... these two problems"" --> ""... these three problems""  2. tIn page 2, the line before the last line, ""... resolbing problem"" --> ""... resolving problem"" ",13,305,21.785714285714285,5.36518771331058,167,1,304,0.0032894736842105,0.0184615384615384,0.8879,113,23,56,14,9,3,"{'ABS': 1, 'INT': 1, 'RWK': 0, 'PDI': 4, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 3}",1,1,0,4,2,7,2,2,0,1,0,1,0,0,0,1,0,0,0,0,3,0,3,0.6447141835497296,0.3347395183447273,0.3254097222528474
ICLR2018-H1O0KGC6b-R1,Reject,"Summary:  Based on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network. This additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments). According to the authors, the contributions are the following: 1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to make sure that the representation learned is used in the most efficient way. 2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression). 3. Experimental results. Clarity: The paper is well-written, the main ideas well-clarified. Importance: While the majority of papers nowadays focuses on the representation part (i.e., how we get to Phi_{L-1}(x)), this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm. This by itself is not enough to boost the performance universally (e.g., if Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures. From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference. On the other hand, to my humble opinion, the experimental results do not show a significant gain in the performances of all networks (esp. Figure 3 and Table 1 are within the range of statistical error). In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better. Originality: It would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer. Comments: 1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer? One reason is convexity in W of the problem (2). Any other? 2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN? The authors state this question in the future direction, but it would make the paper more complete to consider it here. ",16,412,19.61904761904762,4.92191435768262,192,2,410,0.0048780487804878,0.0120772946859903,0.9696,105,53,63,22,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 9, 'EXP': 6, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 1}",0,1,0,1,2,9,6,1,1,0,1,0,0,0,0,0,0,0,1,0,7,3,1,0.5738971780372857,0.4483895420701452,0.32660859452088853
ICLR2018-H1O0KGC6b-R2,Reject,"This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model). Summary of evaluation  There is not much novelty in this idea (of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step), which dates back at least a decade, so the only real contribution would be in the experiments. However the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method. More details  Previous work on the same idea: at least a decade old, e.g., Huang and LeCun 2006. See a review of such work in 'Deep Learning using Linear Support Vector Machines' more recently. Experiments  You should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training). Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct. ",7,204,25.5,5.076530612244898,129,1,203,0.0049261083743842,0.0432692307692307,0.92,51,33,30,19,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 0, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,1,1,0,0,4,1,0,0,0,0,0,0,0,1,0,1,0,0,3,0,0,0.3574014445179831,0.3345772482030192,0.17594433417108257
ICLR2018-H1O0KGC6b-R3,Reject,"This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization.  However, the results do not convince this reviewer to switch to using 'post-training'. Learning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago. However, freezing the layers and continue to train the last layer is of a minor novelty. The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the lambda term which is added for post-training but not added for the baseline. c.f. Eq 3 and Eq 4. Therefore, it's unclear whether the gain in generalization is due to an additional lambda term or from the post-training training itself. A way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise. Other notes,   Remark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments. For table 1, please use decimal points instead of commas. ",9,205,17.083333333333332,4.907692307692308,117,1,204,0.0049019607843137,0.0430622009569378,0.9851,51,23,34,15,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,1,0,0,0,6,2,3,1,0,0,0,0,0,0,1,0,0,0,0,4,1,1,0.358591417288908,0.4463103167489733,0.1974394080795273
ICLR2018-H1OQukZ0--R1,Reject,"Summary of the paper --------------------------- The paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning.  The covered framework is limited to regularization parameters. These hyper-parameters, noted $lambda$, are updated along the training of model parameters $theta$ by relying on the generalization performance (validation error). The paper proposes a dynamical system including the dynamical update of $theta$ and the update of the gradient $y$, derivative of $theta$ w.r.t. to the hyper-parameters. The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system. Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach. Comments ------------- - The materials of the paper sometimes may be quite not easy to follow. Nevertheless the paper is quite well written. - The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016) . As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors. - One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017). The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed. - The goal of the paper is to address automatically the learning of regularization parameters. Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $eta$) which choices are not clearly discussed. It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious. This fact weakens the scope of the online hyper-parameter optimization approach. - It may be helpful to indicate the standard deviations of the experimental results.",17,315,17.5,5.41,153,5,310,0.0161290322580645,0.0277777777777777,0.8608,86,43,47,13,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,1,3,0,0,12,2,1,0,0,0,2,2,0,0,0,0,2,0,0,7,0,2,0.5030570966609959,0.3372633755704352,0.2521916397509535
ICLR2018-H1OQukZ0--R2,Reject,"# Summary of paper The paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method. # Summary of review I find the contribution to be incremental, and the validation weak. Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution. None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject. # Detailed comments  1. The distinction between parameters and hyperparameters (section 3) should be revised. First, the definition of parameters should not include the word parameters. Second, it is not clear what parameters of the regularization means. Typically, the regularization depends on both hyperparameters and parameters. The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are begin learnt, just from different datasets. 2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct. This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015). 3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to. theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used). 4. The hypothesis are not correctly specified. Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated. 5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this. While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm. # Minor  missing . after hypergradient exactly. we could optimization the hyperparam- (typo). References:  Justin  Domke.    Generic  methods  for  optimization-based modeling.  In International Conference on Artificial Intelligence and Statistics, 2012.",20,384,12.8,5.81767955801105,191,3,381,0.0078740157480314,0.03,0.7624,101,36,71,25,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 16, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 13, 'SUB': 0, 'CLA': 4}",0,1,2,0,1,16,0,1,0,0,0,0,3,0,0,0,0,0,0,1,13,0,4,0.432429584190257,0.3410402277640111,0.21712206996104097
ICLR2018-H1OQukZ0--R3,Reject,"Summary of paper:  This work proposes an extension to an existing method (Franceschi 2017) to optimize regularization hyperparameters. Their method claims increased stability in contrast to the existing one. Summary of review:  This is an incremental change of an existing method. This is acceptable as long as the incremental change significantly improves results or the paper presents some convincing theoretical arguments. I did not find either to be the case. The theoretical arguments are interesting but lacking in rigor. The proposed method introduces hyper-hyperparameters which may be hard to tune. The experiments are small scale and it is unclear how much the method improves random grid search. For these reasons, I cannot recommend this paper for acceptance. Comments: 1. Paper should cite Domke 2012 in related work section. 2. Should state and verify conditions for application of implicit function theorem on page 2. 3. Fix notation on page 3. Dot is used on the right hand side to indicate an argument but not left hand side for equation after with respect to lambda. 4. I would like to see more explanation for the figure in Appendix A. What specific optimization is being depicted? This figure could be moved into the paper's main body with some additional clarification. 5. I did not understand the paragraph beginning with This poor estimation. Is this just a restatement of the previous paragraph, which concluded convergence will be slow if eta is too small? 6. I do understand the notation used in equation (8) on page 4. Are <, > meant to denote less than/greater than or something else? 7. Discussion of weight decay on page 5 seems tangential to main point of the paper. Could be reduced to a sentence or two. 8. I would like to see some experimental verification that the proposed method significantly reduces the dropout gradient variance (page 6), if the authors claim that tuning dropout probabilities is an area they succeed where others don't. 9. Experiments are unconvincing. First, only one hyperparameter is being optimized and random search/grid search are sufficient for this. Second, it is unclear how close the proposed method is to finding the optimal regularization parameter lambda. All one can conclude is that it performs slightly better than grid search with a small number of runs. I would have preferred to see an extensive grid search done to find the best possible lambda, then seen how well the proposed method does compared to this. 10. I would have liked to see a plot of how the value of lambda changes throughout optimization. If one can initialize lambda arbitrarily and have this method find the optimal lambda, that is more impressive than a method that works simply because of a fortunate initialization. Typos: 1. Optimization -> optimize (bottom of page 2) 2. Should be a period after sentence starting Several algorithms on page 2. 3. In algorithm box on page 5, enable_projection is never used. Seems like warmup_time should also be an input to the algorithm.",31,496,11.534883720930232,5.256578947368421,230,3,493,0.0060851926977687,0.0199600798403193,0.9962,139,47,98,24,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 17, 'EXP': 5, 'RES': 0, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 1, 'EMP': 21, 'SUB': 0, 'CLA': 2}",0,1,2,0,0,17,5,0,3,0,0,1,1,0,0,0,0,0,4,1,21,0,2,0.5044749408111866,0.4571494349835679,0.28735187853202276
ICLR2018-H1T2hmZAb-R1,Accept,"This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy. The writing is clear, concise and easy to follow. An important argument in favour of using complex-valued networks is said to be the propagation of phase information. However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat.  CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise . It definitely does not preserve phase, like modReLU would. This makes me wonder whether the complex numbers paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in Compressing neural networks with the hashing trick by Chen et al.). Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this. The image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach. The improved performance on the audio tasks seems significant, but how the complex nature of the networks helps achieve this is not really demonstrated. It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague. Once again, a more in-depth analysis of this phase behavior would be very welcome. I'm on the fence about this work: I like the ideas and they are explained well, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used). Comments:  - The related work section is comprehensive but a bit unstructured, with each new paragraph seemingly describing a completely different type of work. Maybe some subsection titles would help make it feel a bit more cohesive. - page 3: (cite a couple of them) should be replaced by some actual references :) - Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer? It would be nice to discuss computational cost as well. REVISION: I have decided to raise my rating from 5 to 7 as I feel that the authors have adequately addressed many of my comments. In particular, I really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network. Regarding the CIFAR results, I may have read over it, but I think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this. With this in mind, it is of course completely fine that the results are not better than for real-valued networks. ",23,540,23.47826086956522,5.142307692307693,266,8,532,0.0150375939849624,0.0383211678832116,0.9957,120,61,108,42,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 14, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 14, 'SUB': 0, 'CLA': 3}",0,1,6,2,0,14,1,3,0,2,0,2,1,0,0,0,1,1,0,1,14,0,3,0.6467529216842048,0.5638032723502223,0.4023999292675924
ICLR2018-H1T2hmZAb-R2,Accept,"The paper presents an extensive framework for complex-valued neural networks. Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more. The contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks. Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc. In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network. Empirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks. Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks, but constructing a solid framework that will enable stable and solid application and research of these well-motivated models.",8,201,28.714285714285715,6.589743589743589,119,1,200,0.005,0.0099502487562189,0.8847,63,48,19,12,3,2,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,4,0,0,6,0,0,0,0,0,0,0,0,0,0,0,2,0,0,4,0,0,0.2157781385617895,0.2242052570839411,0.09834736494599691
ICLR2018-H1T2hmZAb-R3,Accept,"Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions. Their related work section brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations. However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued. Their contributions are:  1. Formulate complex valued convolution 2. Formulate two complex-valued alternatives to ReLU and compare them 3. Formulate complex batch normalization as a whitening operation on complex domain 4. Formulate complex analogue of Glorot weight normalization scheme Since any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case. For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward. However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic. Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance.",11,179,16.272727272727273,6.180232558139535,98,2,177,0.0112994350282485,0.0277777777777777,0.9753,54,30,32,9,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 9, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,2,0,1,9,0,0,0,0,0,0,0,0,0,0,0,1,0,0,4,0,0,0.287796310526947,0.2240880945267511,0.12883896504329384
ICLR2018-H1UOm4gA--R1,Accept,"This paper introduces a new task that combines elements of instruction following and visual question answering: agents must accomplish particular tasks in an interactive environment while providing one-word answers to questions about features of the environment. To solve this task, the paper also presents a new model architecture that effectively computes a low-rank attention over both positions and feature indices in the input image. It uses this attention as a common bottleneck for downstream predictors that select actions and answers to questions. The paper's main claim is that this model architecture enables strong generalization: it allows the model to succeed at the instruction following task even when given words it has only seen in QA contexts, and vice-versa. Experiments show that on the navigation task, the proposed approach outperforms a variety of baselines under both a normal data condition and one requiring strong generalization. On the whole, I think this paper does paper does a good job of motivating the proposed modeling decisions. The approach is likely to be useful for other researchers working on related problems. I have a few questions about the evaluation, but most of my comments are about presentation. EVALUATION  Is it really the case that no results are presented for the QA task, or am I misreading one of the charts here? Given that this paper spends a lot of time motivating the QA task as part of the training scenario, I was surprised not to see it evaluated. Additionally, when I first read the paper I thought that the ZS1 experiments featured no QA training at all. However, your response to one of the sibling comments suggests that it's still a mixed training setting where the sampled QA and NAV instances happen to cover the full space. This should be made more clear in the paper. It would be nice to know (1) how the various models perform at QA in both ZS1 and ZS2 settings, and (2) what the actual performance is NAV alone (even if the results are terrible). MODEL PRESENTATION  I found section 2 difficult to read: in particular, the overloading of Phi with different subscripts for different output types, the general fact that e.g. x and Phi_x are used interchangeably, and the large number of different variables. My best suggestions are to drop the Phis altogether and consider using text subscripts rather than coming up with a new name for every variable, but there are probably other things that will also help. OTHER NOTES  - This paper needs serious proofreading---just in the first few pages the errors   I noticed were in 2D environment (in the title!), such capability, this   characteristics, such language generalization problem, the agent need to,   some early pioneering system, commands is. I gave up on keeping track at   this point but there are many more. - phi in Fig 2 should be explained by the caption. - Here's another good paper to cite for the end of 2.2.1:   https://arxiv.org/pdf/1707.00683.pdf. - The mechanism in 2.2.4 feels a little like   http://aclweb.org/anthology/D17-1015 - I don't think the content on pages 12, 13, and 14 adds much to the   paper---consider moving these to an appendix.",22,519,24.714285714285715,5.094069529652352,272,5,514,0.0097276264591439,0.0203327171903881,0.9558,157,58,87,23,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 10, 'EXP': 7, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 5, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 5}",0,0,2,3,0,10,7,2,1,0,0,3,1,0,0,0,1,2,5,0,3,2,5,0.5745431023377916,0.6687052365804935,0.40421529308410187
ICLR2018-H1UOm4gA--R2,Accept,"[Overview] In this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user's questions as well. To address this problem, the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images. Specifically, By this way, the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly. In the experiments, the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations, including in-domain and out-of-domain cases. [Strengths]  1. I think this paper proposed interesting tasks to combine the vision, language, and actions. As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments. The authors should release the dataset to prompt the research in this area. 2. The authors proposed a simple method to ground the language on visual input. Specifically, the authors grounded each word in a sentence to all locations of the visual map, and then perform a simple concept detection upon it. Then, the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well. 3. From the experiments, it is shown that the proposed model outperforms several baseline methods in both normal tasks and out-of-domain ones. According to the visualizations, the interpreter could generate meaningful attention map given a textual query. [Weakness]  1. The definition of explicit grounding is a bit misleading. Though the grounding or attention is performed for each word at each location of the visual map. It is a still kind of soft-attention, except that is performed for each word in a sentence. As far as I know, this has been done in several previous works, such as: (a). Hierarchical question-image co-attention for visual question answering (https://scholar.google.com/scholar?oi bibs&cluster 15146345852176060026&btnI 1&hl en). Lu et al. NIPS 2016. (b). Graph-Structured Representations for Visual Question Answering. Teney et al. arXiv 2016. At most recent, we have seen some more explicit way for visual grounding like: (c). Bottom-up and top-down attention for image captioning and VQA (https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017. 2. Since the model is aimed at grounding the language on the vision based on interactions, it is worth to show how well the final model could ground the text words to each of the visual objects. Say, show the affinity matrix between the words and the objects to indicate the correlations. [Summary]  I think this is a good paper which integrates vision, language, and actions in a virtual environment. I would foresee more and more works will be devoted to this area, considering its close connection to our daily life. To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc. According to the visualization, the model could attend the right region of the image for finishing a navigation and QA task. As I said, the authors should rephrase the definition of explicit grounding, to make it clearly distinguished with the previous work I listed above. Also, the authors should definitely show the grounding attention results of words and visual signal jointly, i.e., showing them together in one figure instead of separately in Figure 9 and Figure 10. ",26,592,14.095238095238097,5.283662477558348,246,2,590,0.0033898305084745,0.0083892617449664,0.9782,175,71,90,25,10,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 4, 'DAT': 1, 'MET': 17, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 2, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 7, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,0,3,4,1,17,3,1,1,1,2,1,0,0,0,0,2,7,1,0,8,1,0,0.718804186081634,0.5606602695910441,0.4593718378938248
ICLR2018-H1UOm4gA--R3,Accept,"The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks. Agents working in this setting therefore, learn the language of the teacher and efficiently ground words to their respective concepts in the environment. The work also propose a neat model motivated by the environment and outperform various baselines. Further, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase. The robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right? As the paper says, a difference of one word can lead to completely different goals and so, the noise robustness experiments seem to test for the biases learned by the agent in some sense (which is not desirable). Is there any justification for why this method of injecting noise was chosen ? Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ?  Overall, I think the paper proposes an interesting environment and task that is of interest to the community in general. The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). ",10,234,29.25,5.330396475770925,142,2,232,0.0086206896551724,0.0290456431535269,0.9147,68,26,40,14,6,3,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,2,1,2,0,5,3,0,0,0,0,1,0,0,0,0,1,1,0,0,5,0,0,0.4298294074011557,0.3358211630727052,0.2148980915390169
ICLR2018-H1U_af-0--R1,Reject,"The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical-radial rules.  The quadrature rules have smaller variance given the same number of random features, and experiments show its reconstruction error and classification accuracies are better than existing algorithms. It is an interesting paper, but it seems the authors are not aware of some existing works [1, 2] on quadrature for random features. Given these previous works, the contribution and novelty of the paper is limited. [1] Francis Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR, 2017. [2] Tri Dao, Christopher De Sa, Christopher Ru00e9. Gaussian Quadrature for Kernel Features. NIPS 2017",5,114,11.4,5.731481481481482,71,1,113,0.0088495575221238,0.0434782608695652,0.1779,46,15,16,2,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0.3571428571428571,0.2222222222222222,0.15709957235742472
ICLR2018-H1U_af-0--R2,Reject,"The authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems: each feature map evaluates the fourier feature corresponding to the kernel at a set of randomly sampled quadrature points. This gives an unbiased kernel estimator; they prove a bound its variance and provide experiment evidence that for Gaussian and arc-cos kernels, their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression tasks.  The idea is straightforward, the analysis seems correct, and the experiments suggest the method has superior accuracy compared to prior RFMs for shift-invariant kernels. The work is original, but I would say incremental, and the relevant literature is cited. The method seems to give significantly lower kernel approximation errors, but the significance of the performance difference in downstream ML tasks is unclear --- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort.  Since good performance on downstream tasks is the crucial feature that we want RFMs to have, it is not clear that this method represents a true improvement over the state-of-the-art. The exposition of the quadrature method is difficult to follow, and the connection between the quadrature rules and the random feature map is never explicitly stated: e.g. equation 6 says how the kernel function is approximated as an integral, but does not give the feature map that an ML practitioner should use to get that approximate integral. It would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods, which is more important in large-scale ML applications than the kernel approximation error.  It is not clear that the method is *not* more expensive in practice than previous methods (Table 1 gives superior asymptotic runtimes, but I would like to see actual run times, as evaluating the feature maps sound relatively complicated compared to other RFMs). On a related note, I would also like to have seen this method applied to kernels where the probability density in the Bochner integral was not the Gaussian density (e.g., the Laplacian kernel): the authors suggested that their method works there as well when one uses a Gaussian approximation of the density (which is not clear to me) ,  --- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-Gaussian density.",15,409,37.18181818181818,5.3879093198992445,198,3,406,0.0073891625615763,0.0625,0.9718,118,52,70,23,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,1,1,1,0,11,5,0,1,1,0,0,1,0,0,1,0,3,0,0,8,1,0,0.5742752757332529,0.4490324716027252,0.3109528607100913
ICLR2018-H1U_af-0--R3,Reject,"This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature. Pros  1. It is new to apply quadrature rules to improve kernel approximation. The only other work I found is Gaussian Quadrature for Kernel Features NIPS 2017.  The work is pretty recent so the author might not know it when submitting the paper. But in either case, it will be good to discuss the connections. 2. The proposed method is shown to outperform a few baselines empirically. Cons  1. I don't find the theoretical analysis to be very useful. In particular, the theorem shows that the kernel approximation error is O(1/D), which is the same as the original RFF paper. Unless the paper can provide a better characterization of the constants (like the ORF paper), it does not provide much insight in the proposed method. Unlike deep neural networks, since RFF is such a simple model, I think providing precise theoretical understanding is crucial. 2. Approximating an integral is a well-studied topic. I do not find a good discussion on all the possible methods. Why is Genz & Monahan 1998 better than other alternatives such as Monte-Carlo, QMC etc? One argument seems to be ""for kernels with specific specific integrand one can improve on its properties"". But this trick can be used for Monte-Carlo as well. And I do not see benefit of this trick in the curves. 3. When choosing the orthogonal matrix, I think one obvious choice is to sample a matrix from the Stiefel manifold (the Q matrix of a random Gaussian). This baseline should be added in additional to H and B. 4. A wall-time experiment is needed to justify the speedup. Minor comments: ""For kennels with q(w) other than Gaussian... obtain very accurate results with little effort by using Gaussian approximation of q(w)"". What is the citation of this in the kernel approximation context?",16,324,12.0,5.003355704697986,166,4,320,0.0125,0.0273556231003039,0.9397,82,50,63,12,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,1,4,1,0,10,1,0,0,0,0,1,0,0,0,2,0,4,0,0,7,0,0,0.4310646213026425,0.3374616731984792,0.21623462875412386
ICLR2018-H1VGkIxRZ-R1,Accept,"-----UPDATE------  The authors addressed my concerns satisfactorily. Given this and the other reviews I have bumped up my score from a 5 to a 6. ----------------------   This paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs. This is a novel use of existing methods. Some roughly chronological comments follow:  In the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet. The paper is quite well written aside from some grammatical issues. In particular, articles are frequently missing from nouns. Some sentences need rewriting (e.g. in 4.1 which is as well used by Hendrycks..., in 5.2 performance becomes unchanged). It is perhaps slightly unnecessary to give a name to your approach (ODIN) but in a world where there are hundreds of different kinds of GANs you could be forgiven. I'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images. I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications). Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution? When you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again. In terms of experimentation it would be interesting to see the reciprocal of the results between two datasets. For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?  Section 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results. This may just be a matter of taste. n I did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one? (This is in part, addressed in the CIFAR80 20 experiments in the appendices). This appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods). Pros: - Baseline performance is exceeded by a large margin - Novel use of adversarial perturbation and temperature - Interesting analysis Cons: - Doesn't introduce and novel methods of its own - Could do with additional experiments (as mentioned above) - Minor grammatical error",21,425,21.25,5.301507537688442,208,4,421,0.009501187648456,0.0251716247139588,0.8524,105,41,78,26,9,5,"{'ABS': 1, 'INT': 4, 'RWK': 18, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 9, 'RES': 4, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 5, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",1,4,18,0,0,8,9,4,0,2,0,1,1,0,0,4,5,0,3,0,10,0,1,0.6469347301745236,0.5616037796640949,0.4069448056703439
ICLR2018-H1VGkIxRZ-R2,Accept,"The paper proposes a new method for detecting out of distribution samples. The core idea is two fold: when passing a new image through the (already trained) classifier, first preprocess the image by adding a small perturbation to the image pushing it closer to the highest softmax output and second, add a temperature to the softmax. Then, a simple decision is made based on the output of the softmax of the perturbed image - if it is able some threshold then the image is considered in-distribution otherwise out-distribution. This paper is well written, easy to understand and presents a simple and apparently effective method of detecting out of distribution samples. The authors evaluate on cifar-10/100 and several out of distribution datasets and this method outperforms the baseline by significant margins. They also examine the effects of the temperature and step size of the perturbation. My only concern is that the parameter delta (threshold used to determine in/out distribution) is not discussed much. They seem to optimize over this parameter, but this requires access to the out of distribution set prior to the final evaluation. Could the authors comment on how sensitive the method is to this parameter? How much of the out of distribution dataset is used to determine this value, and what are the effects of this size during tuning? What happens if you set the threshold using one out of distribution dataset and then evaluate on a different one? This seems to be the central part missing to this paper and if the authors are able to address it satisfactorily I will increase my score. ",13,266,26.6,5.01937984496124,125,1,265,0.0037735849056603,0.0261194029850746,0.9196,64,23,50,13,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 4, 'DAT': 3, 'MET': 6, 'EXP': 8, 'RES': 0, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,1,5,4,3,6,8,0,0,4,0,2,0,1,0,0,1,0,0,0,6,3,0,0.6454030835928165,0.3366564735241911,0.3238926481265333
ICLR2018-H1VGkIxRZ-R3,Accept,"Detecting out of distribution examples is important since it lets you know when neural network predictions might be garbage. The paper addresses this problem with a method inspired by adversarial training, and shows significant improvement over best known method, previously published in ICLR 2017. Previous method used at the distribution of softmax scores as the measure. Highly peaked -> confidence, spread out -> out of distribution. The authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step. The small step is in the direction of gradient when top class activation is taken as the objective. This is also the gradient used to determine influence of predictors, and it's the gradient term used for adversarial training fast gradient sign method. Their experiments show improvement across the board using DenseNet on collection of small size dataset (tiny imagenet, cifar, lsun). For instance at 95% threshold (detect 95% of out of distribution examples), their error rate goes down from 34.7% for the best known method, to 4.3% which is significant enough to prefer their method to the previous work.",9,184,20.444444444444443,5.3107344632768365,107,1,183,0.0054644808743169,0.010752688172043,0.971,63,21,30,9,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 7, 'DAT': 1, 'MET': 5, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,2,7,1,5,5,0,0,3,0,0,1,0,0,0,3,1,0,0,5,0,0,0.5732598488852958,0.335917236369601,0.2919269648404384
ICLR2018-H1VjBebR--R1,Accept,"The paper addresses the problem of learning mappings between different domains without any supervision. It belongs to the recent family of papers based on GANs . The paper states three conjectures (predictions in the paper): 1. GAN are sufficient to learn u00abu00a0semantic mappingsu00a0u00bb in an unsupervised way, if the considered networks are small enough 2. Controlling the complexity of the network, i.e. the number of the layers, is crucial to come up with what is called u00abu00a0semanticu00a0u00bb mappings when learning in an unsupervised way. More precisely there is tradeoff to achieve between the complexity of the model and its simplicity. A rich model is required in order to minimize the discrepancy between the distributions of the domains, while a  not too complex model is necessary to avoid mappings that are not u00abu00a0meaningfulu00a0u00bb.  To this aim, the authors  introduce a new notion of function complexity which can be seen as a proxy of Kolmogorov complexity. The introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered function. Based on this definition, and assuming identifiability (i.e. uniqueness up to invariants), and for networks with Leaky ReLU activations,  the authors prove that if the number of mappings which preserve a degree of discrepancy (density preserving in the text) is small, then the  set of u00abu00a0minimalu00a0u00bb mappings  of complexity C   that achieve the same degree of  discrepancy is also small.  This result is related to the third conjecture of the paper that is : 3. the number of the number of mappings which preserve a degree of discrepancy  is small. The authors also prove a byproduct result stating that identifiability holds for Leaky ReLU networks with one hidden layer. The paper  comes with a series of experiments to empirically u00abu00a0demonstrateu00a0u00bb the conjectures.  The paper is well written.  The different ideas are clearly stated and discussed, and hence open interesting questions and debates Some of these questions that need to be addressed IMHO:  - A critical general question: if the addressed problem is the alignment between e.g. images and not image generation, why not formalizing the problem as a similarity search one (using e.g. EMD or any other transport metric). The alignment task  hence reduces to computing a ranking from this similarity. I have the impression that we use a jackhammer to break a small brick here (no offence). But maybe that I'm missing something here. - Several works consider the size and the depth of the network as hyper-parameters to optimize, and this is not new. What is the actual contribution of the paper w.r.t. to this body of work? - It is considered that the GAN are trained without any problem, and therefore work in an optimal regime . But the training of the GAN is in itself a problem. How does this affect the paper statements and results? - Are the results still valid for another measure of discrepancy based for instance on another measure, e.g. Wasserstein? Some minor remarks : - p3: the following sentence is not clear  u00abu00a0 Our hypothesis is that the lowest complexity small discrepancy mapping approximates the alignment of the target semantic function. u00a0u00bb - p6: $C^{epsilon_0}_{A,B}$ is used (after Def. 2) before being defined.  - p7: build->built  Section II : A diagram explaining  the different mappings (h_A, h_B, h_AB, etc.) and their spaces (D_A, D_B, D_Z) would greatly help the understanding. Papers 's pros : - clarity - technical results cons: - doubts about the interest and originality   nThe authors provided detailed and convincing answers to my questions. I thank them for that. My scores were changed accrodingly. ",28,589,16.36111111111111,5.229537366548043,275,1,588,0.0017006802721088,0.0126782884310618,0.9591,179,58,98,26,9,6,"{'ABS': 0, 'INT': 4, 'RWK': 7, 'PDI': 6, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 1, 'CLA': 4}",0,4,7,6,0,9,1,3,0,4,0,3,0,2,0,1,0,1,0,1,10,1,4,0.6460003908904752,0.6725076887928156,0.4597254710394739
ICLR2018-H1VjBebR--R2,Accept,"Quality: The paper appears to be correct  Clarity: the paper is clear, although more formalization would help sometimes  Originality The paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know. Significance The points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains. Pros/cons Pros -Adresses an important problem in representation learning ]n-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings -A new cross domain mapping is proposed -Large set of experiments Cons -Some parts deserve more formalization/justification -Too many materials for a conference paper -The cost of the algorithm seems  high   Summary: This paper studies the problem of unsupervised learning of semantic mappings. It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping. A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small. A large set of experiments are provided to support the claims of the paper. Comments:  -The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect .  -In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss . The approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient. n -For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.  Typo: Section 5.1 is build of -> is built of ",13,329,29.90909090909091,5.56957928802589,159,5,324,0.015432098765432,0.0290697674418604,0.9428,106,39,63,8,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 11, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 10, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 8, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,11,3,0,4,1,1,0,10,0,0,0,0,0,0,0,8,0,0,5,0,0,0.5018965295144519,0.2255301898619239,0.2247198250703352
ICLR2018-H1VjBebR--R3,Accept,"This  paper is  on ab important topic : unsupervised learning on unaligned data. The paper shows that is possible to learn the between domains mapping using GAN only without a reconstruction (cyclic) loss . The paper postulates that learning should happen on shallower networks first, then on a deeper network that uses the GAN cost function and regularizing discrepancy between the deeper and the small network.  I did not get the time to go through the proofs, but they handle the fully connected case as far as I understand. Please find my comments are below.  Overall it is an interesting  but long paper, the claims are a bit strong for CNN and need further theoretical and experimental verification. The number of layer as a complexity is not appropriate , as we need to take in account many parameters:  the pooling or the striding for the resolution, the presence or the absence of residual connections (for content preservation), the number of feature maps. More experimentation is needed. Pros:  Important and challenging topic to analyze and any progress on unsupervised learning is interesting. Cons:  I have some questions on the shallow/deep in the context of CNN, and to what extent the cyclic cost is not needed, or it is just distilled from the shallow training:   - Arguably the shallow to deep distillation can be understood as a reconstruction cost , since the shallow network will keep a lot of the spatial information.  If the deep network match the shallow one , this can be understood as a form of ""distilled content "" loss? and the disc of the deep one will take care of the texture , style content? is this intuition correct?   - original cyclic reconstruction constraint is in the pixel space using l1 norm usually, the regularizer introduced matches in a feature space , which is known to produce better results as a ""perceptual loss"", can the author comment on this? is this what is really happening here, moving from cyclic constraint on pixels to a  cyclic constraint in a feature space  (shallow network)?  -  * Spatial resolution*: 1) The analysis seems to be done with respect to DNN not to a  CNN.  did you study the effect of the architectures in terms of striding and pooling how it affects the results? I think just counting number of layers as a complexity is not reasonable when we deal with images, with respect to  what preserves contents and what matches texture or style .   2) - Have you tried resnets generators and discriminators  at various depths , with padding so that the spatial resolution is preserved? - Depth versus width: Another measure that is missing is also the number of feature maps how wide is the network , how does this interplays with the depth? 3) Regularizing deeper networks: in the experiments of varying the length did you see if the results can be stabilized using dropout with deep networks and small feature maps?  4) between training g and h ? how do you initialize h? fully at random ?  5) seems the paper is following implementation by Kim et al. what happens if the discriminator is like in cycle GAN acting on pixels. Pixel GAN rather then only giving a global score for the whole image? ",18,527,35.13333333333333,4.984,236,4,523,0.0076481835564053,0.0157342657342657,0.9914,151,52,99,23,8,3,"{'ABS': 0, 'INT': 3, 'RWK': 3, 'PDI': 5, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 1, 'CLA': 0}",0,3,3,5,0,8,1,3,0,3,0,1,0,0,0,0,0,2,0,0,12,1,0,0.5737899718825374,0.340292027673796,0.2920196774884161
ICLR2018-H1WgVz-AZ-R1,Accept,"  Quality    Overall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well.   Clarity     Overall, the exposition regarding the method is good. I found the setup for the sequence tagging experiments confusing, tough. See more comments below.    Originality / Significance     The paper presents a clever idea that could help make SPENs more practical. The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression.   Major Comment    I'm concerned by the quality of your results and the overall setup of your experiments. In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper.  Most of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network. You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc. The one exception is when you use the tag language model. This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce. In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives. It seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments. It's unclear why SPENs are so important.  The idea of amortizing inference is perhaps more general.  My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions .     Minor Comments     * You should mention 'Energy Based GANs   * I don't understand This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting.  Why would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more?  *  You spend too much space talking about specific hyperparameter ranges, etc. This should be moved to the appendix. You should also add a short summary of the TLM architecture to the main paper body. n * Regarding your footnote discussing using a positive vs. negative sign on the entropy regularization term, I recommend checking out Regularizing neural networks by penalizing confident output distributions.   * You should add citations for the statement In these and related settings, gradient descent has started to be replaced by inference networks.   * I didn't find Table 1 particularly illuminating. All of the approaches seem to perform about the same . What conclusions should I make from it?  * Why not use KL divergence as your Delta function?  * Why are the results in Table 5 on the dev data?  * I was confused by Table 4 . First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network. This idea of training with a standard loss (conditional log lik.) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper. Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy). Why is this?  * I'm confused by the difference between Table 6 and Table 4? Why not just include the TLM results in Table 4?       ",28,620,19.375,5.351443123938879,286,3,617,0.0048622366288492,0.0247093023255813,0.9935,156,88,126,40,9,2,"{'ABS': 0, 'INT': 0, 'RWK': 17, 'PDI': 7, 'DAT': 1, 'MET': 3, 'EXP': 5, 'RES': 4, 'TNF': 5, 'ANA': 12, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 20, 'SUB': 0, 'CLA': 0}",0,0,17,7,1,3,5,4,5,12,0,0,2,0,0,0,0,4,0,0,20,0,0,0.6458905195733964,0.2343909011558086,0.29304073126326785
ICLR2018-H1WgVz-AZ-R2,Accept,"The paper proposes training ``inference networks,'' which are neural network structured predictors.  The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network.  The idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization.  However, it remains unclear why SPENs are the right choice for an energy function.  Experiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent.  However, the experimental results are not clearly presented.  The clarity is poor enough that the paper might not be ready for publication.  Comments and questions:  1) It is unclear whether this paper is motivated by training SPENs or by training structured predictors.  The setup focuses on using SPENs as an inference network, but this seems inessential. Experiments with simpler energy functions seem to be absent, though the experiments are unclear (see below). 2) The confusion over the motivation is confounded by the fact that the experiments are very unclear. Sometimes predictions are described as the output of SPENs (Tables 2, 3, 4, and 7), sometimes as inference networks (Table 5), and sometimes as a CRF (Tables 4 and 6).  In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging, but Tables 4 and 6 indicate both CRFs and BiLSTMS? It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator). 3) The third and fourth columns of Table 5 are identical. The presentation should be made consistent, either with dev/test or -retuning/+retuning as the top level headers. n 4) It is also unclear how to compare Tables 4 and 5. The second to bottom row of Table 5 seems to correspond with the first row of Table 5, but other methods like slack rescaling have higher performance. What is the takeaway from these two tables supposed to be?  5) Part of the motivation for the work is said to be the increasing interest in inference networks:  In these and related settings, gradient descent has started to be replaced by inference networks. Our results below provide more evidence for making this transition. However, no other work on inference networks is directly cited.",21,407,19.38095238095238,5.2650918635170605,183,6,401,0.0149625935162094,0.035799522673031,0.7927,111,44,74,18,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 2, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 2}",0,2,1,3,0,7,3,2,3,0,0,3,1,0,0,0,0,0,1,0,13,1,2,0.6448679412065189,0.4519890687334142,0.3596873203943757
ICLR2018-H1WgVz-AZ-R3,Accept,"This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs. SPENs are an energy-based structured prediction method, where the final prediction is obtained by optimizing min_y E_theta(f_phi(x), y), i.e., finding the label set y with the least energy, as computed by the energy function E(), using a set of computed features f_phi(x) which comes from a neural network. The key innovation in SPENs was representing the energy function E() as an arbitrary neural network which takes the features f(x) and candidate labels y and outputs a value for the energy. At inference time y can be optimized by gradient descent steps. SPENs are trained using maximum-margin loss functions, so the final optimization problem is max -loss(y, y') where y'   argmin_y E(f(x), y). The key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization.  The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem, where the y-predicting network learns to predict labels with low energy (according to the E-computing network) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels (i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator).  The paper explores multiple loss functions and techniques to train these models. They seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based models. The improved understanding of SPENs and potential for further work justify accepting this paper.",9,317,28.818181818181817,5.503378378378378,150,0,317,0.0,0.0093457943925233,0.9424,110,47,48,7,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 5, 'DAT': 0, 'MET': 6, 'EXP': 8, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,3,5,0,6,8,1,0,0,0,1,0,0,0,0,1,1,0,1,5,0,0,0.4308239349345036,0.4469322741838163,0.24549730824667249
ICLR2018-H1Ww66x0--R1,Reject,"CONTRIBUTION The main contribution of the paper is not clearly stated. To the reviewer, It seems ""life-long learning"" is the same as ""online learning"". However, the whole paper does not define what ""life-long learning"" is. The limited budget scheme is well established in the literature. 1. J. Hu, H. Yang, I. King, M. R. Lyu, and A. M.-C. So. Kernelized online imbalanced learning with fixed budgets. In AAAI, Austin Texas, USA, Jan. 25-30 2015. u2028 2. Y. Engel, S. Mannor, and R. Meir. The kernel recursive least-squares algorithm. IEEE Transactions on Signal Processing, 52(8):2275u20132285, 2004. It is not clear what the new proposal in the paper. WRITING QUALITY The paper is not well written in a good shape. Many meanings of the equations are not stated clearly, e.g., $phi$ in eq. (7). Furthermore, the equation in algorithm 2 is not well formatted. DETAILED COMMENTS 1. The mapping function $phi$ appears in Eq. (1) without definition. 2. The last equation in pp. 3 defines the decision function f by an inner product. In the equation, the notation x_t and i_t is not clearly defined. More seriously, a comma is missed in the definition of the inner product. 3. Some equations are labeled but never referenced, e.g., Eq. (4). 4. The physical meaning of Eq.(7) is unclear. However, this equation is the key proposal of the paper.  For example, what is the output of the Eq. (7)? What is the main objective of Eq. (7)? Moreover, what support vectors should be removed by optimizing Eq. (7)? One main issue is that the notation $phi$ is not clearly defined. The computation of f-y_rphi(s_r) makes it hard to understand. Especially,  the dimension of $phi$ in Eq.(7) is unknown. ABOUT EXPERIMENTS 1.tIt is unclear how to tune the hyperparameters. 2.tIn Table 1, the results only report the standard deviation of AUC. No standard deviations of nSV and Time are reported. ",27,315,6.057692307692308,4.809187279151944,150,2,313,0.0063897763578274,0.0440251572327044,-0.8232,92,42,48,25,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 15, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 14}",0,0,0,2,0,15,1,2,0,0,0,1,3,0,0,1,0,0,2,0,5,2,14,0.4322139059286535,0.559266386648155,0.2661770922831753
ICLR2018-H1Ww66x0--R2,Reject,"Summary: The paper proposed a two-dimensional approach to lifelong learning, in the context of multi-task learning. It receives instances in an online setting, where both the prediction model and the relationship between the tasks are learnt using a online kernel based approach. It also proposed to use budgeting techniques to overcome computational costs. In general, the paper is poorly written, with many notation mistakes and inconsistencies. The idea does not seem to be novel, technical novelty is low, and the execution in experiments does not seem to be reliable. Quality: No obvious mistakes in the proposed method, but has very low novelty (as most methods follows existing studies in especially for online kernel learning). Many mistakes in the presentation and experiments. Originality: The ideas do not seem to be novel, and are mostly (trivially) using existing work as different components of the proposed technique. Clarity: The paper makes many mistakes, and is difficult to read. [N] is elsewhere denoted as mathbb{N}. The main equation of Algorithm 2 merges into Algorithm 3. Many claims are made without justification (e.g. 2.2. ""Cavallanti 2012 is not suitable for lifelong learning""... why?; ""simple removal scheme ... highest confidence"" u2013 what is the meaning of highest confidence?), etc. The removal strategy is not at all well explained u2013 the objective function details and solving it are not discussed. Significance: There is no theoretical guarantee on the performance, despite the author's claiming this as a goal in the introduction itself (""goal of lifelong learner ... computation""). The experiments are not reliable. Perceptron obtains a better performance than PA algorithms u2013 which is very odd. Moreover, many of the multi-task baselines obtain a worse performance than a simple perceptron (which does not account for multi-task relationships).  ",16,287,12.478260869565217,5.487179487179487,152,0,287,0.0,0.0274914089347079,-0.7037,79,44,54,19,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 2}",0,1,1,4,0,7,3,2,0,2,0,3,0,1,0,3,0,1,3,0,3,2,2,0.6448798455982623,0.668311746132171,0.4502866368424953
ICLR2018-H1Ww66x0--R3,Reject,"The paper proposes a budgeted online kernel algorithm for multi-task learning. The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks. The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine. The proposed algorithm is tested on 3 data sets and compared with several baselines. Positives: - the output kernel update is well justified - experimental results are encouraging Negatives: - the methodological contribution of the paper is minimal - the proposed approach to maintain the budget is simplistic - no theoretical analysis of the proposed algorithm is provided - there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)",10,215,43.0,5.248803827751196,118,2,213,0.0093896713615023,0.045045045045045,0.84,65,21,42,11,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 2, 'MET': 6, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,0,2,3,2,6,2,1,0,1,0,0,0,0,0,0,0,1,0,0,5,2,0,0.501572334726621,0.3359278395810267,0.2468129169241714
ICLR2018-H1Xw62kRZ-R1,Accept,"The authors consider the task of program synthesis in the Karel DSL. Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output, incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs. Finally they learn a model that predicts correctness of syntax in absence of a syntax checker. While the results in this paper look good, I found many aspects of the exposition difficult to follow. In section 4, the authors define objectives, but do not clearly describe how these objectives are optimized, instead relying on the read to infer from context how REINFORCE and beam search are applied. I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4, or by way of the conditioning introduced in section 5.1. Discussion of the experimental results coould similarly be clearer. The best method very clearly depends on the taks and the amount of available data, but I found it difficult to extract an intuition for which method works best in which setting and why. On the whole this seems like a promising paper. That said, I think the authors would need to convincingly address issues of clarity in order for this to appear. Specific comments   - Figure 2 is too small - Equation 8 is confusing in that it defines a Monte Carlo estimate of the expected reward, rather than an estimator of the gradient of the expected reward (which is what REINFORCE is). - It is not clear the how beam search is carried out. In equation (10) there appear to be two problems. The first is that the index i appears twice (once in i 1..N and once in i in 1..C), the second is that u03bb_r refers to an index that does not appear. More generally, beam search is normally an algorithm where at each search depth, the set of candidate paths is pruned according to some heuristic. What is the heuristic here? Is syntax checking used at each step of token generation, or something along these lines? - What is the value of the learned syntax in section 5.2? Presumaly we need a large corpus of syntax-checked training examples to learn this model, which means that, in practice, we still need to have a syntax-checker available, do we not?",20,387,24.1875,4.912087912087912,196,3,384,0.0078125,0.0279898218829516,0.9929,99,40,71,25,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 10, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 3}",0,0,0,3,1,10,4,2,1,1,0,3,0,0,0,0,0,0,1,0,11,1,3,0.5742040270602816,0.4508262889345823,0.32240910387348964
ICLR2018-H1Xw62kRZ-R2,Accept,"The paper presents a reinforcement learning-based approach for program synthesis. The proposed approach claims two advantages over a baseline maximum likelihood estimation-based approach. MLE-based methods penalize syntactically different but semantically equivalent programs. Further, typical program synthesis approaches don't explicitly learn to produce correct syntax. The proposed approach uses a syntax-checker to limit the next-token distribution to syntactically-valid tokens. The approach, and its constituent contributions, i.e. of using RL for program synthesis, and limiting to syntactically valid programs, are novel. Although both the contributions are fairly obvious, there is of course merit in empirically validating these ideas. The paper presents comparisons with baseline methods. The improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results. However, there is no comparison with other approaches in the literature. The authors claim to improve the state-of-the-art, but fail to mention and compare with the state-of-the-art, such as [1]. I do find it hard to trust papers which do not compare with results from other papers. Pros: 1. Well-written paper, with clear contributions. 2. Good empirical evaluation with ablations. Cons: 1. No SOTA comparison. 2. Only one task / No real-world task, such as Excel Flashfill. [1]: Neural Program Meta-Induction, Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli",17,213,9.681818181818182,6.113300492610837,130,1,212,0.0047169811320754,0.0046728971962616,0.97,72,33,34,11,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,0,6,2,0,7,1,1,0,1,0,1,1,1,0,1,0,4,0,0,4,1,1,0.6448200349136266,0.5577729155316544,0.41243197395920517
ICLR2018-H1Xw62kRZ-R3,Accept,"This is a nice paper. It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space. In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training. I have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted. Specific comments/questions follow:   Figure 2 is too small. It would be much more helpful (and easier to read) if it were enlarged to take the full page width. Page 7: In the supervised setting... This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside. If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7). Is the Karel DSL in your experiments the full Karel language, or a subset designed for the paper? For the versions of the model that use beam search, what beam width was used? Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how?  ",11,266,24.181818181818183,5.241935483870968,155,2,264,0.0075757575757575,0.0222222222222222,0.9836,72,31,48,12,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 5, 'RES': 3, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,0,1,0,2,5,3,3,0,0,2,0,0,0,1,0,0,2,1,7,0,0,0.4293698223669307,0.4482377579773056,0.24185397625369148
ICLR2018-H1Y8hhg0b-R1,Accept,"This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization. The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations.  Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components. The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods. Pros:   - The paper is clearly written, self-contained and a pleasure to read. - Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization Cons:   - It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices  - It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc. - If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant ",9,228,32.57142857142857,5.945945945945946,140,1,227,0.0044052863436123,0.0208333333333333,0.9834,70,31,39,3,6,7,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 8, 'DAT': 0, 'MET': 2, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 1}",0,2,0,8,0,2,4,0,0,2,0,0,0,1,1,1,1,2,0,0,8,2,1,0.4295455454032613,0.78235531888719,0.3384071526577353
ICLR2018-H1Y8hhg0b-R2,Accept,"Learning sparse neural networks through L0 regularisation n Summary:   The authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty. The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks . Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal. The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets  but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations .   Pros:  The paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory.  Optimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters. The work is put in context and related to some previous relaxation approaches to sparsity. The method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training.  Cons:  The method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods . Therefore the main advance is in terms of learning speed to obtain this similar performance. However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout . It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered,e.g. does the proposed sparse learning method converge at the same rate as the others? I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect.  It was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time. It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning. Minor point: it wasn't completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix). I don't see why the spike-and-slab is any more fundamental than the L0 norm prior in (2), it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter.  In the context here this didn't seem a particularly relevant addition to the paper. ",21,507,26.68421052631579,5.7375,247,1,506,0.0019762845849802,0.0381679389312977,0.9901,126,82,90,21,9,7,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 6, 'DAT': 1, 'MET': 16, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 6, 'PNF': 1, 'REC': 1, 'EMP': 14, 'SUB': 0, 'CLA': 2}",0,2,4,6,1,16,3,2,0,2,1,0,0,0,0,2,4,6,1,1,14,0,2,0.647426319366402,0.7867193898174022,0.5034862714248657
ICLR2018-H1Y8hhg0b-R3,Accept,"The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network. The basic problem is empirical risk minimization with a incremental penalty for each non zero weight . To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables. The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights.  The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training. The results presented in the paper are convincing . They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time. They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models. The hard concrete distribution is a small but nice contribution on its own. My only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832). Although the focus is apparently different, these methods are clearly closely related . A discussion of this relationship seems really important. n Specific comments/questions: - The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this. For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation.  This would give a stronger sense of the kind of wins that are possible in this framework . - Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations. Extra evaluations of different relaxations would be appreciated. At the very least a comparison to concrete would be nice. - In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0",19,339,17.842105263157894,5.43125,168,1,338,0.0029585798816568,0.0172413793103448,0.9924,101,48,49,11,8,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 8, 'DAT': 0, 'MET': 18, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 17, 'SUB': 2, 'CLA': 1}",0,2,2,8,0,18,6,1,0,1,1,0,0,0,0,0,4,4,0,0,17,2,1,0.5766339221861438,0.5661091486382781,0.36726196121486887
ICLR2018-H1Yp-j1Cb-R1,Accept,"It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing . Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game).  The present paper proposes to obtain mixed strategy through an online learning approach. Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss. One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M) .In particular, the concave player plays the FTRL algorithm with standard L2 regularization term. The regret of concave player can be bounded using existing result for FTRL. The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically.  Then a lemma by Kalai and Vempala can be used. The theory part of the paper is reasonable and quite well written. Based on the theory developed, the paper presents a practical algorithm. Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration. The paper claims that this may help to prevent model collapsing. However, the experimental part is less satisfying. From figure 2, I don't see much advantage of Checkhov GAN. In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA). The paper didn't really compare other popular GAN models, especially WGAN and its improved version , which is already quite popular by now and should be compared with. Overall, I think it is a borderline paper. ------------------------- I read the response and the new experimental results regarding WGAN. The experimental results make more sense now. It would be interesting to see whether the idea can be applied to more recent GAN models and still perform better . I raised my score to 7.",23,334,15.181818181818182,5.149532710280374,177,3,331,0.0090634441087613,0.0176470588235294,0.9087,92,47,69,26,10,7,"{'ABS': 0, 'INT': 2, 'RWK': 19, 'PDI': 3, 'DAT': 0, 'MET': 12, 'EXP': 12, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 6, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 17, 'SUB': 3, 'CLA': 1}",0,2,19,3,0,12,12,3,1,1,0,1,0,1,1,0,6,3,0,1,17,3,1,0.7196902859274987,0.7884169581085277,0.5722585274375912
ICLR2018-H1Yp-j1Cb-R2,Accept,"This is an interesting paper, exploring GAN dynamics using ideas from online learning, in particular the pioneering sparring follow-the-regularized leader analysis of Freund and Schapire (using what is listed here as Lemma 4). By restricting the discriminator to be a single layer, the maximum player plays over a concave (parameter) space which stabilizes the full sequence of losses so that Lemma 3 can be proved, allowing proof of the dynamics' convergence to a Nash equilibrium. The analysis suggests a practical (heuristic) algorithm incorporating two features which emerge from the theory: L2 regularization and keeping a history of past models. A very simple queue for the latter is shown to do quite competitively in practice. This paper merits acceptance on theoretical merits alone, because the FTRL analysis for convex-concave games is a very robust tool from theory (see also the more recent sequel [Syrgkanis et al. 2016 Fast convergence of regularized learning in games]) that is natural to employ to gain insight on the much more brittle GAN case. The practical aspects are also interesting, because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains; these ideas could clearly be developed in future work.",6,205,29.285714285714285,5.561855670103093,131,1,204,0.0049019607843137,0.0097560975609756,0.9776,59,33,34,13,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,0,5,1,0,2,3,0,1,2,1,3,0,0,0,0,1,0,1,0,4,0,1,0.5722886637320268,0.4463103167489733,0.325771996731434
ICLR2018-H1Yp-j1Cb-R3,Accept,"The paper applies tools from online learning to GANs.  In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no-regret algorithms. The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game. Overall the paper is very well written. The theory is significant to the GAN literature, probably less so to the online learning community. In practice, with deep D, trained by single gradient update steps for G and D, instead of the argmin in Algo 1., the assumptions of the theory break. This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true.  However, this is where I have issues with the work:  1) In all quantitative results, Chekhov GAN do not significantly beat unrolled GAN. Unrolled GAN looks at historical D's through unrolled optimization, but not the history of G. So this lack of significant difference in results raise the question of whether any improvement of Chekhov GAN is coming from the online learning perspective for D and G, or simply due to the fact that it considers historical D models (which could be motivated by sth other than the online learning theory). 2) The mixture GAN approach suggested in Arora et al. (2017) is very related to this work, as acknowledged in Sec. 2.1, but no in-depth analysis is carried out.  I suggest the authors to either discuss why Chekhov GAN is obviously superior and hence no experiments are needed, or compare them experimentally. 3) In the current state, it is hard to place the quantitative results in context with other common methods in the recent literature such as WGAN with gradient penalty. I suggest the authors to either report some results in terms of inception scores on cifar10 with similar architectures used in other methods for comparison. Alternatively please show WGAN-GP and/or other method results in at least one or two experiments using the evaluation methods in the paper. In summary, almost all the experiments in the paper are trying to establish improvement over basic GAN, which would be OK if the gap between theory and practice is small. But in this case, it is not. So it is not entirely convincing that the practical Algo 2 works better for the reason suggested by the theory, nor it drastically improves practical results that it could become the standard technique in the literature.",18,439,21.95,4.990453460620525,212,1,438,0.002283105022831,0.0361173814898419,0.9392,126,57,55,27,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 15, 'PDI': 4, 'DAT': 0, 'MET': 9, 'EXP': 7, 'RES': 7, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 7, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 2}",0,0,15,4,0,9,7,7,0,0,0,2,0,0,0,1,7,1,0,0,8,2,2,0.4327745006152083,0.6714964001804303,0.30054449931040145
ICLR2018-H1YynweCb-R1,Reject,"The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices. Given N inputs and output, this method allows one to specify a linear transformation with O(log(N)) parameters, and perform a forward and backward pass in O(Nlog(N)) time. In addition a relaxation is performed allowing each constituent to deviate a bit from unitarity (""soft unitary constraint""). The paper shows nice results on a number of small tasks. The idea is original to the best of my knowledge and is presented clearly. I especially like the idea of ""soft unitary constraint"" which can be applied very efficiently in this factorized setup. I think this is the main contribution of this work. However the paper in its current form has a number of problems:  - The authors state that a major shortcoming of previous (efficient) unitary RNN methods is the lack of ability to span the entire space of unitary matrices. This method presents a family that can span the entire space, but the efficient parts of this family (which give the promised speedup) only span a tiny fraction of it, as they require only O(log(N)) params to specify an O(N^2) unitary matrix. Indeed in the experimental section only those members are tested. - Another claim that is made is that complex numbers are key, and again the argument is the need to span the entire space of unitary matrices, but the same comment still hold - that is not the space this work is really dealing with, and no experimental evidence is provided that using complex numbers was really needed. - In the experimental section an emphasis is made as to how small the number of recurrent params are, but at the same time the input/output projections are very large, leaving the reader wondering if the workload simply shifted from the RNN to the projections. This needs to be addressed. - Another aspect of the previous points is that it's not clear if stacking KRU layers will work well. This is important as stacking LSTMs is a common practice. Efficient KRU span a restricted subspace whose elements might not compose into structures that are expressive enough. One way to overcome this potential problem is to add projection matrices between layers that will do some mixing, but this will blow the number of parameters. This needs to be explored. - The authors claim that the soft unitary constraint was key for the success of the network, yet no details are provided as to how this constraint was applied, and no analysis was made for its significance.  ",19,426,21.3,4.907317073170732,195,2,424,0.0047169811320754,0.0183908045977011,0.9345,110,55,82,22,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 9, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 5, 'CLA': 1}",0,0,1,4,0,9,6,1,0,3,0,0,0,0,0,1,1,0,0,0,5,5,1,0.4312312827870491,0.5584700913282133,0.2722508412778063
ICLR2018-H1YynweCb-R2,Reject,"Summary of the paper -------------------------------  This paper proposes to factorize the hidden-to-hidden matrix of RNNs into a Kronecker product of small matrices, thus reducing the number of parameters, without reducing the size of the hidden vector. They also propose to use a soft unitary constraint on those small matrices (which is equivalent to a soft unitary constraint on the Kronecker product of those matrices), that is fast to compute. They evaluate their model on 6 small scale RNN experiments. Clarity, Significance and Correctness --------------------------------------------------  Clarity: The main idea is clearly motivated and presented, but the experiment section failed to convince me (see details below). Significance: The idea of using factorization for RNNs is not particularly novel. However, it is really nice to be able to decouple the hidden size and the number of recurrent parameters in a simple way. Also, the combination of Kronecker product and soft unitary constraint is really interesting. Correctness: There are minor flaws. Some of the baselines seems to perform poorly, and some comparisons with the baselines seems unfair (see the questions below). Questions --------------  1. Section 3: You say that you can vary 'pf' and 'qf' to set the trade-off between computational budget and performances. Have you run some experiments where you vary those parameters? 2. Section 4: Are you using the soft unitary constraint in your experiments? Do you have an hyper-parameter that sets the amplitude of the constraint? If yes, what is its value? Are you using it also on the vanilla RNN or the LSTM? 3. Section 4.1: You say that you don't train the recurrent matrix in the KRU version. Do you also not train the recurrent matrix in the other models (RNN, LSTM,...)? If yes, how do you explain the differences? If no, I don't see how those curves compare. 4. Section 4.3: Why does your LSTM in pMNIST performs so poorly? There are way better curves reported in the literature (eg in Unitary Evolution Recurrent Neural Netwkrs or Recurrent Batch Normalization). 5. General: How does your method compares with other factorization approaches, such as in Factorization Tricks for LSTM Networks? 6. Section 4: How does the KRU compares to the other parametrizations, in term of wall-clock time? Remarks ------------  The main claim of the paper is that RNN are over-parametrized and take a long time to train (which I both agree with), but you didn't convinced me that your parametrization solve any of those problems. I would suggest to: 1. Compare more clearly setups where you fix the hidden size. 2. Compare more clearly setups where you fix the number of parameters. With systematic comparisons like that, it would be easier to understand where the gains in performances are coming from. 3. Add an experiment where you vary 'pf' and 'qf' (and keep the hidden size fixed) to show how the optimization/generalization performances can be tweaked. 4. Add computation time (wall-clock) for all the experiments, to see how it compares in practice (this could definitively weight in your favor, since you seems to have a nice CUDA implementation). 5. Present results on larger-scale applications (Text8, Teaching Machines to Read and Comprehend, 3 layers LSTM speech recognition setup on TIMIT, DRAW, Machine Translation, ...), especially because your method is really easy to plug in any existing code available online. Typos / Form ------------------  1. sct 1, par 3: using Householder reflection vectors, it allows a fine-grained -> using Householder reflection vectors, which allows a fine-grained 2. sct 1, par 3: This work called as Efficient -> This work, called Efficient 5. sct 1, par 5: At the heart of KRU is the use of Kronecker -> At the heart of KRU, we use Kronecker 6. sct 1, par 5: Thanks to the properties of Kronecker matrices -> Thanks to the properties of the Kronecker product 7. sct 1, par 5: vanilla real space RNN -> vanilla RNN 8. sct 2, par 1: Consider a standard recurrent -> Consider a standard vanilla recurrent 9. sct 2, par 1: step t RNN -> step t, a vanilla RNN 11. sct 2.1, par 1: U and V, this is efficient using modern BLAS -> U and V, which can be efficiently computed using modern BLAS 12. sct 2.3, par 2: matrices have a determinant of 1 or u22121, i.e., the set of all rotations and reflections respectively -> matrices, i.e., the set of all rotations and reflections, have a determinant of 1 or u22121. 13. sct 3, par 1: are called as Kronecker -> are called Kronecker 14. sct 3, par 3: used it's spectral -> used their spectral 15. sct 3, par 3: Kronecker matrices -> Kronecker products  18. sct 4.4, par 3: parameters are increased -> parameters increases 19. sct 5: There is some more typos in the conclusion (it's -> its) 20. Some plots are hard to read / interpret, mostly because of the round ticks you use on the curves. I suggest you remove them everywhere. Also, in the adding problem, it would be cleaner if you down-sampled a bit the curves (as they are super noisy). In pixel by pixel MNIST, some of the legends might have some typos (FC uRNN), and you should use N instead of   to be consistent with the notation of the paper. 21. Appendix A to E are not necessary, since they are from the literature. 22. sct 3.1, par 2: is approximately unitary. -> is approximately unitary (cf Appendix F). 23. sct 4, par 1: and backward operations. -> and backward operations (cf Appendix G and H). Pros ------  1. Nice Idea that allows to decouple the hidden size with the number of hidden-to-hidden parameters. 2. Cheap soft unitary constraint 3. Efficient CUDA implementation (not experimentally verified) Cons -------  1. Some experimental setups are unfair, and some other could be clearer 2. Only small scale experiments (although this factorization has huge potential on larger scale experiments) 3. No wall-clock time that show the speed of the proposed parametrization.",57,970,14.477611940298509,5.083333333333333,315,4,966,0.0041407867494824,0.0109343936381709,0.9985,292,97,154,44,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 9, 'DAT': 1, 'MET': 10, 'EXP': 20, 'RES': 1, 'TNF': 2, 'ANA': 4, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 8, 'PNF': 2, 'REC': 0, 'EMP': 16, 'SUB': 6, 'CLA': 19}",0,0,2,9,1,10,20,1,2,4,0,2,0,0,0,1,0,8,2,0,16,6,19,0.6474443536315058,0.6788715488304248,0.4645039234072394
ICLR2018-H1YynweCb-R3,Reject,"Typical recurrent neural networks suffer from over-paramterization. Additionally, standard RNNs (non-gated versions) have an ill-conditioned recurrent weight matrix, leading to vanishing/exploding gradients during training. This paper suggests to factorize the recurrent weight matrix as a Kronecker product of matrices. Additionally, in order to avoid vanishing/exploding gradients in standard RNNs, a soft unitary constraint is used. The regularizer is specifically nice in this setting, as it suffices to have the Kronecker factors be unitary. In the empirical section, several RNNs are trained using this approach, using only ~ 100 recurrent parameters, and still achieve comparable results to state-of-the-art approaches. The paper argues that the recurrent state should be high-dimensional (in order to be able to encode the input and extract predictive information) but the recurrent dynamic should be realized by a low-capacity model. Quality: The paper is well written. Clarity: Main ideas are clearly presented. Originality/Significance: Kronecker factorization was introduced for Convolutional networks (citation is in the paper). Soft unitary constraints also have been introduced in earlier work (citations are also in the paper). Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution. Additionally, this approach allows a significant reduction of training time it seems.  ",13,211,14.066666666666666,6.058536585365854,119,2,209,0.0095693779904306,0.02803738317757,0.946,66,29,43,12,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 4, 'DAT': 0, 'MET': 3, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 2}",0,0,4,4,0,3,3,1,0,0,0,1,0,1,0,2,2,2,1,0,2,0,2,0.5010921391037059,0.6675800659625196,0.34944028103042785
ICLR2018-H1a37GWCZ-R1,Reject,"This paper presents simple but useful ideas for improving sentence embedding by drawing from more context.  The authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence; they posit that one can obtain more information about a sentence from other governing sentences in the document such as the title of the document, sentences based on HTML, sentences from table of contents, etc. The way I understand it, previous sentence like in SkipThought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context. Here are the pros of this paper: 1) Useful contribution in terms of using broader context for embedding a sentence. 2) Novel and simple trick for generating OOV words by mapping them to local variables and generating those variables. 3) Outperforms SkipThought in evals .  Cons: 1) Coreference eval: No details are provided for how the data was annotated for the coreference task. This is crucial to understanding the reliability of the evaluation as this is a new domain for coreference. Also, the authors should make this dataset available for replicability. Also, why have the authors not used this embedding for eval on standard coreference datasets like OntoNotes. Please clarify. 2) It is not clear to me how the model learns to generate specific OOV variables. Can the authors clarify how does the decoder learns to generate these words. Clarifications: 1) In section 6.1, what is the performance of skip-thought with the same OOV trick as this paper? 2) What is the exact heuristic in Text Styles in section 3.1? Should be stated for replicability.",14,272,19.428571428571427,5.2684824902723735,134,0,272,0.0,0.0218181818181818,0.9759,75,32,45,7,4,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 8, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,0,0,1,3,8,4,0,0,0,0,0,0,0,0,1,0,0,0,1,10,0,1,0.2877963993656913,0.4500420613580311,0.1624673669809506
ICLR2018-H1a37GWCZ-R2,Reject,"1) This paper proposes a method for learning the sentence representations with sentences dependencies information. It is more like a dependency-based version skip-thought on the sentence level. The idea is interesting to me, but I think this paper still needs some improvements. The introduction and related work part are clear with strong motivations to me. But section 4 and 6 need a lot of details.  2) My comments are as follows: i) this paper claims that this is a general sentence embedding method, however, from what has been described in section 3, I think this dependency is only defined in HTML format document. What if I only have pure text document without these HTML structure information? So I suggest the authors do not claim that this method is a general-purpose sentence embedding model. ii) The authors do not have any descriptions for Figure 3.  Equation 1 is also very confusing. iii) The experiments are insufficient in terms of details. How is the loss calculated? How is the detection accuracy calculated?",10,170,15.454545454545457,5.3202614379084965,99,2,168,0.0119047619047619,0.0290697674418604,0.8921,49,12,36,12,7,1,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,2,1,1,0,3,2,0,1,0,0,1,0,0,0,0,0,0,0,0,4,0,0,0.5006120989485935,0.11297698341564,0.20028448153788292
ICLR2018-H1a37GWCZ-R3,Reject,"This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence. Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related sentences could be encoded, including document title, section title, footnotes, hyperlinked sentences.This is a valid good idea and indeed improves results. The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful. The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution. On the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text.  There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable. The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference. And the coreference experiment isn't that clearly described nor necessarily that meaningful. Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting. Other comments:  - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a topic sentence at the beginning. - p.5. Notation seemed a bit non-standard. I thought most people use sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid. ...  - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common. - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them. A weakness of this section however is that it makes no reference to related work whatsoever. It seems like there's quite a bit of related work. The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015). More generally, a now quite common way to handle this problem is to use pointing or copying, which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too. - p.7. Why such an old Wikipedia dump? Most people use a more recent one! n - p.7. The paraphrase results seem good and prove the idea works. It's a shame they don't let you see the usefulness of the OOV model. - p.8. For various reasons, the coreference results seem less useful than they could have been, but they do show some value for the technique in the area of domain-specific coreference.  ",20,567,14.538461538461538,5.0319548872180455,255,12,555,0.0216216216216216,0.0379310344827586,0.926,141,81,103,43,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 5, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,6,5,0,7,1,3,0,1,0,0,3,0,0,0,1,1,0,0,7,0,1,0.5737055528657159,0.4481761890535022,0.3180986151284037
ICLR2018-H1aIuk-RW-R1,Accept,"After reading rebuttals from the authors: The authors have addressed all of my concerns. THe additional experiments are a good addition. ************************ The authors provide an algorithm-agnostic active learning algorithm for multi-class classification. The core technique is to construct a coreset of points whose labels inform the labels of other points. The coreset construction requires one to construct a set of  points which can cover the entire dataset.  While this is NP-hard problem in general, the greedy algorithm is 2-approximate . The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried. The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space. The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification. The experimental results are convincing enough to show that it outperforms other active learning algorithms. However, I have a few major and minor comments.  Major comments:  1. The proof of Lemma 1 is incomplete. We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label. The proof of lemma 1 only establishes the Lipschitz constant of the CNN function. Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function. 2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset   0. But the equation in proposition 1 also includes the loss on the coreset. Why is this term included. Is this term not equal to 0? n 3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning . UPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf Efficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf A bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf  4. The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function. I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper. For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training. Minor-comment:  1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP.  It would have been great had the authors mentioned that u_j in {0,1}.  2. The authors write on page 4, Moreover, zero training error can be enforced by converting average loss into maximal loss. It is not clear to me what the authors mean here. For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that? Why would that result in zero training error? On the whole this is interesting work and the results are very nice . But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified. Also, important references in active learning literature are missing.",29,597,13.568181818181818,5.146902654867256,244,3,594,0.005050505050505,0.011400651465798,0.8997,200,68,99,16,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 23, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 2, 'CLA': 2}",0,0,0,0,2,23,2,2,0,0,0,1,2,1,0,0,1,1,0,0,12,2,2,0.5056946079324115,0.5625848989180038,0.32282719914524965
ICLR2018-H1aIuk-RW-R2,Accept,"Active learning for deep learning is an interesting topic and there is few useful tool available in the literature. It is happy to see such paper in the field. This paper proposes a batch mode active learning algorithm for CNN as a core-set problem. The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set.  By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT. The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting. Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin. Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop. The proposed algorithm is new and writing is clear. However, the paper is not flawless. The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning. To validate such theoretical result, a non-deep-learning model should be adopted. The ERM for active learning has been investigated in the literature, such as Querying discriminative and representative samples for batch mode active learning in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper. Another interesting question is most of the competing algorithm is myoptic active learning algorithms. The comparison is not fair enough. The authors should provide more competing algorithms in batch mode active learning.",14,270,16.875,5.312977099236641,133,1,269,0.0037174721189591,0.018450184501845,0.9037,88,44,42,7,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 2}",0,1,2,0,2,7,2,1,0,0,0,2,0,0,1,0,0,2,0,0,4,0,2,0.5017493459247447,0.4465086143770174,0.2832016290116544
ICLR2018-H1aIuk-RW-R3,Accept,"This paper studies active learning for convolutional neural networks . Authors formulate the active learning problem as core-set selection and present a novel strategy. Experiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines. Theoretical analysis is presented to show the performance of any selected subset using the geometry of the data points. Authors are suggested to perform experiments on more datasets to make the results more convincing. The initialization of the CNN model is not clearly introduced, which however, may affect the performance significantly.",6,92,15.333333333333334,5.857142857142857,66,1,91,0.0109890109890109,0.053763440860215,0.8039,27,9,20,5,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 3, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,1,0,3,3,2,0,0,0,0,0,0,0,0,1,0,0,0,0,3,1,0,0.3578016186419221,0.3345772482030192,0.17430253604068832
ICLR2018-H1bM1fZCW-R1,Reject,"The paper proposes a method to train deep multi-task networks using gradient normalization. The key idea is to enforce the gradients from multi tasks balanced so that no tasks are ignored in the training. The authors also demonstrated that the technique can improve test errors over single task learning and uncertainty weighting on a large real-world dataset. It is an interesting paper with a novel approach to multi-task learning . To improve the paper, it would be helpful to evaluate the method under various settings. My detailed comments are below.  1. Multi-task learning can have various settings. For example, we may have multiple groups of tasks, where tasks are correlated within groups but tasks in different groups are not much correlated. Also, tasks may have hierarchical correlation structures. These patterns often appear in biological datasets.  I am wondering how a variety of multi-task settings can be handled by the proposed approach. It would be helpful to discuss the conditions where we can benefit from the proposed method. 2. One intuitive approach to task balancing would be to weight each task objective based on the variance of each task.  It would be helpful to add a few simple and intuitive baselines in the experiments. 3. In Section 4, it would be great to have more in-depth simulations (e.g., multi-task learning in various settings). Also, in the bottom right panel in Figure 2, GrandNorm and equal weighting decrease test errors effectively even after 15000 steps but uncertainty weighting seems to reach a plateau. Discussions on this would be useful. 4. It would be useful to discuss the implementation of the method as well. ",15,270,11.73913043478261,5.2109375,137,4,266,0.0150375939849624,0.0181818181818181,0.983,73,34,47,11,8,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 9, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,0,2,1,9,4,0,1,0,0,1,0,1,0,1,0,0,0,0,9,0,0,0.5737479955758306,0.227197881700966,0.260136457880472
ICLR2018-H1bM1fZCW-R2,Reject,"Paper summary: Existing works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks. This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks. Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks. Paper Strengths: - The proposed technique seems simple yet effective for multi-task learning. - Experiments on two different network architectures showcasing the generality of the proposed method. Major Weaknesses: - The main weakness of this work is the unclear exposition of the proposed technique. Entire technique is explained in a short section-3.1 with many important details missing. There is no clear basis for the main equations 1 and 2. How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does 'F' refer to? There is dependency of 'F' on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update? It is very difficult to decipher these details from the short descriptions given in the paper. - Also, several details are missing in toy experiments . What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output? Minor Weaknesses: - There are no training time comparisons between the proposed technique and the standard fixed loss learning. - Authors claim that they operate directly on the gradients inside the network. But, as far as I understood, the authors only update loss weights in this paper. Did authors also experiment with gradient normalization in the intermediate CNN layers? - No comparison with state-of-the-art techniques on the experimented tasks and datasets. Clarifications: - See the above mentioned issues with the exposition of the technique. - In the experiments, why are the input images downsampled to 320x320? - What does it mean by 'unofficial dataset' (page-4) . Any references here ? - Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)? The loss ratios depend on initial loss, which is not important for the final performance of the system. Suggestions: - I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state. - The term 'GradNorm' seem to be not defined anywhere in the paper. Review Summary: Despite promising results, the proposed technique is quite unclear from the paper. With its poor exposition of the technique, it is difficult to recommend this paper for publication.",27,446,21.23809523809524,5.33487297921478,201,1,445,0.0022471910112359,0.0194805194805194,-0.9736,134,52,69,24,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 11, 'EXP': 11, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 1, 'EMP': 12, 'SUB': 3, 'CLA': 0}",0,1,0,0,1,11,11,1,0,0,0,1,1,0,0,0,0,3,1,1,12,3,0,0.5033638790549334,0.5628447654698512,0.3188362257835552
ICLR2018-H1bM1fZCW-R3,Reject,"The paper addresses an important problem in multitask learning. But its current form has several serious issues. Although I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible. There are many things unclear. For example:  -  it starts with talking about multiple tasks, and then immediately talks about a filter F, without defining what the kind of network is being addressed. - Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks. It is not clear how it is used. In particular, it is not clear how it is used to update the task weights - Equation 2 seems sloppy. ""j"" appears as a free index on the right side, but it doesn't appear on the left side. As a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality. The toy experiment is not convincing. - the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task. This is different from the sum of the original losses, which seems to be the one used to train the ""equal weight"" baseline. A more fair baseline is to directly use the evaluation metric as the training loss.  - the curves seem to have not converged. The experiments on NYUv2 involves non-standard settings, without a good justification. So it is not clear if the proposed method can make a real difference on state of the art systems.  And the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth) . However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting u2014 it can in fact magnify gradients of certain tasks and cause over-training and over-fitting. So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. ",17,366,15.25,4.764367816091954,181,6,360,0.0166666666666666,0.0477453580901856,-0.919,83,48,68,19,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 10, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 2}",0,1,2,1,2,10,2,0,0,0,0,1,0,0,0,1,0,2,0,0,9,0,2,0.5024545034580481,0.4496184015512322,0.2762698592437503
ICLR2018-H1bhRHeA--R1,Reject,"The paper presents interesting algorithms for minimizing softmax with many classes. The objective function is a multi-class classification problem (using softmax loss) and with linear model. The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it. At each iteration, SGD samples a subset of training samples and labels. The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach. It seems the implicit SGD approach is better in the experimental comparisons.  I found the paper quite interesting, but meanwhile I have the following comments and questions: - As pointed out by the authors, the idea of this formulation and doubly SGD is not new.  (Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD. The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch. However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck? Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed. - I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity?  I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)? Also, I guess we are assuming the obj is strongly convex? - The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data. As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data). - All the comparisons are based on epochs, but the competing algorithms are quite different and can have very different running time for each epoch. For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster.  - The claim that implicit SGD never overshoots the optimum needs more supports. Is it proved in some previous papers?  - The presentation can be improved. I think it will be helpful to state the algorithms explicitly in the main paper.",22,412,20.6,4.875,189,8,404,0.0198019801980198,0.0188679245283018,0.9652,117,48,77,22,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 2, 'MET': 15, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,3,2,2,15,1,0,0,0,0,2,0,0,0,1,0,5,1,0,4,0,0,0.5038044748075531,0.4467789669777333,0.28431969408128865
ICLR2018-H1bhRHeA--R2,Reject,"The problem of numerical instability in applying SGD to soft-max minimization is the motivation. It would have been helpful if the author(s) could have made a formal statement.  Since the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary.  The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms. The proposed algorithms are variants of SGD but it is not clear why they should converge faster than existing strategies. Some parts of the text are badly written, see for example the following line(see paragraph before Sec 3)  Since the converge of SGD is inversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the formulation to converge faster.   which could have shed more light on the matter. The title is also misleading in using the word exact . I have understand it correct the proposed SGD method solves the optimization problem to an additive error. n In summary the algorithms are novel variants of SGD but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are missing. The choice of word exact is also not clear.",11,222,18.5,4.9672897196261685,117,1,221,0.004524886877828,0.0350877192982456,-0.9606,56,28,45,10,2,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,8,0,0,0,0,0,0,0,0,0,1,0,0,0,0,5,0,1,0.1446084876536355,0.3358211630727052,0.07186313801919493
ICLR2018-H1bhRHeA--R3,Reject,"The paper develops an interesting approach for solving multi-class classification with softmax loss. The key idea is to reformulate the problem as a convex minimization of a double-sum structure via a simple conjugation trick. SGD is applied to the reformulation: in each step samples a subset of the training samples and labels, which appear both in the double sum. The main contributions of this paper are: U-max idea (for numerical stability reasons) and an proposing an implicit SGD idea.  Unlike the first review, I see what the term exact in the title is supposed to mean. I believe this was explained in the paper. I agree with the second reviewer that the approach is interesting. However, I also agree with the criticism (double sum formulations exist in the literature; comments about experiments); and will not repeat it here. I will stress though that the statement about Newton in the paper is not justified. Newton method does not converge globally with linear rate. Cubic regularisation is needed for global convergence. Local rate is quadratic. I believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed. I apologise for short and late review: I got access to the paper only after the original review deadline.",9,208,14.857142857142858,5.194871794871795,121,0,208,0.0,0.0239234449760765,0.5635,59,29,37,8,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,2,0,3,1,0,0,0,0,0,0,3,0,0,0,0,0,1,4,0,0,0.3577211973685963,0.2240880945267511,0.16076096209762597
ICLR2018-H1cKvl-Rb-R1,Reject,"This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning. The algorithm then chooses optimistically over the distribution induced by the ensemble. This leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN. There are several things to like about this paper: - It is a clear paper, with a simple message and experiments that back up the claims. - The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants. - It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that ensemble voting is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?) On the other hand: - The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer. - Something feels wrong/hacky/incomplete about just doing ensemble for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on random initialization + SGD/Adam + specific network architecture to maintain this idea of uncertainty. For example, this wouldn't work for linear value functions! - I think the original bootstrapped DQN used ensemble voting at test time, so maybe you should change the labels or the way this is introduced/discussed.  It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than raw bootstrapped DQN) and UCB still looks like it does better. - I'm not convinced that page 4 and the Bayesian derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say this is similar to particle filter and add the mathematical derivation after, rather than as if it was some complex formula derived. If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead. - I think this paper might miss the point of the bigger problem of efficient exploration in RL... or even how to get deep exploration with deep RL. Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the sub-human games you might hope.) Overall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari. The scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark. Perhaps this will encourage people to dig deeper into some of these issues... I vote accept. ",17,485,20.20833333333333,5.2993630573248405,250,15,470,0.0319148936170212,0.06187624750499,0.9946,129,58,93,43,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 11, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,5,2,0,11,1,0,0,0,0,2,1,0,0,1,2,5,0,1,8,0,1,0.502928591417289,0.6715370555877753,0.3403957811194661
ICLR2018-H1cKvl-Rb-R2,Reject,"The authors propose a new exploration algorithm for Deep RL . They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style. There is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I'm not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn't convinced by the link to Bayesian RL in this paper. n I liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper).  Some questions about the results: -How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise?  -How does the distribution of Q values look like during different phases of learning? -Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2. -What's different between Alg 1 and bootstrapped DQN (other than the action selection)? Minor things: -Missing propto in Eq 7 ? -Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere... -it looks more a Bellman residual update as written in (11). ",10,239,19.916666666666668,5.178571428571429,146,2,237,0.0084388185654008,0.0326530612244898,0.9771,61,35,42,16,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 2, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,8,0,2,1,1,0,1,0,0,0,0,1,2,2,0,2,0,0,0.5018875345915861,0.4452451333602807,0.278932029994532
ICLR2018-H1cKvl-Rb-R3,Reject,"This paper introduces a number of different techniques for improving exploration in deep Q learning. The main technique is to use UCB (upper confidence bound) to speedup exploration. The authors also introduces Ensemble voting facilitate exploitation .  This paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement. The techniques introduced are a small permutation of previous results. The baselines are not particularly strong either. The paper appeared to have be rushed. The presentation is not always clear. I also have the following questions I hope the authors could help me with:  1. I failed to understand how Eqn (5). Could you please clarify. 2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration. 3. Why comparing to A3C+ which is not necessarily better than A3C in final performance? 4. Why not comparing to Bootstrapped DQN since the proposed method is based on it? 5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits? 6. If there is a section on INFOGAIN exploration, why not mention it in the main text?",14,202,10.63157894736842,5.318918918918919,108,0,202,0.0,0.0341463414634146,0.9565,49,19,46,15,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,3,0,0,9,1,0,0,0,0,1,0,0,0,0,1,5,1,0,5,0,0,0.3593053698579024,0.4474009244125762,0.19982205036042247
ICLR2018-H1cWzoxA--R1,Accept,"Pros:  The paper proposes a ""bi-directional block self-attention network (Bi-BloSAN)"" for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient. The paper is written clearly and is easy to follow. The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency. The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off.  Cons: - Methodology of the paper is very incremental compared with previous models.   - Many of the baselines listed in the paper are not competitive; e.g.,  for SNLI, state-of-the-art results are not included in the paper. - The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency, which, however, is not supported by discussion on or comparison with hierarchical CNN. - The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart. This is unnatural, e.g., compared  with alternatives such as using linguistic segments as blocks. - The main originality of paper is the block style. However, the paper doesn't analyze how and why the block brings improvement. -If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse? ",10,252,19.384615384615383,5.353413654618474,144,0,252,0.0,0.0113207547169811,-0.957,72,34,48,16,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,6,1,1,0,0,0,1,0,0,0,1,0,1,0,0,5,0,0,0.4299028770427359,0.3358211630727052,0.21877848287476898
ICLR2018-H1cWzoxA--R2,Accept,"This high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models, and presents strong empirical results across multiple evaluation tasks. The approach is basically to apply self-attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed. It captures local information into so-called blocks using self-attention, and then applies a second level of self-attention over the blocks themselves. The paper is well organized and clearly written, modulo minor language mistakes that should be easy to fix with further proof-reading. The contextualization of the method relative to CNNs/RNNs/Transformers is good, and the beneficial trade-offs between memory, runtime and accuracy are thoroughly investigated, and they're compelling. I am curious how the story would look if one tried to push beyond two levels...? For example, how effective might a further inter-sentence attention level be for obtaining representations for long documents? Minor points: - Text between Eq 4 & 5: W^{(1)} appears twice; one instance should probably be W^{(2)} . - Multiple locations, e.g. S4.1: for NLI, the word is *premise*, not *promise*. - Missing word in first sentence of S4.1: ... reason __ the ...",9,194,19.4,5.679347826086956,133,3,191,0.0157068062827225,0.0346534653465346,0.9564,64,28,29,10,3,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 3}",0,1,0,0,0,6,0,0,0,0,0,2,0,0,0,0,0,0,0,0,3,0,3,0.2155820936626881,0.2236284072336162,0.0958051770744901
ICLR2018-H1cWzoxA--R3,Accept,"This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP. The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications. The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed. The advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance. While I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR. First, there is not much innovation in the model architecture. The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level. I think this more counts as careful engineering of the SAN model rather than a main innovation. Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders. What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?       I appreciate the answers the authors added and I change the score to 6.",11,230,20.90909090909091,5.364055299539171,134,2,228,0.0087719298245614,0.0127118644067796,0.958,72,32,29,10,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,1,0,0,6,4,0,0,0,0,1,0,0,1,0,0,1,0,0,4,0,0,0.3586524050869065,0.3351992056378622,0.18215191372726508
ICLR2018-H1dh6Ax0Z-R1,Accept,"# Update after the rebuttal Thank you for the rebuttal. The authors claim that the source of objective mismatch comes from n-step Q-learning, and their method is well-justified in 1-step Q-learning . However, there is still a mismatch even with 1-step Q-learning because the bootstrapped target is also computed from the TreeQN. More specifically, there can be a mismatch between the optimal action sequences computed from TreeQN at time t and t+1 if the depth of TreeQN is equal or greater than 2. Thus, the author's response is still not convincing to me. I like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of Q-function, which makes implementation simpler compared to VPN. However, the particular method (TreeQN) proposed in this paper introduces a mismatch in the model learning as mentioned above. One could argue that TreeQN is learning an abstract planning rather than grounded planning. However, the fact that reward prediction loss is used to train TreeQN significantly weakens this claim, and there is no such an evidence in the paper. In conclusion, I think the research direction is worth pursuing, but the proposed modification from VPN is not well-justifie d.  # Summary This paper proposes TreeQN and ATreeC which perform look-ahead planning using neural networks. TreeQN simulates the future by predicting rewards/values of the future states and performs tree backup to construct Q-values. ATreeC is an actor-critic architecture that uses a softmax over TreeQN. The architecture is trained through n-step Q-learning with reward prediction loss. The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games. [Pros] - The paper is easy to follow. - The application to actor-critic setting (ATreeC) is novel, though the underlying idea was proposed by [O'Donoghue et al., Schulman et al.]. [Cons] - The proposed method has a technical issue. - The proposed idea (TreeQN) and underlying motivation are almost same as those of VPN [Oh et al.], but there is no in-depth discussion that shows why TreeQN is potentially better than VPN. - Comparison to VPN on Atari is not much convincing. # Novelty and Significance - The underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (n-step Q-learning with reward prediction loss) are same as those of VPN.  But, the paper does not provide in-depth discussion on this. The following is the differences that I found from this paper, so it would be important to discuss why such differences are important. 1) The paper emphasizes the fully-differentiable tree planning aspect in contrast to VPN that back-propagates only through  on-branching trajectories during training. However, differentiating TreeQN also amounts to back-propagating through a single trajectory in the tree that gives the maximum Q-value.  Thus, the only difference between TreeQN and VPN is that TreeQN follows the best (estimated) action sequence, while VPN follows the chosen action sequence in retrospect during back-propagation. Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation (see Technical Soundness section for discussion) ?  2) TreeQN only sets targets for the final Q-value after tree backup, whereas VPN sets targets for all intermediate value predictions in the tree. Why is TreeQN's approach better than VPN's approach? - The application to actor-critic setting (ATreeC) is novel, though the underlying idea of combining Q-learning with policy gradient was proposed by [O'Donoghue et al.] and [Schulman et al.]. # Technical Soundness - The proposed idea of setting targets for the final Q-value after tree backup can potentially make the temporal credit assignment difficult, because the best estimated actions during tree planning does not necessarily match with the chosen actions. Suppose that TreeQN estimated up-right-right as the best future action sequence the during 3-step tree planning, while the agent actually ended up with choosing up-left-left (this is possible because the agent re-plans at every step and follows epsilon-greedy policy). Following n-step Q-learning procedure, we end up with setting target Q-value based on on-policy action sequence up-left-left, while back-propagating through up-right-right action sequence in the TreeQN's plan. This causes a wrong temporal credit assignment, because TreeQN can potentially increase/decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen actions. So, it is unclear why the proposed algorithm is technically correct or better than VPN's approach (i.e., back-propagating through the chosen actions in the search tree). # Quality - Comparison to VPN on Atari is not convincing because TreeQN-1 is actually (almost) equivalent to VPN-1, but the results show that TreeQN-1 performs much better than VPN on many games. Since the authors took the numbers from [Oh et al.] rather than replicating VPN, it is possible that the gap comes from implementation details (e.g., hyperparameter). # Clarity - The paper is overall easy to follow and the description of the proposed method is clear.",38,801,22.25,5.498084291187739,297,1,800,0.00125,0.020631067961165,0.9976,247,114,138,39,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 8, 'PDI': 5, 'DAT': 2, 'MET': 27, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 8, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 0, 'CLA': 2}",0,1,8,5,2,27,3,0,0,0,0,3,0,1,0,2,0,8,0,0,19,0,2,0.5790335010904956,0.45658605882732,0.3294140028550133
ICLR2018-H1dh6Ax0Z-R2,Accept,"The authors propose a new network architecture for RL that contains some relevant inductive biases about planning. This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task. The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end). This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes. The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax. I thought the paper was clear and well-motivated. The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility. Experimental results seem promising but I wasn't fully convinced of its conclusions. In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn't clear to me that this is the right baseline. Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top. Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity? Same comment in Atari, but there it's not really obvious that the proposed architecture is helping. Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc. Page 5, the auxiliary loss on reward prediction seems appropriate, but it's not clear from the text and experiments whether it actually was necessary. Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance? Despite some shortcomings in the result section, I believe this is good work and worth communicating as is.",16,307,21.928571428571427,5.179310344827586,176,2,305,0.0065573770491803,0.0618892508143322,0.9829,82,43,55,10,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,1,5,1,0,10,3,2,0,2,0,0,0,0,0,0,0,2,0,1,9,0,1,0.502826493490341,0.4495372664803782,0.2725693788275625
ICLR2018-H1dh6Ax0Z-R3,Accept,"This was an interesting read. I feel that there is a mismatch between intuition of what a model could do (based on the structure of the architecture) versus what a model does. Just because the transition function is shared and the model could learn to construct a tree, when trained end-to-end the system is not sufficiently constrained to learn this specific behaviour.  More to a point. I think the search tree perspective is interesting, but isn't this just a deeper model with shared weights? And a max operation? It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state, right? Is there any specific attempt to visualize or understand the embeddings inside the tree? The same regarding the rewards. If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards, why would the model use those free latent variables to encode rewards?  I think this is a pitfall that many deep network papers fall, where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution (where the latent have prescribed semantics). I would argue that is rarely the case. When the system is learned end-to-end, the structure does not impose the behaviour of the model, and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree. And this is not by showing final performance on a game. If indeed the model does anything similar to search, than all intermediary representations should correspond to what semantically they should. Ignoring my verbose comment, another view is that the baseline are disadvantaged to the treeQN, because they have less parameters (and are less deep which has a huge impact on the learnability and expressivity of the deep network).",12,322,29.27272727272728,5.0495049504950495,156,3,319,0.0094043887147335,0.0216049382716049,0.9767,76,30,67,15,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,0,1,1,0,11,0,0,0,0,0,1,0,0,0,0,2,0,0,0,9,0,0,0.2882162068521324,0.2272459183494139,0.13023648285743708
ICLR2018-H1eJxngCW-R1,Reject,"Summary: The paper proposes a new dataset for reading comprehension, called DuoRC. The questions and answers in the DuoRC dataset are created from different versions of a movie plot narrating the same underlying story. The DuoRC dataset offers the following challenges compared to the existing reading comprehension (RC) datasets u2013 1) low lexical overlap between questions and their corresponding passages, 2) requires use of common-sense knowledge to answer the question, 3) requires reasoning across multiples sentences to answer the question, 4) consists of those questions as well that cannot be answered from the given passage. The paper experiments with two types of models u2013 1) a model which only predicts the span in a document and 2) a model which generates the answer after predicting the span. Both these models are built off of an existing model on SQuAD u2013 the Bidirectional Attention Flow (BiDAF) model. The experimental results show that the span based model performs better than the model which generates the answers. But the accuracy of both the models is significantly lower than that of their base model (BiDAF) on SQuAD, demonstrating the difficulty of the DuoRC dataset. t Strengths:  1.tThe data collection process is interesting. The challenges in the proposed dataset as outlined in the paper seem worth pushing for. 2.tThe paper is well written making it easy to follow. 3.tThe experiments and analysis presented in the paper are insightful. Weaknesses:  1.tIt would be good if the paper can throw some more light on the comparison between the existing MovieQA dataset and the proposed DuoRC dataset, other than the size. 2.tThe dataset is motivated as consisting of four challenges (described in the summary above) that do not exist in the existing RC datasets. However, the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challenges. Such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset. 3.tIt is not clear from the paper how should the questions which are unanswerable be evaluated. As in, what should be the ground-truth answer against which the answers should such questions be evaluated. Clearly, string matching would not work because a model could say ""don't know"" whereas some other model could say ""unanswerable"". So, does the training data have a particular string as the ground truth answer for such questions, so that a model can just be trained to spit out that particular string when it thinks it can't answer the questions?  4.tOne of the observations made in the paper is that ""training on one dataset and evaluating on the other results in a drop in the performance. "" However, in table 4, evaluating on Paraphrase RC is better when trained on Self RC as opposed to when trained on Paraphrase RC. This seems to be in conflict with the observation drawn in the paper. Could authors please clarify this? Also, could authors please throw some light on why this might be happening? 5.tIn the third phase of data collection (Paraphrase RC), was waiting for 2-3 weeks the only step taken in order to ensure that the workers for this stage are different from those in stage 2, or was something more sophisticated implemented which did not allow a worker who has worked in stage 2 to be able to participate in stage 3? 6.tTypo: Dataset section, phrases --> phases. Overall: The challenges proposed in the DuoRC dataset are interesting. The paper is well written and the experiments are interesting. However, there are some questions (as mentioned in the Weaknesses section) which need to be clarified before I can recommend acceptance for the paper.",29,610,24.4,5.083904109589041,246,2,608,0.0032894736842105,0.0227642276422764,0.9953,164,41,122,25,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 2, 'DAT': 10, 'MET': 18, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 15, 'SUB': 0, 'CLA': 3}",0,1,6,2,10,18,3,1,1,3,0,4,0,0,0,0,0,5,0,1,15,0,3,0.7196836230216722,0.4537827689027141,0.40741415350028254
ICLR2018-H1eJxngCW-R2,Reject,"This paper presents a useful dataset for testing reading comprehension while avoiding significant lexical overlap between question and document. The paper rightly mentions that existing reading comprehension datasets (e.g. SQuAD) where the current methods are already performing at the human level largely due to large lexical overlap between question and document. The authors have devised a clever way to create a reading comprehension dataset without a lot of lexical overlap by using parallel plots of movies from Wikipedia and IMDB. This paper contributes a useful new dataset that fixes some of the shortcomings of existing reading comprehension datasets where the task is made easier by lexical overlap. The authors also present an analysis of the data by applying one of the SOTA techniques on SQuAD to this data.  They also analyze the effect of various span-identification steps and preprocessing steps on the performance. Overall, this paper contributes a useful new dataset that can be quite useful for reading comprehension.",7,159,19.875,5.607843137254902,87,0,159,0.0,0.00625,0.9531,49,21,25,6,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 5, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,0,0,5,3,2,0,0,2,0,0,0,0,0,0,0,0,0,0,6,0,0,0.3578985195023253,0.1142208982853259,0.14667516495270164
ICLR2018-H1eJxngCW-R3,Reject,"1) This paper proposes a new dataset for Reading Comprehension (RC). Different from other existing RC datasets, the authors claim that this new dataset requires background and common-sense knowledge,  and across sentences reasoning in order to answer the questions correctly. Overall, I think this dataset is very useful for RC. The collection process is also carefully designed to reduce the lexical overlap between question and answer pairs. 2) I have the questions as follows: i) in the abstract, authors mentioned the workers set one only takes care of creating questions from version one of the plots, and workers set two is in charge of generating answers from another version of plots. However, in bullet 2 of section 3, it seems that the workers set one is also required to answer the questions in selfRC. Is there any mistake in the description of the abstract? ii) What is the standard for creating the questions? I noticed that the time and location information was used to generate questions sometime, but sometimes these kinds of questions are ignored. iii) Why the SelfRC is about QA pairs but for ParaphraseRC, you need to include documents?  iv) What is the average length of the answers in both ParaphraseRC and SelfRC? I found that the answers are usually very short, which is more like factoid QA. It would be great if the authors could design some non-factoid QA pairs which require more reasoning and background knowledge. v) During NLP pre-processing (section 4), how do you prune the irrelevant documents? ",14,253,25.3,5.070539419087137,132,4,249,0.0160642570281124,0.01171875,0.9245,75,20,47,13,4,2,"{'ABS': 2, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 3, 'MET': 10, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",2,0,1,0,3,10,0,0,0,0,0,0,0,0,0,0,1,0,0,0,11,0,0,0.2880397064767886,0.2284417965706519,0.13160715502636652
ICLR2018-H1kG7GZAW-R1,Accept,"****** Update: revising reviewer score to 6 after acknowledging revisions and improved manuscript ******  The authors propose a new regularization term modifying the VAE (Kingma et al 2013) objective to encourage learning disentangling representations. Specifically, the authors suggest to add penalization to ELBO in the form of -KL(q(z)||p(z)) , which encourages a more global criterion than the local ELBOs. In practice, the authors decide that the objective they want to optimize is unwieldy and resort to moment matching of covariances of q(z) and p(z) via gradient descent. The final objective uses a persistent estimate of the covariance matrix of q and upgrades it at each mini-batch to perform learning. The authors use this objective function to perform experiments measuring disentanglement and find minor benefits compared to other objectives in quantitative terms. Comments: 1. The originally proposed modification in Equation (4) appears to be rigorous and as far as I can tell still poses a lower bound to log(p(x)).  The proof could use the result posed earlier: KL(q(z)||p(z)) is smaller than E_x KL(q(z|x)||p(z|x)). 2.  The proposed moment matching scheme performing decorrelation resembles approaches for variational PCA and especially independent component analysis.  The relationship to these techniques is not discussed adequately.  In addition, this paper could really benefit from an empirical figure of the marginal statistics of z under the different regularizers in order to establish what type of structure is being imposed here and what it results in. 3. The resulting regularizer with the decorrelation terms could be studied as a modeling choice.  In the probabilistic sense, regularizers can be seen as structural and prior assumptions on variables.  As it stands, it is unnecessarily vague which assumptions this extra regularizer is making on variables.  4. Why is using the objective in Equation (4) not tried and tested and compared to?  It could be thought that subsampling would be enough to evaluate this extra KL term without any need for additional variational parameters psi. The reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that Eq (4) has problems.  5. The model seems to be making on minor progress in its stated goal, disentanglement.  It would be more convincing to clarify the structural properties of this regularizer in a statistical sense more clearly given that experimentally it seems to only have a minor effect. 6. Is there a relationship to NICE (Laurent Dinh et al)?  7. The infogan is also an obvious point of reference and comparison here.  8. The authors claim that there are no models which can combine GANs with inference in a satisfactory way, which is obviously not accurate nowadays given the progress on literature combining GANs and variational inference.   All in all I find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change.",19,481,17.178571428571427,5.4449339207048455,248,5,476,0.0105042016806722,0.0280561122244488,0.961,148,48,96,31,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 13, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 17, 'SUB': 1, 'CLA': 0}",0,0,0,1,0,13,1,1,1,2,0,1,1,0,0,0,0,0,0,1,17,1,0,0.5744555961746036,0.3432846522908209,0.2896436409695818
ICLR2018-H1kG7GZAW-R2,Accept,"This paper describes DIP-VAE, an improvement on the beta-VAE framework for learning a disentangled representation of the data generative factors in the visual domain.  The authors propose to augment the standard VAE ELBO objective with an extra term that minimises the covariance between the latents. Unlike the original beta-VAE objective which implicitly minimises such covariance individually for each observation x, the DIP-VAE objective does so while marginalising over x.  This difference removes the tradeoff between reconstruction quality and disentangling reported for beta-VAE, since DIP-VAE maintains sensitivity of q(z|x) to each observation x, and hence achieves disentangling while preserving the sharpness of reconstructions. Pros: - the paper is well written - it makes a contribution to an important line or research (unsupervised disentangled representation learning) - the covariance minimisation proposed in the paper looks like an easy to implement yet impactful change to the VAE objective to encourage disentanglement while preserving reconstruction quality - it directly compares the performance of DIP-VAE to that of beta-VAE showing significant improvements in terms of disentangling metric and reconstruction error Cons: - I am yet to be fully convinced how well the approach works. Table 1 and Figure 1 look good, but other figures are either tangental to the main point of the paper, or impossible to read due to the small scale. For example, the qualitative evaluation of the latent traversals is almost impossible due to the tiny scale of Table 5 (shouldn't this be a Figure rather than a Table?) - The authors concentrate a lot on the CelebA dataset, however I believe the comparison with beta-VAE would be a lot clearer on the dSprites dataset (https://github.com/deepmind/dsprites-dataset) (the authors call it 2D shapes). I would like to see latent traversals of the best DIP-VAE vs beta-VAE to demonstrate good disentangling and the improvements in reconstruction quality.  This (and larger Table 5) would be better use of space compared to Table 2 and Figure 2 for example, which I feel are somewhat tangental to the main message of the paper and are better suited for the appendix. - I wonder how the authors calculated the disentanglement metric on CelebA, given that the ground truth attributes in the dataset are often rather qualitative (e.g. attractiveness), noisy (many can be considered an inaccurate description of the image), and often do not align with the data generative factors discoverable through unsupervised modeling of the data distribution n- Table 3 - the legends for the axes are too small and are impossible to read. Also it would be helpful to normalise the scales of the heat plots in the second row. - Table 3 -  looking at the correlations with the ground truth factors, it seems like beta-VAE did not actually disentangle the latents. Would be nice to see the corresponding latent traversal plots to ensure that the baseline is actually trained well.  I am willing to increase my score for the paper if the authors can address my points. In particular I would like to see a clear comparison in terms of latent traversals on dSprites between beta-VAE and DIP-VAE models presented in Table 3. I would also like to see where these particular models lie in Figure 1.  --------------------------- ---- UPDATE ---------- --------------------------- I have increased my score after reading the revised version of the manuscript.",21,536,29.77777777777778,5.424901185770751,231,3,533,0.0056285178236397,0.014388489208633,0.9974,154,63,84,28,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 2, 'RES': 0, 'TNF': 6, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 14, 'SUB': 0, 'CLA': 1}",0,0,0,2,1,8,2,0,6,0,0,3,0,0,0,0,0,0,1,1,14,0,1,0.4306755964411199,0.452529891097403,0.24554900297900398
ICLR2018-H1kG7GZAW-R3,Accept,"########## UPDATED AFTER AUTHOR RESPONSE ##########  Thanks for the good revision and response that addressed most of my concerns. I am bumping up my score. ###############################################   This paper presents a Disentangled Inferred Prior (DIP-VAE) method for learning disentangled features from unlabeled observations following the VAE framework. The basic idea of DIP-VAE is to enforce the aggregated posterior q(z)   E_x [q(z | x)] to be close to an identity matrix as implied by the commonly chosen standard normal prior p(z). The authors propose to moment-match q(z) given it is hard to minimize the KL-divergence between q(z) and p(z). This leads to one additional term to the regular VAE objective (in two parts, on- and off-diagonal). It has the similar property as beta-VAE (Higgins et al. 2017) but without sacrificing the reconstruction quality. Empirically the authors demonstrate that DIP-VAE can effectively learn disentangled features, perform comparably better than beta-VAE and at the same time retain the reconstruction quality close to regular VAE (beta-VAE with beta   1). The paper is overall well-written with minor issues (listed below). I think the idea of enforcing an aggregated (marginalized) posterior q(z) to be close to the standard normal prior p(z) makes sense, as opposed to enforcing each individual posterior q(z|x) to be close to p(z) as (beta-)VAE objective suggests. I would like to make some connection to some work on understanding VAE objective (Hoffman & Johnson 2016, ELBO surgery: yet another way to carve up the variational evidence lower bound) where they derived something along the same line of an aggregated posterior q(z). In Hoffman & Johnson, it is shown that KL(q(z) | p(z)) is in fact buried in ELBO, and the inequality gap in Eq (3) is basically a mutual information term between z and n (the index of the data point). Similar observations have led to the development of VAMP-prior (Tomczak & Welling 2017, VAE with a VampPrior). Following the derivation in Hoffman & Johnson, DIP-VAE is basically adding a regularization parameter to the KL(q(z) | p(z)) term in standard ELBO. I think this interpretation is complementary to (and in my opinion, more clear than) the one that's described in the paper. My concerns are mostly regarding the empirical studies:   1. One of my main concern is on the empirical results in Table 1. The disentanglement metric score for beta-VAE is suspiciously lower than what's reported in Higgins et al., where they reported a 99.23% disentanglement metric score on 2D shape dataset. I understand the linear classier is different, but still the difference is too large to ignore. Hence my current more neutral review rating.  2. Regarding the correlational plots (the bottom row of Table 3 and 4), I don't think I can see any clear patterns (especially on CelebA). I wonder what's the point of including them here and if there is a point, please explain them clearly in the paper.  3. Figure 2 is also a little confusing to me. If I understand the procedure correctly, a good disentangled feature would imply smaller correlations to other features (i.e., the numbers in Figure 2 should be smaller for better disentangled features). However, looking at Figure 2 and many other plots in the appendix, I don't think DIP-VAE has a clear win here. Is my understanding correct? If so, what exactly are you trying to convey in Figure 2? Minor comments:   1. In Eq (6) I think there are typos in terms of the definition of Cov_q(z)(z)? It appears as only the second term in Eq (5). 2. Hyperparameter subsection in section 3: Shouldn't lambda_od be larger if the entanglement is mainly reflected in the off-diagonal entries? Why the opposite?   3. Can you elaborate on how a running estimate of Cov_p(x)(mu(x)) is maintained (following Eq (6)). It's not very clear at the current state of the paper.  4. Can we have error bars in Table 2? Some of the numbers are possibly hitting the error floor. 5. Table 5 and 6 are not very necessary, unless there is a clear point. ",32,657,16.846153846153847,5.061359867330016,290,8,649,0.012326656394453,0.0321637426900584,0.9833,197,94,108,39,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 2, 'MET': 12, 'EXP': 0, 'RES': 3, 'TNF': 7, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 1}",0,0,1,4,2,12,0,3,7,0,0,5,0,1,0,0,0,0,2,0,12,0,1,0.5748275417875244,0.3402364340404094,0.2877645928149069
ICLR2018-H1kMMmb0--R1,Reject,"Summary: This work is a variant of previous work (Zaremba et al. 2016) that enables the use of (noisy) operators that invoke pre-trained neural networks and is trained with Actor-Critic. In this regard it lacks a bit of originality. The quality of the experimental evaluation is not great. The clarity of the paper could be improved upon but is otherwise fine. The existence of previous work (Zaremba et al. 2016) renders this work (including its contributions) not very significant. Relations to prior work are missing. But let's wait for the rebuttal phase. Pros  -It is confirmed that noisy operators (in the form of neural networks) can be used on the visual arithmetic task Cons -Not very novel -Experimental evaluation is wanting The focus of this paper is on integrating perception and reasoning in a single system. This is done by specifying an interface that consists of a set of discrete operations (some of which involve perception) and memory slots. A parameterized policy that can make use of these these operations is trained via Actor-Critic to solve some reasoning tasks (arithmetics in this case). The proposed system is a variant of previous work (Zaremba et al. 2016) on the concept of interfaces, and similarly learns a policy that utilizes such an interface to perform reasoning tasks, such as arithmetics. In fact, the only innovation proposed in this paper is to incorporate some actions that invoke a pre-trained neural network to ""read"" the symbol from an image, as opposed to parsing the symbol directly. However, there is no reason to expect that this would not function in previous work (Zaremba et al. 2016), even when the network is suboptimal (in which case the operator becomes noisy and the policy should adapt accordingly). Another notable difference is that the proposed system is trained with Actor-Critic as opposed to Q-learning, but this is not further elaborated on by the authors. The proposed system is evaluated on a visual arithmetics task. The input consists of a 2x2 grid of extended MNIST characters. Each location in the grid then corresponds to the 28 x 28 pixel representation of the digit. Actions include shifting the ""fovea"" to a different entry of the grid, invoking the digit NN or the operator NN which parse the current grid entry, and some symbolic operations that operate on the memory. The fact that the input is divided into a 2x2 grid severely limits the novelty of this approach compared to previous work (Zaremba et al. 2016). Instead it would have been interesting to randomly spawn digits and operators in a 56 x 56 image and maintain 4 coordinates that specify a variable-sized grid that glimpses a part of the image. This would make the task severely more difficult, given fixed pre-trained networks. The addition of the salience network is unclear to me in the context of MNIST digits, since any pixel that is greater than 0 is salient? I presume that the LSTM uses this operator to evaluate whether the current entry contains a digit or an operator. If so, wouldn't simply returning the glimpse be enough? In the experiments the proposed system is compared to three CNNs on two different visual arithmetic tasks, one that includes operators as part of the input and one that incorporates operators only in the tasks description. In all cases the proposed method requires fewer samples to achieve the final performance, although given enough samples all of the CNNs will solve the tasks. This is not surprising as this comparison is rather unfair. The proposed system incorporates pre-trained modules, whose training samples are not taken into account. On the other hand the CNNs are trained from scratch and do not start with the capability to recognize digits or operators. Combined with the observation that all CNNs are able to solve the task eventually, there is little insight in the method's performance that can be gained from this comparison. Although the visual arithmetics on a 2x2 grid is a toy task it would at least be nice to evaluate some of the policies that are learned by the LSTM (as done by Zaremba) to see if some intuition can be recovered from there. Proper evaluation on a more complex environment (or at least on that does not assume discrete grids) is much desired. When increasing the complexity (even if by just increasing the grid size) it would be good to compare to a recurrent method (Pyramid-LSTM, Pixel-RNN) as opposed to a standard CNN as it lacks memory capabilities and is clearly at a disadvantage compared to the LSTM. Some detailed comments are:  The introduction invokes evidence from neuroscience to argue that the brain is composed of (discrete) modules, without reviewing any of the counter evidence (there may be a lot, given how bold this claim is). From the introduction it is unclear why the visual arithmetic task is important. Several statements including the first sentence lack citations. The contribution section is not giving any credit to Zaremba et al. (2016) whereas this work is at best a variant of that approach. In the experiment section the role of the saliency detector is unclear. Experiment details are lacking and should be included. The related work section could be more focused on the actual contribution being made. It strikes me as odd that in the discussion the authors propose to make the entire system differentiable, since this goes against the motivation for this work. Relation to prior work:  p 1: The authors write: We also borrow the notion of an interface as proposed in Zaremba et al. (2016). An interface is a designed, task-specific machine that mediates the learning agent's interaction with the external world, providing the agent with a representation (observation and action spaces) which is intended to be more conducive to learning than the raw representations. In this work we formalize an interface as a separate POMDP I with its own state, observation and action spaces.    This interface terminology for POMDPs was actually introduced in:  J.  Schmidhuber. Reinforcement learning in Markovian and non-Markovian environments. In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information Processing Systems 3, NIPS'3, pages 500-506. San Mateo, CA: Morgan Kaufmann, 1991.  p 4: authors write: For the policy u03c0u03b8, we employ a Long Short-Term Memory (LSTM) Do the authors use the (cited) original LSTM of 1997, or do they also use the forget gates (recurrent units with gates) that most people are using now, often called the vanilla LSTM, by Gers et al (2000)? p 4: authors write: One obvious point of comparison to the current work is recent research on deep neural networks designed to learn to carry out algorithms on sequences of discrete symbols. Some of these frameworks, including the Differen-tiable Forth Interpreter (Riedel and Rocktu00e4schel, 2016) and TerpreT (Gaunt et al., 2016b), achieve this by explicitly generating code, while others, including the Neural Turing Machine (NTM; Graves et al., 2014), Neural Random-Access Machine (NRAM; Kurach et al., 2015), Neural Programmer (NP; Neelakan- tan et al., 2015), Neural Programmer-Interpreter (NPI; Reed and De Freitas, 2015) and work in Zaremba et al. (2016) on learning algorithms using reinforcement learning, avoid gen- erating code and generally consist of a controller network that learns to perform actions in a (sometimes differentiable) external computational medium in order to carry out an algorithm.   Here the original work should be mentioned, on differentiable neural stack machines:   G.Z. Sun and H.H. Chen and  C.L. Giles and Y.C. Lee and D. Chen. Connectionist Pushdown Automata that Learn Context-Free Grammars. IJCNN-90, Lawrence Erlbaum, Hillsdale, N.J., p 577, 1990.  Mozer, Michael C and Das, Sreerupa. A connectionist symbol manipulator that discovers the structure of context-free languages. Advances in Neural Information Processing Systems (NIPS), p 863-863, 1993.   ",54,1293,17.24,5.12046204620462,495,3,1290,0.0023255813953488,0.011441647597254,-0.5497,375,151,216,52,8,6,"{'ABS': 0, 'INT': 2, 'RWK': 7, 'PDI': 5, 'DAT': 3, 'MET': 23, 'EXP': 7, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 7, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 32, 'SUB': 2, 'CLA': 1}",0,2,7,5,3,23,7,0,0,0,0,4,7,0,0,3,1,5,0,0,32,2,1,0.5784784144060907,0.6866128890529164,0.4180380572164867
ICLR2018-H1kMMmb0--R2,Reject,"The paper presents an interesting model to reuse specialized models trained for perceptual tasks in order to solve more complex reasoning tasks. The proposed model is based on reinforcement learning with an agent that interacts with an environment C, which is the combination of E and I, the external world and the interface, respectively. This abstraction is nicely motivated and contextualized with respect to previous work. However, the paper evaluates the proposed model in artificial tasks with limited reasoning difficulty: the tasks can be solved with simpler baseline models. The paper argues that the advantage of the proposed approach is data efficiency, which seems to be a side effect of having pre-trained modules rather than a clear superior reasoning capability. The paper discusses other advantages of the model, but these are not tested or evaluated either. A more convincing experimental setup would include complex reasoning tasks, and the evaluation of all the aspects mentioned as benefits: computational time, flexibility of computation, better accuracy, etc.",7,164,23.428571428571427,5.6455696202531644,102,1,163,0.0061349693251533,0.0121951219512195,0.9775,46,21,31,8,4,1,"{'ABS': 1, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",1,0,0,2,0,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0.2862739698037108,0.1142208982853259,0.11662633021307145
ICLR2018-H1kMMmb0--R3,Reject,"Summary: The authors use RL to learn a visual arithmetic task, and are able to do this with a relatively small number of examples, presumably not including the number of examples that were used to pre-train the classifiers that pre-process the images. This appears to be a very straightforward application of existing techniques and networks. Quality: Given the task that the authors are trying to solve, the approach seems reasonable. Clarity: The paper appears quite clearly written for the most part. Originality & Significance: Unless I am missing something important, or misunderstanding something, I do not really understand what is significant about this work, and I don't see it as having originality. Nitpick 1: top of Page 5, it says Figure ??   Nitpick 2: Section 2.3 says M means take the product, A means take the sum, etc. Why choose exactly those terms that obscure the pattern, and then write etc? In Figure 1, X could mean multiply, or take the maximum, but by elimination, it means take the maximum. It would have only added a few characters to the paper to specify the notation here, e.g. Addition(A), Max (X), Min (N), Multiply (M). If the authors insist on making the reader figure this out by deduction, I recommend they just write We leave the symbols-operation mapping as a small puzzle for the reader.   The authors might find the paper Visual Learning of Arithmetic Operations"" by  Yedid Goshen and Shmuel Peleg to also be somewhat relevant, although it's different from what they are doing. Section 3. The story from the figures seems to be that the authors' system works beats a CNN when there are very few examples. But significance of this is not really discussed in any depth other than being mentioned in corresponding places in the text, i.e. it's not really the focal story of the text. Pros: Seems to work OK. Seems like a reasonable application of pre-trained nets to allow solving a different visual problem for which less data might be available. Cons: Unless I am missing an important point, the results are unsurprising, and I am not clear what is novel or significant about them.",15,358,19.88888888888889,4.981927710843373,187,9,349,0.0257879656160458,0.0328767123287671,0.9393,91,33,76,25,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 1, 'TNF': 5, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,0,0,1,0,5,0,1,5,0,0,5,0,0,0,2,2,0,3,0,8,0,1,0.3584184926730245,0.5601255396800291,0.22339692119967056
ICLR2018-H1l8sz-AW-R1,Reject,"GENERAL IMPRESSION:  Overall, the revised version of the paper is greatly improved. The new derivation of the method yields a much simpler interpretation, although the relation to the natural gradient remains weak (see below). The experimental evaluation is now far more solid. Multiple data sets and network architectures are tested, and equally important, the effect of parameter settings is investigated. I enjoyed the investigation of the effect of L_2 regularization on qualitative optimization behavior. CRITICISM:  My central criticism is that the introduction of the L_2 norm as a replacement of KL divergence is completely ad-hoc; how it is related to KL divergence remains unclear. It seems that other choices are equally well justified, including the L_2 norm in parameter space, which then defeats the central argument of the paper. I do believe that L_2 distance is more natural in function space than in parameter space, but I am missing a strict argument for this in the paper. Although related work is discussed in detail in section 1, it remains unclear how exactly the proposed algorithm overlaps with existing approaches. I am confident that it is easy to identify many precursors in the optimization literature, but I am not an expert on this. It would be of particular interest to highlight connections to algorithm regularly applied to neural network training. Adadelta, RMSprop, and ADAM are mentioned explicitly, but what exactly are differences and similarities? The interpretation of figure 2 is off. It is deduced that HCGD generalizes better, however, this is the case only at the very end of training, while SGD with momentum and ADAM work far better initially. With the same plot one could sell SGD as the superior algorithm. Overall, also in the light of figure 4, the interpretation that the new algorithm results in better generalization seems to stand on shaky ground, since differences are small. I like the experiment presented in figure 5 in particular. It adds insights that are of value even if the method should turn out to have significant overlap with existing work (see above), and perform only on par with these:; it adds an interesting perspective to the discussion of how network optimization works, how it handles local optima and which role they play, and how the objective function landscape is perceived by different optimizers. This is where I learned something new. MINOR POINTS:  page 5: the any (typo)  page 5: ture -> true (typo).",21,402,21.157894736842103,5.212041884816754,206,2,400,0.005,0.027027027027027,0.9952,111,50,68,27,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 0, 'DAT': 1, 'MET': 13, 'EXP': 4, 'RES': 0, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,6,0,1,13,4,0,3,0,0,1,0,1,0,0,0,5,1,0,10,0,0,0.5037099281738752,0.3393996004756799,0.2532489853223114
ICLR2018-H1l8sz-AW-R2,Reject,"I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision. It seems that the authors have made an effort to accommodate reviewers' comments. I upgraded the rating. -----------------------------------------------------------------------------------------------------------------------  Summary: The paper considers the use of natural gradients for learning. The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training. The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost. The paper is structured as follows: 1. KL divergence is used as a similarity measure between two distributions. 2. Regularizing the objective with KL div. seems promising, but expensive. 3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix. 4. However, computing and inverting the Fisher information matrix is computationally expensive. 5. One solution is to approximate the solution F^{-1} J using gradient descent. However, still we need to calculate F. There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher'). 6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a good initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective. 7. No large matrices need to be formed or inverted, however more passes needed per outer step. Importance: Somewhat lack of originality and poor experiments lead to low importance. Clarity: The paper needs major revision w.r.t. presenting and highlighting the new main points. E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS). Originality/Novelty: The paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks. Beyond this, the paper does not provide any futher original idea. So, slight to no novelty. Main comments: 1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help? This would require the combination of two Hessian matrices. 2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results. Overall: Rejection.",25,387,10.45945945945946,5.410958904109589,204,3,384,0.0078125,0.0204603580562659,0.9406,121,46,73,16,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 16, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 2, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,1,3,1,0,16,3,0,0,0,0,3,0,2,0,3,0,0,2,2,7,0,2,0.5041864147792136,0.5595400198004721,0.3179918431641464
ICLR2018-H1l8sz-AW-R3,Reject,"The paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction (i.e. adjusting updates based on their size in the output space instead of the input space). This goal is to achieve a similar effect to that of natural gradient, but with lighter computation. The authors claim that their regularization is related to Wasserstein metric (but the connection is not clear to me, read below). Experiments on MNIST with show improved generalization (but the baseline is chosen poorly, read below). The paper is easy to read and organized very well, and has adequate literature review. However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides. On the theory side, the authors claim that their regularization is based on Wasserstein metric (in the title of the paper as well as section 2. 2). However, this connection is not very clear to me [if there is a rigorous connection, please elaborate]. From what I understand, the authors argue that their proposed loss+regularization is equivalent to the Kantorovich-Rubinstein form. However, in the latter, the optimization objective is the f itself (sup E[f_1]-E[f_2]) but in your scheme you propose adding the regularization term (which can be added to any objective function, and then the whole form loses its connection to Wasserstrin metric). On the practical side, the chosen baseline is very poor. The authors only experiment with MNIST dataset. The baseline model lacks both batch normalization and dropout, which I guess is because otherwise the proposed method would under-perform against the baseline. It is hard to tell if the proposed regularization scheme is something significant under such poorly chosen baseline.",14,281,17.5625,5.237226277372263,140,1,280,0.0035714285714285,0.0498220640569395,0.8878,82,28,54,16,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 0, 'DAT': 1, 'MET': 9, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,9,0,1,9,2,0,0,0,0,1,0,0,0,0,0,3,1,0,5,1,1,0.4313030644924865,0.5582777104093074,0.2638198952122699
ICLR2018-H1mCp-ZRZ-R1,Accept,"In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework. The authors also introduce a specific control variate technique based on the so-called Stein's identity. The paper is interesting and well-written. I have some question and some consideration that can be useful for improving the appealing of the paper. - I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq. (1), as also suggested in this work. Are there other alternatives in the literature? Please, please discuss and cite some papers if required. - I suggest to divide Section 3.1 in two subsections. The first one introducing Stein's identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title ""Stein Control Variate"". -  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as  M. U. Gutmann and J. Corander, ""Bayesian optimization for likelihood-free inference of simulator-based statistical mod- els,"" Journal of Machine Learning Research, vol. 16, pp. 4256u2013 4302, 2015.   G. da Silva Ferreira and D. Gamerman, ""Optimal design in geostatistics under preferential sampling,"" Bayesian Analysis, vol. 10, no. 3, pp. 711u2013735, 2015.   L. Martino, J. Vicent, G. Camps-Valls, Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models, IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017. -  Please also discuss the dependence of your algorithm with respect to the starting baseline function phi_0.",11,260,10.0,5.863247863247863,152,0,260,0.0,0.0220588235294117,0.9874,100,34,32,5,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 1, 'CLA': 1}",0,1,3,0,0,6,0,0,0,0,1,2,2,0,0,0,1,2,0,0,0,1,1,0.4300411767580077,0.4445616070016344,0.24138272574780364
ICLR2018-H1mCp-ZRZ-R2,Accept,"This paper proposed a class of control variate methods based on Stein's identity. Stein's identity has been widely used in classical statistics and recently in statistical machine learning literature. Nevertheless, applying Stein's identity to estimating policy gradient is a novel approach in reinforcement learning community. To me, this approach is the right way of constructing control variates for estimating policy gradient. The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies. The experimental results also look promising. It would be nice to include some theoretical analyses like under what conditions, the proposed method can achieve smaller sample complexity than existing works.    Overall this is a strong paper and I recommend to accept.     ",8,121,13.444444444444445,5.793103448275862,84,0,121,0.0,0.0155038759689922,0.9618,38,14,27,5,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,3,1,0,0,1,0,1,0,0,0,1,0,1,0,1,2,0,0,0.4291523007013819,0.4450664018792874,0.24259039289446765
ICLR2018-H1mCp-ZRZ-R3,Accept,"The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein's identity and control functionals. The method relates closely to prior work on action-dependent baselines, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance. A criticism of the paper is that it does not require Stein's identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate. The derivation through Stein's identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick. The empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning. However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:  -FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action. A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.  -The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice. Similar comparison should be done with off-policy fitting in Q-Prop. I wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations. The paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop. As discussed above, it is encouraged to elaborate other potential causes that led to performance differences. The experimental results are presented well for a range of Mujoco tasks.  Pros:  -Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time  -Good empirical evaluation Cons:  -]The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein's identity etc. and does not inherit novel insights due to this derivation. ",16,428,21.4,5.773722627737226,208,5,423,0.0118203309692671,0.0298165137614678,0.9856,140,54,79,27,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 1, 'MET': 13, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,1,4,0,1,13,2,2,0,1,0,0,1,0,0,1,0,5,0,0,5,0,1,0.5748142381855574,0.4474009244125762,0.32106106389046657
ICLR2018-H1meywxRW-R1,Accept,Summary: This paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self-attention. It also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span. The resulting DCN+ model achieved significant improvement over DCN. Strengths: The model and the mixed objective is well-motivated and clearly explained. Near state-of-the-art performance on SQuAD dataset (according to the SQuAD leaderboard). Other questions and comments: The ablation shows 0.7 improvement on EM with mixed objective. It is interesting that the mixed objective (which targets F1) also brings improvement on EM.  ,6,101,12.625,5.887755102040816,65,0,101,0.0,0.0097087378640776,0.9712,35,16,14,3,4,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 3, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,0,1,3,0,3,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0.2863260737272739,0.11297698341564,0.11679257811720611
ICLR2018-H1meywxRW-R2,Accept,"This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks. Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder. The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements. Although DCN+ is an improvement of DCN, I think the improvement is not incremental. One question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?",6,110,22.0,5.542056074766355,75,1,109,0.0091743119266055,0.0181818181818181,0.9423,35,13,23,2,4,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,3,2,1,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0.2863008657335637,0.11297698341564,0.1157969702557268
ICLR2018-H1meywxRW-R3,Accept,"The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR. First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions. The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to  reward the system for making partial matching predictions. Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems. An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance. The paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers. It seems to me that this paper is a significant contribution to the field of question answering systems.",7,182,26.0,5.398876404494382,114,1,181,0.005524861878453,0.0273224043715847,0.9633,66,20,25,3,5,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,3,2,1,0,0,0,1,0,0,0,0,1,1,1,1,2,0,1,0.3577294371621352,0.6672886241015097,0.24957092873096576
ICLR2018-H1pri9vTZ-R1,Reject,"This paper extends the framework of neural networks for finite-dimension to the case of infinite-dimension setting, called deep function machines. This theory seems to be interesting and might have further potential in applications.",2,33,16.5,5.909090909090909,29,2,31,0.064516129032258,0.0909090909090909,0.4939,12,4,5,0,2,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0.1428571428571428,0.1111111111111111,0.053626703847793486
ICLR2018-H1pri9vTZ-R2,Reject,"The main idea of this paper is to replace the feedforward summation y   f(W*x + b) where x,y,b are vectors, W is a matrix by an integral y   f(int W x + b) where x,y,b are functions, and W is a kernel. A deep neural network with this integral feedforward is called a deep function machine.  The motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function x, then one encounters the curse of dimensionality as one obtains finer and finer discretization. The idea of functional PCA is to view x as a function is some appropriate Hilbert space, and expands it in some appropriate basis. This way, finer discretization does not increase the dimension of x (nor its approximation), but rather improves the resolution. This paper takes this idea and applies it to deep neural networks. Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea. This approach amounts to a change of basis - and therefore the resolution invariance is not surprising. In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components). Unfortunately, this was not done. I suspect that in this case, the results would be very similar.   ",10,230,19.166666666666668,5.047169811320755,113,0,230,0.0,0.0165975103734439,-0.4157,82,22,38,14,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,0,3,0,5,1,2,0,0,0,0,0,0,0,0,1,2,0,0,2,0,0,0.3583179272144167,0.3340724533253663,0.18107215533766077
ICLR2018-H1pri9vTZ-R3,Reject,"This paper deals with the problem of learning nonlinear operators using deep learning. Specifically, the authors propose to extend deep neural networks to the case where hidden layers can be infinite-dimensional. They give results on the quality of the approximation using these operator networks, and show how to build neural network layers that are able to take into account topological information from data. Experiments on MNIST using the proposed deep function machines (DFM) are provided. The paper attempts to make progress in the region between deep learning and functional data analysis (FDA). This is interesting. Unfortunately, the paper requires significant improvements, both in terms of substance and in terms of presentation. My main concerns are the following:  1) One motivation of DFM is that in many applications data is a discretization of a continuous process and then can be represented by a function. FDA is the research field that formulated the ideas about the statistical data analysis of data samples consisting of continuous functions, where each function is viewed as one sample element. This paper fails to consider properly the work in its FDA context.  Operator learning has been already studied in FDA. See for e.g. the problem of functional regression with functional responses. Indeed the functional model considered in the linear case is very similar to Eq. 2.5 or Eq. 3.2. Moreover, extension to nonparametric/nonlinear situations were also studied. The authors should add more information about previous work on this topic so that their results can be understood with respect to previous studies. n 2) The computational aspects of DFM are not clear in the paper. From a practical computational perspective, the algorithm will be implemented on a machine which processes on finite representations of data. The paper does not clearly provide information about how the functional nature and the infinite dimensional can be handled in practice. In FDA, generally this is achieved via basis function approximations. 3) Some parts of the paper are hard to read. Sections 3 and 4 are not easy to understand. Maybe adding a section about the notation and developing more the intuition will improve the reading of the manuscript. 4) The experimental section can be significantly improved. It will be interesting to compare more DFM with its discrete counterpart. Also, other FDA approaches for operator learning should be discussed and compared to the proposed approach. ",20,392,13.517241379310343,5.403183023872679,197,1,391,0.0025575447570332,0.0177215189873417,0.7685,111,43,71,19,8,6,"{'ABS': 0, 'INT': 3, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 11, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 2}",0,3,2,0,1,11,3,1,0,0,0,2,0,1,0,1,0,3,1,0,6,2,2,0.5742798287189009,0.670198590534437,0.4056918395647153
ICLR2018-H1q-TM-AW-R1,Accept,"This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T). The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions. VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution. Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved. The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly.  Pros: + The paper is written clearly and easy to read + The idea to keep the decision boundary in the low-density region of the target domain makes sense + The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks + The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks Cons: - Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers - The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger Overall, I think it is a good paper and deserves to be accepted to the conference. I'm especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance. Post-rebuttal revision: After reading the authors' response to my review, I decided to leave the score as is.",14,305,33.888888888888886,5.7898305084745765,177,2,303,0.0066006600660066,0.016025641025641,0.9628,98,44,48,14,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,0,2,0,7,1,0,1,0,0,2,0,1,1,0,0,0,0,0,6,1,1,0.5016058713526144,0.4475542316186593,0.2841048286704078
ICLR2018-H1q-TM-AW-R2,Accept,"As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed. The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption. The experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms. Moreover, we also see the learned model is consistently improved using the proposed Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) approach. The proposed methodology relies on multiple choices that could sometimes be better studied and/or explained. Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7). Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity. On the theoretical side, the discussion could be improved. Namely, Section 3 about limitation of domain adversarial training correctly explained that domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity.  It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based.  The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014). In the latter, the notion of Probabilistic Lipschitzness, which is a relaxation of the cluster assumption seems very related to the actual work.  Reference: Ben-David and Urner. Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell.,  2014  Pros: - Propose a sound approach to mix two complementary strategies for domain adaptation. - Great empirical results. Cons: - Some choices leading to the optimization problem are not sufficiently explained. - The theoretical discussion could be improved. Typos: - Equation 14: In the first term (target loss), theta should have an index t (I think). - Bottom of page 6: ... and that as our validation set (missing word). ",15,354,13.615384615384617,5.637982195845697,196,4,350,0.0114285714285714,0.0190217391304347,0.9485,110,44,63,19,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,4,0,0,10,2,1,0,0,0,1,2,0,0,1,0,2,0,0,6,0,1,0.5025917815277599,0.4476713941758493,0.28384489031736415
ICLR2018-H1q-TM-AW-R3,Accept,"The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption. The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well.  I found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective. This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective. This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors? The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.",6,163,12.538461538461538,5.184210526315789,104,2,161,0.0124223602484472,0.0365853658536585,0.7053,44,22,27,9,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,0,0,3,1,1,0,0,0,0,1,0,0,0,0,0,0,0,3,0,1,0.4290718127989979,0.2234661370919081,0.1902815984338614
ICLR2018-H1sUHgb0Z-R1,Accept,"This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators). The authors provide both theoretical and experimental validation of their idea. Pros: + The paper is generally very clearly written. The motivation, notation, and method are clear. + Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases.  + The approach is a novel twist on an existing method for learning from noisy data. Cons:  - All experiments use simulated workers; this is probably common but still not very convincing. - The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. To re (label), or not to re (label). HCOMP 2014. - The authors should have compared their approach to the base approach of Natarajan et al. - It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these. - The ResNet used for each experiment is different, and there is no explanation of the choice of architecture. Questions:  - How would the model need to change to account for example difficulty? - Why are Joulin 2016, Krause 2016 not relevant? - Best to clarify what the weights in the weighted sum of Natarajan are.  - large training error on wrongly labeled examples -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels? Where does this ground truth come from? - Not clear what Ensure means in the algorithm description. - In Sec. 4.4, why is it important that the samples are fresh?",18,280,14.736842105263158,5.125,164,2,278,0.0071942446043165,0.0234113712374581,-0.9363,72,32,59,20,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 3, 'MET': 9, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 2}",0,1,4,1,3,9,4,0,0,0,0,2,0,0,0,1,1,2,0,0,5,1,2,0.5026191882803929,0.6693527940340825,0.3527551623503944
ICLR2018-H1sUHgb0Z-R2,Accept,"This paper focuses on the learning-from-crowds problem when there is only one (or very few) noisy label per item. The main framework is based on the Dawid-Skene model. By jointly update the classifier weights and the confusion matrices of workers, the predictions of the classifier can help on the estimation problem with rare crowdsourced labels. The paper discusses the influence of the label redundancy both theoretically and empirically. Results show that with a fixed budget, it's better to label many examples once rather than fewer examples multiple times. The model and algorithm in this paper are simple and straightforward. However, I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy. The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed. The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing. About the technique details, this paper is clearly written, but some experimental comparisons and claims are not very convincing. Here I list some of my questions: +About the MBEM algorithm, it's better to make clear the difference between MBEM and a standard EM. Will it always converge? What's its objective? +The setting of Theorem 4.1 seems too simple. Can the results be extended to more general settings, such as when workers are not identical? +When n   O(m log m), the result that epslon_1 is constant is counterintuitive, people usually think large redundancy r can bring benefits on estimation, can you explain more on this? +During CIFAR-10 experiments when r 1, each example only have one label. For the baselines weighted-MV and weighted-EM, they can only be directly trained using the same noisy labels. So can you explain why their performance is slightly different in most settings? Is it due to the randomly chosen procedure of the noisy labels? +For ImageNet and MS-COCO experiments with a fixed budget, you reduced the training set when increasing the redundancy, which is unfair. The reduction of performance could mainly cause by seeing fewer raw images, but not the labels. It's better to train some semi-supervised model to make the settings more comparable.",18,367,21.58823529411765,5.25561797752809,192,3,364,0.0082417582417582,0.005420054200542,0.9804,95,51,58,30,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 12, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 1}",0,1,2,2,0,12,2,2,0,2,0,1,0,0,0,0,2,0,0,0,10,0,1,0.5744870895094768,0.338978986895368,0.295018054795247
ICLR2018-H1sUHgb0Z-R3,Accept,"The authors proposed a supervised learning algorithm for modeling label and worker quality.  Further utilize it to study one of the important problems in crowdsourcing - How much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels. Overall the paper was well written. The motivation of the work is clearly explained and supported with relevant related work. The main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion. Though limited to binary classification, the paper proposed a theoretical framework extending the existing work on VC dimension to compute the upper bound on the risk. The authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better results. More detailed comments 1. Instead of considering multi-class classification as one-vs-all binary classification, can you extend the theoretical guarantee on the risk to multi-class set up like Softmax which is widely used in research nowadays. 2. Can you introduce the Risk -R in the paper before using it in Theorem 4.1 3 . Is there any limit on how many examples each worker has to label? Can you comment more on how to pick that value in real-world settings? Just saying sufficiently many (Section 4.2) is not sufficient. 4. Under the experiments, different variations of Majority Vote, EM and Oracle correction were used as baselines. Can you cite the references and also add some existing state-of-the-art techniques mentioned in the related work section. 5. For the experiments on synthetic datasets, workers are randomly sampled with replacements. Were the scores reported based on average of multiple runs. If yes, can you please report the error bars. 6. For the MS-COCO, examples can you provide more detailed results as shown for synthetic datasets? Majority vote is a very weak baseline. For the novel approach and the theoretical backing, I consider the paper to be a good one. The paper has scope for improvement.",18,337,14.652173913043478,5.315789473684211,174,0,337,0.0,0.0058823529411764,0.9481,94,43,55,15,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 1}",0,1,3,2,1,7,3,2,0,2,0,3,0,0,0,1,0,0,0,0,10,2,1,0.6449222216793188,0.4501487378663527,0.3699604499530455
ICLR2018-H1srNebAZ-R1,Reject,"-------------------- Review updates: Rating 6 -> 7 Confidence 2 -> 4  The rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I've increased my score. ----- ---------------  I want to love this paper. The results seem like they may be very important. However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions. I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained. Unfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1. Without understanding this first result, it's difficult to decide to what extent the rest of the paper's results are to be believed. Fig 1 shows ""the histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers. "" Let's consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons? For now let's assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value. The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs. Let's collect these signs into a long list: [+1, +1, +1, -1, +1, +1, ...]. Now what do we do with this list? As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1. But we can't do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect? Further in this direction, Section 4.1 claims ""Zero partial derivatives are ignored to make the signal more clear ."" Are these zero partial derivatives of the post-relu or pre-relu? The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once). Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope?  In this case we would be excluding a large set (about half!) of the gradient values, and it didn't seem from the context in the paper that this would be desirable. It would be great if the above could be addressed. Below are some less important comments.  Sec 5.1: great results! Fig 3: This figure studies ""the first and last layers of each network"". Is the last layer really the last linear layer, the one followed by a softmax? In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant).  Or is the layer shown (e.g. ""stage3layer2"") the penultimate layer? Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from. Sec 5.2 states ""neuron partitions the inputs in two distinct but overlapping categories of quasi equal size ."" This experiment only shows that this is true in aggregate, not for specific neurons? I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct? Perhaps this statement could be qualified. Table 1: ""52th percentile vs actual 53 percentile shown"".   > Table 1: The more fuzzy, the higher the percentile rank of the threshold This is true for the CIFAR net but the opposite is true for ResNet, right?",32,728,25.103448275862068,4.6895522388059705,290,4,724,0.005524861878453,0.0214477211796246,0.9938,181,92,116,37,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 12, 'MET': 18, 'EXP': 3, 'RES': 6, 'TNF': 5, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 11, 'SUB': 11, 'CLA': 1}",0,0,0,0,12,18,3,6,5,2,0,2,0,1,0,0,2,0,1,1,11,11,1,0.5766930665801969,0.674001042746759,0.40742770240919557
ICLR2018-H1srNebAZ-R2,Reject,"The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks. This is an important area and findings in this paper are interesting! However, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments. - Could we look at the two distributions of inputs that each neuron tries to separate? - Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results). Also, a binarization experiment (and finding) similar to the one in this paper has been done here: [1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition.  2014  + Clarity: The paper is easy to read. A few minor presentation issues: - ReLu --> ReLU  + Originality:  The paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014). + Significance: While the results are interesting, the contribution is not significant as the paper misses an important explanation for the phenomenon. I'm not sure what key insights can be taken away from this.",12,203,20.3,5.415384615384616,126,0,203,0.0,0.0233644859813084,0.938,59,34,35,9,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,1,2,1,0,4,2,2,0,0,0,3,2,0,0,0,2,2,1,0,2,0,1,0.572504763977666,0.5563427121960364,0.3588840238550892
ICLR2018-H1srNebAZ-R3,Reject,"This paper presents an experimental study on the behavior of the units of neural networks. In particular, authors aim to show that units behave as binary classifiers during training and testing. I found the paper unnecessarily longer than the suggested 8 pages. The focus of the paper is confusing: while the introduction discusses about works on CNN model interpretability, the rest of the paper is focused on showing that each unit behaves consistently as a binary classifier, without analyzing anything in relation to interpretability. I think some formal formulation and specific examples on the relevance of the partial derivative of the loss with respect to the activation of a unit will help to understand better the main idea of the paper. Also, quantitative figures would be useful to get the big picture. For example in Figures 1 and 2 the authors show the behavior of some specific units as examples, but it would be nice to see a graph showing quantitatively the behavior of all the units at each layer. It would be also useful to see a comparison of different CNNs and see how the observation holds more or less depending on the performance of the network.",8,198,24.75,5.021164021164021,108,1,197,0.0050761421319796,0.0101010101010101,0.9194,54,18,28,8,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 3}",0,2,2,1,0,3,0,2,2,0,0,1,0,0,0,0,0,1,1,0,1,0,3,0.5006853909126848,0.4446067145861526,0.2844289955289871
ICLR2018-H1tSsb-AW-R1,Accept,"This paper presents methods to reduce the variance of policy gradient using an action dependent baseline. Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state. The paper: (1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free (2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines. (3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case. (4) suggests using MC marginalization and also using the average action to improve computational feasibility (5) combines the method with GAE techniques to further improve convergence by trading off bias and variance The suggested methods are empirically evaluated on a number of settings. Overall action-dependent baseline outperform state-only versions. Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate.  Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance. I find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space. In light of such results, one might change the policy space to enforce such structure. Notes: - Elaborate further on the assumption made in Eqn 9. Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state? - Eqn 11 should use simeq. - How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions? Consider expanding on that in section 4.5. - The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed). How would you train such baselines? - Figure 4 is impossible to read in print. The fonts are too small for the numbers and the legends.",22,356,20.94117647058824,5.479411764705882,175,3,353,0.0084985835694051,0.0220994475138121,0.9498,106,47,62,13,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 16, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 0}",0,1,4,0,0,16,3,1,1,1,0,1,0,0,0,0,1,0,2,0,12,0,0,0.5755953084259107,0.3402364340404094,0.2910636031318311
ICLR2018-H1tSsb-AW-R2,Accept,"In this paper, the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs, in particular when they are conditionally independent ('factored'). With the increasing focus on applying RL methods to continuous control problems and RTS type games, this is an important problem and this technique seems like an important addition to the RL toolbox. The paper is well written, the method is easy to implement, and the algorithm seems to have clear positive impact on the presented experiments. - The derivations in pages 4-6 are somewhat disconnected from the rest of the paper: the optimal baseline derivation is very standard (even if adapted to the slightly different situation situated here), and for reasons highlighted by the authors in this paper, they are not often used; the 'marginalized' baseline is more common, and indeed, the authors adopt this one as well. In light of this (and of the paper being quite a bit over the page limit)- is this material (4.2->4.4) mostly not better suited for the appendix? Same for section 4.6 (which I believe is not used in the experiments). - The experimental section is very strong; regarding the partial observability experiments, assuming actions are here factored as well, I could see four baselines  (two choices for whether the baseline has access to the goal location or not, and two choices for whether the baseline has access to the vector $a_{-i}$). It's not clear which two baselines are depicted in 5b - is it possible to disentangle the effect of providing $a_{-i}$ and the location of the hole to the baseline? (side note: it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy' ; the agent requires information to train, but is not provided the information to act. Out of curiosity, is it intended as an experiment to verify the need for better baselines? Or as a 'fair' training procedure? )  - Minor: in equation 2- is the correct exponent not t'? Also since $rho_pi$ is define with a scaling $(1-gamma)$ (to make it an actual distribution), I believe the definition of $eta$ should also be multiplied by $(1-gamma)$ (as well as equation 2).",12,365,45.625,4.914529914529915,180,3,362,0.0082872928176795,0.0402144772117962,0.9559,93,34,66,32,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,1,5,1,0,9,2,0,0,0,0,0,0,0,0,0,0,0,2,0,8,0,1,0.3595525414543791,0.3377486043010375,0.17810877248669302
ICLR2018-H1tSsb-AW-R3,Accept,"The paper proposes a variance reduction technique for policy gradient methods. The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines. The writing and organization of the paper is very well done.  It is easy to follow, and succinct while being comprehensive. The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively.  There is only one mostly minor issues with the algorithm development and the experiments need to be more polished.  For the algorithm development, there is an relatively strong assumption that z_i^T z_j   0. This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions). However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j   0 rather than z_i^T z_j approx 0. Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied. Otherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms. The paper would be a much stronger contribution, if the experiments could be improved. - More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc. - The experiment in Figure 3 seems to reinforce the influence of lambda as concluded by the Schulman et. al. paper. While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of lambda is contrasted to the state-dependent baseline. What was the goal here? - In general, the graphs are difficult to read; fonts should be improved and the graphs polished. - The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline? - It'd be great if Plot (a) and (b) in Figure 5 are swapped. Overall I think the idea proposed in the paper is beneficial. Better discussing the strong theoretical assumption should be incorporated. Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner. Particularly with some clarity on the experiments, I would be willing to increase the score. Minor comments: 1. In Equation (28) how is the optimal-state dependent baseline obtained? This should be explicitly shown, at least in the appendix. 2. The listed site for videos and additional results is not active. 3. Some typos - Section 2 - 1st para - last line: These methods are therefore usually more sample efficient, but can be less stable than critic-based methods.. - Section 4.1 - Equation (7) - missing subscript i for b(s_t,a_t^{-i}). - Section 4.2 - hat{Q} is just Q in many places.",29,467,15.064516129032258,5.355855855855856,229,4,463,0.0086393088552915,0.016359918200409,0.9881,114,53,88,39,8,4,"{'ABS': 0, 'INT': 2, 'RWK': 8, 'PDI': 2, 'DAT': 1, 'MET': 12, 'EXP': 9, 'RES': 0, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 1, 'EMP': 17, 'SUB': 0, 'CLA': 5}",0,2,8,2,1,12,9,0,3,0,0,2,0,0,0,0,0,0,4,1,17,0,5,0.5756104998511952,0.4549050104567582,0.32831591773945357
ICLR2018-H1u8fMW0b-R1,Reject,"This paper introduces a machine learning adaptation of the active inference framework proposed by Friston (2010), and applies it to the task of image classification on MNIST through a foveated inspection of images. It describes a cognitive architecture for the same, and provide analyses in terms of processing compression and confirmation biases in the model. u2013 Active perception, and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency.  The problem and its potential applications are well motivated. u2013 The perception-driven control formulation is well-detailed and simple to follow. u2013 The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling Questions and comments: u2013 While an 85% compression rate is significant, 88% accuracy on MNIST seems poor. A plot demonstrating the tradeoff of  accuracy for compression (by varying Href or other parameters) would provide a more complete picture of performance. Knowing baseline performance (without active inference) would help put numbers in perspective by providing a performance bound due to modeling choices. u2013u00a0What does the distribution of number of saccades required per recognition (for a given threshold) look like over the entire dataset, i.e. how many are dead-easy vs difficult? u2013 Steady state assumption: How can this be relaxed to further generalize to non-static scenes? u2013 Figure 3 is low resolution and difficult to read. Post-rebuttal comments:  I have revised my score after considering comments from other reviewers and the revised paper. While the revised version contains more experimental details, the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context. The paper also contains grammatical errors and is somewhat difficult to understand.  Finally, while it proposes an interesting formulation of a well-studied problem , more comparisons and analysis are required to validate the approach.",18,321,22.928571428571427,5.930463576158941,179,2,319,0.006269592476489,0.0153374233128834,0.975,95,52,55,9,11,7,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 2, 'MET': 8, 'EXP': 3, 'RES': 2, 'TNF': 2, 'ANA': 2, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 2, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 1}",0,1,2,2,2,8,3,2,2,2,1,2,0,0,0,0,2,2,2,1,4,2,1,0.787963038640412,0.7799770947200693,0.6227161860563848
ICLR2018-H1u8fMW0b-R2,Reject,"It is rather difficult to evaluate the manuscript. A large part of the manuscript reviews various papers from the active vision domain and subsequently proposes that this can directly be modeled using Friston's free energy principle, essentially, by ""analogy"", as the authors state. This extends up to page 4. I would argue, that this is quite a stretch, as the free energy principle is essentially blind to the idea of rewards and preferable states such that all tasks are essentially evaluated in terms surprise reduction. This is very much different from large part of the cited classic active vision literature. The authors furthermore introduce a simplification of the setting, i.e. that nothing changes in a scene during saccadic exploration, which is rather unusual for active vision problems.  The authors provide some detail about the actual implementation of their model, section 4, but the in depth details required at ICLR are missing. No comparisons to other gaze selection models or saliency models are given. Furthermore, the manuscript seems to suggest, that the simulation results are somehow related to human vision as it is stated: ""The model provides apparently realistic saccades, for they cover the full range of the image and tend to point over regions that contain class-characteristic pixels."" but no actual comparisons or evaluations are provided. ",7,216,19.63636363636364,5.349282296650718,131,1,215,0.0046511627906976,0.0275229357798165,0.5807,60,26,38,14,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 0}",0,0,3,2,0,3,0,1,0,1,0,0,0,0,1,0,0,2,0,0,2,2,0,0.3578635170370501,0.4452902409447989,0.19992033886926244
ICLR2018-H1u8fMW0b-R3,Reject,"In this paper, the authors present a computational framework for the active vision problem. Motivating the study biologically, the authors explain how the control policy can be learned to reduce the entropy of the posterior belief, and present an application (MNIST digit classification) to substantiate their proposal. I am not convinced about the novelty and contribution of the work. The active vision/sensing problem has been well studied and both the information theory and Bayes risk formulations have already been considered in previous works (see Najemnik and Geisler, 2005; Butko and Movellan, 2010; Ahmad and Yu, 2013). The paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision. Examples: foveate inspection the data (abstract), may allow to (motivation), tu put it clear (motivation), on contrary to animals retina (footnote 1), minimize at most the current uncertainty (perception-driven control), center an keep (fovea-based implementation), degrade te recognition (outlook and perspective). The citations are in non-standard format (section 1.2: Kalman (1960)). Overall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking. 1. J Najemnik and W S Geisler. Optimal eye movement strategies in visual search. Nature, 434(7031):387u201391, 2005. 2. N J Butko and J R Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2(2):91u2013107, 2010. 3. S Ahmad and A J Yu. Active sensing as Bayes-optimal sequential decision-making. Uncertainty in Artificial Intelligence, 2013.",8,243,12.15,5.71689497716895,151,2,241,0.0082987551867219,0.0205761316872428,0.8312,89,32,33,7,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 2}",0,1,1,1,0,3,0,0,0,0,0,1,3,0,0,1,0,0,1,0,1,0,2,0.4290965988086724,0.4445255795152985,0.24120121772631267
ICLR2018-H1uP7ebAW-R1,Reject,"The paper proposes to combine the recently proposed DenseNet architecture with LSTMs to tackle the problem of predicting different pathologic patterns from chest x-rays. In particular, the use of LSTMs helps take into account interdependencies between pattern labels. Strengths: - The paper is very well written. Contextualization with respect to previous work is adequate. Explanations are clear. Novelties are clearly identified by the authors. - Quantitative improvement with respect to the state the art. Weaknesses: - The paper does not introduce strong technical novelties -- mostly, it seems to apply previous techniques to the medical domain. It could have been interesting to know if there are more insights / lessons learned in this process. This could be of interest for a broader audience. For instance, what are the implications of using higher-resolution images as input to DenseNet / decreasing the number of layers? How do the features learned at different layers compare to the ones of the original network trained for image classification? How do features of networks pre-trained on ImageNet, and then fine-tuned for the medical domain, compare to features learned from medical images from scratch? - The impact of the proposed approach on medical diagnostics is unclear. The authors could better discuss how the approach could be adopted in practice. Also, it could be interesting also to discuss how the results in Table 2 and 3 compare to human classification capabilities, and if that performance would be already enough for building a computer-aided diagnosis system. Finally -- is it expected that the ordering of the factorization in Eq. 3 does not count much (results in Table 3)? As a non-expert in the field, I'd expect that ordering between pathologic patterns matters more.",18,276,18.4,5.446096654275093,149,1,275,0.0036363636363636,0.0316901408450704,0.9802,78,28,52,15,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 2}",0,1,2,1,2,10,0,2,1,0,0,3,0,2,0,2,0,1,0,0,7,3,2,0.6453813181004501,0.5596268958366285,0.4025334053029585
ICLR2018-H1uP7ebAW-R2,Reject,"This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images. Authors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification. Experiments are clearly described and results are significantly better compared to state of the art. The only issue with this paper is, that their proposed method, in practice is not tractable for inference on estimating probability of a single output, a task which would be critical in medical domain. Considering that their paper is titled as a work to use dependencies among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation. On the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently. To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well. That said, the paper has several positive aspects in all areas:  Originality - the paper presents first combination of DenseNets with LSTM-based output factorization, Writing clarity - the paper is very well written and clear. Quality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done, Significance - Apart from the issue of intractable inference which is arguably a large limitation of this work,",11,267,26.7,5.268199233716475,147,0,267,0.0,0.0073260073260073,0.9826,79,33,43,17,8,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 0, 'DAT': 2, 'MET': 6, 'EXP': 3, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 1}",0,2,2,0,2,6,3,4,0,0,0,1,0,1,0,1,0,1,0,0,5,3,1,0.5731611268306335,0.5582567383115704,0.3658039765865577
ICLR2018-H1uP7ebAW-R3,Reject,"Well written and appropriately structured. Well within the remit of the conference. Not much technical novelty to be found, but the original contributions are adequately identified and they are interesting on their own. My main concern (and complaint) is not technical, but application-based. This study is (unfortunately) typical in that it focuses on and provides detail of the technical modeling issues, but ignores the medical applicability of the model and results. This is exemplified by the fact that the data set is hardly described at all and the 14 abnormalities/pathologies, the rationale behind their choice and the possible interrelations and dependencies are never described from a medical viewpoint. If I were a medical expert, I would not have a clue about how these results and models could be applied in practice, or about what medical insight I could achieve. The bottom line seems to be: my model and approach works better than the other guys' model and approach, but one is left with the impression that these experiments could have been made with other data, other problems, other fields of application and they would not have not changed much ",9,189,23.625,5.1923076923076925,107,1,188,0.0053191489361702,0.0157894736842105,0.4019,42,20,34,14,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,0,0,0,2,3,1,1,0,2,0,3,0,1,1,1,0,0,1,0,5,0,1,0.5006520319641802,0.5580433852949275,0.31676659978432065
ICLR2018-H1uR4GZRZ-R1,Accept,"This paper investigates a new approach to prevent a given classifier from adversarial examples.  The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks. Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training. The proposed algorithm is clearly described. However there are issues in the presentation. In section 2-3, the problem setting is not suitably introduced. In particular one sentence that can be misleading: ""Given a classifier, one common way to generate an adversarial example is to perturb the input in direction of the gradient..."" You should explain that given a classifier with stochastic output, the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient.  The practical way in which the adversarial examples are generated is not known to the player. An adversary could choose any policy. The only thing the player knows is the best adversarial policy. In section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient. Does it come from a high variance? If it is the case, you should explain that the optimal policy of the adversary is approximated by ""fast gradient sign method"".  In comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples. SAP does not perform as well as adversarial training, but SAP could be used with a trained network. Overall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training. The presentation could be improved.",17,274,17.125,5.4106463878327,121,0,274,0.0,0.0180505415162454,-0.8238,72,29,55,15,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",0,1,10,3,0,7,5,0,0,1,0,3,0,1,0,0,2,2,2,0,7,1,1,0.5742083357393827,0.6706251794051657,0.4056809132352105
ICLR2018-H1uR4GZRZ-R2,Accept,"This paper propose a simple method for guarding trained models against adversarial attacks.  The method is to prune the network's activations at each layer and renormalize the outputs. It's a simple method that can be applied post-training and seems to be effective. n The paper is well written and easily to follow. Method description is clear. The analyses are interesting and done well. I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods. Could you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images. Would be good to know if the authors have any intuition why is that the case. There are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text. I would be interested to know more about the intuition behind the proposed method. It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method. Also would like to see some notes about computation complexity of sampling multiple times from a larger multinomial. Again I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014. Would be good to see the performance against other forms of adversarial attacks as well if they exist.",15,253,16.866666666666667,4.897540983606557,135,2,251,0.0079681274900398,0.0549019607843137,0.9504,52,35,51,14,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 3, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 3, 'EXT': 4}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 2}",0,0,8,3,0,6,3,0,1,2,0,2,3,4,0,0,1,1,0,0,5,3,2,0.6450854184526956,0.5583378733824245,0.3926107458025408
ICLR2018-H1uR4GZRZ-R3,Accept,"The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations. Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking. On one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks. On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness. For example, how does this compare to random perturbation (say, zero-mean) of the weights? This adds stochasticity as well so why and why not this work?  The authors do not give any insight in this regard. Overall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner. The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).",9,158,22.571428571428573,5.437908496732026,108,1,157,0.0063694267515923,0.025,0.9711,39,20,23,16,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 5, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,2,5,0,4,3,1,0,1,0,2,0,0,0,0,0,1,0,1,6,2,0,0.5012860740826289,0.4476609081269808,0.2814170836716694
ICLR2018-H1vCXOe0b-R1,Reject,"The paper intends to interpret a well-trained multi-class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction making. However, these discovered core units are specific to a particular class, which are retained to maintain the deep neural network's ability to separate that particular class from the other ones. Thus, these non-core units for a particular class could be core units for separating another class from the remaining ones. Consequently, the aggregation of all class-specific core units could include all hidden units of a layer. Therefore, it is hard for me to understand what's the motivation to identify the core units in a one-vs-remaining manner. At this moment, these identified class-specific core units are useful for neither reducing the size of the network, nor accelerating computation. ",6,133,19.0,5.546875,77,0,133,0.0,0.0074626865671641,0.743,37,19,21,4,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,4,1,0,4,6,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0.3585658761499065,0.1142208982853259,0.1467705686181372
ICLR2018-H1vCXOe0b-R2,Reject,"Pros - The paper proposes a novel formulation of the problem of finding hidden units   that are crucial in making a neural network come up with a certain output. - The method seems to be work well in terms of isolating a few hidden units that   need to be kept while preserving classification accuracy. n Cons - Sections 3.1 and 3.2 are hard to understand. There seem to be inconsistencies   in the notation. For example, (1) It would help to clarify whether y^b_n is the prediction score or its transformation into [0, 1]. The usage is inconsistent. (2) It is not clear how y^b_n can be expressed as sum_{k 1}^K z_{nk}f_k(x_n) in general. This is only true for the penultimate layer, and when y^b_n denotes the input to the output non-linearity. However, this analysis seems to be applied for any hidden layer and y^b_n is the output of the non-linearity unit (The new prediction scores are transformed into a scalar ranging from 0 to 1, denoted as y^b_n. ) (3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the same classifier as f(.). (4) Why is r_n called the center ? I could not understand in what sense is this the center, and of what ? It seems that the max value has been subtracted from all the logits into a softmax (which is a fairly standard operation). - The analysis seems to be about finding neurons that contribute evidence for   a particular class. This does not address the issue of understanding why the network makes a certain prediction for a particular input. Therefore this approach will be of limited use. n - The paper should include more analysis of how this method helps interpret the   actions of the neural net, once the core units have been identified. Currently, the focus seems to be on demonstrating that the classifier performance is maintained as a significant fraction of hidden units are masked. However, there is not enough analysis on showing whether and how the identified hidden units help interpret the model. n Quality The idea explored in the paper is interesting and the experiments are described in enough detail.  However, the writing still needs to be polished. Clarity The problem formulation and objective function (Section 3.1) was hard to follow. n Originality This approach to finding important hidden units is novel .  Significance The paper addresses an important problem of trying to have more interpretable neural networks. However, it only identifies hidden units that are important for a class, not what are important for any particular input. Moreover, the main thesis of the paper is to describe a method that helps interpret neural network classifiers. However, the experiments only focus on identifying important hidden units and fall short of actually providing an interpretation using these hidden units.",26,461,18.44,5.098360655737705,195,6,455,0.0131868131868131,0.0311203319502074,0.9829,118,54,97,21,7,6,"{'ABS': 0, 'INT': 6, 'RWK': 22, 'PDI': 4, 'DAT': 0, 'MET': 16, 'EXP': 12, 'RES': 2, 'TNF': 0, 'ANA': 6, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 9, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 17, 'SUB': 6, 'CLA': 3}",0,6,22,4,0,16,12,2,0,6,0,0,0,0,0,3,9,0,1,0,17,6,3,0.5068760743935645,0.677788146664089,0.3560012917025379
ICLR2018-H1vCXOe0b-R3,Reject,"The paper develops a technique to understand what nodes in a neural network are important for prediction. The approach they develop consists of using an Indian Buffet Process  to model a binary activation matrix with number of rows equal to the number of examples. The binary variables are estimated by taking a relaxed version of the  asymptotic MAP objective for this problem. One question from the use of the  Indian Buffet Process: how do the asymptotics of the feature allocation determine  the number of hidden units selected? Overall, the results didn't warrant the complexity of the method. The results are neat, but  I couldn't tell why this approach was better than others. Lastly, can you intuitively explain the additivity assumption in the distribution for p(y')",7,125,20.83333333333333,5.216666666666667,82,0,125,0.0,0.0307692307692307,0.7742,40,13,19,4,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,2,3,0,4,4,2,0,0,0,1,0,0,0,0,2,1,0,0,6,0,0,0.5012639532352849,0.3364911571559961,0.2503486988649655
ICLR2018-H1vEXaxA--R1,Accept,"Summary:   This paper proposes a multi-agent communication task where the agents learn to translate as a side-product to solving the communication task.  Authors use the image modality as a bridge between two different languages and the agents learn to ground different languages to same image based on the similarity . This is achieved by learning to play the game in both directions. Authors show results in a word-level translation task and also a sentence-level translation task. They also show that having more languages help the agent to learn better .  My comments:  The paper is well-written and I really enjoyed reading this paper. While the idea of pivot based common representation learning for language pairs with no parallel data is not new, adding the communication aspect as an additional supervision is novel. However I would encourage authors to rephrase their claim of emergent translation (the title is misleading) as the authors pose this as a supervised problem and the setting has enough constraints to learn a common representation for both languages (bridged by the image) and hence there is no autonomous emergence of translation out of need. I see this work as adding communication to improve the translation learning. Is your equation 1 correct? I understand that your logits are reciprocal of mean squared error. But don't you need a softmax before applying the NLL loss mentioned in equation 1? In current form of equation 1, I think you are not including the distractor images into account while computing the loss? Please clarify. What is the size of the vocabulary used in all the experiments? Because Gumbel Softmax doesn't scale well to larger vocabulary sizes and it would be worth mentioning the size of your vocabulary in all the experiments. Are you willing to release the code for reproducing the results? Minor comments:  In appendix C, Table 4 caption: you say target sentence is ""Trg"" but it is ""Ref"" in the table. Also is the reference sentence for skateboard example typo-free?",14,329,23.5,5.22508038585209,170,1,328,0.0030487804878048,0.0148367952522255,0.6883,94,35,63,12,10,3,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 5, 'DAT': 1, 'MET': 1, 'EXP': 6, 'RES': 1, 'TNF': 1, 'ANA': 6, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 2}",0,2,2,5,1,1,6,1,1,6,0,2,0,0,0,0,0,0,0,0,5,2,2,0.715228915234512,0.3360089746518808,0.362772106751404
ICLR2018-H1vEXaxA--R2,Accept,"-------------- Summary and Evaluation: -------------- This work present a novel multi-agent reference game designed to train monolingual agents to perform translation between their respective languages -- all without parallel corpora. The proposed approach closely mirrors that of Nakayama and Nishida, 2017 in that image-aligned text is encouraged to map to similarly to the grounded image. Unlike in this previous work, the approach proposed here induces this behavior though a multi-agent reference game.  The key distinction being that in this gamified setting, the agents sample many more descriptions from their stochastic policies than would otherwise be covered by the human ground truth. The authors demonstrate that this change results in significantly improved BLEU scores across a number of translation tasks. Furthermore, increasing the number of agents/languages in this setting seems to   Overall I think this is an interesting paper . The technical novelty is somewhat limited to a minor (but powerful) change in approach from Nakayama and Nishida, 2017; however, the resulting translators outperform this previous method. I have a few things listed in the weaknesses section that I found unclear or think would make for a stronger submission. -------------- Strengths: --------------  - The paper is fairly clearly written and the figures appropriately support the text. - Learning translation without parallel corpora is a useful task and leveraging a pragmatic reference game to induce additional semantically valid samples of a source language is an interesting approach to do so. - I'm also excited by the result that multi-agent populations tend to improve the rate of convergence and final translation abilities of these models; though I'm slightly confused about some of the results here (see weaknesses). -------------- Weaknesses: --------------  - Perhaps I'm missing something, but shouldn't the Single EN-DE/DE-EN results in Table 2 match the not pretrained EN-DE/DE-EN Multi30k Task 1 results? I understand that this is perhaps on a different data split into M1/2 but why is there such a drastic difference? - I would have liked to see some context as how these results compare to an approach trained with aligned corpora.  Perhaps a model trained on the human-translated pairs from Task 1 of Multi30k? Obviously, outperforming such a model is not necessary for this approach to be interesting, but it would provide useful context on how well this is doing. n - A great deal of the analysis and qualitative examples are pushed to the supplement which is a bit of a shame given they are quite interesting.   ",15,395,26.33333333333333,5.547683923705722,208,7,388,0.018041237113402,0.0167464114832535,0.993,103,51,69,25,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 1, 'RES': 4, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,0,4,2,1,8,1,4,2,0,0,3,0,0,0,2,0,1,2,0,4,1,1,0.5737619210490079,0.6686392154795171,0.4050892213941008
ICLR2018-H1vEXaxA--R3,Accept,"Summary: The authors show that using visual modality as a pivot they can train a model to translate from L1 to L2. Please find my detailed comments/questions/suggestions below:  1) IMO, the paper could have been written much better. At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}. There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works. IMO, this leads to unnecessary confusion and does more harm than good. For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside). n 2) Continuing on the above point, this is essentially about learning a common multimodal representation and then decode from this common representation. However, the authors do not cite enough work on such multimodal representation learning (for example, look at Spandana et. al. : Image Pivoting for Learning Multilingual Multimodal Representations, EMNLP 2017 for a good set of references) 3) This omission of related work also weakens the experimental section. At least for the word translation task many of these common representation learning frameworks could have been easily evaluated. For example, find the nearest german neighbour of the word dog in the common representation space. The authors instead compare with very simple baselines. 4) Even when comparing with simple baselines, the proposed model does not convincingly outperform them.  In particular,  the P@5 and P@20 numbers are only slightly better .   5) Some of the choices made in the Experimental setup seem questionable to me: - Why  use a NMT model without attention? That is not standard and does not make sense to use when a better baseline model (with attention) is available ? - It is mentioned that While their model unit-normalizes the output of every encoder, we found this to consistently hurt performance, so do not use normalization for fair comparison with our models.  I don't think this is a fair comparison. The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance. Please mention the numbers with unit normalization to give a better picture. It does not make sense to weaken an existing baseline and then compare with it. 6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph . This again leads to poor readability and it is hard to read and compare the corresponding numbers from Table 1. I am not sure why this cannot be accommodated in the Table itself. 7) In Figure 2, what exactly do you mean by Results are averaged over 30 translation scenarios. Can you please elaborate ?",25,495,19.8,5.1400437636761485,216,1,494,0.0020242914979757,0.0137524557956778,0.9255,115,54,97,34,9,5,"{'ABS': 1, 'INT': 2, 'RWK': 9, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 2, 'TNF': 4, 'ANA': 7, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 8, 'PNF': 3, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 3}",1,2,9,1,0,6,3,2,4,7,0,0,0,0,0,0,0,8,3,0,4,1,3,0.6452238292163978,0.558526973749729,0.40926865417979924
ICLR2018-H1vrqSxAb-R1,Reject,"The paper proposes a large experimental analysis with the goal of evaluating the generalization capabilities of CNNs. Specifically the analysis involves three datasets and two visual domains for each dataset: besides the original version of each image a new version is created by inverting its colors, i.e. simply rescaling the color channels in [0,1] and then applying (1-pixel_value). + Several possible combinations between datasets and domains are considered to evaluate the network behaviour. The analysis is also performed by varying the network architectures, considering data augmentation and/or fine tuning. - The text is confused in several points and the final overall conclusions are not fully clear. Some experimental settings are well defined to shed light on few generalization aspects of the networks. Other do not add a significant novelty and their contribution is not clear. Deatailed comments: 1) at the end of page 2 most conventional data transformation only introduce slight variations...limited in evaluating  the generalization capabilities. Conventional data transformations are introduced for data augmentation without supposing the existance of a significand domain shift between training and test data. If this shift exists (as for the case of negative images) it is possible to refer to the extensive deep domain adaptation literature. 2) With reference to the previous point, the experiment 1 in Figure 2 provides a standard example of domain shift. The fact that 1-layer softmax and 2-layers MLP perform worse than VGG is not surprising, I do not see it as an  interesting contribution. It would it make more sense if the comparison was between VGG and ResNet or other different deep structures. 3) Also the results of experiments 2 and 3 in figure 2 are not surprising. In my understanding, in experiment 2 the network, while learning to recognize the numbers, it also learns to be invariant to color thanks to a tailored data augmentation and the good final results are expected. In experiment 3 instead, the defined setting is supposed to give imporance to colors, as stated at the end of page 4. However, there is no reason to automatically expect that this will decrease the importance of shape. Every category  is defined by a specific combination of color and shape that is well recognized at test time. 4) the experiments in figure 3 show that the tailored data augmentation can be done even for a subset of the classes and still work well for all of them. It would be interesting to investigate the limits of this statement: what would happen by augmenting only 8 or 7 or 6 or 5... categories instead of 9? 5) the experiment in section 6.1, figure 5 is just slightly different from that in figure 3. I would suggest to  put the two together since they both demonstrate that the network can learn to be invariant to a certain domain aspect as far as data augmentation is used to cover that aspect for at least a part of the observed categories. 6) I do not see any novel contribution in the analysis of the batch normalization (end of section 5): bn has been  previously used for  domain adaptation Revisiting Batch Normalization For Practical Domain Adaptation, arXiv:1603.04779 AutoDIAL: Automatic DomaIn Alignment Layers, ICCV 2017 7) the experiment in section 6.4 should be presented as an extreme case of that of figure 7. So the two can be presented together in the same section. Dividing them makes more complicated to  draw general conclusions from this particular data augmentation setting. 8) the fine-tuning experiments do not bring significant novelty. In figure 8, my interpretation of (a) is that the initial model has learned to be invariant to color and this remains true even if the fine-tuning data do not contain any negative data. Given this conclusion, I would have expected a discussion about  the difference between learning with all data at the same time or with fine-tuning in two different  steps. Morever this fine-tuning experiment needs more details: is it based only on parameter initialization or there are some fully frozen network layers? This work shows few interesting results but the paper is not easy to read, the presentation is sparse  and the bit and pieces of information do not allow to derive strong final conclusions.   ",32,702,22.64516129032258,5.23206106870229,293,0,702,0.0,0.0223152022315202,0.8258,185,79,118,42,9,7,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 4, 'MET': 5, 'EXP': 17, 'RES': 2, 'TNF': 9, 'ANA': 6, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 3, 'PNF': 6, 'REC': 0, 'EMP': 12, 'SUB': 3, 'CLA': 2}",0,0,2,1,4,5,17,2,9,6,0,2,0,0,0,2,2,3,6,0,12,3,2,0.6458369053911792,0.7855491116149101,0.5096788965682203
ICLR2018-H1vrqSxAb-R2,Reject,"Pros:  The paper is clearly written and studies an interesting problem. The transferrability of features described in Section 6 is interesting. Cons:  1. The title ""assessing the generalization capability of CNNs"" is too broad, as the paper is only studying a narrow aspect of generalization. 2. One of the conclusions of the paper is: ""Although CNNs do not intrinsically classify objects based on their shapes, they can learn to do so when trained with enough number of images with the same shape and different colors. "" The conclusion is obvious when we train the network to make it invariant to colors and textures. If the emphasis of this conclusion is on  the number of images needed, then it will be good to show more analysis on Figure 6: e.g. Why does the accuracy curve drops to zero before going up? Are the negative images messing up the training when the number is between 10^1 and 10^2? 3. Negative images are fast to obtain, but they are oversimplified, and can be obtained via linear transform which is easy for neural networks. Therefore it is not very convincing whether the conclusion applies to other types of data, e.g. train/test on RGB&Gray image pairs, which are more commonly seen. Larger scale experiments on ImageNet is also recommended to show how general the conclusion of the paper is. 4. The transferrability of features in Section 6 is an interesting problem to explore. It could provide more insights to practical problems if more experiments were done: e.g. can this technique help domain adaptation?  ",12,257,15.117647058823527,4.91497975708502,140,0,257,0.0,0.0190114068441064,0.5828,67,25,49,15,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 3, 'EXP': 5, 'RES': 3, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 5, 'CLA': 1}",0,0,0,1,3,3,5,3,1,2,0,1,0,0,0,0,0,0,0,0,5,5,1,0.5724820434687976,0.3362478691059911,0.28922873107383285
ICLR2018-H1vrqSxAb-R3,Reject,"The paper presents a series of empirical studies of the generalization ability of convolutional neural networks (CNNs) applied to image recognition tasks related to shape images.  The paper makes a number of observations about the learning or transfer learning of shape bias vs. color by controlling and manipulating the input of training data based on shapes, negative shapes, and random images. A concluding observation is that CNN models learn and generalize the structure content of images. The work is described in sufficient detail including the experimental setups, data set, neural networks, and results. The experiments should be reproducible given the descriptions in the paper. The overall significance of the results is not very strong since the paper focuses solely on experiments conducted on very limited data and artificially manipulated data. It is not clear how the observations made from the experiments generalize beyond these specific learning tasks or how one may take advantage of them in practice. One way to greatly improve the impact of the paper would be to take the observations made from the simulated data experiments (e.g., MNIST) and use them to make changes to how CNN training is done on another real task (e.g., ImageNet) and show improvements in performance.",8,204,22.666666666666668,5.268656716417911,114,1,203,0.0049261083743842,0.0292682926829268,-0.6489,68,17,35,7,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 4, 'MET': 2, 'EXP': 7, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,0,0,1,4,2,7,2,0,0,0,0,0,0,0,0,1,0,0,0,4,2,0,0.3580741981192838,0.3353058821461837,0.17889120212738618
ICLR2018-H1wt9x-RW-R1,Reject,"This is a well written paper on a compelling topic: how to train an automated teacher to use intuitive strategies  that would also apply to humans. The introduction is fairly strong, but this reviewer wishes that the authors would have come up with an intuitive example that illustrates why the strategy 1) train S on random exs; 2) train T to pick exs for S makes sense. Such an example would dramatically improve the paper's readability. The paper appears to be original, and the related work section is quite extensive. A second significant improvement would be to add an in-depth  running example in section 3, so that the authors could illustrate why the BR strategy makes sense (Algorithm 2).",5,119,23.8,5.064220183486238,70,2,117,0.017094017094017,0.024793388429752,0.952,32,12,20,7,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,2,1,0,1,0,0,0,0,0,1,0,1,0,1,1,0,0,0,1,0,1,0.4286519164738125,0.4444444444444444,0.23991777538600573
ICLR2018-H1wt9x-RW-R2,Reject,"The authors define a novel method for creating a pair of models, a student and a teacher model, that are co-trained in a manner such that the teacher provides useful examples to the student to communicate a concept that is interpretable to people. They do this by adapting a technique from computational cognitive science called rational pedagogy.  Rather than jointly optimize the student and teacher (as done previously), they have form a coupled relation between the student and teacher where each is providing a best response to the other.  The authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teaching. Understanding how to make complex models interpretable is an extremely important problem in ML for a number of reasons (e.g., AI ethics, explainable AI).  The approach proposed by the authors is an excellent first step in this direction, and they provide a convincing argument for why a previous approach (joint optimization) did not work. It is an interesting approach that builds on computational cognitive science research and the authors provide strong evidence their method creates interpretable examples. They second part of their article, where they test the examples created by their models using behavioral experiments was less convincing.  This is because they used the wrong statistical tests for analyzing the studies and it is unclear whether their results would stand with proper tests (I hope they will! u2013 it seems clear that random samples will be harder to learn from eventually, but I also hoped there was a stronger baseline.). For analysis, the authors use t-tests directly on KL-divergence and accuracy scores; however, this is inappropriate (see Jaeger, 2008; Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434-446.). This is especially applicable to the accuracy score results and the authors should reanalyze their data following the paper referenced above. With respect to KL-divergence, a G-test can be used (see https://en.wikipedia.org/wiki/G-test#Relation_to_Kullback.E2.80.93Leibler_divergence). I suspect the results will still be meaningful, but the appropriate analysis is essential to be able to interpret the human results. Also, a related article: One article testing rational pedagogy in more ML contexts and using it to train ML models that is Ho, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016). Showing versus Doing. Teaching by Demonstration. For future work, it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks, which motivated the current push for explainable AI in the first place.",17,446,21.23809523809524,5.504784688995216,225,1,445,0.0022471910112359,0.0199556541019955,0.9874,124,66,77,18,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 7, 'DAT': 0, 'MET': 6, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 3, 'FWK': 1, 'OAL': 0, 'BIB': 5, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,0,9,7,0,6,4,3,0,3,1,0,5,0,1,1,2,0,0,0,7,0,1,0.5741482363288277,0.5593353368130614,0.3622256271665551
ICLR2018-H1wt9x-RW-R3,Reject,"This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy. The teacher's strategy should also be  learned from data. In this case the authors look at finding interpretable teaching strategies. The authors define the good strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans. The suggested method follow an iterative process in which the student and teacher are interchangeably used. At each iteration the teacher generates  examples based on the students current concept. I found it very difficult to follow the claims in the paper. Why is it assumed that human intuition is necessarily good?  The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition. There are not enough details to get a good grasp of the suggested method and the different choices for it,  and similarly the experiments are not described in a very convincing way. Specifically - the domains picked seem very contrived,  there actual results are not reported, the size of the data seems minimal so it's not clear what is actually learned. How would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing.",12,232,21.09090909090909,5.238938053097345,126,1,231,0.0043290043290043,0.0378151260504201,0.8563,56,28,44,16,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 12, 'DAT': 1, 'MET': 0, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,1,3,12,1,0,2,1,0,1,0,0,0,0,0,0,2,0,0,0,6,0,1,0.50089947007689,0.3364911571559961,0.2484102103111252
ICLR2018-H1xJjlbAZ-R1,Reject,"The authors study cases where interpretation of deep learning predictions is extremely fragile. They systematically characterize the fragility of several widely-used feature-importance interpretation methods. In general, questioning the reliability of the visualization techniques is interesting. Regarding the technical details, the reviewer has the following comments:   - What's the limitation of this attack method? - How reliable are the interpretations? - The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity. - Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting.  ",9,123,15.375,6.360655737704918,90,0,123,0.0,0.0229007633587786,-0.2031,44,14,23,3,5,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,0,1,0,5,3,1,0,0,0,0,0,1,0,0,0,0,0,0,4,0,0,0.3583160171814131,0.11297698341564,0.14424187047936535
ICLR2018-H1xJjlbAZ-R2,Reject,"The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output. Thus the authors claim that one has to be careful about using feature importance maps. Pro:  The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes. -  The examples in the paper seem to be cherry picked to illustrate dramatic effects. The experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs. The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior.",6,145,20.714285714285715,5.659574468085107,96,1,144,0.0069444444444444,0.0533333333333333,0.91,49,24,25,3,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 0, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 0}",0,0,1,2,0,0,2,1,0,2,0,0,0,0,0,0,0,1,0,0,1,2,0,0.3573130721770378,0.3334400098416548,0.17394149844818674
ICLR2018-H1xJjlbAZ-R3,Reject,"The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs). The authors perturb input images and create explanations using different methods. Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps. This is true even for random perturbations. The images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind. Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change. For one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability. Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation. In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer. It can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent. The explanation maps mirror for the considered methods the model's reaction to the input. They are thus not meaningless, but are a measure to model reaction instead of an independent process. The excellent Figure 2 supports this point. Not the interpretation itself is fragile, but the model. Adding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane. The fragility of DNN models to marginally perturbed inputs themselves is well known. This especially true for adversial perturbations, which have been used as test cases in this work. The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches. The authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps. In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method. Further do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model. It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input. Further remarks: It would be interesting to see the size and position of the center of mass attacks in the appendix. The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations. The reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the DeepLift method. Can bugs and suboptimal configurations be ruled out during the experiments? The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point). However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis. Have the authors considered training the net with small random perturbations added to the samples, to compare the vanilla model to the more robust one, which has seen noisy samples, and compared explanations? Why not train (finetune) the considered models using softplus activations instead of exchanging activation nodes? Appendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation Conclusion: The paper follows an interesting approach, but ultimately takes the wrong view point: The authors try to attribute fragility to explaining methods, which visualize/measure the reaction of the model to the perturbed inputs. A major rework should be considered.",34,752,22.78787878787879,5.313432835820896,304,0,752,0.0,0.0079787234042553,0.9469,235,69,114,49,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 25, 'EXP': 3, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,0,1,1,0,25,3,0,1,1,0,1,0,2,0,0,0,3,0,0,10,1,0,0.5776149018109777,0.3391652753613,0.2959214178191184
ICLR2018-H1zRea1Mf-R1,,"This paper proposed an end-to-end network that generates computer tokens from a single GUI screenshot as input.  Even though the introduced dataset in this paper is interesting, there are some issues: - On the model side, this paper used the same architecture as the Karpathy & Fei-Fei (2015). As a result, in my view, the paper has limited novelty and originality. - Experiments: The experiments are not enough at all, More specifically, the paper didn't establish any baseline to show the difficulty of this problem and the dataset. Without a reasonable baseline, it is hard to see how difficult is this dataset and this problem and can't say anything about the significance of this problem in this paper. - The related works: Some recent papers in program synthesis are missing and should have been included in this paper such as: RobustFill: Neural Program Learning under Noisy I/O, ICML 2017 Some other comments: - In the last paragraph of section 3 and first paragraph section 3.1, the authors mentioned that ...CNN to perform unsupervised feature learning... . In my view, it is not a correct statement to call the feature extraction from a CNN unsupervised feature learning as the referred CNNs in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as well.  Unfortunately, At this point, I do not see a sufficient contribution to warrant publication in ICLR.",9,232,25.77777777777778,5.113636363636363,122,0,232,0.0,0.0333333333333333,-0.9205,71,23,33,12,9,5,"{'ABS': 0, 'INT': 3, 'RWK': 6, 'PDI': 3, 'DAT': 4, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 5, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 4, 'CLA': 0}",0,3,6,3,4,4,2,2,0,1,0,0,2,0,0,1,5,1,0,0,4,4,0,0.644442359201695,0.5579336039788404,0.4006089057916578
ICLR2018-H1zRea1Mf-R2,,"The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface . Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time . Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released. Pros: - Generating programs with neural networks is an exciting direction - Novel task of generating UI code from UI screenshots - Three new datasets of UI images and corresponding code n- Paper is clearly written n Cons: - Limited technical novelt y - Limited experiments I agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research. Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest. The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task .  My main concern with this paper is a lack of technical novelty. The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now. Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand. The experiments in this paper are also lacking. Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments . The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples. I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods. I would have also liked to see more innovation in evaluation. Are there metrics other than overall accuracy that could be used to measure performance?  Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task? In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics? Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases? How does performance change with different amounts of training data ? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another ? I don't expect the authors to answer any of these questions in particular; I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset. On the whole I appreciate the novelty of the task and dataset, but the paper suffers from a lack of technical novelty in the model and limited experimental validation.",30,499,31.1875,5.239495798319328,208,0,499,0.0,0.0078125,0.9194,143,65,88,16,9,7,"{'ABS': 0, 'INT': 9, 'RWK': 4, 'PDI': 11, 'DAT': 9, 'MET': 11, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 5, 'IMP': 9, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 15, 'SUB': 1, 'CLA': 1}",0,9,4,11,9,11,6,1,0,1,1,0,0,0,0,5,9,2,0,1,15,1,1,0.6471173826329139,0.7871670679484251,0.5189887839315418
ICLR2018-H1zRea1Mf-R3,,"This paper proposes to use a hybrid of convolutional and recurrent networks to predict the DSL specification of a GUI given a screenshot of the GUI. n  Pros:   The paper is clear and the proposed problem is novel and well-defined. The training data is synthetic, allowing for arbitrarily large training sets to be generated. The authors have made their synthetic dataset publicly available. n The method seems to work well based on the samples and ROC curves presented. Cons:  This is mostly an application of an existing method to a new domain - - as stated in the related work section, effectively the same convnet+RNN architecture has been in common use for image captioning and other vision applications. The UIs that are represented in the dataset seem quite simple; it's not clear that this will transfer to arbitrarily complex and multi-page UIs. The main motivation for the proposed system seems to be for non-technical designers to be able to implement UIs just by drawing a mockup screenshot .  However, the paper hasn't shown that this is necessarily possible assuming the hand-designed mockups aren't pixel-for-pixel matches with a screenshot that could be generated by the ""DSL code -> screenshot"" mapping that this system learns to invert. There exist a number of ""drag and drop"" style UI design products (at least for HTML) that would seem to accomplish the same basic goal as the proposed system in a more reliable way. (Though the proposed system does have the advantage of only requiring a screenshot created using any software, rather than being restricted to a particular piece of software.) Overall, the paper is well-written but the novelty and applicability seems a bit limited.",13,276,25.09090909090909,5.17490494296578,158,3,273,0.0109890109890109,0.0491228070175438,0.3834,71,37,55,16,7,7,"{'ABS': 0, 'INT': 3, 'RWK': 4, 'PDI': 5, 'DAT': 3, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 2}",0,3,4,5,3,5,1,0,0,1,0,0,0,0,0,2,4,1,1,0,7,2,2,0.5016026509481315,0.7818865514958729,0.38397291596817124
ICLR2018-H1zriGeCZ-R1,Accept,"This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis. The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter. In the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search . Their result also has an application in learning decision trees where it significantly improves the sample complexity bound. n The main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong. However, I find this result to be out of the context with the main theme of the paper. I find it highly unlikely that a person interested in using Harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complexity. Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments .  When it comes to the experiments only one real-world experiment is present . It is hard to conclude which method is better based on a single real-world experiment . Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time. I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations. If this is the case, some configurations could easily require more time to evaluate than the others. It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots. It is not clear what is the stopping criterion for each of the methods used in the experiments . One weakness of Harmonica is that it has 6 hyperparameters itself to be tuned.  It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods. Few more questions:  Which problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?  How do you find best t minimizers of g_i in line 4 of Algorithm 3? ",19,388,21.55555555555556,5.1462765957446805,177,1,387,0.0025839793281653,0.0175879396984924,0.987,97,53,65,15,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 1, 'DAT': 3, 'MET': 13, 'EXP': 8, 'RES': 6, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 7, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 2, 'CLA': 0}",0,2,2,1,3,13,8,6,0,5,0,1,0,0,0,0,7,4,0,0,16,2,0,0.6470183940620183,0.4545201900376677,0.36845072105522947
ICLR2018-H1zriGeCZ-R2,Accept,- algorithm 1 has a lot of problem specific hyperparametes that may be difficult to get right . Not clear how important they are - they analyze the simpler (analytically and likely computationally) Boolean hyperparameter case (each hyperparameter is binary). Not a realistic setting. In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials. - interesting idea but I think it's more theoretical than practical. Feels like a hammer in need of a nail.,6,86,14.333333333333334,5.518987341772152,66,3,83,0.036144578313253,0.0888888888888888,0.238,20,15,15,9,3,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 1, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,0,0,4,0,1,4,0,0,0,0,0,0,0,0,1,1,0,0,0,5,1,1,0.2147222012464076,0.5580433852949275,0.12627973963028738
ICLR2018-H1zriGeCZ-R3,Accept,"The paper is about hyperparameter optimization, which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithms .  At a high-level, hyperparameter optimization (for the challenging case of discrete variables) can be seen as a black-box optimization problem where we have only access to a function evaluation oracle (but no gradients etc.). In the entirely unstructured case, there are strong lower bounds with an exponential dependence on the number of hyperparameters . In order to sidestep these impossibility results, the current paper assumes structure in the unknown function mapping hyperparameters to classification accuracy.  In particular, the authors assume that the function admits a representation as a sparse and low-degree polynomial . While the authors do not empirically validate whether this is a good model of the unknown function, it appears to be a reasonable assumption (the authors *do* empirically validate their overall approach). Based on the sparse and low-degree assumption, the paper introduces a new algorithm (called Harmonica) for hyperparameter optimization. The main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements (i.e., function evaluations). The authors derive relevant sample complexity results for their approach. Moreover, the method also yields new algorithms for learning decision trees. In addition to the theoretical results , the authors conduct a detailed study of their algorithm on CIFAR10 . They compare to relevant recent work in hyperparameter optimization (Bayesian optimization, random search, bandit algorithms) and find that their method significantly improves over prior work.  The best parameters found by Harmonica improve over the hand-tuned results for their base architecture (ResNets). n Overall, I find the main idea of the paper very interesting and well executed, both on the theoretical and empirical side.  Hence I strongly recommend accepting this paper. Small comments and questions :  1. It would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial (e.g., MSE of the best fit) .  2. A comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem. 3. The current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques (e.g., Zoph & Le, ICLR 2017). While it might be hard to compare to this approach directly in experiments, it would still be good to mention this work and discuss how it relates to the current paper. 4. Did the authors tune the hyperparameters directly using the CIFAR10 test accuracy? Would it make sense to use a slightly smaller training set and to hold out say 5k images for hyperparameter evaluation before making the final accuracy evaluation on the test set? The current approach could be prone to overfitting .  5. While random search does not explicitly exploit any structure in the unknown function, it can still implicitly utilize smoothness or other benign properties of the hyperparameter space. It might be worth adding this in the discussion of the related work. n 6. Algorithm 1: Why is the argmin for g_i  (what does the index i refer to)?  7 . Why does PSR truncate the indices in alpha? At least in standard compressed sensing, the Lasso also has recovery guarantees without truncation (and empirically works sometimes better without). n 9. Definition 3: Should C be a class of functions mapping {-1, 1}^n to R?  (Note the superscript.)  10. On Page 3 we assume that K   1, but Theorem 6 still maintains a dependence on K . It might be cleaner to either treat the general K case throughout, or state the theorem for K   1.  11. On CIFAR10, the best hyperparameters do not improve over the state of the art with other models (e.g., a wide ResNet) . It could be interesting to run Harmonica in the regime where it might improve over the best known models for CIFAR10.  12 . Similarly, it would be interesting to see whether the hyperparameters identified by Harmonica carry over to give better performance on ImageNet. The authors claim in C.3 that the hyperparameters identified by Harmonica generalize from small networks to large networks. Testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as well.",36,704,17.170731707317074,5.539170506912442,281,6,698,0.0085959885386819,0.0136239782016348,0.999,198,89,106,31,9,6,"{'ABS': 0, 'INT': 5, 'RWK': 17, 'PDI': 17, 'DAT': 1, 'MET': 11, 'EXP': 14, 'RES': 7, 'TNF': 0, 'ANA': 17, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 6, 'CMP': 10, 'PNF': 0, 'REC': 2, 'EMP': 18, 'SUB': 5, 'CLA': 0}",0,5,17,17,1,11,14,7,0,17,0,0,1,0,0,3,6,10,0,2,18,5,0,0.6495480106784172,0.6790713109904336,0.46485432903292184
ICLR2018-HJ1HFlZAb-R1,Reject,"The main idea is to use the accuracy of a classifier trained on synthetic training examples produced by a generative model to define an evaluation metric for the generative model.  Specifically, compare the accuracy of a classifier trained on a noise-perturbed version of the real dataset to that of a classifier trained on a mix of real data and synthetic data generated by the model being evaluated. Results are shown on MNIST and Fashion MNIST. The paper should discuss the assumptions needed for classifier accuracy to be a good proxy for the quality of a generative model that generated the classifier's training data. It may be the case that even a bad generative model (according to some other metric) can still result in a classifier that produces reasonable test accuracy. Since a classifier can be a highly nonlinear function, it can potentially ignore many aspects of its input distribution such that even poor approximations (as measured by, say, KL) lead to similar test accuracy as good approximations. The sensitivity of the evaluation metric defined in equation 2 to the choice of hyperparameters of the classifier and the metric itself (e.g., alpha) is not evaluated.  Is it possible that a different choice of hyperparameters can change the model ranking ? Should the hyperparameters be tuned separately for each generative model being evaluated? The intuition behind comparing against a classifier trained on a noise-perturbed version of the data is not explained clearly.  Why not compare a classifier trained on only (unperturbed) real data to a classifier trained on both real and synthetic data? Evaluation on two datasets is not sufficient to provide insight into whether the proposed metric is useful. Other datasets such as ImageNet, Cifar10/100, Celeb A, etc., should also be included.",13,290,29.0,5.376383763837638,127,1,289,0.0034602076124567,0.0340136054421768,-0.4936,72,38,48,15,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 5, 'DAT': 9, 'MET': 12, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 6, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 6, 'CLA': 1}",0,0,3,5,9,12,0,2,0,6,0,0,0,0,0,0,2,4,0,0,13,6,1,0.4321897417901895,0.5639519516352964,0.2701029155610224
ICLR2018-HJ1HFlZAb-R2,Reject,"The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only. Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution. In addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for . If the aim is to have realistic samples, a visual turing test is probably the best metric. If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option. PROS: The idea is interesting.  CONS: 1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction). It would be interesting to understand how the different metrics relate . Moreover, the new metric is introduced with the following motivation ""[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution"".  The mode collapse issue is never discussed elsewhere in the paper.  n 2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays. Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start.  3. The authors should clarify if the method is specifically designed for GANs and VAEs . If not, section 2.1 should contain several other works (as in Table 1).   4 . One of the main statements of the paper ""Our approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)"" is never proved, nor discussed. n 5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive.  Further, how many tau's one should evaluate? In order to evaluate a generative model one should test on the generated data only (tau 1) I believe. In the worst case, the generator experiences mode collapse and performs badly. Differently, it can memorize the training data and performs as good as the baseline model.  If it does actual data augmentation, it should perform better .  6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models. In fact, the limit of training with a fixed dataset is that the model 'sees' the data multiple times across epochs with the risk of memorizing . In the proposed protocol, the model 'sees' the generated data D_gen (which is fixed before training) multiple time across epochs.  This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability. Minor:  Section 2.2 might be more readable it divided in two (exploitation and evaluation).",26,529,17.06451612903226,5.147117296222664,230,2,527,0.0037950664136622,0.0329670329670329,0.9,148,57,96,31,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 14, 'PDI': 14, 'DAT': 12, 'MET': 15, 'EXP': 15, 'RES': 0, 'TNF': 0, 'ANA': 10, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 5, 'IMP': 8, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 23, 'SUB': 14, 'CLA': 1}",0,0,14,14,12,15,15,0,0,10,0,0,0,0,0,5,8,3,0,0,23,14,1,0.4357175727478268,0.6824875368329789,0.3068977924054532
ICLR2018-HJ3d2Ax0--R1,Reject,"This paper investigates an effect of time dependencies in a specific type of RNN. The idea is important and this paper seems sound . However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently. --Main comment About the deep network case in Theorem 1, how $L$ affects the bound on ranks? In the current statement, the result seems independent to $L$ when $L geq 2$. I think that this paper should quantify the effect of an increase of $L$. --Sub comment Numerical experiments for calculating the separation rank is necessary to provide evidence of the main result. Only a simple example will make this paper more convincing.",8,113,16.142857142857142,5.009803921568627,67,3,110,0.0272727272727272,0.0789473684210526,0.6287,32,19,15,5,9,3,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 0}",0,2,5,1,0,1,3,3,0,1,1,0,0,1,0,0,4,0,0,0,2,2,0,0.6434884087648305,0.3342060772218415,0.30556506140012757
ICLR2018-HJ3d2Ax0--R2,Reject,"After reading the authors's rebuttal I increased my score from a 7 to a 6. I do think the paper would benefit from experimental results , but agree with the authors that the theoretical results are non-trivial and interesting on their own merit. ------------------------ The paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l} )  The work is inspired by previous results for feed forward nets and CNNs . However, what is unique to RNNs is their ability to model long term dependencies across time. To analyze this specific property, the authors propose a concept called start-end rank that essentially models the richness of the dependency between two disjoint subsets of inputs . Specifically, let S   {1, . . . , T/2} and E     {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points. Specifically sep_{S,E}(y)   K means there exists g_s^k and g_e^k for k 1...K such that y(x)   sum_{k} g_s^k(x_S) g_e^k(x_E) .  Therefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E).  If sep_{S,E} 1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution) . A higher rank would correspond to more dependence across time .   (Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank, since I think it much makes it clearer to explain . Right now the authors explain separation rank first and then discuss tensors / matricization). Using this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs. I overall like the paper's theoretical results, but I have the following complaints:  (1)  I have the same question as the other reviewer. Why is Theorem 1 not a function of L? Do the papers that prove similar theorems about ConvNets able to handle general L ? What makes this more challenging? I feel if comparing L 2 vs L 3 is hard, the authors should be more up front about that in the introduction/abstract .  (2) I think it would have been stronger if the authors would have provided some empirical results validating their claims.   ",23,386,14.846153846153848,5.208823529411765,193,3,383,0.0078328981723237,0.0210280373831775,0.9964,128,55,63,19,10,4,"{'ABS': 1, 'INT': 3, 'RWK': 13, 'PDI': 4, 'DAT': 3, 'MET': 9, 'EXP': 14, 'RES': 5, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 1}",1,3,13,4,3,9,14,5,0,4,0,0,0,3,0,0,4,0,1,0,13,0,1,0.7189903920897981,0.4520520436079038,0.40702393862656283
ICLR2018-HJ3d2Ax0--R3,Reject,"The paper proposes to use the start-end rank to measure the long-term dependency in RNNs. It shows that deep RNN is signficantly better than shallow one in this metric. The theory part seems to be technical enough and interesting, though I haven't checked all the details . The main concern with the paper is that I am not sure whether the RAC studied by the paper is realistic enough for practice.  Certain gating in RNN is very useful but I don't know whether one can train any reasonable RNN with all multiplicative gates . The paper will be much stronger if it has some experiments along this line. ",7,106,15.142857142857142,4.660194174757281,73,1,105,0.0095238095238095,0.1,0.8108,21,18,20,7,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,0,6,1,0,4,3,1,0,0,0,0,0,2,0,0,2,0,0,0,4,1,0,0.4299061640762769,0.3352472422863101,0.20009939219550915
ICLR2018-HJ4IhxZAb-R1,Reject,"The approach solves an important problem as getting labelled data is hard. The focus is on the key aspect, which is generalisation across heteregeneous data. The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.  Pros:  1. The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on. 2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets, which isnt very common in most meta-learning papers today, so it's encouraging that the method works in that regime.  Cons:  1. The embedding strategy, especially the representative and discriminative histograms, is complicated. It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images . More evidence in the paper for why it would work on harder problems would be great. 2. The policy network would have to output a probability for each datapoint in the dataset U, which could be fairly large, thus the method is computationally much more expensive than random sampling. A section devoted to showing what practical problems could be potentially solved by this method would be useful. 3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM,  or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner. Significance/Conclusion: The idea of meta-learning or learning to learn is fairly common now . While they do show good performance, it's unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks .   Comments: There's lots of typos, please proof read to improve the paper. Revision: I thank the authors for the updates and addressing some of my concerns. I agree the computational budget makes sense for cross data transfer, however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks . I update my review to 6",22,346,15.727272727272728,5.172727272727273,181,3,343,0.0087463556851311,0.0307262569832402,0.9606,95,36,63,17,10,8,"{'ABS': 0, 'INT': 1, 'RWK': 17, 'PDI': 6, 'DAT': 10, 'MET': 9, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 2, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 2, 'EMP': 11, 'SUB': 3, 'CLA': 2}",0,1,17,6,10,9,4,2,1,2,0,0,0,1,1,2,2,0,1,2,11,3,2,0.7185353822509073,0.8955158960299467,0.6276637467507702
ICLR2018-HJ4IhxZAb-R2,Reject,"This reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements: 1) you should include in your comparison Query-by- Bagging & Boosting, which are two of the best out-of-the-box active learning strategies n2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones but many questions are still unanswered: -  would any 7-7 split work just as well (ie, cross-validate over the 14 domains) - do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains? are the results significantly different? OTHER COMMENTS: - p3: both images in Figure 1 are labeled Figure 1.a - p3: typo theis --> this   Abe & Mamitsuksa (ICML-1998) . Query Learning Strategies Using Boosting and Bagging.",8,121,60.5,5.59047619047619,78,0,121,0.0,0.0227272727272727,0.9759,33,16,20,7,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 3, 'MET': 1, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,0,4,2,3,1,5,0,1,0,0,0,1,0,0,0,2,1,1,0,2,0,0,0.5007177282156294,0.4451144385277353,0.2810210819833231
ICLR2018-HJ4IhxZAb-R3,Reject,"Overview  The authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous datasets. The reinforcement learning part is based on a policy network, which selects the data instance to be labeled next. They use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation. The authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myopically. The experiments show the proposed approach is effective on 14 UCI datasets .  strength  * The paper is mostly clear and easy to follow . * The overall idea is interesting and has many potentials. * The experimental results are promising on multiple datasets. * There are thorough discussion with related works .  weakness  * The graph in p.3 don't show the architecture of the network clearly. * The motivation of using feature histograms as embedding is not clear . * The description of the 2-D histogram on p.4 is not clear. The term posterior value sounds ambiguous . * The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non-linear learners. Also, the experiments takes a fixed 20K iterations for training, and the convergence status (e.g. whether the accumulated gradient has stabilized the policy) is not clear. * Are there particular reasons in using policy learning instead of other reinforcement learning approaches ? * The term A(Z) in the objective function can be more clearly described . * While many loosely-related works were surveyed, it is not clear why literally none of them were compared. There is thus no evidence on whether a myopic bandit learner (say, Chu and Lin's work) is really worse than the RL policy. There is also no evidence on whether adaptive learning on the fly is needed or not. * In Equation 2, should there be a balancing parameter for the reconstruction loss? * Some typos     - page 4: some duplicate words in discriminative embedding session     - page 4: auxliary -> auxiliary     - page 7: tescting -> testing  ",22,320,15.238095238095235,5.549668874172186,172,1,319,0.0031347962382445,0.0714285714285714,0.9205,92,50,65,21,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 15, 'PDI': 4, 'DAT': 9, 'MET': 9, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 12, 'SUB': 1, 'CLA': 4}",0,2,15,4,9,9,5,0,1,1,0,1,0,0,0,0,4,3,2,0,12,1,4,0.6468224380016613,0.6741916076460285,0.4347776948363064
ICLR2018-HJ4YGZ-AW-R1,Reject,"I have a huge amount of sympathy for this work, and was really hoping to read a well-presented paper setting out how to cleanly integrate Smolensky's theory with deep learning, ideally (but not necessarily) with some decent empirical results. If that had been the case, I would certainly have been recommending acceptance, since ICLR would benefit from the alternative perspective that Smolensky's work provides, compared to the majority of work in Deep Learning. The empirical results are decent, but the presentation requires too much work to warrant acceptance at ICLR for this year. There are also some questionable decisions made regarding the NLP evaluations. More detailed comments.  o Just call the pos tagging task POS tagging. Talking about classification of the part of speech of *a* word makes it sound like you're tagging a single word in isolation. o The statement that language structures can't be integrated with DL is a hopeless misrepresentation of current practice in NLP, and misses a large body of existing work. There's obviously Richard Socher's work on integrating DL and the output of eg the Stanford parser, but also a current raft of work on trying to induce tree structures automatically via a DL framework and task-based objective. Two examples, by no means exhaustive:  Learning to Compose Words into Sentences with Reinforcement Learning Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling. ICLR 2017. Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs Jean Maillard, Stephen Clark, Dani Yogatama. o It's not at all clear to me what the third task is - something like chunking. Identification of the phrase structure sounds like parsing, but you're not doing full parsing. This needs explaining fully. There are various standard NLP tasks related to identifying phrase structure - I would just do one of those, using one of the standard datasets, then there won't be any confusion. o The description of tagging on p.2 mentions MEMMs too much - these were superseded by CRFs, which I think is what you mean to refer to. o The reference to a maximum entropy language model is a little odd, since as far as I know these never became mainstream (assuming you mean Rosenfeld's whole sentence maxent language models). o N is terrible name for a system!  o Not sure about the 5-role schema example on p.4, since presumably these would still be generated one word at a time? So in what sense is the model encoding a schema? o Section 5 is the key section in the paper. Unfortunately I found it hard to follow. I guess the LSTM equations are needed for completeness, but what I really needed was a clear paragraph explaining how the LSTM is used to build the vectors and matrices used by the binding/unbinding network. o There are various oddities in the POS tagging experiments.  Why use the Stanford tagger? Are you using this to get the train/test data? Just use the Penn Treebank, or another standard pos tag dataset. o Why precision and recall for tagging? Doesn't each word get assigned a single tag? In which case we just need accuracy. o There are a number of minor comments I could have made re. the presentation, eg funny refs with both names (Chen and Laurence Zitnick). ",31,541,20.03703703703704,5.117529880478088,285,3,538,0.0055762081784386,0.0309653916211293,0.8795,157,58,106,35,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 1, 'DAT': 4, 'MET': 12, 'EXP': 6, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 1, 'EMP': 18, 'SUB': 0, 'CLA': 1}",0,0,6,1,4,12,6,2,0,0,0,2,0,0,1,0,0,4,1,1,18,0,1,0.5037949246425352,0.6775914307305672,0.3538224836784079
ICLR2018-HJ4YGZ-AW-R2,Reject,"**Strengths** The approach to sentence generation makes a lot of sense -- and provides a potentially elegant manner to incorporate or provide the model with the inductive bias that language has syntax and semantics, by leveraging a classical idea called Tensor Product Representation and showing how to adapt it to modern deep learning architectures and ""learn"" syntax and semantics end to end. Results on image captioning models indicate that the proposed approach might be promising. As a by-product, the paper also evaluates the model on POS tagging and shows that one can do fairly well using the representations learned in the TPGN.  **Weakness** My main concerns are the lack of appropriate baselines to establish concretely the contribution of the TPGN. It would be good to address issues under ""Baselines"" below. Approach: 1. It would be nice to explain clearly why the pretraining of the TPGN with the LSTM input is needed.  Is the idea that one would want to feed the representation of the entire representation as input in order to infer what ""S_n"" should be? Why is it then justified to feed in the image input instead? Also, in the second stage, the image features need not correspond to the LSTM feature dimensions, which means that the pretraining seems unprincipled. A better solution would have been to learn a joint embedding of image captions and labels (say via. ranking), and then use the embedding for the caption as input to the TPGN. This would ensure that when we use images, the model sees input that is appropriately ""aligned"". A discussion why this is not needed or implementing this seems important. Minor Points: 1.  ""There are mainly two approaches to natural language generation in image captioning. The first approach takes the words detected by a CNN as input, and uses a probabilistic model, such as a maximum entropy (ME) language model, to arrange the detected words into a sentence. "" -- can cite Fang. et.al [A]  Baselines: 1. The arXiv version and the PAMI version of the Neural Image Captioning paper (Vinyals, 2015) does report numbers on METEOR and CIDEr metrics, so they should be used to populate Table. 1 for completeness. Also, it would be good to clarify which split of MSCOCO Table. 1 reports results on -- is it the 40K large validation split or the 5K validation/test split released by (Karpathy, 2015)? Clarifying this would be nice since the numbers seem a bit on the lower side. 2. What are the relative number of parameters in the Vinyals et.al. baseline and the proposed TPGN model? Would having an LSTM with twice the number of layers (by stacking them) or twice the size of the hidden state do better? 3. What if we used the hidden state of a regular LSTM decoder to do POS tagging? How well would that do? Does the TPN capture any more syntactic structure than an LSTM decoder (Table. 2). This seems to be an important result to report. Clarity: 1. Page 7.: ""We also implemented the latest ResNet feature"" -- would be good be explicit which resnet model is used. References: [A]: Fang, Hao, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollu00e1r, Jianfeng Gao, et al. 2014. ""From Captions to Visual Concepts and Back."" arXiv [cs.CV]. arXiv.",25,542,16.424242424242426,4.85631067961165,253,5,537,0.0093109869646182,0.0235934664246823,0.9916,155,54,102,22,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 16, 'EXP': 0, 'RES': 3, 'TNF': 8, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 21, 'SUB': 0, 'CLA': 1}",0,0,4,1,1,16,0,3,8,0,0,0,1,0,0,0,0,1,0,0,21,0,1,0.5042688349242872,0.3457724820301928,0.25602844308174016
ICLR2018-HJ4YGZ-AW-R3,Reject,"The paper claims that Deep Learning (DL) has not been able to explicitly represent and enforce grammatical structures, which is false, see Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks, Ask Me Anything: Dynamic Memory Networks for Natural Language Processing, DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks or Deep Compositional Question Answering with Neural Module Networks. The Introduction triple challenge is confusing, not clear what are the challenges this paper tries to address. The representation learned in a crucial layer of the TPGN can be interpreted as encoding grammatical roles Doesn't refer to any specific kind of layer, or what it make it special. The idea of using outer product as a layer has already been explored in Multimodal compact bilinear pooling for visual question answering and visual grounding   The following paragraph in page 2 is not clear, very confusing: The work reported here .... their categories In page 3 authors claim that the vectors are linearly independent but didn't specify how they enforce that. Figure 3 contradicts Figure 1, not clear what are the inputs for module S. The experiments reported in Table1 are useless, there a tons of previous work with much better results, see  https://competitions.codalab.org/competitions/3221#results Even the numbers reported for Vinyals et al. (2015) are much higher in the leaderboard. There is no comparison with other models that use attention or analysis of the impact of the increased number of parameters of the method proposed. The experiments about POS tagger and Phrase Classifier are reported on 5000 from the COCO test set, which is useful for comparisons. Should report numbers on PennTreeBank or other common POS dataset. The text is missing a lot of references, for example:  - page 2 GSC  - page 2 The first approach takes the detected by a CNN ....  - page 3 previous work where TPRs are hand-crafted",13,304,23.384615384615383,5.458620689655173,180,0,304,0.0,0.0285714285714285,0.2338,88,46,58,13,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 2, 'MET': 2, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,1,2,2,2,2,2,2,1,1,0,0,0,0,0,0,0,1,1,0,7,0,2,0.6434251111594789,0.4482573241243563,0.3601395295497244
ICLR2018-HJ5AUm-CZ-R1,Reject,"This paper presents an alternative approach to constructing variational lower bounds on data log likelihood in deep, directed generative models with latent variables. Specifically, the authors propose using approximate posteriors shared across groups of examples, rather than posteriors which treat examples independently. The group-wise posteriors allow amortization of the information cost KL(group posterior || prior) across all examples in the group, which the authors liken to the KL annealing tricks that are sometimes used to avoid posterior collapse when training models with strong decoders p(x|z) using current techniques for approximate variational inference in deep nets. The presentation of the core idea is solid, though it did take two read-throughs before the equations really clicked for me. I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments (as illustrated in Figure 3). E.g., explicitly describing how group-wise and per-example posteriors are composed in this model, using Equations and pseudo-code for the main training loop, would have saved me some time. For readers less familiar with amortized variational inference in deep nets, the benefit would be larger. I appreciate that the authors developed extensions of the core method to more complex group structures, though I didn't find the related experiments particularly convincing. Overall, I like this paper and think the underlying group-wise posterior construction trick is worth exploring further. Of course, the elephant in the room is how to determine the groups across which the posteriors can be shared and their information costs amortized.",11,253,25.3,5.672064777327935,148,3,250,0.012,0.0157480314960629,0.9748,76,32,49,11,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,0,0,4,0,7,2,0,0,0,1,1,0,1,0,0,2,0,2,0,2,1,0,0.4303366766314124,0.4451760074515387,0.24322716625215013
ICLR2018-HJ5AUm-CZ-R2,Reject,"- Good work on developing VAEs for few-shot learning. - Most of the results are qualitative and I reckon the paper was written in haste. - The rest of the comments are below:  - 3.1: I got a bit confused over what X actually is:  -- We would like to learn a generative model for **sets X** of the form. --... to refer to the **class X_i** .... -- we can lower bound the log-likelihood of each **dataset X** ... - 3.2: In general, if we wish to learn a model for X in which each latent variable ci affects some arbitrary subset Xi of the data (**where the Xi may overlap**), ...: Which is just like learning a Z for a labeled X but learning it in an unsupervised manner, i.e. the normal VAE, isn't it? If not, could you please elaborate on what is different (in the case of 3.2 only, I mean)? i.e. Could you please elaborate on what's different (in terms of learning) between 3.2 and a normal latent Z that is definitely allowed to affect different classes of the data without knowing the classes? - Figure 1 is helpful to clarify the main idea of a VHE. - In a VHE, this recognition network takes only small subsets of a class as input, which additionally ...: And that also clearly leads to loss of information that could have been used in learning. So there is a possibility for potential regularization but there is definitely a big loss in estimation power. This is obviously possible with any regularization technique, but I think it is more of an issue here since parts of the data are not even used in learning. - Table 4.1 compares these log likelihoods, with VHE achieving state-of-the-art. To: Where is Table 4.1?? - This is a minor point and did not have any impact on the evaluation but VAE --> VHE, reparameterization trick --> resampling trick. Maybe providing rather original headings is better? It's a style issue that is up to tastes anyway so, again, it is minor. - However, sharing latent variables across an entire class reduces the encoding cost per element is significantly: typo. - Figure ?? illustrates. ",20,347,19.27777777777778,4.725552050473186,178,3,344,0.0087209302325581,0.0296495956873315,0.9746,97,37,67,25,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 10, 'EXP': 0, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 4, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 6}",0,0,1,0,2,10,0,1,4,0,0,2,0,0,0,0,0,3,4,0,5,0,6,0.4309745610255545,0.4477569814238765,0.24084569312980106
ICLR2018-HJ5AUm-CZ-R3,Reject,"The paper presents some conceptually incremental improvements over the models in ""Neural Statistician"" and ""Generative matching networks"". Nevertheless, it is well written and I think it is solid work with reasonable convincing experiments and good results. Although, the authors use powerful PixelCNN priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive components.",3,65,21.666666666666668,5.890625,53,1,64,0.015625,0.0307692307692307,0.936,16,11,10,5,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 0, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,0,1,0,0,2,2,0,0,0,1,0,0,0,0,0,0,0,0,2,0,1,0.2858561833987038,0.2228441796570651,0.1274295646692945
ICLR2018-HJ8W1Q-0Z-R1,Reject,"The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one slow trained as usual and one fast that gets updated in every time-step based on the slow network. The authors generalize this idea in a nice  way and present results on 1 experiment. On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original. On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors. The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors. For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from Attention is All You Need). Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection.",7,182,26.0,5.0344827586206895,104,0,182,0.0,0.0054644808743169,0.8163,51,21,26,7,7,7,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 5, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,0,3,2,0,1,5,4,0,0,0,3,0,1,0,1,3,1,0,1,3,1,1,0.5008230021276879,0.7791177659443594,0.3993770805237207
ICLR2018-HJ8W1Q-0Z-R2,Reject,"Summary The paper proposes a neural network architecture for associative retrieval based on fast weights with context-dependent gated updates. The architecture consists of a 'slow' network which provides weight updates for the 'fast' network which outputs the predictions of the system. The experiments show that the architecture outperforms a couple of related models on an associative retrieval problem. Quality The authors evaluate their architecture on an associative retrieval task which is similar to the variable assignment task used in Danihelka et al. (2016). The difference with the original task seems to be that the network is also trained to predict a 'blank' symbol which indicates that no prediction has been made. While this task is artificial, it does make sense in the context of what the authors want to show. The fact that the authors compare their results with three sensible baselines and perform some form of hyper-parameter search for all of the models, adds to the quality of the experiment. It is somewhat unfortunate that the paper doesn't give more detail about the precise hyper-parameters involved and that there is no comparison with the associative LSTM from Danihelka et al. Did these hyper-parameters also include the sizes of the models?  Otherwise it's not very clear to me why the numbers of parameters are so much higher for the baseline models. While I think that this experiment is well done, it is unfortunate that it is the only experiment the authors carried out and the paper would be more impactful if there would have been results for a wider variety of tasks. It is commendable that the authors also discuss the memory requirements and increased wall clock time of the model. Clarity I found the paper hard to read at times and it is often not very clear what the most important differences are between the proposed methods and earlier ones in the literature.  I'm not saying those differences aren't there, but the paper simply didn't emphasize them very well and I had to reread the paper from Ba et al. (2016) to get the full picture.  Originality/Significance While the architecture is new, it is based on a combination of previous ideas about fast weights, hypernetworks and activation gating and I'd say that the novelty of the approach is average. The architecture does seem to work well on the associative retrieval task, but it is not clear yet if this will also be true for other types of tasks. Until that has been shown, the impact of this paper seems somewhat limited to me. Pros Experiments seem well done. Good baselines. Good results. Cons Hard to extract the most important changes from the text. Only a single synthetic task is reported.  ",22,451,18.791666666666668,4.96127562642369,209,4,447,0.0089485458612975,0.0526315789473684,0.9317,118,50,83,29,10,5,"{'ABS': 0, 'INT': 0, 'RWK': 12, 'PDI': 5, 'DAT': 1, 'MET': 11, 'EXP': 10, 'RES': 2, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 5, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 4, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 4}",0,0,12,5,1,11,10,2,0,3,0,5,1,1,0,1,4,3,0,0,11,0,4,0.7189727798087302,0.562396970176271,0.4403583775532771
ICLR2018-HJ8W1Q-0Z-R3,Reject,"The paper proposed an extension to the fast weights from Ba et al. to include additional gating units for changing the fast weights learning rate adaptively. The authors empirically demonstrated the gated fast weights outperforms other baseline methods on the associative retrieval task. Comment:  - I found the paper very hard to follow. The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest. The authors should emphasize the first half of the method section are from existing works and should go into a separate background section. - Overall, the only contribution of the paper seems to be the modification to Ba et al. is the Eq. (8). The authors have only evaluated the method on a synthetic associative retrieval task. Without additional experiments on other datasets, it is hard for the reader to draw any meaningful conclusion about the proposed method in general.",8,150,13.636363636363637,5.219178082191781,86,1,149,0.0067114093959731,0.0261437908496732,0.818,44,18,26,7,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 3, 'DAT': 1, 'MET': 2, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,0,4,3,1,2,3,0,0,0,0,3,1,0,0,0,0,1,0,1,2,1,1,0.5008734847441666,0.5561775129903985,0.31305762088291156
ICLR2018-HJ94fqApW-R1,Accept,"In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations. The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter gamma which is seen as a ""gate"" to the information flow . Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency . In the experiments section, the authors apply their pruning approach on a few representative problems and networks.  The concept of applying sparsity on gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights . However, the ISTA, which is equivalent to L1 penalty on gamma is in spirit same as ""smaller-norm-less-informative"" assumption.  Hence, the title seems a bit misleading. The quality and clarity of the paper can be improved in some sections . Some specific comments by section:  3. Rethinking Assumptions: -tWhile both issues outlined here are true in general, the specific examples are either artificial or can be resolved fairly easily . For example: L-1 norm penalties only applied on alternate layers is artificial and applying the penalties on all Ws would fix the issue in this case.  Also, the scaling issue of W can be resolved by setting the norm of W to 1, as shown in He et. al., 2017 . Can the authors provide better examples here? -tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity? 4. Channel Pruning -tThe notation can be improved by defining or replacing ""sum_reduced"" -tISTA u2013 is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative.  Can the authors address the earlier comment about ""a theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms"" ? -tCan the authors address the earlier comment on ""how to set thresholds for weights across different layers"", by providing motivation for choice of penalty for each layer? -tCan the authors address the earlier comment on how their approach provides ""guarantees for preserving neural net functionality approximately""?   5. Experiments -tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size?  (like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection) -tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this.  However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify? n-tFore/Background: Can the authors add citations for datasets, metrics for this problem? Overall, the channel pruning with sparse gammas is an interesting concept and the numerical results seem promising . The authors have started with right motivation and the initial section asks the right questions, however, some of those questions are left unanswered in the subsequent work as detailed above.",24,491,22.318181818181817,5.502118644067797,240,2,489,0.0040899795501022,0.0098231827111984,0.9736,147,64,81,16,11,5,"{'ABS': 0, 'INT': 2, 'RWK': 16, 'PDI': 8, 'DAT': 1, 'MET': 12, 'EXP': 10, 'RES': 4, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 3, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 14, 'SUB': 0, 'CLA': 3}",0,2,16,8,1,12,10,4,1,3,0,3,3,0,0,0,4,2,1,0,14,0,3,0.7912222656544972,0.5640645448527559,0.5066499259205672
ICLR2018-HJ94fqApW-R2,Accept,"This paper is well written and it was easy to follow . The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers . This is achieved by forcing the output of some channels being constant during training. This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter .   The authors evaluate the performance of the proposed approach on different classification and segmentation tasks . The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet. Some of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc. Could the authors explain their choices? How sensible is the algorithm to these hyperparameters? nIt would be nice to see empirically how much of computation the proposed approach takes during training.  How much longer does it takes to train the model with the ISTA based constraint ?  Overall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above.",12,179,19.88888888888889,5.293785310734464,105,1,178,0.0056179775280898,0.0319148936170212,0.9526,44,20,43,5,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 3, 'DAT': 1, 'MET': 8, 'EXP': 7, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 2}",0,2,3,3,1,8,7,3,0,2,0,1,0,0,1,0,1,2,0,1,8,0,2,0.6455668800277177,0.6712186663386115,0.45008325973759633
ICLR2018-HJ94fqApW-R3,Accept,"This paper proposes an interesting  approach to prune a deep model from a computational point of view.  The idea is quite simple as pruning using the connection in the batch norm layer . It is interesting to add the memory cost per channel into the optimization process.  The paper suggests normal pruning does not necessarily preserve the network function. I wonder if this is also applicable to the proposed method and how can this be evidenced. As strong points, the paper is easy to follow and does a good review of existing methods. Then, the proposal is simple and easy to reproduce and leads to interesting results . It is clearly written (there are some typos / grammar errors). As weak points: 1) The paper claims the selection of alpha is critical but then, this is fixed empirically without proper sensitivity analysis. I would like to see proper discussion here.  Why is alpha set to 1.0 in the first experiment while set to a different number elsewhere. 2) how is the pruning (as post processing) performed for the base model (the so called model A). In section 4, in the algorithmic steps . How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network? 3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected.",15,238,17.0,4.907079646017699,136,1,237,0.0042194092827004,0.024390243902439,0.947,58,30,44,15,8,4,"{'ABS': 0, 'INT': 3, 'RWK': 9, 'PDI': 3, 'DAT': 1, 'MET': 7, 'EXP': 7, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 6, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,3,9,3,1,7,7,2,0,1,0,0,0,0,0,0,6,0,0,0,9,1,1,0.5743161415556554,0.4496602871654276,0.32325777088556723
ICLR2018-HJBhEMbRb-R1,Reject,"Deep neural networks have found great success in various applications. This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach. Specifically, the authors develop a Fourier-based generalization bound. Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions. Numerical experimental results are also presented to verify the theory. (1) The scope is a bit limited.  The paper only considers 2-layer NNs. Is there an essential difficulty in extending the result here to NNs with more layers? Also, the analysis for gradient-based method in section 6  is only for squared-error loss, SINE activation and a deterministic target variable.  What would happen if Y is random or the activation is ReLU? (2) The generalization bound in Corollary 3 is only for the gradient w.r.t. alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W. It would be intersting to present some analysis regarding the gradient w.r.t. W. (3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function. However, no comparison is clearly made. It would be better if the authors could explain this more? In summary, the application domain of the theoretical results seems a bit restricted. Minor comments: Eq. (1): dxi should be dx Lemma 2: one hat{g} should be hat{f}",16,240,12.63157894736842,5.2152466367713,130,2,238,0.0084033613445378,0.0205761316872428,0.4084,67,38,39,11,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 11, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 6, 'CLA': 1}",0,0,11,2,0,3,5,1,0,4,0,3,0,1,0,0,0,1,0,0,6,6,1,0.5733029134666211,0.4480876141602666,0.32341156407058747
ICLR2018-HJBhEMbRb-R2,Reject,"This work proposes to study the generalization of learning neural networks via the Fourier-based method. It first gives a Fourier-based generalization bound, showing that Rademacher complexity of functions with small bandwidth and Fourier l_1 norm will be small. This leads to generalization for 2-layer networks with appropriate bounded size. For 2-layer networks with sine activation functions, assuming that the data distribution has nice spectral property (ie bounded bandwidth), it shows that the local minimum of the population risk (if with isolated component condition) will have small size, and also shows that the gradient of the empirical risk is close to that of the population risk. Empirical results show that the size of the networks learned on random labels are larger than those learned on true labels, and shows that a regularizer implied by their Fourier-based generalization bound can effectively reduce the generalization gap on random labels.  The idea of applying the Fourier-based method to generalization is interesting. However, the theoretical results are not very satisfactory. -- How do the bounds here compared to those obtained by directly applying Rademacher complexity to the neural network functions? -- How to interpret the isolated components condition in Theorem 4? Basically, it means that B(P_X) should be a small constant. What type of distributions of X will be a good example? -- It is not easy to put together the conclusions in Section 6.1 and 6.2. Suppose SGD leads to a local minimum of the empirical loss. One can claim that this is an approximate local minimum (ie, small gradient) by Corollary 3. But to apply Theorem 4, one will need a version of Theorem 4 for approximate local minima. Also, one needs to argue that the local minimum obtained by SGD will satisfy the isolated component condition. The argument in Section 8.6 is not convincing, ie, there is potentially a large approximation error in (41) and one cannot claim that Lemma 1 and Theorem 4 are still valid without the isolated component condition. ",17,327,21.8,5.353896103896104,144,0,327,0.0,0.0331325301204819,-0.882,80,51,51,16,7,2,"{'ABS': 0, 'INT': 0, 'RWK': 13, 'PDI': 3, 'DAT': 1, 'MET': 9, 'EXP': 12, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,13,3,1,9,12,1,0,0,0,0,0,1,0,0,3,0,0,0,5,0,0,0.5040341451713922,0.2248061252584899,0.22459922894192183
ICLR2018-HJBhEMbRb-R3,Reject,"This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis. Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017). However, I am not satisfied with the results in the current version. 1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions.  2) Even if for sin activation functions, the analysis is NOT complete.  The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima. However, Corollary 3 is only a concentration bound on the gradient. There is a gap that how this corollary implies generalization. The paragraph below this corollary is only a high level intuition. ",9,127,12.7,5.601694915254237,79,0,127,0.0,0.0076923076923076,-0.8286,39,17,17,9,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,7,0,0,5,1,2,0,1,0,1,1,0,0,0,1,0,0,0,2,1,0,0.5729679691907235,0.3339552907681763,0.2920990035445331
ICLR2018-HJCXZQbAZ-R1,Accept,"The paper presents a method for hierarchical object embedding by Gaussian densities for lexical entailment tasks.Each word is represented  by a diagonal Gaussian and the KL divergence is used as a directional distance measure. if D(f||g) < gamma then the concept represented by f entails the concept represented by g. The main technical difference of the present work compared from the  main prior work (Vendrov, 2015) is that in addition to mean vector representation they use here also a variance component. The main modeling challenge here to to define a  good directional measure that can be suitable for lexical entailment. in Vendrov work they defined a partial ordering. Here,  the KL is not symmetric but its directional aspect is not significant. For example if we set all the variances to be a unit matrix than the KL is collapsed to be a simple symmetrical Euclidean distance. We can also see from Table 1 that if we replace KL by its symmetrical variant we get similar results. Hence, I was not convinced that the propose KL+Gaussian modeling is suitable for directional relations. The paper also presents several methods for negative samplings and according to table 4 there is a lot of performance variability based on the method that is used for selecting negative sampling. I find this component of the proposed algorithm  very heuristic. To summarize, I don't think there is enough interesting novelty in this paper. If the focus of the  paper is on  obtaining good entailment results, maybe an NLP conference can be a more suitable venue. ",13,257,18.357142857142858,5.179166666666666,130,2,255,0.0078431372549019,0.0112781954887218,-0.8412,64,42,48,14,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,2,3,0,7,0,1,2,0,0,2,0,0,1,1,1,1,0,0,3,0,0,0.4303403856489892,0.5567994704252415,0.2731709432595759
ICLR2018-HJCXZQbAZ-R2,Accept,"The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one. The goal is to capture hypernyms of some synsets, even if their occurrence is scarce on the training data. +++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical ackground, even if no methodological novelty has been introduced (this is also a cons!) ---cons: 1) Badly presented: the writing of the paper fails in let the reader aware of what the paper actually serves n COMMENTS: The introduction puzzled me:  the authors, once they stated the problem (the scarceness of the hypernyms' occurrences in the texts w.r.t. their hyponyms), proposed a solution which seems not to directly solve this problem. So I suggest the authors to better explain the connection between the told problem and their proposed solution, and how this can solve the problem. This aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset. How the 4000 hypernyms have been selected? Why a diagonal covariance has been estimated, and not a full covariance one? n Figure 4 middle, it is not clear whether the location and city concepts are intersecting the other synsets. It shouldn't be, but the authors should spend a little on this. Apart from these comments, I found the paper interesting especially for the big amount fo comparisons carried out. As a final general comment, I would have appreciated a paper more self explanative, without referring to the paper [Vilnis & McCallum, 2014] which makes appear the paper a minor improvement of what it is actually. ",15,295,26.818181818181817,5.157509157509158,159,1,294,0.0034013605442176,0.0436241610738255,0.8443,76,29,50,22,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 4, 'DAT': 2, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 2}",0,1,1,4,2,3,1,1,2,0,0,3,0,0,0,1,0,1,3,0,4,2,2,0.6436856529869807,0.6688434883979778,0.44249528887495315
ICLR2018-HJCXZQbAZ-R3,Accept,"The paper introduces a novel method for modeling hierarchical data. The work builds on previous approaches, such as Vilnis and McCallum's Word2Gauss and Vendrov's Order Embeddings, to establish a partial order over probability densities via encapsulation, which allows it to model hierarchical information. The aim is to learn embeddings from supervised structured data, such as WordNet. The work also investigates various schemes for selecting negative samples. The evaluation consists of hypernym detection on WordNet and graded lexical entailment, in the shape of HyperLex. This is good work: it is well written, the experiments are thorough and the proposed method is original and works well. Section 3 could use some more signposting. Especially for 3.3 it would be good to explain (either at the beginning of section 3, or the beginning of section 3.3) why these measures matter and what is going to be done with them. It's good that LEAR is mentioned and compared against, even though it was very recently published. Please do note that the authors' names are misspelled: Vuli'c not Vulic, Mrkv{s}i'c instead of Mrksic. If I am not mistaken, the Vendrov WordNet test set is a set of positive pairs. I would like to see more details on how the evaluation is done here: presumably, the lower I set the threshold, the higher my score? Or am I missing something? It would be useful to describe exactly the extent to which supervision is used - the method only needs positive and negative links, and does not require any additional order information (i.e., WordNet strictly contains more information than what is being used). I don't see what Socher et al. (2013) has to do with the loss in equation (7). Or did they invent the margin loss? Word2gauss also evaluates on similarity and relatedness datasets. Did you consider doing that here too? hypothesis proposed by Santus et al. which says is not a valid reference.",19,317,18.647058823529413,5.033112582781457,181,1,316,0.0031645569620253,0.0188679245283018,0.9548,81,39,71,22,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 4, 'DAT': 2, 'MET': 4, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 3, 'CLA': 2}",0,0,7,4,2,4,4,0,0,2,0,2,1,1,0,1,0,2,0,1,2,3,2,0.6446333625020545,0.6677002747461966,0.45373533938718486
ICLR2018-HJDUjKeA--R1,Reject,"This paper learns to construct masks and feature representations from an input image, in order to represent objects. This is applied to the relatively simple domain of Atari games video input (compared to natural images). The paper is completely inadequate in respect to related work; it re-invents known techniques like non-maximum suppression and matching for tracking; fails to learn convincing objects according to visual inspection; and fails to compare with earlier methods for these tasks. (The comment above about re-invention is the most charitable intepretation -- the worst case would be using these ideas without citation.) 1) The related work section is outrageous, containing no references before 2016. Do the authors think researchers never tried to do this task before then? This is the bad side of the recent deep nets hype, and ICLR is particularly susceptible to this. Examples include  @article{wang-adelson-94,   author          Wang,  J. Y. A. and Adelson, E. H.,   title           {{Representing Moving Images with Layers}},   journal         {{IEEE Transactions on Image Processing}},   year            1994,   volume          3(5),   pages           {625-638} } see http://persci.mit.edu/pub_pdfs/wang_tr279.pdf  and  @article{frey-jojic-03,    author      {Frey, B. J. and Jojic, N.},    title       {{Transformation Invariant Clustering Using the EM Algorithm}},    journal     {IEEE Trans Pattern Analysis and Machine Intelligence},    year        {2003},    volume      {25(1)},    pages       {1-17} } where mask and appearances for each object of interest are learned.  There is a literature which follows on from the F&J paper. The methods used in Frey & Jojic are different from what is proposed in the paper, but there needs to be comparisons. The AIR paper also contains references to relevant previous work. 2) p 3 center -- this seems to be reinventing non-maximum suppression 3) p 4 eq 3 and sec 3.2 -- please justify *why* it makes sense to use the concrete transform. Can you explain better (e.g. in the supp mat) the effect of this for different values of q_i? 4) Sec 3.5 Matching objects in successive frames using the Hungarian  algorithm is also well known, e.g. it is in the matlab function assignDetectionsToTracks . 5) Overall: in this paper the authors come up with a method for learning objects from Atari games video input. This is a greatly restricted setting compared to real images. The objects learned as shown in Appendix A are quite unconvincing, e.g. on p 9. For example for Boxing why are the black and white objects broken up into 3 pieces, and why do they appear coloured in col 4? Also the paper lacks comparisons to other methods (including ones from before 2016) which have tackled this problem. It may be that the methods in this paper can outperform previous ones -- that would be interesting, but it would need a lot of work to address the issues raised above. Text corrections:  p 2 we are more precise -> we give more details  p 3 and p 2 -- local maximum (not maxima) for a single maximum.  [occurs many times] ",23,473,17.51851851851852,5.175925925925926,242,3,470,0.0063829787234042,0.0065252854812398,0.7136,144,60,86,20,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 2, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 1}",0,0,9,2,0,9,0,0,0,0,0,0,1,2,0,0,0,4,0,0,4,3,1,0.3598569252022192,0.4468751574371862,0.20562546736485318
ICLR2018-HJDUjKeA--R2,Reject,"The paper proposes a method  for learning object representations from pixels and then use such representations for doing reinforcement learning. This method is based on convnets that map raw pixels to a mask and feature map. The mask contains information about the presence/absence of objects in different pixel locations and the feature map contains information about object appearance. I believe that the current method can only learn and track simple objects in a constant background, a problem which is  well-solved in computer vision. Specifically, a simple method such as background subtraction can easily infer the mask (the outlying pixels which correspond to moving objects)  while simple tracking methods (see a huge literature over decades on computer vision) can allow to track these objects across frames.  The authors completely ignore all this previous work and their related work section  starts citing papers from 2016 and onwards!  Is it any benefit of learning objects with the current (very expensive) method compared to simple methods such as  background subtraction? Furthermore, the paper is very badly written since it keeps postponing the actual explanations to later sections (while these  sections eventually refer to the appendices). This makes reading the paper very hard. For example, during the early sections you  keep referring to a loss function which will allow for learning the objects, but you never really give the form of this loss (which you should as soon as  you mentioning it) and the reader needs to search into the appendices to find out what is happening.  Also, experimental results are very preliminary and not properly analyzed.  For example the results in Figure 3 are unclear and need to be discussed in detail in the main text. ",11,282,25.63636363636364,5.305147058823529,149,0,282,0.0,0.0135593220338983,-0.7326,75,24,52,22,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 4}",0,0,1,3,0,2,1,2,1,0,0,3,0,1,0,0,0,0,2,0,3,1,4,0.5719439027748782,0.4459933334504959,0.32423706356456267
ICLR2018-HJDUjKeA--R3,Reject,"The paper proposes a neural architecture to map video streams to a discrete collection of objects, without human annotations, using an unsupervised pixel reconstruction loss. The paper uses such object representation to inform state representation for reinforcement learning. Each object is described by a position, appearance feature and confidence of existence (presence). The proposed network predicts a 2D mask image, where local maxima  correspond to object locations, and values of the maxima correspond to presence values. The paper uses a hard decision on the top-k objects (there can be at most k objects) in the final object list, based on the soft object presence values (I have not understood if these top k are sampled based on the noisy presence values or are thresholded, if the authors could kindly clarify). The final presence values though are sampled using Gumbell-softmax. Objects are matched across consecutive frames using non parametric (not learnable) deterministic matching functions, that takes into account the size and appearance of the objects. For the unsupervised reconstruction loss, a static background is populated with objects, one at a time, each passing its state and feature through deconvolution layers to generate RGB object content. Then a policy network is trained with deep Q learning whose architecture takes into account the objects in the scene, in an order agnostic way, and pairwise features are captured between pairs of objects, using similar layers as visual interaction nets. Pros The paper presents interesting ideas regarding unsupervised object discovery n Cons: The paper shows no results. The objects discovered could be discovered with mosaicing (since the background is static) and background subtraction.  ",12,268,22.33333333333333,5.662745098039216,147,0,268,0.0,0.014760147601476,0.9393,95,34,47,4,3,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 7, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,0,0,7,0,4,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,1,1,0.2153920897982028,0.3339552907681763,0.10903350102941346
ICLR2018-HJDV5YxCW-R1,Reject,"This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase. Though this is an important direction to investigate, there are several issues:  1. Comparison with previous results is misleading: a.t1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%. b.tHubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper.  Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit.  Therefore, it is not clear that the proposed methods improve over previous approaches. 2. It is not clear to me: in which dimension of the tensors are we saving the scale factor? If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?  3. The review of the literature is inaccurate. For example, it is not true that Courbariaux et al. (2016) ""further improved accuracy on small datasets"": the main novelty there was binarizing the activations (which typically decreased the accuracy). Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed a significant improvement over previous work in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors). Lastly, the statement ""Typical approaches include linearly placing the quantization points"" is inaccurate: it was observed that logarithmic quantization works better in various cases. For example, see Miyashita, Lee and Murmann 2016, and Hubara et al. %%% After Author's Clarification %%% This paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly.",12,313,17.38888888888889,5.351170568561873,160,1,312,0.0032051282051282,0.0595611285266457,0.9647,100,41,53,20,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 3, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 4}",0,0,9,3,0,1,1,1,0,0,0,3,3,0,0,1,3,0,0,0,7,1,4,0.5008781265685591,0.5596267786740713,0.30381788557753037
ICLR2018-HJDV5YxCW-R2,Reject,"The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights. 1.  The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them. 2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet. Indeed, the authors have commented: AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.  So, please show that. 3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy? My concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016]. 4. Is trade-off between 1 to 2 bits really important?  Compared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1).  Is such improvement really important? Reference: [a]. Trained Ternary Quantization. ICLR 2017 [b]. Extremely low bit neural network: Squeeze the last bit out with ADMM.",18,230,9.2,5.196172248803828,125,1,229,0.0043668122270742,0.0127659574468085,0.9786,73,27,37,21,10,2,"{'ABS': 0, 'INT': 0, 'RWK': 11, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 6, 'RES': 1, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 3, 'EXT': 4}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 3, 'CLA': 0}",0,0,11,1,0,5,6,1,1,2,0,2,3,4,0,0,0,0,0,0,2,3,0,0.7166452491260488,0.2230575326737081,0.32534056380014315
ICLR2018-HJDV5YxCW-R3,Reject,"This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory).  The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension. The description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages. Experiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain. Results are promising. Overall, I am leaning towards a rejection mostly due to limited novelty.",6,126,21.0,5.463414634146342,87,1,125,0.008,0.0234375,0.8726,34,25,22,6,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 2, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,0,4,3,0,2,3,1,0,1,0,2,0,0,0,1,1,1,0,0,4,2,0,0.5008280659361161,0.5575281043684059,0.31430173525042965
ICLR2018-HJFcmshbM-R1,,"The paper adopts the concept of saliency to explain how the deep model makes decisions with adversarial perturbations.  The paper is not written well and the idea seems to be trivial compared with previous methods. The paper uses three manners of adversarial attacks and formulate saliency as the gradient that is particularly influential to the final classification output. From what I can see, there is barely new that is proposed by the paper. The experiment lacks the C&W's attack. The second paragraph of introduction listed some related work and yet failed to compare with them well.  The method resembles quite a lot the Grad-CAM method and the conclusions from the experiment (shallow layers are robust enough to adversarial examples and middle layers ....) seem shallow as well. The presentation of the paper is poor. Many syntax errors/ format issues. For example, in the abstract, saliency simply explain how -> explains. Discussion section, how xxx contributing to wrong xxx -> contributes to.",10,158,14.363636363636363,5.205128205128205,98,1,157,0.0063694267515923,0.0184049079754601,-0.9415,46,23,26,11,6,6,"{'ABS': 1, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 3}",1,0,1,3,0,3,2,0,0,0,0,3,0,0,0,1,0,3,1,0,1,1,3,0.4293674459305192,0.6670632619227548,0.30216445543946646
ICLR2018-HJFcmshbM-R2,,"This paper proposed an approach for detecting adversarial examples using saliency maps. The key idea is exploiting saliency maps as additional inputs to detector, which reveals importance of each pixel in classification. Overall, this reviewer leans towards rejecting this paper due to its limited contribution/novelty and the incomprehensive experiment results. The proposed method simply concatenates a saliency map with the corresponding raw pixel image as an input to adversarial perturbation detector. Despite of its simplicity, there are no discussions/empirical comparisons with other approaches, and simple ablative analysis such as performance of detector with and without saliency maps.    ",5,97,16.166666666666668,6.125,68,0,97,0.0,0.0099009900990099,-0.8885,35,15,11,1,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,2,1,1,0,1,0,1,0,0,0,1,0,1,0,1,4,0,0,0.428880920547069,0.4463103167489733,0.2441026009398242
ICLR2018-HJFcmshbM-R4,,"The paper describes a method for detecting adversarial examples using a second detector classifier. The key innovation here is to use pixel saliency as a channel for the adversarial example detector. Pixel saliency, in this case, is defined as the partial derivative of the output of the classification network with respect to each input pixel.  Several examples are used to show that even visually similar adversarial examples can have very different saliency maps, which motivates using these saliency maps to detect adversaries. The experiments show that this approach can be used to successfully detect adversaries for several datasets, including MNIST, CIFAR10, and a small subset of ImageNet, and also investigates the robustness across different variations of attack. n In general, I think the idea of using saliency to detect adversarial attacks is very good and represents an interesting avenue of research. However, the paper as it stands seems rather preliminary, and there are several issues with the paper I think need to be addressed. 1) Sec 2.2 introduces the C&W attack. However, this does not appear to be used anywhere else in the paper. It is not clear from the paper why the authors do not test their systems resiliency to this form of attack. More advanced attacks need to be considered. 2) I would have liked to have seen an ablative study where the detectors are trained on pixels alone and directly compared with the detectors trained with saliency as an input channel. This would give a much more convincing account of the value of saliency in this context. 3) It is easy to envision a (full knowledge) attack where the attacker knows the architecture and parameters of the detector and then devises an attack to fool both the detector and the original classifier.  This could, for example, be done by optimizing a joint objective which modifies the input to both maximize error in the classifier and in the detector simultaneously, while remaining close to the original input.  Unfortunately, there is no investigation of this kind of attack in the paper.  Of course, it is fine if the assumption is that the attacker has no knowledge of the detector (black box case), but if this is the case, this assumption needs to be explicitly acknowledged up front and attention drawn to the obvious limitations of this approach. 4) There is no direct comparison with other state-of-the-art in the paper. Related work is mentioned in 2.3, but a direct comparison in which the experimental settings are identical in the evaluation would have been helpful. 5) The mathematical notation needs a bit of improvement. E.g. argmin, sgn, min, etc. should be set in roman and parentheses should be enlarged to match the size of the enclosed expressions. There is also a bit of overloading going on: at the start of 2.1 it appears f_theta refers to the true function, then later the classification model, and then in eq (4) a linear approximation of the classification model. 6) The writing needs improvement and the paper in general needs some copy editing. There are quite a few grammatical errors and typos which, unfortunately, can make the text difficult to comprehend in parts. Captions also run into the text in a few places. ",24,539,19.25,5.0546875,243,4,535,0.0074766355140186,0.0202205882352941,-0.7928,144,51,92,34,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 1, 'MET': 15, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 5, 'FWK': 1, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 2, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 3}",0,0,2,1,1,15,5,0,0,5,1,4,0,0,0,0,1,2,2,0,5,3,3,0.5755916660373923,0.6697088510453829,0.4054050776509153
ICLR2018-HJGcNz-0W-R1,Reject,"The paper is generally clear, and proposes to use a convolutional autoencoder based on 3D meshes. The novelty here how the problem is formulated. Pros: - Interesting formulation. I have not seen this particular setup for processing meshes with neural networks in an autoencoder setting. - This work collected a new dataset for 3D face expression representation, which is great (the state of 3D face databases which are available to researchers is very limited, so this is a step in the right direction). Cons: - The visual depiction of the auto-encoded meshes looks a bit strange. In particular, they exhibit some high frequency artefacts. These do not appear in the smoother PCA version. From a human standpoint, in those cases, the smoother meshes would in fact be preferable. I did not see a discussion about this, given that such problems are not captured by the metrics. - I am a bit confused by the requirement that all meshes need to have the same adjacency matrix. Does this mean that you need to convert the raw meshes coming from the 3D camera into a particular topology before you can use this algorithm? If yes, this seems like a rather large limitation. - Regarding the evaluation, you wrote:In order to evaluate the interpolation capability of the autoencoder, we split the dataset in training and test samples in the ratio of 1:9. The test samples are obtained by picking consecutive frames of length 10 uniformly at random across the sequences.  - To me this is very unclear. You have very few sequeces/subjects. Did  you split by *subject*? I think this is CRUCIAL, and a lot of the results hinge on this answer. General Questions  I am  wondering how come you didn't consider a geometry image representation of the meshes, and went for a slightly more general, and yet very confined alternative (the adjacency requirement, which in some sense is the same type of constraint as geometry images). On geometry images, in particular it would be possible to apply standard convolutional architectures without any special processing. Another question that I had is why use a L1 loss when in the evaluation you're using L2? It would make a lot of sense to use the same loss as the evaluation metric (not to mention the properties of PCA).",21,377,18.85,5.051428571428572,197,2,375,0.0053333333333333,0.0258397932816537,-0.717,103,42,65,18,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 1, 'MET': 13, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 2, 'CLA': 1}",0,0,0,4,1,13,5,1,0,0,0,1,0,0,0,1,0,0,0,0,16,2,1,0.4320964166892465,0.4538804824754104,0.2438832041839095
ICLR2018-HJGcNz-0W-R2,Reject,"Paper summary: Authors extend [1] to form an auto-encoder CNN network for face mesh representation. Face mesh graph is represented by Fourier basis of graph Laplacian and therefore convolution operator is defined in Fourier space. Chebyshev polynomial is used for faster computations. Max pooling on graph is done by using Graclus multilevel clustering algorithm. Binary tree generated in pooling layers are kept for unpooling layers in decoder network. Authors captured a new facial dataset for their evaluation and reported better results than PCA. Positive points: Authors tackle irregular data feature extraction and learning using CNNs which is a hot topic in deep learning. Negative points: Although proposed idea is interesting, paper has a number of critical problems.  Firstly, experiments are the main weakness of the paper. Set of experiments does not prove claims of the paper. - It is not clear how authors uses PCA to reconstruct faces in the test set. - Authors do not compare to any state of the art on 3D face representation and reconstruction (e.g. [2]) using public datasets (e.g. BU-3DFE). - How network behaves by introducing noise on vertices? - What is the effect of network hyper-parameters? Secondly, paper has a lack of novelty. It is a simple extension of [1] without considering and solving problems in [1]. Also, it is not mentioned what is the loss function to train the network. I suppose it is L2 norm loss, but it must be clear in the paper. [1] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844u20133852, 2016. [2] A. Brunton, T. Bolkart, and S. Wuhrer.  Multilinear wavelets: A statistical shape space for human faces. In European Conference on Computer Vision, pp. 297u2013312, 2014a. After rebuttal: The current version of the paper still needs significant amount of work regarding the experimental part.",25,311,9.424242424242424,5.334494773519164,178,0,311,0.0,0.0220820189274447,0.34,118,44,55,8,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 4, 'MET': 8, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 5, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,0,1,3,4,8,6,1,0,0,0,3,5,0,0,3,0,1,0,0,9,2,0,0.5739781767624497,0.449616995600546,0.32380010084712313
ICLR2018-HJGcNz-0W-R3,Reject,"This paper introduces a convolutional autoencoder for irregular graphs, specifically surfaces in the form of discrete meshes in 3D. The underlying technique that is used to operate on the irregular graph is spectral decomposition, which enables convolutions in the spectral domain. The spectral convolution methods have been applied to mesh data structures for about 5 years now, as stated in the paper as well [Bruna et al. 2013], [Defferrard et al. 2016], [Bronstein et al. 2017], [Li et al. 2017] ... The paper states that it builds upon the formulation in [Defferrard et al. 2016] as explained in section 3. Given the facts above, I am having a hard time to understand the novelty of this paper? Is it the Mesh Upsampling operation defined at the end of page 4? If that is the case, it is not demonstrated in the paper that it actually works. The main reason is that the original face mesh graph that goes into the convolution/downsampling  operations is topologically preserved through the upconvolutions. This means that the upsampling operation is not really upsampling a true graph/mesh. The topology is already known, the upsampling just predicts a function on this topology. The choice of the face domain is also suspicious, since all faces are topologically the same graph (even though there are geometric variations). Convolutions/downsampling-convolutions/upsampling that are demonstrated in the paper basically boil down to function prediction on the same exact global graph. Face topologies are so regular that they can even be represented with a height map like geometry encoding in the image plane (See [1'] below). To demonstrate the mesh/graph generation capability truly, the authors need to experiment on novel topology generation. As is, the paper does not bear enough novelty on top of [Defferrard et al. 2016], or is not demonstrating it even if there exists any. [1'] Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman, D. Samaras. Neural Face Editing with Intrinsic Image Disentangling. CVPR 2017 MINOR:  First paragraph of Section 3:  - The definition of a mesh (F (V,E,A)) is not correct: Both E and A essentially define the same connectivity. Base your definition on F (V,E) or F (V,A). Since you are using A later in the section, probably the latter makes more sense for you.  ",19,376,12.129032258064516,5.183673469387755,189,1,375,0.0026666666666666,0.010443864229765,0.9086,118,35,69,29,7,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,0,1,1,0,11,1,0,0,0,0,2,1,1,0,2,0,0,0,0,11,0,0,0.5025473399458972,0.2284869041551701,0.22992033612905435
ICLR2018-HJIhGXWCZ-R1,Reject,"This paper introduce a times-series prediction model that works in two phases. First learns a deterministic mapping from x to y.phases. And then train another net to predict future frames given the input and residual error from the first network. And does sampling for novel inputs by sampling the residual error collected from the training set.  Pros: The paper is well written and easy to follow. Good cover of relevant work in sec 3. Cons The paper emphasis on the fact the their modeling multi-modal time series distributions, which is almost the case for most of the video sequence data. But unfortunately doesn't show any results even qualitative like generated samples for other  work on next frame video prediction. The shown samples from model looks extremely, low quality and really hard to see the authors interpretations of it. There are many baselines missing. One simple one would be what if they only used the f and draw z samples for N(0,1)? VAE is very power latent variable model which also not being compared against. It is not clear what implantation of GAN they are using?. Vanilla GAN is know to be hard to train and there has been many variants recently that overcome some of those difficulties and its mode collapse problem.  ",14,212,15.142857142857142,4.878048780487805,137,0,212,0.0,0.0138888888888888,-0.9626,62,23,39,14,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 1}",0,0,1,4,0,6,0,1,0,0,0,1,0,0,0,0,0,0,0,0,7,2,1,0.3585717172973477,0.3371717544507126,0.18225586543792727
ICLR2018-HJIhGXWCZ-R2,Reject,"The paper proposes a model for prediction under uncertainty where the separate out deterministic component prediction and uncertain component prediction. They propose to have a predictor for deterministic information generation using a standard transformer trained via MSE. For the non-deterministic information, they have a residual predictor that uses a low-dimensional latent space.  This low-dim latent space is first predicted from the residual of the (deterministic prediction - groundtruth), and then the low-dim encoding goes into a network that predicts a corrected image. The subtleness of this work over most other video prediction work is that it isn't conditioned on a labeled latent space (like text to video prediction, for example). Hence inferring a structured latent space is a challenge. The training procedure follows an alternative minimization in EM style. The biggest weakness of the paper (and the reason for my final decision) is that the paper completely goes easy on baseline models. It's only baseline is a GAN model that isn't even very convincing (GANs are finicky to train, so is this a badly tuned GAN model? or did you spend a lot of time tuning it?). Because of the plethora of VAE models used in video prediction [1] (albeit, used with pre-structured latent spaces), there has to be atleast one VAE baseline. Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try. In fact, a VAE would be nicely suited when proposing to work with low-dimensional latent spaces. The main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline. If the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.  [1]  http://openaccess.thecvf.com/content_ICCV_2017/papers/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.pdf",16,298,19.866666666666667,5.498220640569395,164,0,298,0.0,0.0231788079470198,0.6164,82,43,54,19,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 6, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 0, 'SUB': 0, 'CLA': 0}",0,0,6,6,0,3,1,0,0,0,0,1,0,3,0,0,0,5,0,1,0,0,0,0.4297894077565107,0.2226908724509821,0.1930226238048425
ICLR2018-HJIhGXWCZ-R3,Reject,"Summary:   I like the general idea of learning output stochastic noise models in the paper, but the idea is not fully explored (in terms of reasonable variations and their comparative performance). I don't fully understand the rationale for the experiments: I cannot speak to the reasons for the GAN's failure (GANs are not easy to train and this seems to be reflected in the results); the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample. I.e., with enough throws, I can always hit the bullseye with a dart even when blindfolded. Comments:  The model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i). The trained residual function can be used to predict a residual z_i for x_i. Then for out-of-sample prediction for x*, the paper appears to propose sampling a z uniformly from the training data {z_i}_i (it is not clear from the description on page 3 that this uniformly sampled z*   z_i depends on the actual x* -- as far as I can tell it does not). The paper does suggest learning a p(z|x) but does not provide implementation details nor experiment with this approach. I like the idea of learning an output stochastic model -- it is much simpler to train than an input stochastic model that is more standard in the literature (VAE, GAN) and there are many cases where I think it could be quite reasonable.  However, I don't think the authors explore the idea well enough -- they simply appear to propose a non-parametric way of learning the stochastic model (sampling from the training data z_i's) and do not compare to reasonable alternative approaches.  To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution. Second, why not simply propose learning exponential family models where the parameters of these models are (deep nets) conditioned on the input? One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x.  If the contribution of the paper is the output stochastic noise model, I think it is worth experimenting with the design options one has with such a model. The experiments range over 4 video datasets.  PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7.  The new model EEN is compared to a deterministic model and conditional GAN. The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason? Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.  If I understand what is being evaluated correctly (i.e., best random guess) then I am not surprised the EEN can perform better with enough random samples. Have we learned anything? ",22,523,29.05555555555556,4.829268292682927,228,9,514,0.0175097276264591,0.0683918669131238,0.9761,139,61,101,40,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 15, 'EXP': 12, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 2, 'CLA': 0}",0,0,1,1,1,15,12,1,0,0,0,0,0,1,0,0,0,3,0,0,12,2,0,0.5044508433017808,0.3405158667393074,0.24361628826625228
ICLR2018-HJJ23bW0b-R1,Accept,"I was very confused by some parts of the paper that are simple copy-past from the paper of Downey et al. which has been accepted for publication in NIPS. In particular, in section 3, several sentences are taken as they are from the Downey et al.'s paper. Some examples :  u00abu00a0provide a compact representation of a dynamical system by representing state as a set of predictions of features of future observations. u00a0u00bb   u00abu00a0a predictive state is defined as... , where...  is a vector of features of future observations and ...  is a vector of features of historical observations. The features are selected such that ...  determines the distribution of future observations ... Filtering is the process of mapping a predictive state...u00a0u00bb Even the footnote has been copied & pasted: u00abu00a0For convenience we assume that the system is k-observable: that is, the distribution of all future observations is determined by the distribution of the next k observations. (Note: not by the next k observations themselves.) At the cost of additional notation, this restriction could easily be lifted. u00a0u00bb u00abu00a0 This approach is fast, statistically consistent, and reduces to simple linear algebra operations. u00a0u00bb   Normally, I should have stopped reviewing, but I decided to continue  since those parts only concerned the preliminaries part. A key element in PSRNN is to used as an initialization a kernel ridge regression. The main result here, is to show that using orthogonal random features approximates well the original kernel comparing to random fourrier features as considered in PSRNN. This result is formally stated and proved in the paper. The paper comes with some experiments in order to empirically demonstrate the superiority  orthogonal random features over RFF. Three data sets are considered (Swimmer,  Mocap and  Handwriting). I found it that the contribution of the paper is very limited. The connexion to PSRNN is very tenuous since the main results are about the regression part. in Theorems 2 and 3 there are no mention to PSRNN. Also the experiment is not very convincing. The datasets are too small with observations in low dimensions, and I found it not very fair to consider LSTM in such settings. Some minor remarks:  - p3: We use RFs-> RFFs - p5: ||X||, you mean |X| the size of the dataset - p12: Eq (9). You need to add u00abu00a0with probability $1-rho$ as in Avron's paper. - p12: the derivation of Eq (10) from Eq (9) needs to be detailed. I thank the author for their detailed answers. Some points have been clarified but other still raise issues. In particular, I continue thinking that the contribution is limited. Accordingly, I did not change my scores.",27,432,13.935483870967742,5.165432098765432,211,0,432,0.0,0.0153846153846153,-0.4412,123,50,85,25,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 0, 'DAT': 3, 'MET': 6, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 4}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 4, 'CLA': 0}",0,0,6,0,3,6,2,3,0,0,0,3,0,4,0,0,0,0,0,1,9,4,0,0.5020420029583301,0.3386290223370415,0.255169541887091
ICLR2018-HJJ23bW0b-R2,Accept,"The paper tackles the problem of training predictive state recurrent neural networks (PSRNN), which  uses large kernel ridge regression (KRR) problems as a subprimitive, and makes two main contributions: - the suggestion to use orthogonal random features (ORFs) in lieu of standard random fourier features (RFFs) to reduce the size of the KRR problems - a novel analysis of the risk of KRR using ORFs which shows that the risk of ORFs is no larger than that of using RFFs The contribution to the practice of PSRNNs seems significant (to my non-expert eyes): when back-propagation through time is used, using ORFs to do the two-stage KRR training needed visibly outperforms using standard RFMs to do the KRR. I would like the authors to have provided results on more than the current three datasets, as well as an explanation of how meaningful the MSEs are in each dataset (is a MSE of 0.2 meaningful for the Swimmer Dataset, for instance? the reader does not know apriori). The contribution in terms of the theory of using random features to perform kernel ridge regression is novel, and interesting. Specifically, the author argue that the moment-generating function for the pointwise kernel approximation error of ORF features grows slower than the moment-generating function for the pointwise kernel approximation error of RFM features, which implies that error bounds derived using the MGF of the RFM features will also hold for ORF features. This is a weaker result than their claim that ORFs satisfy better error, but close enough to be of interest and certainly indicates that their method is principled. Unfortunately, the proof of this result is poorly written: - equation (20) takes a long time to parse --- more effort should be put into making this clear - give a reference for the expressions given for A(k,n) in 24 and 25 - (27) and (28) should be explained in more detail. My staying power was exhausted around equation 31. The proof should be broken up into several manageable lemmas instead of its current monolithic and taxing form.  ",12,336,37.333333333333336,5.062111801242236,174,2,334,0.0059880239520958,0.0405797101449275,-0.3818,103,34,57,15,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",0,0,0,2,1,7,3,3,0,2,0,0,0,0,0,2,0,1,0,0,7,1,1,0.4304403958654448,0.5593324077491315,0.2660173570211843
ICLR2018-HJJ23bW0b-R3,Accept,"This paper investigates the Predictive State Recurrent Neural Networks (PSRNN) model that embed the predictive states in a Reproducible Hilbert Kernel Space and then update the predictive states given new observation in this space. While PSRNN usually uses random features to project the map the states in a new space where dot product approximates the kernel well, the authors proposes to leverage orthogonal random features. In particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness. Authors then empirically validate their model on several small-scale datasets where they compare their model with PSRNN and LSTM. They observe that PSRNN with orthogonal random features leads to lower MSE on test set than both PSRNN and LSTM and seem to reach lower value earlier in training. Question: -tWhat is the cost of constructing orthogonal random features compared to RF? -tWhat is the definition of H the Hadamard matrix in the discrete orthogonal joint definition? -tWhat are the hyperparameters values use for the LSTM n-tEmpirical evaluations seem to use relatively small datasets composed by few dozens of temporal trajectories. Did you consider larger dataset for evaluation? -tHow did you select the maximum number of epochs in Figure 5? It seems that the validation error is still decreasing after 25 epochs? Pros: -tProvide theoretical guarantee for the use of orthogonal random features in the context of PSRNN Cons: -tEmpirical evaluation only on small scale datasets. ",14,253,31.625,5.506072874493927,137,2,251,0.0079681274900398,0.0196850393700787,0.3736,81,37,40,11,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 3, 'MET': 8, 'EXP': 7, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 3, 'CLA': 0}",0,0,0,4,3,8,7,0,1,2,0,0,0,0,0,0,0,0,1,0,9,3,0,0.4311147485641438,0.33852234582872,0.21815626089134785
ICLR2018-HJLPel-CW-R1,Reject,"This paper introduces two new loss functions which can be used along with the existing reconstruction and adversarial losses for language style transfer. The first one is a style discrepancy loss to enforce that the discrepancy between the learnt style representation for a source sentence and the target style representation is consistent with a pre-trained discriminator. The second one is a cycle consistency loss to make sure that the the original sentence can be recovered from the content of the transferred sentence and the style of the original sentence. The proposed method does not assume that the source sentences have only one style and allows them to have unknown styles. Generally speaking, this paper is well written and easy to follow. The ideas are straightforward and make sense given the current trends in the field. This paper does not bring completely new perspectives for the task, but the contribution is valuable to the community. The experimental results show the effectiveness of the approach. It can be trained with source sentences having various styles and it can produce sentences in a different style without changing the content much. These are pros of the approach. The cons of the approach may be the fact that it needs a pre-trained classifier for the style discrepancy loss. If it can be integrated into the training end-to-end, it might be better. The experimental results show the results without the cyclic loss. The experimental results could also include results without style discrepancy loss. It looks that this paper denotes the style of the target domain as y^* and assume that it is shared by all samples in X_2. However, y^* is also estimated by the encoder as shown in Eq. (2) and it varies depending on the input to the encoder. It is not very clear to me how y^* is chosen in Section 3.3. Please make that part clear. Please explain the reason why STB is better than the proposed method with 100k positive samples. The difference between STB and the proposed method is not clear without reading the reference. The algoroithm of STB should be briefly explained in Section 4.2.",20,356,16.181818181818183,4.997076023391813,155,2,354,0.0056497175141242,0.0337078651685393,0.9807,101,32,64,14,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 1, 'MET': 10, 'EXP': 7, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,0,0,4,1,10,7,3,0,1,0,2,0,0,0,0,2,1,1,0,9,1,1,0.5031036259533508,0.6716903627938584,0.35029605288215904
ICLR2018-HJLPel-CW-R2,Reject,"In this paper, the authors proposed a method to transfer the text style to a specific target style. To this end, the authors combined a text reconstruction loss, an adversarial decoding loss, a cyclic consistency loss and a style discrepancy loss. The method seems solid, and the writing is pretty good. But there are a few key issues that are not clearly addressed and the experimental results are not convincing. Pros: This method combines the contributions of a few previous works, and obtains a stronger and more general model. Especially it borrows the cyclic loss from the image style transfer, which provides a reasonable regularization to the text style transfer model. Cons: There are a few key technical issues that are not clearly addressed. Hence I'm not fully convinced that this model indeed works as claimed. 1. Why use CNN for the style representation layer? CNN is known to be usually unable to capture long-range correlations in natural language (unless enhanced with attentions). Are long-range correlations irrelevant to the text style? 2. The authors made an essential assumption that all target samples have the same style embedding y*. But seems none of the 4 loss functions incorporates this constraint. If y* is simply fixed somewhere in the model, then I'm worried that it may cause mode collapse (i.e. the encoder always output similar values), and one possible bad consequence is that y1_i of different sentences in the source domain may have very similar values. 3. The experiments are toyish and not convincing. The given examples seem to exhibit certain kind of mode collapse, i.e. different examples have similar wording from a very limited vocabulary. It is possible that the generator just learned to overfit the sentiment classifier, so that the classifier thought the transferred sentences have the desired sentiment, but the transferred sentences may lack variations and hence lacks practical use. It would be convincing if the authors could transfer an English paragraph into the style of a certain author, such as Shakespeare, which can be easily evaluated by a human instead of a trained classifier.",18,346,16.476190476190474,5.273006134969325,170,6,340,0.0176470588235294,0.0491329479768786,-0.9463,103,47,57,22,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 12, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 2}",0,0,0,2,0,12,5,1,0,0,0,2,0,2,0,0,0,0,0,0,12,0,2,0.4317823717279579,0.2291448890763489,0.19017289522656908
ICLR2018-HJLPel-CW-R3,Reject,"This paper is an extension of the recent language style transfer method without parallel training pairs (Shen et al. 2017). The author extend the  original algorithm from two-folds:  1. A style discrepancy loss that pushes the style vector of the sentences away from target style 2. A cycle consistency loss that makes sure the content vector of transferred sentences and style vector of the original sentence should be able to reconstruct the original sentences. Both extensions are reasonable to me. The experiments further verify the performance gain compared against the baseline. I have several concerns:  Metrics are not very reasonable to me:  - It does not measure how well the content is preserved. Style transfer has two key components, the first is how well it is transferred to the target style; second is how well it preserves the original contents. However, the metric that using a pre-trained style classification network has issues in terms of evaluating how well the original content is preserved. In fact, from the qualitative examples of Yelp, I do not think that the contents are well preserved. Also it is not clear how good the classifier is. - It seems there are only two styles in Yelp dataset, and it's not clear how many styles are there in the On Chat dataset. The paper does not clearly describe how D_s is obtained. Also in the formulation D_S is not optimised. I am wondering how D_S is trained, how the GT labels are obtained and whether it is trained jointly. Moreover, is this D_S same as the style classifier used in the metric? These are the crucial questions related with fair comparisons, which I would like to know specific answers to make my final decisions. My final decision is dependent on the author's response to my questions. ",17,298,15.68421052631579,4.899305555555555,146,2,296,0.0067567567567567,0.0327868852459016,0.9444,80,31,61,23,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 7, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,0,2,0,1,7,6,0,0,1,0,0,0,3,0,0,0,1,0,0,8,1,0,0.4306027042513781,0.3376870353772341,0.21488134997936995
ICLR2018-HJMN-xWC--R1,Reject,"There is a vast literature on structure learning for constructing neural networks (topologies, layers, learning rates, etc.) in an automatic fashion. Your work falls under a similar category. I am a bit surprised that you have not discussed it in the paper not to mention provided a baseline to compare your method to. Also, without knowing intricate details about each of 17 tasks you mentioned it is really hard to make any judgement as to how significant is improvement coming from your approach. There has been some work done on constructing interpretable neural networks, such as stimulated training in speech recognition, unfortunately these are not discussed in the paper despite interpretability being considered important in this paper. ",5,117,19.5,5.4774774774774775,83,0,117,0.0,0.0084745762711864,0.5944,27,11,27,6,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 0}",0,0,1,0,0,3,0,0,0,1,0,1,0,1,0,0,1,1,0,0,1,2,0,0.3576432413704265,0.4445511209527659,0.20348911239945802
ICLR2018-HJMN-xWC--R2,Reject,"This paper introduces a skip-connection based design of fully connected networks, which is loosely based on learning latent variable tree structure learning via mutual information criteria.  The goal is to learn sparse structures across layers of fully connected networks. Compared to prior work (hierarchical latent tree model), this work introduces skip-paths. Authors refer to prior work for methods to learn this backbone model. Liu et.al (http://www.cse.ust.hk/~lzhang/ltm/index.htm) and Chen et.al. (https://arxiv.org/abs/1508.00973) and (https://arxiv.org/pdf/1605.06650.pdf). As far as I understand, the methods for learning backbone structure and the skip-path are performed independently, i.e. there is no end-to-end training of the structure and parameters of the layers. This will limit the applicability of the approach in most applications where fully connected networks are currently used. Originality - The paper heavily builds upon prior work on hierarchical latent tree analysis and adds 'skip path' formulation to the architecture, however the structure learning is not performed end-to-end and in conjunction with the parameters. Clarity - The paper is not self-contained in terms of methodology. Quality and Significance - There is a disconnect between premise of the paper (improving efficiency of fully connected layers by learning sparser structures) and applicability of the approach (slow EM based method to learn structure first, then learn the parameters). As is, the applicability of the method is limited. Also in terms of experiments, there is not enough exploration of simpler sparse learning methods such as heavy regularization of the weights. ",11,236,15.733333333333333,5.537190082644628,121,0,236,0.0,0.0041493775933609,0.6524,84,21,42,17,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 5, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 2, 'CLA': 1}",0,0,2,3,0,5,2,0,0,0,0,2,0,0,0,1,2,0,0,0,1,2,1,0.3584743278238505,0.555710268712325,0.2289303797149584
ICLR2018-HJMN-xWC--R3,Reject,"The main strengths of the paper are the supporting experimental results in comparison to plain feed-forward networks (FNNs). The proposed method is focused on discovering sparse neural networks. The experiments show that sparsity is achieved and still the discovered sparse networks have comparable or better performance compared to dense networks. The main weakness of the paper is lack of cohesion in contributions and difficulty in delineating the scope of their proposed approach. Below are some suggestions for improving the paper:  Can you enumerate the paper's contributions and specify the scope of this work? Where is this method most applicable and where is it not applicable? Why is the paper focused on these specific contributions?  What problem does this particular set of contributions solve that is not solvable by the baselines?  There needs to be a cohesive story that puts the elements together. For example, you explain how the algorithm for creating the backbone can use unsupervised data. On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems. The motivation section in the beginning of the paper motivates using the backbone structure to get a sparse network. However, it does not adequately motivate the skip-path connections or applications of the method to supervised tasks. Is this work extending the applicability of baselines to new types of problems? Or is this work focused on improving the performance of existing methods? Answers to these questions can automatically determine suitable experiments to run as well.  It's not clear if Pruned FNNs are the most suitable baseline for evaluating the results. Can your work be compared experimentally with any of the constructive methods from the related work section? If not, why?  When contrasting this work with existing approaches, can you explain how existing work builds toward the same solution that you are focusing on? It would be more informative to explain how the baselines contribute to the solution instead of just citing them and highlighting their differences. Regarding the experimental results, is there any insight on why the dense networks are falling short? For example, if it is due to overfitting, is there a correlation between performance and size of FNNs?  Do you observe a similar performance vs FNNs in existing methods? Whether this good performance is due to your contributions or due to effectiveness of the baseline algorithm, proper analysis and discussion is required and counts as useful research contribution. ",24,413,29.5,5.28117359413203,187,0,413,0.0,0.0119047619047619,0.9715,109,37,82,21,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 5, 'DAT': 0, 'MET': 8, 'EXP': 6, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 0}",0,0,8,5,0,8,6,5,0,2,0,2,0,0,0,1,0,1,0,0,13,1,0,0.5032758842786517,0.4519079336625601,0.28747665852138055
ICLR2018-HJNGGmZ0Z-R1,Reject,"This paper analyzes the effect of image features on image captioning. The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on. The MSCOCO captioning and Flickr30K datasets are used for evaluation. Introduction - The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later. - The introduction also states that the authors will propose ways to improve image captioning. This is never discussed. Captioning Model and Table 1 - The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting. In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods. - Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard. The top 50 entries have METEOR scores >  0.25, while the maximum METEOR score reported by the authors is 0.22. Similar trend holds for other metrics like BLEU-4. - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding. Experimental Setup and Training Details - How was the model optimized? No training details are provided. Did you use dropout? Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1? Main claim of the paper Devlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.  Metrics for evaluation - Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers. Choice of Datasets - If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps. The authors should evaluate on these datasets to make their findings stronger and more valuable. Minor comments - Figure 1 is hard to read on paper. Please improve it. - Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.",30,555,17.903225806451612,5.0093984962406015,238,2,553,0.0036166365280289,0.0157068062827225,0.9912,160,51,117,32,10,5,"{'ABS': 0, 'INT': 3, 'RWK': 5, 'PDI': 2, 'DAT': 7, 'MET': 4, 'EXP': 4, 'RES': 2, 'TNF': 7, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 5, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 1}",0,3,5,2,7,4,4,2,7,1,0,1,0,0,0,0,0,3,5,0,11,2,1,0.716139201428527,0.5623624072219,0.45510591150703916
ICLR2018-HJNGGmZ0Z-R2,Reject,"The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect. The paper lacks novelty, just reports some results without proper analysis or insights. Main weakness of the paper:  - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results) - According to SPICE: Semantic Propositional Image Caption Evaluation current metrics used in image captioning don't correlate with human judgement. - Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be . - The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results - To provide a fair comparison authors, should compare their results with other paper results. - Tables 2 and 3 are missing the original baselines. The evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference. - Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3. - The claim The bag of objects model clusters these group the best is not supported by any evidence or metric. One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task. Typos:  - synsest-level -> synsets-level",14,251,17.928571428571427,5.591836734693878,154,0,251,0.0,0.0075757575757575,0.529,84,32,48,15,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 3, 'CLA': 1}",0,0,3,1,0,1,2,3,1,2,0,1,0,3,0,1,0,5,0,0,1,3,1,0.6432590937559688,0.5562375588009584,0.40961159830156835
ICLR2018-HJNGGmZ0Z-R3,Reject,"This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings. The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup. I recomend this paper for a workshop presentation. ",7,93,11.625,5.181818181818182,56,1,92,0.0108695652173913,0.0531914893617021,-0.5489,27,14,16,4,3,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 0, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,0,2,0,0,0,0,3,0,0,0,3,0,0,1,0,1,0,0,0.2145220475553798,0.3334235485023696,0.1044433248169445
ICLR2018-HJOQ7MgAW-R1,Reject,"This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate. It shows comparable results with standard LSTM. I believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results. However, the formulation is very similar to [1] Semi-supervised Question Retrieval with Gated Convolutions 2016 by Lei, and Deriving Neural Architectures from Sequence and Graph Kernels which give theoretical view from string kernel about why this type of networks works. Both of the two paper don't have output gate and non-linearity of Wx_t and results on PTB also stronger than this paper. It also have some visualization about how the model decay the weights. Other AnonReviewer also point out some similar work. I won't repeat it here. In the paper, the author argued we propose and evaluate the minimal changes... but I think the these type of analysis also been covered by [1], Figure 5. On the experimental side, to draw the conclusion, weighted sum is enough for LSTM. I think at least Machine Translation and other classification results should be added. I'm not very familiar with SQuAD dataset, but the results seems worse than Reading Wikipedia to answer open-domain questions Table 4 which seems use a vanilla LSTM setup. Update: the revised version of the paper addresses all my concerns about experiments. So I increased my score.  ",14,228,14.25,5.296296296296297,140,4,224,0.0178571428571428,0.0434782608695652,-0.2692,71,33,37,13,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 2, 'DAT': 1, 'MET': 0, 'EXP': 3, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,0,4,2,1,0,3,4,1,1,0,2,0,2,0,0,0,3,0,1,3,1,0,0.6435521505639039,0.4459226844285103,0.3540533080005379
ICLR2018-HJOQ7MgAW-R2,Reject,"This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the ""content update"" values computed at each time step. The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition. Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context. This reduced form of the LSTM is shown to perform comparably to ""full"" LSTMs. The decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs. The proposed model variations (which replaces the ""content update"" that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary. First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update. You get rid of one, not the other. You can make an argument for why the one that was ablated was ""more interesting"", but really this is an obvious empirical question that should be addressed. The second problem of what tasks to evaluate on is a general problem with comparing RNNs. One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible. Although I don't think this is the responsibility of this paper (although something that should be considered). Finally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs. When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those. Notes on clarity: Before Eq 1 it's hard to know what the antecedent of ""which"" is without reading ahead. For componentwise multiplication, you have been using circ, but then for the iterated component wise product, prod is used. To be consistent, notation like odot and bigodot might be a bit clearer. The discussion of dynamic programming: the dynamic program is also only available because the attention pattern is limited in a way that self attention is not. This might be worth mentioning. When presenting Eq 11, the definition of w_j^t elides a lot of complexity. Indeed, w_j^t is only ever implicitly defined in Eq 8, whereas things like the input and forget gates are defined multiple times in the text.. Since w_j^t can be defined iteratively and recursively (as a dynamic program), it's probably worth writing both out, for expository clarity. Eq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content. Table 4 is unclear. In a language model, the figure looks like it is attending to the word that is being generated, which is clearly not what you want to convey since language models don't condition on the word they are predicting. Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word? In any case, this figure should be corrected to reflect this. This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text).",28,624,23.11111111111111,4.965174129353234,292,8,616,0.0129870129870129,0.0256410256410256,0.9896,155,67,115,35,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 6, 'DAT': 0, 'MET': 14, 'EXP': 6, 'RES': 0, 'TNF': 5, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 9}",0,0,0,6,0,14,6,0,5,1,0,0,0,1,0,0,0,1,1,0,7,2,9,0.4326445961612779,0.5600430572397673,0.27159680404619185
ICLR2018-HJOQ7MgAW-R3,Reject,"Summary: the paper proposes a new insight to LSTM in which the core is an element-wise weighted sum. The paper then argues that LSTM is redundant by keeping only input and forget gates to compute the weights. Experimental results show that the simplified versions work as well as the full LSTM. Comment: I kinda like the idea and welcome this line of research. The paper is very well written and has nice visualisation of demonstrating weights. I have only one question:  in the simplified versions, content(x_t)   Wx_t , which works very well (outperforming full LSTM). I was wondering if the problem is from the tanh activation function (eq 2). What if content(x_t)   W_1 . h_{t-1} + W_2 . x_t? ",8,115,11.5,4.888888888888889,72,0,115,0.0,0.024,0.7713,35,16,23,9,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,0,0,4,0,3,1,1,0,0,0,1,0,0,0,0,0,0,0,0,3,0,1,0.3578211409559937,0.2234661370919081,0.1606935913533233
ICLR2018-HJRV1ZZAW-R1,Reject,"This paper borrows the idea from dilated CNN and proposes a dilated convolution based module for fast reading comprehension, in order to deal with the processing of very long documents in many reading comprehension tasks.  The method part is clear and well-written. The results are fine when the idea is applied to the BiDAF model, but are not very well on the DrQA model. (1) My biggest concern is about the motivation of the paper:   Firstly, another popular approach to speed up reading comprehension models is hierarchical (coarse-to-fine) processing of passages, where the first step processes sentences independently (which could be parallelized), then the second step makes predictions over the whole passage by taking the sentence processing results. Examples include , Attention-Based Convolutional Neural Network for Machine Comprehension, A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data, and Coarse-to-fine question answering for long documents This paper does not compare to the above style of approach empirically, but the hierarchical approach seems to have more advantages and seems a more straightforward solution. Secondly, many existing works on multiple passage reading comprehension (or open-domain QA as often named in the papers) found that dealing with sentence-level passages could result in better (or on par) results compared with working on the whole documents. Examples include QUASAR: Datasets for question answering by search and reading, SearchQA: A new q&a dataset augmented with context from a search engine, and Reinforced Ranker-Reader for Open-Domain Question Answering. If in many applications the sentence-level processing is already good enough, the motivation of doing speedup over LSTMs seems even waker. Even on the SQuAD data, the sentence-level processing seems sufficient: as discussed in this paper about Table 5, the author mentioned (at the end of Page 7) that the Conv DrQA model only encode every 33 tokens in the passage, which shows that such a small context is ENOUGH for most of the questions. Moreover, the proposed method failed to give any performance boost, but resulted in a big performance drop on the better-performed DrQA system. Together with the above concerns, it makes me doubt the motivation of this work on reading comprehension. I would agree that the idea of using dilated CNN (w/ residual connections) instead of BiLSTM could be a good solution to many online NLP services like document-level classification tasks. Therefore, the motivation of the paper may make more sense if the proposed method is applied to a different NLP task. (2) A similar concern about the baselines: the paper did not compare with ANY previous work on speeding up RNNs, e.g. Training RNNs as Fast as CNNs. The example work and its previous work also accelerated LSTM by several times without significant performance drop on some RC models (including DrQA). (3) About the speedup: it could be imaged that the speedup from the usage of dilated CNN largely depends on the model architecture. Considering that the DrQA is a better system on both SQuAD and TriviaQA, the speedup on DrQA is thus more important. However, the DrQA has less usage of LSTMs, and in order to cover a large reception field, the dilated CNN version of DrQA has a 2-4 times speedup, but still works much worse. This makes the speedup less impressive. (4) It seems that this paper was finished in a rush. The experimental results are not well explained and there is not enough analysis of the results. (5) I do not quite understand the reason for the big performance drop on DrQA. Could you please provide more explanations and intuitions?",26,587,24.45833333333333,5.276292335115865,260,7,580,0.0120689655172413,0.0135363790186125,0.992,177,74,87,39,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 7, 'DAT': 1, 'MET': 16, 'EXP': 1, 'RES': 3, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 18, 'SUB': 2, 'CLA': 0}",0,0,2,7,1,16,1,3,1,2,0,1,0,0,0,0,0,3,2,0,18,2,0,0.6471824348034664,0.4554202913832797,0.3703735672276799
ICLR2018-HJRV1ZZAW-R2,Reject,"The paper proposes a simple dilated convolutional network as drop-in replacements for recurrent networks in reading comprehension tasks. The first advantage of the proposed model is short response time due to parallelism of non-sequential output generation, proved by experiments on the SQuAD dataset. The second advantage is its potentially better representation, proved by better results compared to models using recurrent networks on the TriviaQA dataset. The idea of using dilated convolutional networks as drop-in replacements for recurrent networks should have more value than just reading comprehension tasks. The paper should stress on this a bit more. The paper also lacks discussion with other models that use dilated convolution in different ways, such as WaveNet[1]. In general, the proposed model has novelty. The experimental results also sufficiently demonstrate the proposed advantages of the model. Therefore I recommend acceptance for it. [1] Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 (2016).",10,172,14.333333333333334,5.813253012048193,107,0,172,0.0,0.0232558139534883,0.9259,65,30,24,7,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,0,1,2,7,2,0,0,0,0,1,1,0,0,1,0,2,0,1,4,0,0,0.4301948677857387,0.4464274793061633,0.24163056100378388
ICLR2018-HJRV1ZZAW-R3,Reject,"This paper proposes a convnet-based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed-up. The proposed architecture combines a few recent DL techniques: residual networks, dilated convolutions and gated linear units. I understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring. However, I think the proposed architecture in this paper is less motivated. Why is GLU chosen? Why is dilation used? According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why? The architecture search (Table 3 and Figure 4) seems to quite arbitrary. I  would like to see more careful architecture search and ablation studies. Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF? I would like to see more explanations of Figure 4. How important is # of layers and residual connections? Minor: - It'd be helpful to add the formulation of gated linear units and residual layers. - It is necessary to put Table 5 in the main paper instead of Appendix. These are still the main results of the paper.",14,198,19.8,5.308108108108108,110,3,195,0.0153846153846153,0.0198019801980198,0.9686,46,37,38,13,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,0,1,1,10,0,2,4,1,0,0,0,0,0,0,0,2,1,0,6,2,0,0.4309487533703198,0.4477780706841707,0.24321972960631924
ICLR2018-HJYQLb-RW-R1,Reject,"Although GAN recently has attracted so many attentions, the theory of GAN is very poor. This paper tried to make a new insight of GAN from theories and I think their approach is a good first step to build theories for GAN. However, I believe this paper is not enough to be accepted. The main reason is that the main theorem (Theorem 4.1) is too restrictive. 1.tThere is no theoretical result for failed conditions. 2.tTo obtain the theorem, they assume the optimal discriminator. However, most of failed scenarios come from the discriminator dynamics as in Figure 2. 3.tThe authors could make more interesting results using the current ingredients. For instance, I would like to check the conditions on eta and T to guarantee d_TV(G_mu*, G_hat{mu})<  delta_1 when |mu*_1 u2013 mu*_2| >  delta_2 and |hat{mu}_1 u2013 hat{mu}_2| >  delta_3. In Theorem 4.1, the authors use the same delta for delta_1, delta_2, delta_3. So, it is not clear which initial condition or target performance makes the eta and T. ",11,165,13.75,4.836601307189542,96,1,164,0.0060975609756097,0.0292397660818713,0.5003,57,27,28,11,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,0,1,0,6,0,4,1,0,0,1,0,1,0,0,0,0,0,1,6,2,0,0.4299894948184802,0.3365497970158697,0.2150930679538219
ICLR2018-HJYQLb-RW-R2,Reject,"The authors proposes to study the impact of GANS in two different settings: 1. at each iteration, train the discriminator to convergence and do a (or a few) gradient steps for updating the generator 2. just do a few gradient steps for the discriminator and the generator This is done in a very toy example: a one dimensional equally weighted mixture of two Gaussian distributions. Clarity: the text is reasonably well written, but with some redundancy (e.g. see section 2.1) , and quite a few grammatical and mathematical typos here and there.  (e.g. Lemma 4.2., $f$ should be $g$, p7 Rect(0) is actually the empty set, etc..)  Gaining insights into the mechanics of training GANs is indeed important. The authors main finding is that, in this very particular setting, it seems that training the discriminator to convergence leads to convergence. Indeed, in real settings, people have tried such strategies for WGAN for examples. For standard GANs, if one adds a little bit of noise to the labels for example, people have also reported good result for such a strategy (although, without label smoothing, this will indeed leads to problems). Although I have not checked all the mathematical fine details, the approach/proof looks sound (although it is not at all clear too me why the choice of gradient step-sizes does not play a more important roles the the stated results). My biggest complain is that the situation analyzed is so simple (although the convergence proof is far from trivial) that I am not at all convinced that this sheds much light on more realistic examples. Since this is the main meat of the paper (i.e. no methodological innovations), I feel that this is too little an innovation for deserving publication in ICLR2018.",11,290,20.714285714285715,4.985294117647059,160,1,289,0.0034602076124567,0.0477815699658703,-0.0503,72,39,44,24,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 2}",0,0,0,1,0,6,1,2,0,1,0,3,0,1,1,0,0,0,0,1,4,0,2,0.5013975000777339,0.4463914518198273,0.27479052542577365
ICLR2018-HJYQLb-RW-R3,Reject,"Summary:  This paper studies the dynamics of adversarial training of GANs for Gaussian mixture model. The generator is a mixture of two Gaussians in one dimension. Discriminator is union of two intervals. Synthetic data is generated from a mixture of two Gaussians in one dimension. On this data, adversarial training is considered under three different settings depending on the discriminator updates: 1) optimal discriminator updates, 2) standard single step gradient updates, 3) Unrolled gradient updates with 5 unrolling steps. The paper notices through simulations that in a grid search over the initial parameters of generator optimal discriminator training always succeeds in recovering the true generator parameters, whereas the other two methods fail and exhibit mode collapse. The paper also provides theoretical results showing global convergence for the optimal discriminator updates method. Comments: 1) This is an interesting paper studying the dynamics of GANs on a simpler model (but rich enough to display mode collapse).  The results establish the standard issues noticed in training  GANs. However no intuition is given as to why the mode collapse happens or why the single discriminator updates fail (see for ex. https://arxiv.org/abs/1705.10461)? 2) The proposed method of doing optimal discriminator updates cannot be extended when the discriminator is a neural network. Does doing more unrolling steps simulate this behavior? What happens in your experiments as you increase the number of unrolling steps? 3) Can you write the exact dynamics used for Theorem 4.1 ? Is the noise added in each step? 4) What is the size of the initial discriminator intervals used for experiments in figure 2? ",15,261,21.75,5.5910931174089065,127,0,261,0.0,0.0037593984962406,-0.3939,85,27,45,6,6,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 10, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,0,1,0,11,10,3,1,0,0,1,0,0,0,0,0,0,0,0,10,0,0,0.4319605156200722,0.1167087280246978,0.1779110158905502
ICLR2018-HJZiRkZC--R1,Reject,"This paper presents a convolutional auto-encoder architecture for text encoding and generation. It works on the character level and contains a recursive structure which scales with the length of the input text. Building on the recent state-of-the-art in terms of architectural components, the paper shows the feasibility of this architecture and compares it to LSTM, showing the cnn superiority for auto-encoding. The authors have decided to encode the text into a length of 1024 - Why? Would different lengths result in a better performance? You write Minimal pre-processing is applied to them since our model can be applied to all languages in the same fashion.  Please be more specific. Which pre-processing do you apply for each dataset? I wonder if the comparison to a simple LSTM network is fair. It would be better to use a 2- or 3-layer network. Also, BLSTM are used nowadays. A strong part of this paper is the large amount of investigation and extra experiments. Minor issues: Please correct minor linguistic mistakes as well as spelling mistakes. In Fig. 3, for example, the t of Different is missing. An issue making it hard to read the paper is that most of the figures appear on another page than where they are mentioned in the text. the authors have chosen to cite a work from 1994 for the vanishing gradient problem. Note, that many (also earlier) works have reported this problem in different ways. A good analysis of all researches is performed in Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. (2001) Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.",17,267,15.705882352941176,5.133603238866397,151,0,267,0.0,0.0111524163568773,0.7972,80,30,45,6,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 2}",0,0,0,3,1,7,2,1,2,0,0,1,2,0,0,0,0,1,1,0,5,0,2,0.5731702105922435,0.4470134092546703,0.32604770895467305
ICLR2018-HJZiRkZC--R2,Reject,"The authors propose autoencoding text using a byte-level encoding and a convolutional network with shared filters such that the encoder and decoder should exhibit recursive structure. They show that the model can handle various languages and run various experiments testing the ability of the autoencoder to reconstruct the text with varying lengths, perturbations, depths, etc. The writing is fairly clear, though many of the charts and tables are hard to decipher without labels (and in Figure 8, training errors are not visible -- maybe they overlap completely?). Main concern would be the lack of experiments showing that the network learns meaningful representations in the hidden layer. E.g. through semi-supervised learning experiments or experiments on learning semantic relatedness of sentences. Obvious citations such as https://arxiv.org/pdf/1511.06349.pdf and https://arxiv.org/pdf/1503.00075.pdf are missing, along with associated baselines. Although the experiment with randomly permuting the samples is nice, would hesitate to draw any conclusions without results on downstream tasks and a clearer survey of the literature.",8,159,19.875,5.767295597484277,110,2,157,0.0127388535031847,0.0375,0.4939,49,22,26,5,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 3, 'CLA': 2}",0,0,1,2,0,1,3,1,1,0,0,1,1,0,0,0,0,0,1,0,1,3,2,0.5716602628738446,0.4447389325319414,0.3160131420320606
ICLR2018-HJZiRkZC--R3,Reject,"The paper aims to illustrated the representation learning ability of the convolutional autoencoder with residual connections is  proposed by to encode text at the byte level. The authors apply the proposed architecture to 3 languages and run comparisons with an LSTM.  Experimental results  with different perturbation of samples, pooling layers, and sample lengths are presented. The writing is fairly clear, however the presentation of tables and figures could be done better, for example, Fig. 2 is referred  to in page 3,  Table 2 which contains results is referred to on page 5, Fig 4 is referred to in page 6 and appears in page 5, etc. What kind of minimal preprocessing is done on the text? Are punctuations removed?  Is casing retained?  How is the space character encoded? Why was the encoded dimension always fixed at 1024? What is the definition of a sample here? The description of the various data sets could be moved to a table/Appendix, particularly since most of the results are presented on the enwiki dataset, which would lead to better readability of the paper.  Also results are presented only on a random 1M sample selected from these data sets, so the need for this whole page goes away. Comparing Table 2 and Table 3, the LSTM is at 67% error on the test set while the proposed convolutional autoencoder is at 3.34%.  Are these numbers on the same test set? While the argument that the LSTM does not generalize well due to the inherent memory learnt is reasonable, the differences in performance cannot be explained away with this. Can you please clarify this further? It appears that the byte error shoot up for sequences of length 512+ (fig. 6 and fig. 7) and seems entirely correlated with the amount of data than recursion levels. How do you expect these results to change for a different subset selection of training and test samples? Will Fig. 7 and Fig. 6 still hold? In Fig, 8, unless the static train and test error are exactly on top of the  recursive errors, they are not visible. What is the x-axis in Fig. 8? Please also label axes on all figures. While the datasets are large and would take a lot of time to process for each case study, a final result on the complete data set, to illustrate if the model does learn well with lots of data would have been useful.  A table showing generated sample text would also clarify the power of the model. With the results presented,  with a single parameter setting, its hard to determine what exactly the model learns and why.",26,437,21.85,4.803921568627451,201,4,433,0.0092378752886836,0.0334075723830735,0.6753,118,33,82,22,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 4, 'MET': 10, 'EXP': 4, 'RES': 2, 'TNF': 7, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 5, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 1}",0,0,0,2,4,10,4,2,7,0,0,3,0,0,0,0,0,0,5,0,12,0,1,0.5029642157537746,0.3404211408118194,0.25152253662075524
ICLR2018-HJcSzz-CZ-R1,Accept,"This paper is an extension of the ""prototypical network"" which will be published in NIPS 2017. The classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode.  The paper solves a new semi-supervised situation, which is more close to the setting of the real world, with an extension of the prototype network.  Sufficient implementation detail and analysis on results. However, this is definitely not the first work on semi-supervised formed few-shot learning. There are plenty of works on this topic [R1, R2, R3].  The authors are advised to do a thorough survey of the relevant works in Multimedia and computer vision community. Another concern is that the novelty. This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data. The experiments are also not enough. Not only some other works such as [R1, R2, R3]; but also the other nau00efve baselines should also be compared, such as directly nearest neighbor classifier, logistic regression, and neural network in traditional supervised learning. Additionally, in the 5-shot non-distractor setting on tiered ImageNet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset? [R1] ""Videostory: A new multimedia embedding for few-example recognition and translation of events,"" in ACM MM, 2014  [R2] ""Transductive Multi-View Zero-Shot Learning"", IEEE TPAMI 2015  [R3] ""Video2vec embeddings recognize events when examples are scarce,"" IEEE TPAMI 2014 ",13,260,21.666666666666668,5.493877551020408,152,0,260,0.0,0.0037593984962406,0.3366,82,47,37,17,10,5,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 4, 'DAT': 1, 'MET': 4, 'EXP': 7, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 0}",0,0,8,4,1,4,7,1,0,1,0,2,1,1,0,1,3,1,0,0,8,2,0,0.7163401990876261,0.5601120074046736,0.4596695222590572
ICLR2018-HJcSzz-CZ-R2,Accept,"In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes.  The studied problem is interesting, and the paper is well-written. Extensive experiments are performed to demonstrate the effectiveness of the proposed methods.  While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning). On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers. Overall, I would like to vote for a weakly acceptance regarding this paper.",6,118,19.666666666666668,5.640350877192983,79,1,117,0.0085470085470085,0.0166666666666666,0.5927,30,14,22,5,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,1,1,3,0,3,2,0,0,0,0,2,0,0,0,0,3,0,0,1,3,1,1,0.4293220271224686,0.5568955437221372,0.2710860126468103
ICLR2018-HJcSzz-CZ-R3,Accept,"This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible  strategies. One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the  assigned pseudo-labels. Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to any of the known categories. In practice this second solution is analogous to the first, but a general 'distractor' class is added. Finally the third technique learns to weight the samples according to their distance to the original prototypes. These strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained  on some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting multiple times a large dataset), then they are used on a final target task with again few labeled data and large  unlabeled samples but beloning to a different set of categories. + the paper is well written, well organized and overall easy to read +/-  this work builds largely on previous work. It introduces only some small technical novelty inspired by soft-k-means clustering that anyway seems to be effective. + different aspect of the problem are analyzed by varying the number of disctractors and varying the level of semantic relatedness between the source and the target sets  Few notes and questions 1) why for the omniglot experiment the table reports the error results?  It would be better to present accuracy as for the other tables/experiments 2) I would suggest to use source and target instead of train and test -- these two last terms are confusing because actually there is a training phase also at test time. 3) although the paper indicate that there are different other few-shot methods that could be applicable here ,  no other approach is considered besides the prothotipical network and its variants.  An further external reference  could be used to give an idea of what would be the experimental result at least in the supervised case.",16,331,27.58333333333333,5.255451713395638,183,1,330,0.003030303030303,0.0229885057471264,0.9393,78,51,63,14,10,6,"{'ABS': 0, 'INT': 0, 'RWK': 12, 'PDI': 3, 'DAT': 3, 'MET': 5, 'EXP': 7, 'RES': 3, 'TNF': 1, 'ANA': 2, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 2}",0,0,12,3,3,5,7,3,1,2,1,1,0,0,0,1,2,0,1,0,8,2,2,0.7170159287868625,0.6712562169381909,0.5043980647895888
ICLR2018-HJcjQTJ0W-R1,Reject,"1. This is an interesting paper - introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm 2. From the initial part of the paper, it seems that the proposed PrivyNet is supposed to be a meta-learning framework to split a DNN in order to improve privacy while maintaining a certain accuracy level 3. However, the main issue is that the meta-learning mechanism is a bit ad-hoc and empirical - therefore not sure how seamless and user-friendly it will be in general, it seems it needs empirical studies for every new application - this basically involves generation of a pareto front and then choose pareto-optimal points based on the user's requirements, but it is unclear how a privy net construction based on some data set considered from the internet has the ability to transfer and help in maintaining privacy in another type of data set, e.g., social media pictures",3,156,39.0,5.129251700680272,99,2,154,0.0129870129870129,0.0440251572327044,0.8488,41,21,26,5,4,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,0,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0.2857735855761414,0.1111111111111111,0.11242493925560605
ICLR2018-HJcjQTJ0W-R2,Reject,"Summary: The paper studies the problem of effectively training Deep NN under the constraint of privacy. The paper first argues that achieving privacy guarantees like differential privacy is hard, and then provides frameworks and algorithms that quantify the privacy loss via Signal-to-noise ratio. In my opinion, one of the main features of this work is to split the NN computation to local computation and cloud computation, which ensures that unnecessary amount of data is never released to the cloud. Comments: I have my concerns about the effectiveness of the notion of privacy introduced in this paper. The definition of privacy loss in Equation 5 is an average notion, where the averaging is performed over all the sensitive training data samples. This notion does not seem to protect the privacy of every individual training example, in contrast to notions like differential privacy. Average case notions of privacy are usually not appreciated in the privacy community because of their vulnerability to a suite of attacks. The paper may have a valid point that differential privacy is hard to work with, in the case of Deep NN. However, the paper needs to make a much stronger argument to defend this claim.",9,198,22.0,5.139896373056994,101,2,196,0.010204081632653,0.0151515151515151,-0.8856,63,19,28,9,3,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 2, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,0,0,4,0,2,3,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0.2148861975684835,0.111733068545954,0.0870680614002565
ICLR2018-HJcjQTJ0W-R3,Reject,"1. Paper summary   This paper describes a technique using 3 neural networks to privatize data and make predictions: a feature extraction network, an image classification network, and an image reconstruction network. The idea is to learn a feature extraction network so that the image classification network performs well and the image reconstruction network performs poorly. 2. High level paper - subjective  I think the presentation of the paper is somewhat scattered: In section 2 the authors introduce their network and their metric for utility and privacy and then immediately do a sensitivity analysis. Section 3 continues with a sensitivity analysis now considering performance and storage of the method. Then 2.5 pages are spent on channel pruning. I would have liked if the authors spent more time justifying why we should trust their method as a privacy preserving technique (described in detail below). The authors clearly performed an impressive amount of sensitivity experiments. Assuming the privacy claims are reasonable (which I have some doubts about below) then this paper is clearly useful to any company wanting to do privacy preserving classification. At the same time I think the paper does not have a significant amount of machine learning novelty in it. 3. High level technical  I have a few doubts about this method as a privacy-preserving technique: - Nearly every privacy-preserving technique gives a guarantee, e.g., differential privacy guarantees a statistical notion of privacy and cryptographic methods guarantee a computational notion of privacy. In this work the authors provide a way to measure privacy but there is no guarantee that if someone uses this method their data will be private, by some definition, even under certain assumptions. - Another nice thing about differential privacy and cryptography is that they are impervious to different algorithms because it is statistically hard or computationally hard to reveal sensitive information. Here there could be a better image reconstruction network that does a better job of reconstructing images than the ones used in the paper. - It's not clear to my why PSNR is a useful way to measure privacy loss. I understand that it is a metric to compare two images that is based on the mean-squared error so a very private image should have a low PSNR while a not private image should have a high PSNR, but I have no intuition about how small the PSNR should be to afford a useful amount of privacy. For instances, in nearly all of the images of Figures 21 and 22 I think it would be quite easy to guess the original images. 4. 1/2 sentence summary  While the authors did an extensive job evaluating different settings of their technique I have serious doubts about it as a privacy-preserving method.",16,450,21.428571428571427,5.479115479115479,195,4,446,0.0089686098654708,0.0413943355119825,0.9587,137,47,72,22,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 4, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 5, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,9,1,0,1,4,0,3,0,0,0,1,0,0,5,0,7,0,0,0.4307972611015116,0.3373113536376044,0.21305590705050567
ICLR2018-HJdXGy1RW-R1,Reject,"The paper presents a new CNN architecture: CrescendoNet. It does not have skip connections yet performs quite well. Overall, I think the contributions of this paper are too marginal for acceptance in a top tier conference. The architecture is competitive on SVHN and CIFAR 10 but not on CIFAR 100. The performance is not strong enough to warrant acceptance by itself. FractalNets amd DiracNets (https://arxiv.org/pdf/1706.00388.pdf) have demonstrated that it is possible to train deep networks without skip connections and achieve high performance. While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet. Hence, CrescendoNet does not have the best performance among skip connection free networks. You claim that FractalNet shows no ensemble behavior. This is clearly not true because FractalNet has ensembling directly built in, i.e. different paths in the network are explicitly averaged. If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet. While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble. Besides, as Veit showed, ResNet also shows ensemble behavior. Hence, using ensembling in deep networks is not a significant contribution. The authors claim that Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance. I don't think the experiments show that ensemble behavior leads to high performance.  Just because a network performs averaging of different paths and individual paths perform worse than sets of paths doesn't imply that ensembling as a mechanism is in fact the cause of the performance of the entire architecture. Similary, you say On the other hand, the ensemble model can explain the performance improvement easily.  Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet. Path-wise training is not original enough or indeed different enough from drop-path to count as a major contribution. Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet. You claim that the number of layers increase exponentially in FractalNet. This is misleading. The number of layers increases exponentially in the number of paths, but not in the depth of the network. In fact, the number of layers is linear in the depth of the network. Since depth is the meaningful quantity here, CrescendoNet does not have an advantage over FractalNet in terms of layer number. Also, it is always possible to simply add more paths to FractalNet if desired without increasing depth. Instead of using 1 long paths, one can simply use 2, 3, 4 etc. While this is not explicitly mentioned in the FractalNet paper, it clearly would not break the design principle of FractalNet which is to train a path of multiple layers by ensembling it with a path of fewer layers.  CrescendoNets do not extend beyond this design principle. You say that First, path-wise training procedure significantly reduces the memory requirements for convolutional layers, which constitutes the major memory cost for training CNNs. For example, the higher bound of the memory required can be reduced to about 40% for a Crescendo block with 4 paths where interval   1. This is misleading, as you need to store the weights of all convolutional layers to compute the forward pass and the majority of the weights of all convolutional layers to compute the backward pass, no matter how many weights you intend to update. In a response to a question I posed, you mentioned that we you meant was we use about 40% memory for the gradient computation and storage. Fair enough, but gradient computation and storage is not mentioned in the paper. Also, the reduction to 40% does not apply e.g. to vanilla SGD because the computed gradient can be immediately added to the weights and does not need to be stored or combined with e.g. a stored momentum term. Finally, nowhere in the paper do you mention which nonlinearities you used or if you used any at all. In future revisions, this should be rectified. While I can definitely imagine that your network architecture is well-designed and a good choice for image classification tasks, there is a very saturated market of papers proposing various architectures for CIFAR-10 and related datasets. To be accepted to ICLR, either outstanding performance or truly novel design principles are required.",39,744,17.302325581395348,5.177622377622377,290,3,741,0.0040485829959514,0.02803738317757,0.9886,196,73,137,58,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 1, 'DAT': 3, 'MET': 25, 'EXP': 7, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 3, 'EMP': 12, 'SUB': 3, 'CLA': 0}",0,0,7,1,3,25,7,3,0,2,0,3,0,2,0,1,1,5,0,3,12,3,0,0.6501703260826112,0.6742298026396724,0.45642609320892036
ICLR2018-HJdXGy1RW-R2,Reject,"This paper proposes a new convolutional network architecture, which is tested on three image classification tasks. Pros: The network is very clean and easy to implement, and the results are OK. Cons: The idea is rather incremental compared to FractalNet. The results seem to be worse than existing networks, e.g., DenseNet (Note that SVHN is no longer a good benchmark dataset for evaluating state-of-the-art CNNs). Not much insights were given. One additional question: Skip connections have been shown to be very useful in ConvNets. Why not adopt it in CrescendoNet?  What's the point of designing a network without skip connections? ",8,100,14.285714285714286,5.237113402061856,77,0,100,0.0,0.0196078431372549,0.7777,30,12,23,7,4,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,0,1,2,0,5,0,3,0,0,0,0,0,0,0,0,0,1,0,0,4,1,1,0.286885757816699,0.4463103167489733,0.16170708053798005
ICLR2018-HJdXGy1RW-R3,Reject,"In this paper, the authors propose  a new network architecture, CrescendoNet, which is a simple stack of building blocks without residual connections. To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet. The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections. Contributions:  1 The authors proposed Crescendo block that consists of convolution paths with increasing depth. 3 The authors conducted experiments on three benchmark datasets and show promising performance of CrescendoNet . 3 The authors proposed a path-wise training procedure to reduce memory requirement in training. Negative points:  1 The motivation of the paper is not clear. It is well known that the residual connections are important in training deep CNNs and have shown remarkable performance on many tasks. The authors propose the CrescendoNet which is without residual connections. However, the experiments 2show that CrescendoNet is worse than ResNet. 2  The contribution of this paper is not clear. In fact, the performance of CrescendoNet is worse than most of the variants of residual networks, e.g., Wide ResNet, DenseNet, and ResNet with pre-activation. Besides, it seems that the proposed path-wise training procedure also leads to significant performance degradation. 3 The novelty of this paper is insufficient. The CrescendoNet is like a variant of the FractalNet, and the only difference is that the number of convolutional layers in Crescendo blocks grows linearly. 4 The experimental settings are unfair. The authors run 700 epochs and even 1400 epochs with path-wise training on CIFAR, while the baselines only have 160~400 epochs for training. 5 The authors should provide the experimental results on large-scale data sets (e.g. ImageNet) to prove the effectiveness of the proposed method, as they only conduct experiments on small data sets, including CIFAR-10, CIFAR-100, and SVHN. 6 The model size of CrescendoNet is larger than residual networks with similar performance. Minor issues:  1In line 2, section 3.4, the period after ""(128, 256, 512)"" should be removed. ",20,337,15.318181818181818,5.682539682539683,138,1,336,0.0029761904761904,0.0261627906976744,-0.8825,111,43,44,10,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 6, 'DAT': 4, 'MET': 9, 'EXP': 8, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,0,0,6,4,9,8,3,0,0,0,2,0,1,0,1,0,4,0,0,5,1,1,0.5031665015746667,0.5583948729664974,0.31518293448054546
ICLR2018-HJewuJWCZ-R1,Accept,"This paper focuses on the problem of machine teaching, i.e., how to select a good strategy to select training data points to pass to a machine learning algorithm, for faster learning. The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns, and use policy gradient to update the teacher parameters. I find the definition of the state in this case very interesting. The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster. Overall I think that this paper is decent. The angle the authors took is interesting (essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup). The problem formulation is mostly reasonable, and the evaluation seems quite convincing. The paper is well-written: I enjoyed the mathematical formulation (Section 3). The authors did a good job of using different experiments (filtration number analysis, and teaching both the same architecture and a different architecture) to intuitively explain what their method actually does. At the same time, though, I see several important issues that need to be addressed if this paper is to be accepted. Details below.   1. As much as I enjoyed reading Section 3, it is very redundant. In some cases it is good to outline a powerful and generic framework (like the authors did here with defining teaching in a very broad sense, including selecting good loss functions and hypothesis spaces) and then explain that the current work focuses on one aspect (selecting training data points). However, I do not see it being the case here. In my opinion, selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it. But that is not very interesting (if you can propose new loss functions, that would be way cooler). I also do not see how to define an intuitive set of states in that case. Therefore, I think this section should be shortened. I also think that the authors should not discuss the general framework and rather focus on data teaching, which is the only focus of the current paper. The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions. 2. The authors should do a better job at explaining the details of the state definition, especially the student model features and the combination of data and current learner model. 3. There is only one definition of the reward - related to batch number when the accuracy first exceeds a threshold. Is accuracy stable, can it drop back down below the threshold in the next epoch? The accuracy on a held-out test set is not guaranteed to be monotonically increasing, right?  Is this a problem in practice (it seems to happen on your curves)? What about other potential reward definitions? And what would they potentially lead to?  n4. Experimental results are averaged over 5 repeated runs - a bit too small in my opinion. 5. Can the authors show convergence of the teacher parameter theta? I think it is important to see how fast the teacher model converges, too. 6. In some of your experiments, every training method converges to the same accuracy after enough training (Fig.2b), while in others, not quite (Fig. 2a and 2c). Why is this the case? Does it mean that you have not run enough iterations for the baseline methods? My intuition is that if the learner algorithm is convex, then ultimately they will all get to the same accuracy level, so the task is just to get there quicker. I understand that since the learner algorithm is an NN, this is not the case - but more explanation is necessary here - does your method also reduces the empirical possibility to get stuck in local minima? 7. More explanation is needed towards Fig.4c. In this case, using a teacher model trained on a harder task (CIFAR10) leads to much improved student training on a simpler task (MNIST). Why? 8. Although in terms of effective training data points the proposed method outperforms the other methods, in terms of time (Fig.5) the difference between it and say, NoTeach, is not that significant (especially at very high desired accuracy). More explanation needed here.  Read the rebuttal and revision and slightly increased my rating.",39,729,18.225,5.0813953488372094,295,7,722,0.0096952908587257,0.020297699594046,0.9893,205,74,131,54,12,5,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 24, 'EXP': 7, 'RES': 5, 'TNF': 2, 'ANA': 6, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 2, 'EMP': 22, 'SUB': 8, 'CLA': 2}",1,1,1,2,1,24,7,5,2,6,0,4,0,1,0,0,0,0,2,2,22,8,2,0.8639797092307898,0.5695259017123308,0.5504727673269948
ICLR2018-HJewuJWCZ-R2,Accept,"The authors define a deep learning model composed of four components:  a student model, a teacher model, a loss function, and a data set. The student model is a deep learning model (MLP, CNN, and RNN were used in the paper). The teacher model learns via reinforcement learning which items to include in each minibatch of the data set. The student model learns according to a standard stochastic gradient descent technique (Adam for MLP and CNN, Momentum-SGD for RNN), appropriate to the data set (and loss function), but only uses the data items of the minibatch chosen by teacher model. They evaluate that their method can learn to provide learning items in an efficient manner in two situations: (1) the same student model-type on a different part of the same data set, and (2) adapt the teaching model to teach a new model-type for a different data set. In both circumstances, they demonstrate the efficacy of their technique and that it performs better than other reasonable baseline techniques: self-paced learning, no teaching, and a filter created by randomly reordering the data items filtered out from a teaching model. This is an extremely impressive manuscript and likely to be of great interest to many researchers in the ICLR community. The research itself seems fine, but there are some issues with the discussion of previous work. Most of my comments focuses on this. The authors write that artificial intelligence has mostly overlooked the role of teaching, but this claim is incorrect. There is a long history of research on teaching in artificial intelligence. Two literatures of note are intelligent tutoring and machine teaching in the computational learnability literature. A good historical hook to intelligent tutoring is Anderson, J. R., Boyle, C. F., & Reiser, B. J. (1985). Intelligent tutoring systems. Science, 228. 456-462. The literature is still healthy today. One offshoot of it has its own society with conferences and a journal devoted to it (The International Artificial intelligence in Education Society: http://iaied.org/about/). For the computational learnability literature, complexity analysis for teaching has a subliterature devoted to it (analogous to the learning literature). Here is a hook into that literature: Goldman, S., & Kerns. M. (1995). On the complexity of teaching. Journal of Computer and Systems Sciences, 50(1), 20-31. One last related literature is pedagogical teaching from computational cognitive science. This one is a more recent development. Here are two articles, one that provides a long and thorough discussion that is a definitive start to the literature, and another that is most relevant to the current paper, on applying pedagogical teaching to inverse reinforcement learning (a talk at NIPS 2016). Shafto, P., Goodman, N. D., & Griffiths, T. L. (2014). A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive Psychology, 71, 55-89.  Ho, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016).  I hope all of this makes it clear to the authors that it is inappropriate to claim that artificial intelligence has ""largely overlooked"" or ""largely neglected"". One other paper of note given that the authors train a MLP is an optimal teaching analysis of a perceptron: (Zhang, Ohannessian, Sen, Alfeld, & Zhu, 2016; NIPS).   ",27,528,12.0,5.461702127659574,222,2,526,0.0038022813688212,0.0074211502782931,0.9973,182,71,65,12,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 15, 'PDI': 0, 'DAT': 2, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 4}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,15,0,2,6,0,0,0,0,0,2,0,4,1,0,0,1,0,0,1,0,0,0.3596301420975716,0.3333333333333333,0.18327510010730919
ICLR2018-HJewuJWCZ-R3,Accept,"This paper suggests a learning to teach framework. Following a similar intuition as self-paced learning and curriculum learning, the authors suggest to learn a teaching strategy,  corresponding to choices over the data presented to the learner (and potentially other decisions  about the learner, such as the  algorithm used).  The problem is framed as RL problem, where the state space corresponds to learning configurations, and teacher actions change the state.  Supervision is obtained by observing the learner's performance. I found it very difficult to understand the evaluation. First, there is quite a bit of recent work on learning to teach and curriculum learning. It would be helpful if there are comparisons to these models, and use similar datasets.  It's not clear if an evaluation on the MNIST data set is particularly meaningful.  The implementation of SPL seems to hurt performance in some cases (slower convergence on the IMDB dataset), can you explain it? In other text learning task (e.g., [1]) SPL showed improved performance. The results over the IMDB dataset in the original paper [2] are higher than the ones reported here, using a simple model (BoW). Second, in non-convex problems, one can expect curriculum learning approaches to also perform better, not just converge faster.  This aspect is not really discussed. Finally,  I'm not sure I understand the X axis in Figure 2, the (effective) number of examples is much higher than the size of the dataset. Does it indicate the number of  iterations over the same dataset? I would also like to see some analysis of what's actually being learned by the teacher. Some qualitative analysis, or even feature ablation study would be helpful. [1] Easy Questions First? A Case Study on Curriculum Learning for Question Answering. Sachan et-al. [2] Learning Word Vectors for Sentiment Analysis. Maas et-al.",16,298,15.68421052631579,5.243816254416961,167,2,296,0.0067567567567567,0.0324675324675324,0.8269,92,36,54,20,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 5, 'MET': 7, 'EXP': 3, 'RES': 3, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 0}",0,0,1,2,5,7,3,3,2,2,0,0,1,0,0,0,0,3,2,0,4,3,0,0.6448937710714396,0.4468195638037996,0.35904137715822776
ICLR2018-HJg1NTGZRZ-R1,Reject,This paper proposes a direct way to learn low-bit neural nets. The idea is introduced clearly and rather straightforward. pros: (1) The idea is introduced clearly and rather straightforward. (2) The introduction and related work are well written. cons: The provided experiments are weak to demonstrate the effectiveness of the proposed method. (1) only small networks on relatively small datasets are tested. (2) the results on MNIST and CIFAR 10 are not good enough for practical deployment.,7,77,11.0,5.507042253521127,50,0,77,0.0,0.0259740259740259,0.2946,20,10,14,9,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 2, 'MET': 2, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 3}",0,1,1,3,2,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,2,3,0.5004048825773897,0.3348461948530488,0.25070819792596716
ICLR2018-HJg1NTGZRZ-R2,Reject,"This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4. 1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong. 2. The experimental section of this paper needs improvement. a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with.    b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc. c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these. d. No wall-time and real memory numbers are reported.",12,179,8.95,5.225609756097561,114,1,178,0.0056179775280898,0.0273224043715847,0.5083,60,26,36,9,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,0,3,2,1,7,4,1,0,0,0,0,0,0,0,0,0,3,0,0,6,3,0,0.4305514442958863,0.3368907986385711,0.21586111395774885
ICLR2018-HJg1NTGZRZ-R3,Reject,"The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used). Updates are made to the parameter representing the # of bits via the sign of its gradient. Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10. Overall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. I have a few concerns: First, I find the discussion around the training methodology insufficient. Inherently, the objective is discontinuous since # of bits is a discrete parameter. This is worked around by updating the parameter using the sign of its gradient. This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume. It's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss). We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either. I would be less concerned about the above points if I found the experiments compelling. Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful. Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad. Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve. Overall, while I like the and think the goal is good, I think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning. I can't recommend acceptance. ",23,484,21.043478260869566,5.085836909871245,220,6,478,0.0125523012552301,0.0574948665297741,-0.9223,127,57,75,32,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 3, 'MET': 14, 'EXP': 12, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 13, 'SUB': 2, 'CLA': 0}",0,0,0,4,3,14,12,3,0,0,0,1,0,0,0,0,0,0,0,1,13,2,0,0.4331335646101978,0.3409034990597705,0.21154860358514113
ICLR2018-HJjePwx0--R1,Reject,"**I am happy to see some good responses from the authors to my questions. I am raising my score a bit higher. Summary:  A new stochastic method based on trust region (TR) is proposed. Experiments show improved generalization over mini-batch SGD, which is the main positive aspect of this paper.  The main algorithm has not been properly developed; there is too much focus on the convergence aspects of the inner iterations, for which there are many good algorithms already in the optimization literature. There are no good explanations for why the method yields better generalization. Overall, TR seems like an interesting idea, but it has neither been carefully expanded or investigated. Let me state the main interesting results before going into criticisms: 1. TR method seems to generalize better than mini-batch SGD. 2. TR seems to lose generalization more gracefully than SGD when batch size is increased. [But note here that mini-batch SGD is not a closed chapter. With better ways of adjusting the noise level via step-size control (larger step sizes mean more noise) the loss of generalization associated with large mini-batch sizes can be brought down. See, for example: https://arxiv.org/pdf/1711.00489.pdf.] 3. Hybrid method is even better. This only means that more understanding is needed as to how TR can be combined with SGD. Trust region methods are generally batch methods. Algorithm 1 is also stated from that thinking and it is a well-known optimization algorithm. The authors never mention mini-batch when Algorithm 1 is introduced. But the authors clearly have only the stochastic min-batch implementation of the algorithm in mind. One has to wait till we go into the experiments section to read something like: Lastly, although in theory, we need full gradient and full Hessian to guarantee convergence, calculating them in each iteration is not practical, so we calculate both Hessian and gradient on subsampled data to replace the whole dataset for readers to realize that the authors are talking about a stochastic mini-batch method. This is a bad way of introducing the main method. This stochastic version obviously requires a step size; so it would have been proper to state the stochastic version of the algorithm instead of the batch algorithm in Algorithm 1. Instead of saying that in passing why not explicitly state it in key places, including the abstract and title? I suggest TR be replaced by Stochastic TR everywhere. Also, what does step size mean in the TR method? I suggest that all these are fully clarified as parts of Algorithm 1 itself. Trust region subproblem (TRS) has been analyzed and developed so much in the optimization literature. For example, the conjugate gradient-based method leading to the Steihaug-Toint point is so much used. [Note: Here, the gradient refers to the gradient of the quadratic model, and it uses only Hessian-vector products.] http://www.ii.uib.no/~trond/publications/papers/trust.pdf. The authors spend so much effort developing their own algorithm! Also, in actual implementation, they only use a crude version of the inner algorithm for reasons of efficiency. The paper does not say anything about the convergence of the full algorithm. How good are the trust region updates based on q_t given the huge variability associated with the mini-batch operation?  The authors should look at several existing papers on stochastic trust region and stochastic quasi-Newton methods, e.g., papers from Katya Scheinberg (Lehigh) and Richard Byrd (Colorado)'s groups. The best-claimed method of the method, called Hybrid method is also mentioned only in passing, and that too in a scratchy fashion (see end of subsec 4.3): To enjoy the best of both worlds, we also introduce a ""hybrid"" method in the Figure 3, that is, first run TR method for several epochs to get coarse solution and then run SGD for a while until fully converge. Our rule of thumb is, when the training accuracy raises slowly, run SGD for 10 epochs (because it's already close to minimum). We find this ""hybrid"" method is both fast and accurate, for both small batch and large batch.   Explanations of better generalization properties of TR over SGD are important. I feel this part is badly done in the paper. For example, there is this statement: We observe that our method (TR) converges to solutions with much better test error but worse training error when batch size is larger than 128. We postulate this is because SGD is easy to overfit training data and ""stick"" to a solution that has a high loss in testing data, especially with the large batch case as the inherent noise cannot push the iterate out of loss valley while our TR method can.  Frankly, I am unable to decipher what is being said here. There is an explanation indicating that switching from SGD to TR causes an uphill movement (which I presume, is due to the trust region radius r being large); but statements such as - this will lead to climbing over to a wide minimum etc. are too strong; no evidence is given for this. There is a statement - even if the exact local minima is reached, the subsampled Hessian may still have negative curvature - again, there is no evidence. Overall, the paper only has a few interesting observations, but there is no good and detailed experimental analysis that help explain these observations. The writing of the paper needs a lot of improvement.       ",45,883,19.622222222222224,5.010538641686183,374,4,879,0.0045506257110352,0.0244716351501668,0.9975,242,127,158,54,12,6,"{'ABS': 1, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 24, 'EXP': 6, 'RES': 7, 'TNF': 1, 'ANA': 7, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 2, 'REC': 1, 'EMP': 30, 'SUB': 4, 'CLA': 1}",1,1,1,2,2,24,6,7,1,7,0,3,0,3,0,0,0,3,2,1,30,4,1,0.8640156667125666,0.6853193558402606,0.6155022628741647
ICLR2018-HJjePwx0--R2,Reject,"The paper proposes training neural networks using a trust region method, in which at each iteration a (non-convex) quadratic approximation of the objective function is found, and the minimizer of this quadratic within a fixed radius is chosen as the next iterate, with the radius of the trust region growing or shrinking at each iteration based on how closely the gains of the quadratic approximation matched those observed on the objective function. The authors claim that this approach is better at avoiding  arrow local optima, and therefore will tend to generalize better than minibatched SGD. The main novelty seems to be algorithm 2, which finds the minimizer of the quadratic approximation within the trust region by performing GD iterations until the boundary is hit (if it is--it might not, if the quadratic is convex), and then Riemannian GD along the boundary. The paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge. I had particular difficulty accepting that the phase 1 GD iterates would never hit the boundary if the quadratic was strongly convex, although I accept that it is true due to the careful choice of step size and initialization (assumptions 1 and 2). The central claim of the paper, that a trust region method will be better at avoiding narrow basins, seems plausible, since if the trust region is sufficiently large then it will simply pass straight over them. But if this is the case, wouldn't that imply that the quadratic approximation to the objective function is poor, and therefore that line 5 of algorithm 1 should shrink the trust region radius? Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4. I don't see why this is true--the quadratic approximation would be likely to capture the narrow basin only. This skepticism aside, the experiments in figure 2 do clearly show that, while the proposed approach doesn't converge nearly as quickly as SGD in terms of training loss, it does ultimately find a solution that generalizes better, as long as both SGD and TR use the same batch size (but I don't see why they should be using the same batch size). How does SGD with a batch size of 1 compare to TR with the batch sizes of 512 (CIFAR10) or 1024 (STL10)? Section 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid). Furthermore, 64 was the smallest batch size considered, but SGD was performing monotonically better as the batch size decreased, so one would expect it to be still better for 32, 16, etc. Smaller comments:  You say that you base the Hessian and gradient estimates on minibatched samples. I assume that the same is true for the evaluations of F on line 4 of Algorithm 1? Do these all use the same minibatch, at each iteration? On the top of page 3: M is the matrix size. Is this the number of elements, or the number of rows/columns? Lemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)? You cite Nocedal & Wright, but could you please provide a page number (or at least a chapter)? On the top of page 5, Line 10 of Algorithm 1: I think you mean Line 11 of Algorithm 2.",21,627,44.785714285714285,4.855172413793103,253,7,620,0.0112903225806451,0.0396825396825396,0.9952,155,61,109,38,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 17, 'EXP': 3, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 2}",0,0,0,2,2,17,3,1,2,0,0,1,1,0,0,1,0,1,0,0,11,1,2,0.5757227253949992,0.5618562649748394,0.35702288757012085
ICLR2018-HJjePwx0--R3,Reject,"The paper develops an efficient algorithm to solve the subproblem of the trust region method with an asymptotic linear convergence guarantee, and they demonstrate the performances of the trust region method incorporating their efficient solver in deep learning problems. It shows better generation errors by trust region methods than SGD in different tasks, despite slower running time, and the authors speculate that trust-region method can escape sharp minima and converge to wide minima and they illustrated that through some hybrid experiment. The paper is organized well. 1.  The result in Section 4.3 empirically showed that Trust Region Method could escape from sharp local minimum. The results are interesting but not quite convincing. The terms about sharp and wide minima are ambiguous. At best, this provides a data point in an area that has received attention, but the lack of precision about sharp and wide makes it difficult to know what the more general conclusions are. It might help to show the distance between the actual model parameters that those algorithms converge to. 2. As well know, VGG16 with well training strategy (learning rate decay) could achieve at least 92 percent accuracy. In the paper, the author only got around 83 percent accuracy with SGD and 85 percent accuracy with TR. Why is this. 3. In section 4.2, it said Although we can also define Hessian on ReLU function, it is not well supported on major platforms (Theano/PyTorch). Likewise, we find max-pooling is also not supported by platforms to calculate higher order derivative, one way to walk around is to change all the max-pooling layers to avg- pooling, it hurts accuracy a little bit, albeit this is not our primary concern.  It is my understanding that Pytorch support higher order derivative both for ReLu and Max-pooling. Hence, it is not an explanation for not using ReLu and Max-pooling. Please clarify 4. In section 4.3, the authors claimed that numerical diffentiation only hurts 1 percent error for second derivative. Please provide numerical support. 5. The setting of numerical experiments is not clear, e.g. value of N1 and N2. This makes it hard to reproduce results. 5. It's not clear whether this is a theoretical paper or an empirical paper. For example, there is a lot of math, but in Section 4.5 the authors seem to hedge and say We give an intuitive explanation ... and leave the rigorous analysis to future works.  Please clarify.  ",22,401,12.935483870967742,5.102362204724409,203,3,398,0.007537688442211,0.0294840294840294,0.7766,110,50,64,23,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 2, 'RES': 9, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 16, 'SUB': 2, 'CLA': 0}",0,0,0,1,0,10,2,9,0,2,1,2,0,1,0,0,0,0,1,0,16,2,0,0.5742822495746845,0.3427693713642994,0.2889941198428534
ICLR2018-HJqUtdOaZ-R1,Reject,"The paper presents a method for feature projection which uses a two level neural network like structure to generate new features from the input features. The weights of the NN like structure are optimised using a genetic search algorithm which optimises the cross-validation error of a nearest neighbor classifier. The method is tested on four simple UCI datasets. There is nothing interesting or novel about the paper. It is not clear whether the GA optimisation takes place on the level of cross validation error estimation or within an internal validation set as it should have been the case. The very high accuracies reported seem to hint the latter, which is a serious methodological error. The poor language and presentation does not help in clearing that, as it does not help in general. ",7,132,16.5,5.102362204724409,88,0,132,0.0,0.0451127819548872,-0.8186,37,15,25,5,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,0,1,1,4,1,0,0,0,0,2,0,0,0,1,0,0,0,0,2,0,1,0.3579388522922617,0.3339552907681763,0.17577498398465868
ICLR2018-HJqUtdOaZ-R2,Reject,"This paper proposes using a feedforward neural network (FFNN) to extract intermediate features which are input to a 1NN classifier. The parameters of the FFNN are updated via a genetic algorithm with a fitness function defined as the error on the downstream classification, on a held-out set. The performance of this approach is measured on several UCI datasets and compared with baselines. u2013 The paper's main contribution seems to be a neural network with a GA optimization for classification that can learn ""intelligent combinations of features"", which can be easily classified by a simple 1NN classifier. But isn't this exactly what neural networks do u2013 learn intelligent combinations of features optimized (in this case, via GA) for a downstream task? This has already been successfully applied in multiple domains eg. in computer vision (Krizhevsky et al, NIPS 2011), NLP (Bahdanau et al 2014), image retrieval (Krizhevsky et al. ESANN 2011) etc, and also studied comprehensively in autoencoding literature. There also exists prior work on optimizing neural nets via GA (Leung, Frank Hung-Fat et al., IEEE Transactions on Neural networks 2003). However, this paper claims both as novelties while not offering any improvement / comparison.  u2013 The claim ""there is no need to use more powerful and complex classifier anymore"" is unsubstantiated, as the paper's approach still entails using a complex classifier (a FFNN) to learn an optimal intermediate representation. u2013 The choice of activations is not motivated, and performance on variants is not reported. For instance, why is that particular sigmoid formulation used? u2013 The use for a genetic algorithm for optimization is not motivated, and no comparison is made to the performance and efficiency of other approaches (like standard backpropagation). So it is unclear why GA makes for a better choice of optimization, if at all. u2013 The primary baselines compared to are unsupervised methods (PCA and LDA), and so demonstrating improvements over those with a supervised representation does not seem significant or surprising. It would be useful to compare with a simple neural network baseline trained for K-way classification with standard backpropagation (though the UCI datasets may potentially be too small to achieve good performance). u2013 The paper is poorly written, containing several typos and incomplete, unintelligible sentences, incorrect captions (eg. Table 4) etc. ",16,375,20.83333333333333,5.577586206896552,196,2,373,0.0053619302949061,0.0238095238095238,0.9932,103,57,61,20,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 3, 'DAT': 2, 'MET': 11, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,0,4,3,2,11,5,0,0,0,0,2,0,0,0,1,0,3,0,0,6,1,1,0.4318597058549174,0.5588996678441505,0.2712906959000172
ICLR2018-HJqUtdOaZ-R3,Reject,"The main issue is the scientific quality. What the authors call intelligent mapping and combining system for the proposed system is simply a fully connected neural network. Such systems have been largely investigated in the literature. The use of genetic algorithms has also been considered. Moreover, mapping features to some appropriate feature space has been widely investigated, including the choice of appropriate mapping. We didn't find anything intelligent in the proposed mapping. There are many spelling and grammatical errors. ",7,79,9.875,5.730769230769231,56,0,79,0.0,0.0125,-0.2216,20,12,20,7,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,0,0,3,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,1,0.2858328854379972,0.2222222222222222,0.12968773800105768
ICLR2018-HJr4QJ26W-R1,Reject,"+ Quality: The paper discusses an interesting direction of incorporating humans in the training of a generative adversarial networks in the hope of improving generated samples. I personally find this exciting/refreshing and will be useful in the future of machine learning. However, the paper shows only preliminary results in which the generator trained to maximize the PIR score (computed based on VGG features to simulate human aesthetics evaluation) indeed is able to do so. However, the paper lacks discussion / evidence of how hard it is to optimize for this VGG-based PIR score. In addition, if this was challenging to optimize, it'd be useful to include lessons for how the authors manage to train their model successfully. In my opinion, this result is not too surprising given the existing power of deep learning to fit large datasets and generalize well to test sets. Also, it is not clear whether the GAN samples indeed are improved qualitatively (with the incorporation of the PIR objective score maximization objective) vs. when there is no PIR objective. The paper also did not report sample quantitative measures e.g. Inception scores / MS-SSIM. I'd be interested in how their proposed VGG-based PIR actually correlates with human evaluation. + Clarity:  - Yosinski et al. 2014 citation should be Nguyen et al. 2015 instead (wrong author order / year). - In the abstract, the authors should emphasize that the PIR model used in this paper is based on VGG features. + Originality:  - The main direction of incorporating human feedback in the loop is original. + Significance:  - I think the paper contribution is lighter vs. ICLR standard. Results are preliminary. Overall, I find this direction exciting and hope the authors would keep pushing in this direction! However, the current manuscript is not ready for publication.",15,287,14.35,5.227758007117438,160,1,286,0.0034965034965034,0.0265780730897009,0.9861,83,38,59,20,11,6,"{'ABS': 1, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 1, 'CLA': 1}",1,0,1,1,1,4,4,3,0,1,0,2,1,1,0,1,3,0,0,1,4,1,1,0.7868802720242352,0.6686286122680913,0.5501146130639765
ICLR2018-HJr4QJ26W-R2,Reject,"Summary: This paper proposes an approach to generate images which are more aesthetically pleasing, considering the feedback of users via user interaction. However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture. Pros: + The paper is well-written and has just a few typos: 2.1: ""an Gan"". + The idea is very interesting.  Cons:  - Page 2- section 2- The reasoning that a deep-RL could not be more successful is not supported by any references and it is not convincing. - Page 3- para 3 - mathematically the statement does not sound since the 2 expressions are exactly equivalent. The slight improvement may be achieved only by chance and be due to computational inefficiency, or changing a seed. - Page 3- 2.2. Using a crowd-sourcing technique, developing a similarly small dataset (1000 images with 100 annotations) would normally cost less than 1k$. - Page 3- 2.2.It is highly motivating to use users feedback in the loop but it is poorly explained how actually the user's' feedback is involved if it is involved at all. - Page 4- sec 3 .. it should be seen as a success; the claim is not supported well. - Page 4- sec 3.2- last paragraph. This claim lacks scientific support, otherwise please cite proper references. The claim seems like a subjective understanding of conscious perception and unconscious perception of affective stimuli is totally disregarded. The experimental setup is not convincing. - Page 4. 3.3) Note that.. outdoor images this is implicitly adding the designers' bias to the results. The statement lacks scientific support. - Page 4. 3.3) the importance of texture and shape is disregarded. ""In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings, Yanulevskaya et al"" The architecture may lead in overfitting to users' feedback (being over-fit on the data with PIR measures)  - Page 6-Sec 4.2)  It had more difficulty optimizing for the three-color result why? please discuss it.  - The expectation which is set in the abstract and the introduction of the paper is higher than the experiments shown in the Experimental setup. ",21,350,14.0,5.284375,181,3,347,0.0086455331412103,0.0243243243243243,0.9405,93,38,66,27,9,4,"{'ABS': 1, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 6, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 2}",1,1,0,3,1,6,2,4,0,0,0,2,1,0,0,0,0,0,1,0,9,1,2,0.6445254234276653,0.4495012389940422,0.36274176249536044
ICLR2018-HJr4QJ26W-R3,Reject,"This paper proposes a technique to improve the output of GANs by maximising a separate score that aims to mimic human interactions. Summary: The goal of the technique to involve human interaction in generative processes is interesting. The proposed addition of a new loss function for this purpose is an obvious choice, not particularly involved. It is unclear to me whether the paper has value in its current form, that is without experimental results for the task it achieves. It feels to premature for publication. More comments: The main problem with this paper is that the proposed systems is designed for a human interaction setting but no such experiment is done or presented. The title is misleading, this may be the direction where the authors of the submission want to go, but the title  "".. with human interactions"" is clearly misleading. ""Model of human interactions"" may be more appropriate. The technical idea of this paper is to introduce a separate score in the GAN training process. This modifies the generator objective. Besides ""fooling"" the discriminator, the generator objective is to maximise user interaction with the generated batch of images. This is an interesting objective but since no interactive experiments presented in this paper, the rest of the experiments hinges on the definition of ""PIR"" (positive interaction rate)using a model of human interaction. Instead of real interactions, the submission proposes to maximise the activations of hidden units in a separate neural network. By choosing the hierarchy level and type of filter the results of the GAN differ. I could not appreciate the results in Figure 2 since I was missing the definition of PIR, how it is drawn in the training setup. Further I found it not surprising that the PIR changes when a highly parameterised model is trained for this task. The PIR value comes from a separate network not directly accessible during training time, nonetheless I would have been surprised to not see an increase. Please comment in the rebuttal and I would appreciate if the details of the synthetic PIR values on the training set could be explained. - Technically it was a bit unclear to me how the objective is defined. There is a PIR per level and filter (as defined in C4) but in the setup the L_{PIR} was mentioned to be a scalar function, how are the values then summarized? There is a PIR per level and feature defined in C4. - What does the PIR with the model in Section 3 stand for? Shouldn't be something like ""uniqueness"", that is how unique is an image in a batch of images be a better indicator? Besides, the intent of what possibly interesting PIR examples will be was unclear. E.g., the statement at the end of 2.1 is unclear at that point in the document.  How is the PIR drawn exactly? What does it represent? Is there a PIR per image? It becomes clear later, but I suggest to revisit this description in a new version. - Also I suggest to move more details from Section C4 into the main text in Section 3. The high level description in Section 3.  ",32,521,19.296296296296298,4.928425357873211,214,3,518,0.0057915057915057,0.0302457466918714,0.8999,146,54,90,20,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 11, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 5, 'CLA': 3}",0,0,0,2,0,10,11,5,0,2,0,1,0,0,1,0,0,0,0,0,15,5,3,0.4319919423258871,0.4537408247072401,0.24271842195180135
ICLR2018-HJrJpzZRZ-R1,,"This is a fine paper that generally reads as a new episode in a series on motion-based video prediction with an eye towards robotic manipulation [Finn et al. 2016, Finn and Levine 2017, Ebert et al. 2017]. The work is rather incremental but is competently executed. It is in line with current trends in the research community and is a good fit for ICLR. The paper is well-written, reasonably scholarly, and contains stimulating insights. I recommend acceptance, despite some reservations. My chief criticism is a matter of research style: instead of this deluge of barely distinguishable least-publishable-unit papers on the same topic, in every single conference, I wish the authors didn't slice so thinly, devoted more time to each paper, and served up a more substantial dish. Some more detailed comments:  - The argument for evaluating visual realism never quite gels and is not convincing. The paper advocates two primary metrics: accuracy of the predicted motion and perceptual realism of the synthesized images. The argument for motion accuracy is clear and is clearly stated: it's the measure that is actually tied to the intended application, which is using action-conditional motion prediction for control. A corresponding argument for perceptual realism is missing. Indeed, a skeptical reviewer may suspect that the authors needed to add perceptual realism to the evaluation because that's the only thing that justifies the adversarial loss. The adversarial loss is presented as the central conceptual contribution of the paper, but doesn't actually make a difference in terms of task-relevant metrics. A skeptical perspective on the paper is that the adversarial loss just makes the images look prettier but makes no difference in terms of task performance (control). This is an informative negative result. It's not how the paper is written, though.  - The ""no adversary""/""no adv"" condition in Table 1 and Figure 4 is misleading. It's not properly controlled. It is not the case that the adversarial loss was simply removed. The regression loss was also changed from l_1 to l_2. This is not right. The motivation for this control is to evaluate the impact of the adversarial loss, which is presented as the key conceptual contribution of the paper. It should be a proper control. The other loss should remain what it is in the full ""Ours"" condition (i.e., l_1). - The last sentence in the caption of Table 1 -- ""Slight improvement in motion is observed by training with an adversary as well"" -- should be removed. The improvement is in the noise. - Generally, the quantitative impact of the adversarial loss never comes together. The only statistically significant improvement is on perceptual image realism. The relevance of perceptual image realism to the intended task (control) is not substantiated, as discussed earlier. - In the perceptual evaluation procedure, the ""1 second"" restriction is artificial and makes the evaluated methods appear better than they are. If we are serious about evaluating image realism and working towards passing the visual Turing test, we should report results without an artificial time limit. They won't look as flattering, but will properly report our progress on this journey. If desired, the results of timed comparisons can also be reported, but reporting just a timed comparison with an artificial limit of 1 second may mislead some readers into thinking that we are farther along than we actually are. There are some broken sentences that mar an otherwise well-written paper:  - End of Section 1, ""producing use a learned discriminator and show improvements in visual quality"" - Beginning of Section 3, ""We first present the our overall network architecture""  - page 4, ""to choose to copy pixels from the previous frame, used transformed versions of the previous frame"" - page 4, ""convolving in the input image with""  - page 5, ""is know to produce""  - page 5, ""an additional indicating"" - page 5, ""Adam Kingma & Ba (2015)"" (use the other cite command)  - page 5, ""we observes""  - page 5, ""smaller batch sizes degrades""  - page 5, ""larger batch sizes provides"" ",35,651,18.6,5.262214983713355,302,2,649,0.0030816640986132,0.0250368188512518,-0.9806,179,84,108,47,8,7,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 5, 'TNF': 5, 'ANA': 2, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 13, 'SUB': 2, 'CLA': 7}",0,0,1,4,0,9,0,5,5,2,0,5,0,1,1,0,0,1,2,1,13,2,7,0.5741304019508988,0.7858963228531426,0.45201531211491236
ICLR2018-HJrJpzZRZ-R2,,"This paper is concerned with video prediction, for use in robotic motion planning. The task is performed on tabletop videos of a robotic arm manipulator interacting with various small objects. They use a prior model proposed in Finn et al. 2016, make several incremental architectural improvements, and use an adversarial loss function instead of an L2 loss. They also propose a new metric, motion accuracy, which uses the accuracy of the predicted position of the object instead of conventional metrics like PSNR, which is more relevant for robotic motion planning. They obtain significant quantitative improvements over the previous 2 papers in this domain (video prediction on tabletop with robotic arm and objects) on both type of metrics - image assessment and motion accuracy. They also evaluate realism images using AMT fooling - asking turks to chose the fake between between real and generated images, and obtain substantial improvements on this metric as well. A major point of concern is that they do not use the public dataset proposed in Finn et al. 2016, but use their own (smaller) dataset. They do not mention whether they train the previous methods on the new dataset, and some of their reported improvements may be because of this. They also do not report results on unseen objects, when occlusions are present, and on human motion video prediction, unlike the other papers. The adversarial loss helps significantly only with AMT fooling or realism of images, as expected because GANs produce sharp images rather than distributions, and is not very relevant for robot motion planning. The incremental architectural changes, different dataset and training are responsible for most of the other improvements.",11,273,21.0,5.286792452830189,137,1,272,0.0036764705882352,0.0181818181818181,0.3818,74,51,37,16,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 4, 'DAT': 2, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,0,3,4,2,4,2,2,0,0,0,0,0,0,0,0,1,1,0,0,3,2,0,0.4298388687274294,0.4457950358224518,0.24250531610312667
ICLR2018-HJrJpzZRZ-R3,,"1) Summary This paper proposes a flow-based neural network architecture and adversarial training for multi-step video prediction. The neural network in charge of predicting the next frame in a video implicitly generates flow that is used to transform the previously observed frame into the next. Additionally, this paper proposes a new quantitative evaluation criteria based on the observed flow in the prediction in comparison to the groundtruth. Experiments are performed on a new robot arm dataset proposed in the paper where they outperform the used baselines. 2) Pros: + New quantitative evaluation criteria based on motion accuracy. + New dataset for robot arm pushing objects. 3) Cons: Overall architectural prediction network differences with baseline are unclear: The differences between the proposed prediction network and [1] seem very minimal. In Figure 3, it is mentioned that the network uses a U-Net with recurrent connections. This seems like a very minimal change in the overall architecture proposed. Additionally, there is a paragraph of ""architecture improvements"" which also are minimal changes. Based on the title of section 3, it seems that there is a novelty on the ""prediction with flow"" part of this method. If this is a fact, there is no equation describing how this flow is computed. However, if this ""flow"" is computed the same way [1]  does it, then the title is misleading. Adversarial training objective alone is not new as claimed by the authors: The adversarial objective used in this paper is not new. Works such as [2,3] have used this objective function for single step and multi-step frame prediction training, respectively. If the authors refer to the objective being new in the sense of using it with an action conditioned video prediction network, then this is again an extremely minimal contribution. Essentially, the authors just took the previously used objective function and used it with a different network. If the authors feel otherwise, please comment on why this is the case. Incomplete experiments: The authors only show experiments on videos containing objects that have already been seen, but no experiments with objects never seen before. The missing experiment concerns me in the sense that the network could just be memorizing previously seen objects. Additionally, the authors present evaluation based on PSNR and SSIM on the overall predicted video, but not in a per-step paradigm. However, the authors show this per-step evaluation in the Amazon Mechanical Turk, and predicted object position evaluations. Unclear evaluation: The way the Amazon Mechanical Turk experiments are performed are unclear and/or not suited for the task at hand. Based on the explanation of how these experiments are performed, the authors show individual images to mechanical turkers. If we are evaluating the video prediction task for having real or fake looking videos, the turkers need to observe the full video and judge based on that. If we are just showing images, then they are evaluating image synthesis, which do not necessarily contain the desired properties in videos such as temporal coherence. Additional comments: The paper needs a considerable amount of polishing. 4) Conclusion: This paper seems to contain very minimal changes in comparison to the baseline by [1]. The adversarial objective is not novel as mentioned by the authors and has been used in [2,3]. Evaluation is unclear and incomplete. References: [1] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NIPS, 2016. [2] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016. [3] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee. Decomposing Motion and Content for Natural Video Sequence Prediction. In ICLR, 2017 ",34,605,14.404761904761903,5.422338568935428,244,3,602,0.0049833887043189,0.0197044334975369,-0.9417,190,79,103,36,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 14, 'EXP': 12, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 4, 'EXT': 1}","{'APR': 0, 'NOV': 6, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 10, 'SUB': 3, 'CLA': 0}",0,0,3,2,1,14,12,0,1,0,0,1,4,1,0,6,0,1,1,0,10,3,0,0.6473152487262744,0.561592063408376,0.4094116186873266
ICLR2018-HJrJpzZRZ-R4,,"In this paper a neural-network based method for multi-frame video prediction is proposed. It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015]. What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images. Time series modelling is performed via convolutional LSTMs. The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames. My main concerns with this paper are novelty, reproducibility and evaluation. * Novelty. The network design builds heavily on the work of [Finn et al., 2106]. A number of design decisions (such as instance normalization) seem to help yield better results, but are minor contributions. A major contribution is certainly the combination with an adversarial loss, which is a non-trivial task. However, the authors claim that their method is the first to combine multi-frame video prediction with an adversarial loss, which is not true. A recent work, presented at CVPR this year also does multi-frame prediction featuring an adversarial loss and explicitly models and captures the full dense optical flow (though in the latent space) that allows non-trivial motion extrapolation to future frames. This work is neither mentioned in the related work nor compared to. Lu et al. , Flexible Spatio-Temporal Networks for Video Prediction, CVPR 2017  This recent work builds on another highly relevant work, that is also not mentioned in the paper:  Patraucean et al. Spatio-temporal video autoencoder with differentiable memory, arxiv 2017  Since this is prior state-of-the-art and directly applicable to the problem, a comparison is a must.  * Reproducibility and evaluation The description of the network is quite superficial. Even if the authors released their code used for training (which is not mentioned), I think the authors should aim for a more self-contained exposition. I doubt that a PhD student would be able to reimplement the method and achieve comparable results given the paper at hand only. It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset. Hence, it remains unclear to what extend the achieved improvements are due to the proposed network design changes or the particular dataset they use for training. The authors also don't show any results on previous datasets, which would allow for a more objective comparison to existing state of the art. Another point of criticism is the way the Amazon Mechanical Turk evaluation was performed. Since only individual images were shown, the evaluation mainly measures the quality of the generated images. Since the authors combine their method with a GAN, it is not surprising that the generated images look more realistic. However, since the task is *video* prediction, it seems more natural to show small video snippets rather than individual images, which would also evaluate temporal consistency. * Further comments: The paper contains a number of broken sentences, typos and requires a considerable amount of polishing prior to publication. ",26,560,18.666666666666668,5.381750465549349,255,2,558,0.003584229390681,0.031634446397188,-0.9426,175,78,93,31,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 4, 'DAT': 3, 'MET': 11, 'EXP': 5, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 4, 'CLA': 0}",0,0,6,4,3,11,5,4,0,0,0,2,0,1,0,2,0,3,0,0,13,4,0,0.5751403207967059,0.4525073958864226,0.32230135619658085
ICLR2018-HJsk5-Z0W-R1,Reject,"This paper proposes to improve time complexity of factorization machine. Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong. Specifically, the dot product can be computed as (which is linear to feature size)  (sum x_i beta_i)^T (sum x_i beta_i) - sum_i x_i^2 beta_i^T beta_i The projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one. When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)",5,94,23.5,4.97872340425532,61,0,94,0.0,0.0208333333333333,-0.3182,35,9,19,3,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,3,2,0,1,1,0,0,2,0,0,0,0,0,0,0,1,0,0,5,0,0,0.4288164236186685,0.2247100519615941,0.19342805392736398
ICLR2018-HJsk5-Z0W-R2,Reject,"The authors introduce a novel novel for collaborative filtering. The proposed model combines some of the strengths of factorization machines and of polynomial regression. Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected). The paper is well written overall and relatively easy to understand. The study seems fairly thorough (both vanilla and cold-start experiments are reported). Overall the paper feels a little bit incomplete . This is particularly apparent in the empirical study. Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results. Here are some suggestions about how to achieve that:   1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets. 2) Report results on well-known CF datasets. Movielens comes to mind. 3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)  4) Explore the models and shed some lights on where the gains are coming from. Minor:   - How do you deal with unobserved preferences in the implicit case? n - I found the idea of Figure 1 very good but in its current form I didn't find it particularly insightful (these clouds are hard to interpret). - It may also be worth adding this reference when discussing neural factorization: http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf ",15,231,16.5,5.324324324324325,150,4,227,0.0176211453744493,0.0248962655601659,0.9001,62,30,38,16,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 5, 'DAT': 2, 'MET': 3, 'EXP': 1, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 1}",0,1,6,5,2,3,1,3,1,0,0,2,0,1,0,2,0,0,0,0,7,2,1,0.7156186508059895,0.4483279731463419,0.4020510706843336
ICLR2018-HJsk5-Z0W-R3,Reject,"This paper presents a method for matrix factorization using DNNs. The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7). From the methodology point of view, such extensions are relatively straightforward. As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically . The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features. While the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer. In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem).  I think this paper does not connect very well with that literature. When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework. I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods, or if the success of this paper is mostly due to clever processing of text features using DNNs. Some detailed comments: 1. eqn 4 does not indicate any rank-r factors.  2. some statements do not seem straightforward/justified to me:       -- the paper uses the word inference several times without definition -- if we were interested in interpreting the parameters, we could constrain w to be non-negative ... . Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples? -- Note that if the dot product is replaced with a neural function, fast inference for cold-start ... .  3. the experimental setup seems quite unusual to me: since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency.  This seems very problematic if most of the entries are not observed. Why cannot you use the typical evaluation procedure for collaborative filtering, where you hide some known entries during model training, and evaluate on these entries during test? ",20,401,19.09523809523809,5.454308093994778,213,5,396,0.0126262626262626,0.0333333333333333,0.898,110,46,70,24,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 11, 'DAT': 2, 'MET': 5, 'EXP': 8, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 4, 'CLA': 0}",0,1,10,11,2,5,8,3,0,2,0,3,0,0,0,0,1,0,0,0,13,4,0,0.6460417231162855,0.3411168520764134,0.32332163679279724
ICLR2018-HJzgZ3JCW-R1,Accept,"This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand. The resulting Winograd-ReLU CNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool-CNN-C, and ImageNEt with ResNet-18).  The proposed method seems to improve over the two baseline approaches (Winograd and sparsity, respectively). Overall, the paper is well-written and the experiments seems to be quite thorough and clear. Note that I am not an expert in this field and I might miss important references along this direction. I am leaving it to other reviewers to determine its novelty.   Putting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration. Also, I am curious about the performance after weight pruning but before retraining).",8,177,19.666666666666668,5.658959537572255,113,4,173,0.0231213872832369,0.0333333333333333,0.8774,53,26,31,5,8,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 1, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,1,2,1,1,2,1,0,0,0,1,0,2,0,0,0,0,0,0,2,0,1,0.571583395150293,0.2228441796570651,0.2541491354575155
ICLR2018-HJzgZ3JCW-R2,Accept,"This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. The resultant CNN can achieve ~10x theoretical speedup with little performance loss. The paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting.  The results on Cifar-10 and ImageNet are promising. In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup. The results on ImageNet using ResNet-18 architecture are also promising. However, no results are provided for deeper networks, so it is unclear how this method can benefit the computation of very deep neural networks  A general limitation of the proposed method is the network architecture inconsistency with the ordinary CNNs. Due to the location change of ReLUs, it is unclear how to transform a pretrained ordinary CNNs to the new architectures accurately. It seems training from scratch using the transformed architectures is the simplest solution. The paper does not report the actual speedup in the wall clock time. The actual implementation is what matters in the end. It will be more informative to present Figure 2,3,4 with respect to the workload in addition to the weight density.   ",15,243,16.2,5.3940677966101696,124,1,242,0.0041322314049586,0.0161290322580645,0.8982,73,33,38,9,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 4, 'EXP': 1, 'RES': 5, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 1}",0,0,0,2,2,4,1,5,1,2,0,2,0,0,0,1,0,0,0,0,7,3,1,0.5725674841311793,0.4483895420701452,0.3245342619868212
ICLR2018-HJzgZ3JCW-R3,Accept,"Summary:  The paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. This modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way: - weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity - the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations  This way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers. Review: The paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. I only have a couple of questions/comments: 1) I'm not familiar with the term m-specific (""Matrices B, G and A are m-specific. "") and didn't find anything that seemed related in a very quick google search.  Maybe it would make sense to add at least an informal description. 2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs? 3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015) n4) Figure 5 caption has a typo: ""acrruacy"" References: Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations.  In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015. Lin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with few multiplications. arXiv preprint arXiv:1510.03009 (2015). Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.  Xnor-net: Imagenet classification using binary convolutional neural networks.  In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.",19,392,14.51851851851852,5.805555555555555,221,2,390,0.0051282051282051,0.0149625935162094,0.9773,140,49,53,22,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 6, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,1,4,0,8,1,2,2,0,0,0,6,0,0,0,0,0,0,0,5,1,1,0.5734987806882338,0.3358211630727052,0.2907937869428491
ICLR2018-Hk-FlMbAZ-R1,Reject,"The authors argue that good classifiers naturally represent the classes in a classification as well-separated manifolds, and that adversarial examples are low-confidence examples lying near to one of these manifolds. The authors suggest fixing adversarial examples by projecting them back to the manifold, essentially by finding a point near the adversarial example that has high confidence. There are numerous issues here, which taken together, make the whole story pretty unconvincing. The term manifold is used very sloppily. To be fair, this is unfortunately common in modern machine learning. An actual manifold is a specific mathematical structure with specific properties. In ML, what is generally hypothesized is that the data (often per class) lives  ear to some low-dimensional structure. In this paper, even the low-dimensionality isn't used --- the manifold assumption is used as a stand-in for the regions associated with different classes are well-separated.  (This is partially discussed in Section 6, where the authors point out correctly that the same defense as used here could be used with a 1-nn model.) This is fine as far as it goes, but the paper refs Basri & Jacobs 2016 multiple times as if it says anything relevant about this paper: Basri & Jacobs is specifically about the ability of deep nets to fit data that falls on (actual, mathematical) manifolds. This reference doesn't add much to the present story. The essential argument of the paper rests on the Postulate: (A good model) F is confident on natural points drawn from the manifolds, but has low confidence on points outside of the manifolds.    This postulate is sloppy and speculative. For instance, taken in its strong form, if believe the postulate, then a good model: 1. Can classify all  atural points from all classes with 100% accuracy. 2. Can detect adversarial points with 100% accuracy because all high-confidence points are correct classifications and all low-confidence points are adversarial. 3. All adversarial examples will be low-confidence. Point 1 makes it clear that no good model F fully satisfying the postulate exists --- models never achieve 100% accuracy on difficult real-world distributions. But the method for dealing with adversarial examples seems to require Points 2 and 3 being true. To be fair, the paper more-or-less admits that how true these points are is not known and is important.  Nevertheless, I think this paper comes pretty close to arguing something that I *think* is not true, and doesn't do much to back up its argument. Because of the quality of the writing (generally sloppy), it's hard to tell, but I believe the authors are basically arguing that: a. You can generally easily detect adversarial points because they are low confidence. b. If you go through a procedure to find a point near your adversarial point that is high-confidence, you'll get the correct (or perhaps original) class back. I think b follows from a, but a is extremely suspect. I do not personally work in adversarial examples, and briefly looking at the literature, it seems that most authors *do* focus on how something is classified and not its confidence, but I don't think it's *that* hard to generate high-confidence adversarial examples. Early work by Goodfellow et al. (Explaining and Harnessing Adversarial Examples, Figure 1, shows an example where the incorrect classification has very high confidence. The present paper only uses Carlini-Wagner attacks. From a read of Carlini-Wagner, it seems they are heavily concerned with finding *minimal* perturbations to achieve a given misclassification; this will of course produce low-confidence adversaries, but I see no reason why this is a general property of all adversarial examples. The experiments are weak. I applaud the authors for mentioning the experiments are very preliminary, but that doesn't make them any less weak. What are we to make of the one image discussed at the end of Section 5 and shown in Figure 1? The authors note that the original image gives low-confidence for the correct class. (Does this mean that the classifier isn't good? Is it evidence against some kind of manifold assumption?) The authors note the adversarial category has significantly higher confidence, and say in this case, it seems that it is the vagueness of the signals/data that lead to a natural difficulty.  But the signals and data are ALWAYS vague. If they weren't, machine learning would be easy. This paper proposes something, looks at a tiny number of examples, and already finds a counterexample to the theory. What's the evidence *for* the theory? A lot of writing is given over to how this method is semantic, and I just don't buy it. The connection to manifolds is weak. The basic argument here is really (1) If our classifiers produce smooth well-separated high-confidence regions, (2) then we can detect adversaries because they're low-confidence, and (3) we can correct adversaries by projecting them back to high-confidence. (1) seems vastly unlikely to me based on all my experience: neural nets often get things wrong, they often get things wrong with high confidence, and when they're right, the confidence is at least sometimes low.  The authors use a sloppy postulate about good models and so could perhaps argue I've never seen a good model, but the methods of this paper require a good model. (2) seems to follow logically from (1). (3) is also suspect --- perturbations which are *minimal* can be corrected as this paper does (and Carlini-Wagner attacks are minimal by design), but there's no reason to expect general perturbations to be minimal. The writing is poor throughout. It's generally readable, but the wordings are often odd, and sometimes so odd it's hard to tell what was meant. For instance, I spent awhile trying to decide whether the authors assumed common classifiers are good (according to the postulate) or whether this paper was about a way to *make* classifiers good (I eventually decided the former).",48,967,19.73469387755102,5.223568281938326,350,18,949,0.0189673340358271,0.0356778797145769,-0.977,226,129,201,75,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 10, 'DAT': 1, 'MET': 13, 'EXP': 3, 'RES': 3, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 7, 'BIB': 0, 'EXT': 7}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 20, 'SUB': 1, 'CLA': 3}",0,0,4,10,1,13,3,3,3,0,0,7,0,7,0,0,0,1,1,0,20,1,3,0.647293483233908,0.5675350169592801,0.4053119166022897
ICLR2018-Hk-FlMbAZ-R2,Reject,"The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples. The two objective functions are based on assigning low confidence values to points that are near or off the underlying (learned) data manifold while assigning high confidence values to points lying on the data manifold. In particular, for an adversarial example that is distinguishable from the points on the manifold and assigned a low confidence by the model, is projected back onto the designated manifold such that the model assigns it a high confidence value. The authors claim that the two objective functions proposed in this manuscript provide such a projection onto the desired manifold and assign high confidence for these adversarial points. These mechanisms, together with the so-called shell wrapper around the model (a deep learning model in this case) will provide the desired defense mechanism against adversarial examples. The manuscript at the current stage seems to be a preliminary work that is not well matured yet. The manuscript is overly verbose and the arguments seem to be weak and not fully developed yet. More importantly, the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments.",8,202,25.25,5.49746192893401,101,2,200,0.01,0.0148514851485148,0.9106,51,28,29,16,3,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,0,0,2,0,6,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1,0,0.2155959747164933,0.2228441796570651,0.09765089896030027
ICLR2018-Hk-FlMbAZ-R3,Reject,"1) Summary This paper proposes a new approach to defending against adversarial attacks based on the manifold assumption of natural data. Specifically, this method takes inputs (possibly coming from an adversarial attack), project their semantic representation into the closest data class manifold. The authors show that adversarial attack techniques can be with their algorithm for attack prevention. In experiments, they show that using their method on top of a base model achieves perfect success rate on attacks that the base model is vulnerable to while retaining generalizability. 2) Pros: + Novel/interesting way of defending against adversarial attacks by taking advantage of the manifold assumption. + Well stated formulation and intuition. + Experiments validate the claim, and insightful discussion about the limitations and advantages of the proposed method. 3) Cons: Number of test examples used too small: As mentioned in the paper, the number of testing points is a weakness. There needs to be more test examples to make a strong conclusion about the method's performance in the experiments. Comparison against other baselines: Even though the method proposes a new approach for dealing with adversarial attacks using Madry et al. as base model, it would be useful to the community to see how this method works with other base models. Algorithm generalizability: As mentioned by the authors, their method depends on assumptions of the learned embeddings by the model being used. This makes the method less attractive for people that may be interested in dealing with adversarial examples in, for example, reinforcement learning problems. Can the authors comment on this? Additional comments: The writing needs to be polished. 4) Conclusion: Overall, this is a very interesting work on how to deal with adversarial attacks in deep learning, while at the same time, it shows encouraging results of the application of the proposed method. The experimental section could improve a little bit in terms of baselines and test examples as previously mentioned, and also the authors may give some comments on if there is a simple way to make their algorithm not depend on assumptions of the learned embeddings. ",16,344,20.23529411764705,5.448795180722891,165,3,341,0.0087976539589442,0.0172413793103448,-0.6705,108,37,54,10,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 12, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,0,1,2,1,12,4,1,0,0,0,2,0,0,0,1,0,1,1,0,8,1,1,0.5031154192966637,0.6710203687105675,0.35588177927044157
ICLR2018-Hk0wHx-RW-R1,,"This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based). The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform. This explicit approach is shown on synthetic experiments to preserve more information about the target, yield better reconstruction and converge faster than the baseline. The authors further develop a sparse extension to this Deep Copula Information Bottleneck (DCIB), which yields improved representations (in terms of disentangling and sparsity) on a UCI dataset. (significance) This is a promising idea. This paper builds on the information theoretic perspective of representation learning, and makes progress towards characterizing what makes for a good representation. Invariance to transforms of the marginal distributions is clearly a useful property, and the proposed method seems effective in this regard. Unfortunately, I do not believe the paper is ready for publication as it stands, as it suffers from lack of clarity and the experimentation is limited in scope. (clarity) While Section 3.3 clearly defines the explicit form of the algorithm (where data and labels are essentially pre-processed via a copula transform), details regarding the ""implicit form"" are very scarce. From Section 3.4, it seems as though the authors are optimizing the form of the gaussian information bottleneck I(x,t), in the hopes of recovering an encoder $f_beta(x)$ which gaussianizes the input (thus emulating the explicit transform) ? Could the authors clarify whether this interpretation is correct, or alternatively provide additional clarifying details ?  There are also many missing details in the experimental section: how were the number of ""active"" components selected ? Which versions of the algorithm (explicit/implicit) were used for which experiments ? I believe explicit was used for Section 4.1, and implicit for 4.2 but again this needs to be spelled out more clearly. I would also like to see a discussion (and perhaps experimental comparison) to standard preprocessing techniques, such as PCA-whitening. (quality) The experiments are interesting and seem well executed. Unfortunately, I do not think their scope (single synthetic, plus a single UCI dataset) is sufficient. While the gap in performance is significant on the synthetic task, this gap appears to shrink significantly when moving to the UCI dataset. How does this method perform for more realistic data, even e.g. MNIST ? I think it is crucial to highlight that the deficiencies of DIB matter in practice, and are not simply a theoretical consideration. Similarly, the representation analyzed in Figure 7 is promising,but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the beta-VAE paper. I would have also liked to see a more direct and systemic validation of the claims made in the paper. For example, the shortcomings of DIB identified in Section 3.1, 3.2 could have been verified more directly by plotting I(y,t) for various monotonic transformations of x. A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion. Pros: * Theoretically well motivated * Promising results on synthetic task * Potential for impact  Cons: * Paper suffers from lack of clarity (method and experimental section) * Lack of ablative / introspective experiments * Weak empirical results (small or toy datasets only).",27,560,25.454545454545453,5.630188679245283,259,6,554,0.0108303249097472,0.0226480836236933,0.9902,163,74,92,39,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 18, 'PDI': 8, 'DAT': 5, 'MET': 8, 'EXP': 17, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 2, 'PNF': 2, 'REC': 0, 'EMP': 16, 'SUB': 4, 'CLA': 4}",0,1,18,8,5,8,17,1,1,0,0,4,0,0,0,0,5,2,2,0,16,4,4,0.6480516330782181,0.6769303410016226,0.45863405727124296
ICLR2018-Hk0wHx-RW-R2,,"This paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-Gaussian information bottleneck and solved via variational auto-encoder stochastic optimization. The authors suggest Gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual information. Experiments include both artificial data and real data. The paper is unclear at some places and writing gets confusing. For example, it is unclear whether and when explicit or implicit transforms are used for x and y in the experiments, and the discussion at the end of Section 3.3 also sounds confusing. It would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practice. Moreover, what is the form of f_beta and how beta is optimized? In the first equation on page 5, is tilde y involved? How to choose lambda? If MI is invariant to monotone transformations and information curves are determined by MIs, why ""transformations basically makes information curve arbitrary""? Can you elaborate? Although the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves, more compact representation and better reconstruction quality, it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attacks. Minor comments:   - What is the meaning of the dashed lines and the solid lines respectively in Figure 1?  - Section 3.3 at the bottom of page 4: what is tilde t_j? and x in the second term? Is there a typo? - typo, find the ""most orthogonal"" representation if the inputs -> of the inputs  Overall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental. The paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough. *************** Updates:  *************** The authors have clarified some questions that I had and further demonstrated the benefits of copula transform with new experiments in the revised paper. The new results are quite informative and addressed some of the concerns raised by me and other reviewers. I have updated my score to 6 accordingly. ",21,379,29.15384615384616,5.532967032967033,211,1,378,0.0026455026455026,0.0204081632653061,0.9051,136,54,60,15,11,3,"{'ABS': 0, 'INT': 1, 'RWK': 8, 'PDI': 6, 'DAT': 1, 'MET': 1, 'EXP': 8, 'RES': 2, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 4}",0,1,8,6,1,1,8,2,1,2,0,3,0,3,0,0,2,0,0,0,4,0,4,0.7873674859079542,0.3354906474988723,0.3969369964247452
ICLR2018-Hk0wHx-RW-R3,,"[                                    REVISION                                                       ] Ok so the paper underwent major remodel, which significantly improved the clarity. I do agree now on Figure 5, which tips the scale for me to a weak accept.  [                                    END OF REVISION                                                 ]  This paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning. Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib. They then go on to explore the sparsity of the latent space My main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny - 2K instances) and some of the plots (like Figure 5) are not convincing to me. On top of that, it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence (if it does) Minor comments Page 1: forcing an compact -> forcing a compact ""and and""  >and Section 2: mention that I is mutual information, it is not obvious for everyone  Figure 3: circles/triangles are too small, hard to see  Figure 5: not really convincing. B does not appear much more structured than a, to me it looks like a simple transformation of a. ",9,216,27.0,4.989949748743719,123,0,216,0.0,0.0223325062034739,0.1716,56,26,37,18,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 2, 'DAT': 1, 'MET': 3, 'EXP': 4, 'RES': 0, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,0,5,2,1,3,4,0,2,1,0,3,0,1,0,0,2,2,0,0,3,1,1,0.6441115015080376,0.5569646696308793,0.40444025047356
ICLR2018-Hk0wHx-RW-R4,,"The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version. The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'. However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations? Quality: Ok. The claims appear to be sufficiently verified in the experiments. However, it would have been great to have an experiment that actually makes use of the learned features to make predictions. I struggle a little to see the relevance of the proposed method without a good motivating example. Clarity: Below average. Section 3 is a little hard to understand. Is q(t|x) in Fig 1 a typo? How about t_j in equation (5)? There is a reference that appeared twice in the bibliography (1st and 2nd). Originality and Significance: Average. The paper (if I understood it correctly) appears to be mainly about borrowing the key ideas from Rey et. al. 2014 and applying it to the existing DVIB model.",12,204,15.692307692307692,5.3052631578947365,112,1,203,0.0049261083743842,0.0196078431372549,0.7845,53,27,36,10,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 1}",0,0,6,2,0,2,4,0,1,1,0,3,1,1,0,0,1,1,0,0,2,1,1,0.6439184993359304,0.5561775129903985,0.4051031937861699
ICLR2018-Hk2MHt-3--R1,Reject,"This work proposed a reconfiguration of the existing state-of-the-art CNN model architectures including ResNet and DensNet. By introducing new branching architecture, coupled ensembles, they demonstrate that the model can achieve better performance in classification tasks compared with the single branch counterpart with same parameter budget. Additionally, they also show that the proposed ensemble method results in better performance than other ensemble methods (For example, ensemble over independently trained models)  not only in combined mode but also in individual branches. Paper Strengths: * The proposed coupled ensembles method truly show impressive results in classification benchmark (DenseNet-BC L   118 k   35 e   3). * Detailed analysis on different ensemble fusion methods on both training time and testing time. * Simple but effective design to achieve a better result in testing time with same total parameter budget. t Paper Weakness: * Some detail about different fusing method should be mentioned in the main paper instead of in the supplementary material. * In practice, how much more GPU memory is required to train the model with parallel branches (with same parameter budgets) because memory consumption is one of the main problems of networks with multiple branches. * At least one experiment should be carried out on a larger dataset such as ImageNet to further demonstrate the validity of the proposed method. * More analysis can be conducted on the training process of the model. Will it converge faster? What will be the total required training time to reach the same performance compared with single branch model with the same parameter budget?",12,250,22.727272727272727,5.691666666666666,132,0,250,0.0,0.0113636363636363,0.9723,80,44,37,12,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 6, 'DAT': 1, 'MET': 6, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 3, 'CLA': 0}",0,0,9,6,1,6,5,3,0,4,0,0,0,0,0,1,2,2,0,0,7,3,0,0.5027217081913764,0.5596658523868941,0.3200057514722158
ICLR2018-Hk2MHt-3--R2,Reject,"Strengths: * Very simple approach, amounting to coupled training of e identical copies  of a chosen net architecture, whose predictions are fused during training.  This forces the different model instances to become more complementary. * Perhaps counterintuitively, experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate training. * The practical advantages of the proposed approach are twofold: 1. Given a fixed parameter budget, coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained nets. 2. For the same accuracy, coupled ensembling yields significant parameter savings. Weaknesses: * Although results are very strong, the proposed models do not outperform the state-of-the-art, except for the models reported in Table 4, which however were obtained by *traditional* ensembling of coupled ensembles.  * Coupled ensembling requires joint training of all nets in the ensemble and thus is limited by the size of the model that can be fit in memory.  Conversely, traditional ensembling involves separate training of the different instances and this enables the learning of an arbitrary number of individual nets. * I am surprised by the results in Table 2, which suggest that the optimal number of nets in the ensemble is remarkably low (only 3!). It'd be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net. * Strictly speaking it is correct to refer to the individual nets in the ensembles as branches and basic blocks.  Nevertheless, I find the use of these terms confusing in the context of the proposed approach, since they are commonly used to denote concepts different from those represented here.  I would recommend refraining from using these terms here. Overall, the paper provides limited technical novelty. Yet, it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensemble.",15,309,18.176470588235293,5.562289562289562,163,1,308,0.0032467532467532,0.0217391304347826,0.9705,75,50,60,17,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 6, 'DAT': 0, 'MET': 4, 'EXP': 6, 'RES': 5, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 3, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 0}",0,0,9,6,0,4,6,5,2,0,0,3,0,1,0,2,3,2,1,0,7,2,0,0.573887472404465,0.6707634312226499,0.4040172749328114
ICLR2018-Hk2MHt-3--R3,Reject,"This paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores; the network is trained in end-to-end, thus training the parallel branches jointly. Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers). In effect, this paper extends the existing literature suggesting end-to-end branching.  While the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community. + Paper is well written and easy to follow. + Proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem + Detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches. + [Ease of implementation] Suggested architecture can be easily implemented using existing deep learning frameworks. - Although joint end-to-end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet. While authors suggests a way to circumvent such limitations by training branches on separate GPUs but this would still impose limits on the number of branches as well as its ease of implementation. - Adding an overview figure of the architecture in the main paper (instead of supplementary) would be helpful. - Branched architecture serve as a regularization by distributing the gradients across different branches; however this also suggests that early layers on the network across branches would be independent.  It would helpful if authors would consider an alternate archiecture where early layers may be shared across branches, suggesting a delayed branching, with fusion at the final layer. - One of the benefits of architectures such as DenseNet is their usefulness as a feature extractor (output of lower layers) which generalizes even to domain other that the dataset; the branched architecture could potentially diminish this benefit. Minor edits: Page 1. 'significantly match and improve'  > 'either match or improve'  Additional notes: - It would interesting to compare this approach with a conditional training pipeline that sequentially adds branches, keeping the previous branches fixed. This may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branches.",17,430,26.875,5.875598086124402,208,5,425,0.0117647058823529,0.0247747747747747,0.9938,127,59,71,25,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 10, 'PDI': 4, 'DAT': 1, 'MET': 11, 'EXP': 11, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,2,10,4,1,11,11,0,1,0,1,1,0,0,0,1,1,3,0,1,6,0,1,0.6471488315484148,0.6700107789552615,0.4538706772235875
ICLR2018-Hk2aImxAb-R1,Accept,"This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time. The paper is very well written, experiments are clearly presented and convincing and, most importantly, the research question is exciting (and often overlooked). My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). The authors add a hierarchical, multi-scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig. 3). They investigate pros and cons in detail adding more valuable analysis in the appendix. However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in-depth analysis.  Some more minor comments:   -tPlease enlarge Fig. 4. -tI did not fully grasp the details in the first Solution paragraph on P5. Please extend and describe in more detail. In conclusion, this is a very well written paper that designs the network architecture (of DenseNet) such that it is optimized to include CPU budgets at test time. I recommend acceptance to ICLR18.",11,181,12.928571428571429,5.197674418604652,111,1,180,0.0055555555555555,0.0163043478260869,0.9879,56,20,27,15,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 3, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 2}",0,0,6,3,0,2,1,0,2,1,0,2,2,1,0,1,0,0,1,0,6,1,2,0.6437094839801534,0.5587464778006245,0.407356874377914
ICLR2018-Hk2aImxAb-R2,Accept,"This paper presents a method for image classification given test-time computational budgeting constraints. Two problems are considered:  any-time classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images. A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features. In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage. Evaluations are performed on ImageNet and CIFAR-100. I would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well? Also on p.6 I'm not entirely clear on how the  etwork reduction is performed --- it looks like finer scales are progressively dropped in successive blocks, but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is lazy evaluation). A picture would help here, showing where the depth-layers are divided between blocks. n I was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:  It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation. So this isn't really a hard constraint (just an expected result for batches that are large and varied enough). This is fine, but could perhaps be pointed out if that is indeed the case. Overall, this seems like a natural and effective approach, and achieves good results.",15,295,26.818181818181817,5.275985663082437,176,4,291,0.0137457044673539,0.07,0.9356,74,35,60,26,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 5, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 2}",0,1,9,5,0,4,2,3,0,4,0,1,0,1,0,0,1,0,0,0,8,0,2,0.6447605795839682,0.3377681704480882,0.31002322805775095
ICLR2018-Hk2aImxAb-R3,Accept,"This paper introduces a new model to perform image classification with limited computational resources at test time.  The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer. The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image.  The multi-scale representation allows for better performance at early stages of the network. Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers. A thorough evaluation on ImageNet and Cifar100 shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of computation. Pros: - The presentation is clear and easy to follow. - The structure of the network is clearly justified in section 4. - The use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interesting. - The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios. - Results are very promising, with 5x speed-ups and same or better accuracy that previous models. - The extensive experimentation shows that the proposed network is better than previous approaches under different regimes. Cons: - Results about the more efficient densenet* could be shown in the main paper Additional Comments: - Why in training you used logistic loss instead of the more common cross-entropy loss? Has this any connection with the final performance of the network? - In fig. 5 left for completeness I would like to see also results for DenseNet^MT and ResNet^MT - In fig. 5 left I cannot find the 4% and 8% higher accuracy with 0.5x10^10 to 1.0x10^10 FLOPs, as mentioned in section 5.1 anytime prediction results - How the budget in terms of Mul-Adds is actually estimated? n I think that this paper present a very powerful approach to speed-up the computational cost of a CNN at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them. The experimental evaluation is complete and accurate.   ",21,358,21.058823529411764,5.391044776119403,178,1,357,0.0028011204481792,0.0053475935828877,0.9911,110,51,44,14,10,6,"{'ABS': 0, 'INT': 2, 'RWK': 12, 'PDI': 8, 'DAT': 0, 'MET': 6, 'EXP': 7, 'RES': 4, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 4, 'CLA': 1}",0,2,12,8,0,6,7,4,2,3,0,1,2,0,1,0,2,0,1,0,13,4,1,0.717632069898324,0.6744982220581947,0.515484702897495
ICLR2018-Hk3ddfWRW-R1,Accept,"The authors propose a new sampling based approach for inference in latent variable models. They apply this approach to multi-modal (several intentions) imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks. The proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled.  The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior. The objective to be maximized is a lower bound to 1/alpha * the likelihood. Quality: The empirical results (including a video of an actual robotic arm system performing the task) looks good. This reviewer is a bit sceptical to the methodology. I am not convinced that the proposed bound will have low enough variance. It is mentioned in a footnote that variational autoencoders were tested but that they failed. Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework.  Also, the recently proposed continuous relaxation of random variables seemed relevant. Clarity: The paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribution. Significance: Hard to say. New method proposed and shown to work well in one case. Too early to tell about significance. Pro: 1. Challenging and relevant problem solved better than other approaches. 2. New latent variable model bound that might work better than classic approaches. Con: 1. Not entirely convincing that it should work better than already existing methods. 2. Missing some investigation of the properties of the estimator on simple problem to be compared to standard methods. ",19,304,12.666666666666666,5.530821917808219,169,2,302,0.0066225165562913,0.0227272727272727,0.9653,75,46,61,17,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 12, 'DAT': 0, 'MET': 7, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 2, 'CLA': 3}",0,0,3,12,0,7,5,1,0,0,0,4,0,1,0,0,3,0,0,0,14,2,3,0.5027954665588756,0.4528949110443284,0.2843912976845308
ICLR2018-Hk3ddfWRW-R2,Accept,"The authors provide a method for learning from demonstrations where several modalities of the same task are given. The authors argue that in the case where several demonstrations exists and a deterministic (i.e., regular network) is given, the network learns some average policy from the demonstrations. The paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming. They put the this specific work in the right context of imitation learning and IRL. Afterward, the authors argue that deterministic network cannot adequately several modalities. The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs. I find that recent paper by Tamar et al 2016. on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network. Even the control task is very similar to the current proposed task in this paper. The authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs.  Again, Tamar et al. 2016 deals with this 3 points. I went over the math. It seems right and valid. Indeed, SNN is a good choice for adding (Bayesian) context to a task. Also, I see the advantage of referring only to the good quantiles when needed. It is indeed a good method for dealing with the variance. I must say that I was impressed with the authors making the robot succeed in the tasks in hand (although reaching to an object is fairly simple task).  My concerns are as follows: 1) Seems like that the given trajectories are naturally divided with different tasks, i.e., a single trajectory consists only a single task. For me, this is not the pain point in this tasks. the pain point is knowing when tasks are begin and end. 2) I'm not sure, and I haven't seen evidence in the paper (or other references) that SNN is the only (optimal?) method for this context. Why not adding (non Bayesian) context (not label) to the task will not work as well? 3) the robot task is impressive. but proving the point, and for the ease of comparing to different tasks, and since we want to show the validity of the work on more than 200 trials, isn't showing the task on some simulation is better for understanding the different regimes that this method has advantage? I know how hard is to make robotic tasks work...    4) I'm not sure that the comparison of the suggested architecture to one without any underlying additional variable Z or context (i.e., non-Bayesian setup) is fair.  Vanilla NN indeed may fail miserably . So, the comparison should be to any other work that can deal with similar environment but different details. To summarize, I like the work and I can see clearly the motivation. But I think some more work is needed in this work: comparing to the right current state of the art, and show that in principal (by demonstrating on other simpler simulations domains) that this method is better than other methods.",29,543,18.724137931034484,4.851272015655577,232,5,538,0.0092936802973977,0.0253623188405797,0.9757,148,68,91,36,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 25, 'PDI': 5, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 4, 'CLA': 2}",0,1,25,5,0,9,1,0,0,0,0,2,2,0,0,1,1,1,0,0,9,4,2,0.5042282578278038,0.6720434907412289,0.3541463443986365
ICLR2018-Hk3ddfWRW-R3,Accept,"This paper focuses on imitation learning with intentions sampled  from a multi-modal distribution. The papers encode the mode as a hidden  variable in a stochastic neural network and suggest stepping around posterior  inference over this hidden variable (which is generally required to  do efficient maximum likelihood) with a biased importance  sampling estimator. Lastly, they incorporate attention for large visual inputs. The unimodal claim for distribution without randomness is weak. The distribution  could be replaced with a normalizing flow.  The use of a latent variable  in this setting makes intuitive sense, but I don't think multimodality motivates it. Moreover, it really felt like the biased importance sampling approach should be  compared to a formal inference scheme. I can see how it adds value over sampling  from the prior, but it's unclear if it has value over a modern approximate inference  scheme like a black box variational inference algorithm or stochastic gradient MCMC. How important is using the pretrained weights from the deterministic RNN? Finally, I'd also be curious about how much added value you get from having  access to extra rollouts.",10,180,20.0,5.644970414201183,121,2,178,0.0112359550561797,0.03125,0.9693,50,26,34,8,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 4, 'DAT': 0, 'MET': 7, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,1,6,4,0,7,4,0,0,0,0,1,0,0,0,0,2,0,0,0,6,0,0,0.4309115077267497,0.2253800460448849,0.1924878041331759
ICLR2018-Hk5elxbRW-R1,Accept,"The paper is clear and well written. The proposed approach seems to be of interest and to produce interesting results. As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better. The paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models. As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps. I have two main concerns with the presentation. A/ In addition to the main contribution, the paper devotes a significant amount of space to explaining how to compute the smoothed loss. This can be done by evaluating elementary symmetric polynomials at well-chosen values. The paper argues that classical methods for such evaluations (e.g., using the usual recurrence relation or more advanced methods that compensate for numerical errors) are not enough when using single precision floating point arithmetic. The paper also advances that GPU parallelization must be used to be able to efficiently train the network. Those claims are not substantiated, however, and the method proposed by the paper seems to add substantial complexity without really proving that it is useful. The paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value. I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch? I believe the paper could justify this approach better by providing a bit more insights as to why it is required.  For instance:  - What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials? - How do those compare with CE and L_{5, 1} with the proposed method? - Are numerical instabilities making this completely unfeasible? This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss. B/ No mention is made of the object detection problem, although multiple of the motivating examples in Figure 1 consider cases that would fall naturally into the object detection framework. Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting. Additional comments:  - Figure 2b: this visualization is confusing. This is presented in the same figure and paragraph as the CIFAR results, but instead uses a single synthetic data point in dimension 5, and k 1. This is not convincing. An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting.",25,486,24.3,5.417391304347826,233,3,483,0.0062111801242236,0.020242914979757,0.9888,124,68,91,25,10,6,"{'ABS': 0, 'INT': 0, 'RWK': 12, 'PDI': 7, 'DAT': 3, 'MET': 1, 'EXP': 4, 'RES': 1, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 7, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 6, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 5, 'CLA': 4}",0,0,12,7,3,1,4,1,2,2,0,7,0,1,0,0,6,1,1,0,9,5,4,0.716178179427612,0.672552620633498,0.5058492477776614
ICLR2018-Hk5elxbRW-R2,Accept,"This paper made some efforts in smoothing the top-k losses proposed in Lapin et al. (2015).  A family of smooth surrogate loss es was proposed, with the help of which the top-k error may be minimized directly. The properties of the smooth surrogate losses were studied and the computational algorithms for SVM with these losses function were also proposed. Pros: 1, The paper is well presented and is easy to follow. 2, The contribution made in this paper is sound, and the mathematical analysis seems to be correct. 3, The experimental results look convincing. Cons: Some statements in this paper are not clear to me. For example, the authors mentioned sparse or non-sparse loss functions. This statement, in my view, could be misleading without further explanation (the non-sparse loss was mentioned in the abstract).",9,134,13.4,5.0310077519379846,79,2,132,0.0151515151515151,0.0444444444444444,-0.8802,37,15,27,4,7,5,"{'ABS': 1, 'INT': 0, 'RWK': 5, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 3}",1,0,5,0,0,1,2,1,0,0,0,3,1,0,0,0,2,0,1,0,4,1,3,0.5004989850173457,0.5576317346502404,0.30759773096318055
ICLR2018-Hk5elxbRW-R3,Accept,"This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks. The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the temperature parameter.  The paper is well organized and clearly written. The idea deserves a publication. On the other hand, there might be better and more direct solutions to reduce the combinatorial complexity. When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}.",6,140,23.33333333333333,4.923664122137405,74,2,138,0.0144927536231884,0.0141843971631205,0.9594,37,21,19,9,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,0,4,1,0,1,4,0,0,0,0,2,0,0,0,0,1,0,1,1,3,0,1,0.3576883270331857,0.5567994704252415,0.22639405464802476
ICLR2018-Hk6WhagRW-R1,Accept,"The authors describe a variant of the negotiation game in which agents of different type, selfish or prosocial, and with different preferences. The central feature is the consideration of a secondary communication (linguistic) channel for the purpose of cheap talk, i.e. talk whose semantics are not laid out a priori. The essential findings include that prosociality is a prerequisite for effective communication (i.e. formation of meaningful communication on the linguistic channel), and furthermore, that the secondary channel helps improve the negotiation outcomes. The paper is well-structured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel. Finally, agent societies are represented by injecting individuals' ID into the input representation. The positive: - The authors attack the challenging task of given agents a means to develop communication patterns without apriori knowledge. - The paper presents the problem in a well-structured manner and sufficient clarity to retrace the essential contribution (minor points for improvement). - The quality of the text is very high and error-free. - The background and results are well-contextualised with relevant related work.  The problematic: - By the very nature of the employed learning mechanisms, the provided solution provides little insight into what the emerging communication is really about. In my view, the lack of interpretable semantics hardly warrants a reference to 'cheap talk'. As such the expectations set by the well-developed introduction and background sections are moderated over the course of the paper. - The goal of providing agents with richer communicative ability without providing prior grounding is challenging, since agents need to learn about communication partners at runtime. But it appears as of the main contribution of the paper can be reduced to the decomposition of the learnable feature space into two communication channels. The implicit relationship of linguistic channel on proposal channel input based on the time information (Page 4, top) provides agents with extended inputs, thus enabling a more nuanced learning based on the relationship of proposal and linguistic channel. As such the well-defined semantics of the proposal channel effectively act as the grounding for the linguistic channel. This, then, could have been equally achieved by providing agents with a richer input structure mediated by a single channel. From this perspective, the solution offers limited surprises. The improvement of accuracy in the context of agent societies based on provided ID follows the same pattern of extending the input features. - One of the motivating factors of using cheap talk is the exploitation of lying on the part of the agents. However, apart from this initial statement, this feature is not explicitly picked up. In combination with the previous point, the necessity/value of the additional communication channel is unclear. Concrete suggestions for improvement:  - Providing exemplified communication traces would help the reader appreciate the complexity of the problem addressed by the paper. - Figure 3 is really hard to read/interpret. The same applies to Figure 4 (although less critical in this case). - Input parameters could have been made explicit in order to facilitate a more comprehensive understanding of technicalities (e.g. in appendix). - Emergent communication is effectively unidirectional, with one agent as listener. Have you observed other outcomes in your evaluation? In summary, the paper presents an interesting approach to combine unsupervised learning with multiple communication channels to improve learning of preferences in a well-established negotiation game. The problem is addressed systematically and well-presented, but can leave the reader with the impression that the secondary channel, apart from decomposing the model, does not provide conceptual benefit over introducing a richer feature space that can be exploited by the learning mechanisms.[PDI-POS,[MET-POS], [EMP-POS], [APC], [MAJ]] Combined with the lack of specific cheap talk features, the use of actual cheap talk is rather abstract. Those aspects warrant justification.",32,628,19.03030303030303,5.838499184339315,295,1,627,0.0015948963317384,0.0046801872074883,0.9848,198,91,92,28,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 6, 'DAT': 0, 'MET': 13, 'EXP': 0, 'RES': 5, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 4, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 2}",0,1,1,6,0,13,0,5,2,0,0,2,0,1,0,0,0,1,4,0,19,1,2,0.5750189004428611,0.5670166312249931,0.37119938889320164
ICLR2018-Hk6WhagRW-R2,Accept,"This paper explores how agents can learn to communicate to solve a negotiation task. They explore several settings: grounded vs. ungrounded communication, and self-interested vs. prosocial agents. The main findings are that prosocial agents are able to learn to ground symbols using RL, but self-interested agents are not.  The work is interesting and clearly described, and I think this is an interesting setting for studying emergent communication. My only major comment is that I'm a bit skeptical about the claim that ""self-interested agents cannot ground cheap talk to exchange meaningful information"". Given that the agents' rewards would be improved if they were able to make agreements, and humans can use 'cheap talk' to negotiate, surely the inability to do so here shows a failure of the learning algorithm (rather than a general property of self-interested agents)? I am also concerned about the dangers posed by robots inventing their own language, perhap the authors should shut this down :-) ",7,157,19.625,5.377483443708609,103,1,156,0.0064102564102564,0.01875,0.9477,40,23,35,9,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,0,0,4,0,2,0,0,0,0,1,1,0,0,0,0,1,0,0,0,3,0,1,0.2861423774136376,0.3345772482030192,0.14436094967539365
ICLR2018-Hk6WhagRW-R3,Accept,"The experimental setup is clear, although the length of the utterances and the number of symbols in them is not explicitly stated in the text (only the diagrams). Experiment 1 confirms that agents who seek only to maximise their own rewards fail to coordinate over a non-binding communication channel. The exposition of the experiments, however, is unclear. In Fig 1, it is not clear what Agent 1 and Agent 2 are. Do they correspond to arbitrary labels or the turns that the agent takes in the game? Why is Agent 1 the one who triumphs in the no-communication channel game? Is there any advantage to going first generally? Where are the tests of robustness on the curves demonstrated in Figure 2a? Has figure 2b been cherry picked? This should be demonstrated over many different negotiations with error bars. In the discussion of the agents being unable to ground cheap talk, the symbolic nature of the linguistic channel clouds the fact that it is not the symbolic, ungrounded aspect but the non-binding nature of communication on this channel. This would be more clearly demonstrated and parsimonious by using a non-binding version of the proposal channel and saving the linguistic discussion for later. Experiment 2 shows that by making the agents prosocial, they are able to learn to communicate on the linguistic channel to achieve pretty much optimal rewards, a very nice result. The agents are not able to reach the same levels of cooperation on the proposal channel, in fact performing worse than the no-communication baseline. Protocols could be designed that would allow the agents to communicate their utilities over this channel (within 4 turns), so the fact they don't suggests it is the learning procedure that is not able to find this optimum. Presenting this as a result about the superiority of communication over the linguistic channel is not well supported. Why do they do worse with random termination than 10 turns in the proposal channel? 4 proposals should contain enough information to determine the utilities. Why are the 10 turn games even included in this table? It seems that this was dismissed in the environment setup section, due to the first mover advantage. Why do no-communication baselines change so much between random termination and 10 turns in the prosocial case? Why do self-interested agents for 10 turns on the linguistic channel terminate early? Table 1 might be better represented using the median and quartiles, since the data is skewed. Analysis of the communication, i.e. what is actually sent, is interesting and the division into speaker and listener suggests that this is a simple protocol that is easy for agents to learn. Experiment 3 aims to determine whether an agent is able to negotiate against a community of other agents with mixed levels of prosociality. It is shown that if the fixed agent is able to identify who they are playing against they can do better than not knowing, in the case where the fixed agent is self interested. The pca plot of agent id embeddings related is good. Both Figure 4 and Table 3 use Agent 1 and Agent 2 rather than Agent A and Agent B and is not clear whether this is a mistake or Agent 1 is different from Agent A. The no-communication baseline is referred to in the text but the results are not shown in the table. There are no estimates of the uncertainty of the results in table 3, how robust are these results to different initial conditions? This section seems like a bit of an add-on to address criticisms that might arise about the initial experiment being only two agents. Overall, the paper has some nice results and an interesting ideas  but could do with some tightening up of the results to make it really good. ",34,634,26.416666666666668,4.951747088186356,250,6,628,0.0095541401273885,0.0455974842767295,0.9926,144,79,109,31,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 16, 'EXP': 9, 'RES': 10, 'TNF': 9, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 5, 'REC': 0, 'EMP': 24, 'SUB': 0, 'CLA': 2}",0,0,1,1,0,16,9,10,9,1,0,0,0,0,0,0,0,1,5,0,24,0,2,0.5051301487604775,0.4590768762119002,0.28098337624804215
ICLR2018-Hk6kPgZA--R1,Accept,"This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples.  The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution. In particular, the authors adopt the Wasserstein distance to define the ambiguity sets. This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost. The theoretical results in the paper are supported by experiments. Overall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem.",6,130,21.666666666666668,5.704,85,1,129,0.0077519379844961,0.0076335877862595,0.9582,35,17,19,7,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,4,3,0,1,2,0,0,1,0,1,0,0,0,0,0,0,0,0,4,0,1,0.5004462592225721,0.2240880945267511,0.22859565074106225
ICLR2018-Hk6kPgZA--R2,Accept,"This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). I think the idea is intuitive and reasonable, the result is nice. Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ",7,119,23.8,5.452991452991453,84,3,116,0.0258620689655172,0.0416666666666666,-0.09,28,23,22,7,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 6, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,1,6,0,1,2,1,0,1,0,0,0,0,0,0,2,0,0,0,4,0,0,0.5003826951009874,0.224136131175199,0.22092928222346891
ICLR2018-Hk6kPgZA--R3,Accept,"In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0. n Since the min-max problem is intractable in general, what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem. If the duality parameter is large enough, the functions become convex given that the initial losses are smooth. What follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions. Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure.",6,138,27.6,5.162790697674419,88,2,136,0.0147058823529411,0.0215827338129496,-0.4012,34,15,25,9,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 5, 'DAT': 0, 'MET': 1, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,1,5,0,1,4,0,0,1,0,1,0,0,0,0,1,0,0,0,3,0,0,0.4290672153939776,0.2234661370919081,0.19315577528328
ICLR2018-Hk8XMWgRb-R1,Accept,"The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM. Instead of using Monte Carlo approximation as in the traditional random features literature, the main point of the paper is to learn these Fourier features in a min-max sense. This perspective leads to some interesting theoretical results and some new interpretation. Synthetic and some simple real-world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bases. I like the idea of trying to formulate the feature learning problem as a two-player min-max game and its connection to boosting. As for the related work, it seems the authors have missed some very relevant pieces of work in learning these Fourier features through gradient descent [1, 2]. It would be interesting to compare these algorithms as well. [1] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang. Deep Fried Convnets. ICCV 2015. [2] Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson. A la Carte u2014 Learning Fast Kernels. AISTATS 2015.",9,185,13.214285714285714,5.348837209302325,115,1,184,0.0054347826086956,0.0162162162162162,0.7783,65,29,26,4,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,1,4,3,0,3,1,1,0,0,0,0,2,0,0,0,0,0,0,0,3,2,0,0.5008728406632701,0.2235728136002296,0.22671843093518304
ICLR2018-Hk8XMWgRb-R2,Accept,"In this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together.  The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem.  They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity. Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity. Empirically, they show the performances are better than random feature and the LKRF. I like the way they handle the nonconvexity component of the model. However, there are several issues need to be addressed. 1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly. 2, In the paper, there is an assumption about the peak of random feature it is a natural assumption on realistic data that the largest peaks are close to the origin. I was wondering where this assumption is used? Could you please provide more justification for such assumption? 3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning. Please clarify this in the paper explicitly. 4, The experiment is weak. The algorithm is closely related to boosting and MKL, while there is no such comparison. Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison. Suggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning.",17,278,16.352941176470587,5.322222222222222,143,0,278,0.0,0.0071428571428571,0.9718,71,36,46,11,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 16, 'PDI': 2, 'DAT': 1, 'MET': 6, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 4, 'CLA': 0}",0,0,16,2,1,6,0,1,0,0,0,0,0,0,0,0,4,1,0,0,3,4,0,0.359660436109396,0.4461524987844385,0.20523794122003855
ICLR2018-Hk8XMWgRb-R3,Accept,"In this paper the authors consider learning directly Fourier representations of shift/translation invariant kernels for machine learning applications. They choose the alignment of the kernel to data as the objective function to optimize. They empirically verify that the features they learned lead to good quality SVM classifiers. My problem with that paper is that even though at first glance learning adaptive feature maps seems to be an attractive approach, authors' contribution is actually very little. Below I list some of the key problems. First of all the authors claim in the introduction that their algorithm is very fast and with provable theoretical guarantees. But in fact later they admit that the problem of optimizing the alignment is a non-convex problem and the authors end up with a couple of heuristics to deal with it. They do not really provide any substantial theoretical justification why these heuristics work in practice even though they observe it empirically. The assumptions that large Fourier peaks happen close to origin is probably well-justified from the empirical point of view, but it is a hack, not a well established well-grounded theoretical method (the authors claim that in their experiments they found it easy to find informative peaks, even in hundreds of dimensions, but these experiments are limited to the SVM setting, I have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive features). The Langevin dynamics algorithm used by the authors to find the peaks (where the gradient is available) gives only weak theoretical guarantees (as the authors actually admit) and this is a well known method, certainly not a novelty of that paper. Finally, the authors notice that In the rotation-invariant case, where u03a9 is a discrete set, heuristics are available. That is really not very informative (the authors refer to the Appendix so I carefully read that part of the Appendix, but it is extremely vague, it is not clear at all how the Langevin dynamics can be emulated by a discrete Markov chain that mixes fast; the authors do not provide any justification of that approach, what is the mixing time ?; how the good emulation property is exactly measured ?). In the conclusions the authors admit that: Many theoretical questions remain, such as accelerating the search for Fourier peaks. I think that the problem of accelerating this approach is a critical point that this publication is missing. Without this, it is actually really hard to talk about general mechanism of learning adaptive Fourier features for kernel algorithms (which is how the authors present their contribution); instead we have a method heavily customized and well-tailored to the (not particularly exciting) SVM scenario (with optimization performed by the standard annealing method; it is not clear at all whether for other downstream kernel applications this approach for optimizing the alignment would provide good quality models) that uses lots of task specific hacks and heuristics to efficiently optimize the alignment. Another problem is that it is not clear at all to me how authors' approach can be extended to non shift-invariant kernels that do not benefit from Bochner's Theorem. Such kernels are very related to neural networks (for instance PNG kernels with linear rectifier nonlinearities correspond to random layers in NNs with ReLU) and in the NN context are much more interesting that radial basis function or in general shift-invariant kernels. A general kernel method should address this issue (the authors just claim in the conclusions that it would be interesting to explore the NN context in more detail). To sum it up, it is a solid submission, but in my opinion without a substantial contribution and working only in a  very limited setting when it is heavily relying on many unproven hacks and heuristics.",23,623,32.78947368421053,5.262809917355372,260,3,620,0.0048387096774193,0.0399361022364217,0.8741,159,71,102,52,8,5,"{'ABS': 0, 'INT': 3, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 16, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 5, 'CLA': 0}",0,3,1,4,0,16,2,1,0,1,0,2,0,0,1,1,0,2,0,0,7,5,0,0.5755420051792989,0.5598311687550893,0.3560984654794992
ICLR2018-Hk99zCeAb-R1,Accept,"The paper describes a number of modifications of GAN training that enable synthesis of high-resolution images. The modifications also support more automated longer-term training, and increasing variability in the results. The key modification is progressive growing. First, a GAN is trained for image synthesis at very low resolution. Then a layer that refines the resolution is progressively faded in. (More accurately, a corresponding pair of layers, one in the generation and one in the discriminator.) This progressive fading in of layers is repeated, one octave at a time, until the desired resolution is reached. Another modification reported in the paper is a simple parameter-free minibatch summary statistic feature that is reported to increase variation. Finally, the paper describes simple schemes for initialization and feature normalization that are reported to be more effective than commonly used initializers and batchnorm. It's a very nice paper. It does share the bag of tricks nature of many GAN papers, but as such it is better than most of the lot.  I appreciate that some of the tricks actually simplify training, and most are conceptually reasonable. The paper is also very well written. My quibbles are minor. First, I would discuss [Huang et al., CVPR 2017] and the following paper more prominently:  [Zhang et al., ICCV 2017] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017. I couldn't find a discussion of [Huang et al., CVPR 2017] at all, although it's in the bibliography. (Perhaps I overlooked the discussion.) And [Zhang et al., ICCV 2017] is quite closely related, since it also tackles high-resolution synthesis via multi-scale refinement.  These papers don't diminish the submission, but they should be clearly acknowledged and the contribution of the submission relative to these prior works should be discussed. Also, [Rabin et al., 2011] is cited in Section 5 but I couldn't find it in the bibliography. ",18,327,11.678571428571429,5.366666666666666,160,1,326,0.0030674846625766,0.0120845921450151,0.948,109,35,52,30,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 8, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 4, 'CLA': 1}",0,0,4,8,0,5,1,1,0,0,0,2,1,1,0,0,0,0,0,0,3,4,1,0.5731313214319028,0.3348972777279837,0.29117590185929104
ICLR2018-Hk99zCeAb-R2,Accept,"Before the actual review I must mention that the authors provide links in the paper that immediately disclose their identity (for instance, the github link). This is a violation of double-blindness, and in any established double-blind conference this would be a clear reason for automatic rejection. In case of ICLR, double-blindness is new and not very well described in the call for papers, so I guess it's up to ACs/PCs to decide. I would vote for rejection. I understand in the age of arxiv and social media double-blindness is often violated in some way, but here the authors do not seem to care at all. u2014  The paper proposes a collections of techniques for improving the performance of Generative Adversarial Networks (GANs). The key contribution is a principled multi-scale approach, where in the process of training both the generator and the discriminator are made progressively deeper and operate on progressively larger images. The proposed version of GANs allows generating images of high resolution (up to 1024x1024) and high visual quality. Pros: 1) The visual quality of the results is very good, both on faces and on objects from the LSUN dataset. This is a large and clear improvement compared to existing GANs. 2) The authors perform a thorough quantitative evaluation, demonstrating the value of the proposed approach. They also introduce a new metric - Sliced Wasserstein Distance. 3) The authors perform an ablation study illustrating the value of each of the proposed modifications. Cons: 1) The paper only shows results on image generation from random noise. The evaluation of this task is notoriously difficult, up to impossible (Theis et al., ICLR 2016). The authors put lots of effort in the evaluation, but still: - it is unclear what is the average quality of the samples - a human study might help - it is unclear to which extent the images are copied from the training set.  The authors show some nearest neighbors from the training set, but very few and in the pixel space, which is known to be pointless (again, Theis et al. 2016). Interpolations in the latent space is a good experiment, but in fact the interpolations do not look that great on LSUN - it is unclear if the model covers the full diversity of images (mode collapse) It would be more convincing to demonstrate some practical results, for instance inpainting, superresolution, unsupervised or semi-supervised learning, etc. 2) The general idea of multi-scale generation is not new, and has been investigated for instance in LapGAN (Denton et al., ICLR 2015) or StackGAN (Zhang et al., ICCV2017, arxiv 2017). The authors should properly discuss this.  3) The authors mention ""unhealthy competition"" between the discriminator and the generator several times, but it is not quite clear what exactly they mean - a more specific definition would be useful. (This conclusion does not take the anonymity violation into account. Because of the violation I believe the paper should be rejected. Of course I am open to discussions with ACs/PCs.)  To conclude, the paper demonstrates a breakthrough in the quality and resolution of images generated with a GAN. The experimental evaluation is thorough, to the degree allowed by the poorly defined task of generating images from random noise. Results on some downstream tasks, such as inpainting, image processing or un-/semi-supervised learning would make the paper more convincing. Still, the paper should definitely be accepted for publication. Normally, I would give the paper a rating of 8.",30,569,19.620689655172413,5.190298507462686,261,3,566,0.0053003533568904,0.0276338514680483,-0.7529,157,67,89,32,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 12, 'EXP': 3, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 11, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 8, 'EMP': 14, 'SUB': 2, 'CLA': 0}",0,0,2,0,1,12,3,6,0,0,0,11,0,0,1,1,0,2,0,8,14,2,0,0.4323091188529141,0.6751145556902924,0.30387656704013677
ICLR2018-Hk99zCeAb-R3,Accept,"This paper proposes a number of ideas for improving GANs for image generation, highlighting in particular a curriculum learning strategy to progressively increase the resolution of the generated images, resulting in GAN generators capable of producing samples with unprecedented resolution and visual fidelity. Pros:  The paper is well-written and the results speak for themselves! Qualitatively they're an impressive and significant improvement over previous results from GANs and other generative models. The latent space interpolations shown in the video (especially on CelebA-HQ) demonstrate that the generator can smoothly transition between modes and convince me that it isn't simply memorizing the training data. (Though I think this issue could be addressed a bit better -- see below.) Though quantification of GAN performance is difficult and rapidly evolving, there is a lot of quantitative analysis all pointing to significant improvements over previous methods. A number of new tricks are proposed, with the ablation study (tab 1 + fig 3) and learning curves (fig 4) giving insight into their effects on performance.  Though the field is moving quickly, I expect that several of these tricks will be broadly adopted in future work at least in the short to medium term. The training code and data are released. Cons/Suggestions:  It would be nice to see overfitting addressed and quantified in some way. For example, the proposed SWD metric could be recomputed both for the training and for a held-out validation/test set, with the difference between the two scores measuring the degree of overfitting. Similarly, Danihelka et al. [1] show that an independently trained Wasserstein critic (with one critic trained on G samples vs. train samples, and another trained on G samples vs. val samples) can be used to measure overfitting. Another way to go could be to generate a large number of samples and show the nearest neighbor for a few training set samples and for a few val set samples. Doing this in pixel space may not work well especially at the higher resolutions, but maybe a distance function in the space of some high-level hidden layer of a trained discriminator could show good semantic nearest neighbors. The proposed SWD metric is interesting and computationally convenient, but it's not clear to me that it's an improvement over previous metrics like the independent Wasserstein critic proposed in [1]. In particular the use of 7x7 patches would seem to limit the metric's ability to capture the extent to which global structure has been learned, even though the patches are extracted at multiple levels of the Laplacian pyramid. The ablation study (tab 1 + fig 3) leaves me somewhat unsure which tricks contribute the most to the final performance improvement over previous work.  Visually, the biggest individual improvement is easily when going from (c) to (d), which adds the ""Revised training parameters"", with the improvement from (a) to (b) which adds the highlighted progressive training schedule appearing relatively minor in comparison.  However, I realize the former large improvement is due to the arbitrary ordering of the additions in the ablation study, with the small minibatch addition in (c) crippling results on its own.  Ablation studies with large numbers of tweaks are always difficult and this one is welcome and useful despite the ambiguity. On a related note, it would be nice if there were more details on the ""revised training hyperparameters"" improvement ((d) in the ablation study) -- which training hyperparameters are adjusted, and how? ""LAPGAN"" (Denton et al., 2015) should be cited as related work: LAPGAN's idea of using a separate generator/discriminator at each level of a Laplacian pyramid conditioned on the previous level is quite relevant to the progressive training idea proposed here. Currently the paper is only incorrectly cited as ""DCGAN"" in a results table -- this should be fixed as well.   Overall, this is a well-written paper with striking results and a solid effort to analyze, ablate, and quantify the effect of each of the many new techniques proposed.  It's likely that the paper will have a lot of impact on future GAN work. [1] Danihelka et al., ""Comparison of Maximum Likelihood and GAN-based training of Real NVPs"" https://arxiv.org/abs/1705.05263",24,681,26.192307692307693,5.367029548989113,312,4,677,0.0059084194977843,0.0230215827338129,0.9955,176,102,111,34,10,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 4, 'MET': 7, 'EXP': 11, 'RES': 4, 'TNF': 3, 'ANA': 5, 'FWK': 1, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 0}",0,0,1,1,4,7,11,4,3,5,1,0,1,0,0,2,1,5,0,0,16,0,0,0.7170696762271962,0.454287563780367,0.4060348504109912
ICLR2018-HkAClQgA--R1,Accept,"The paper proposes a model for abstractive document summarization using a self-critical policy gradient training algorithm, which is mixed with maximum likelihood objective. The Seq2seq architecture incorporates both intra-temporal and intra-decoder attention, and a pointer copying mechanism. A hard constraint is imposed during decoding to avoid trigram repetition. Most of the modelling ideas already exists, but this paper show how they can be applied as a strong summarization model. The approach obtains strong results on the CNN/Daily Mail and NYT datasets. Results show that intra-attention improves performance for only one of the datasets. RL results are reported with only the best-performing attention setup for each dataset. My concern with that is that the authors might be using the test set for model selection; It is not a priori clear that the setup that works better for ML should also be better for RL, especially as it is not the same across datasets. So I suggest that results for RL should be reported with and without intra-attention on both datasets, at least on the validation set. It is shown that intra-decoder attention decoder improves performance on longer sentences. It would be interesting to see more analysis on this, especially analyzing what the mechanism is attending to, as it is less clear what its interpretation should be than for intra-temporal attention. Further ablations such as the effect of the trigram repetition constraint will also help to analyse the contribution of different modelling choices to the performance. For the mixed decoding objective, how is the mixing weight chosen and what is its effect on performance? If it is purely a scaling factor, how is the scale quantified?  It is claimed that readability correlates with perplexity, so it would be interesting to see perplexity results for the models. The lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work. This is a strong paper that presents a significant improvement in document summarization.",17,333,22.2,5.46749226006192,159,1,332,0.003012048192771,0.0449101796407185,0.9935,92,40,55,14,8,3,"{'ABS': 0, 'INT': 0, 'RWK': 12, 'PDI': 2, 'DAT': 4, 'MET': 6, 'EXP': 8, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 2}",0,0,12,2,4,6,8,5,0,2,0,4,0,0,0,0,5,0,0,0,12,0,2,0.5747196249228212,0.3404481467812517,0.2841267041912523
ICLR2018-HkAClQgA--R2,Accept,"This is a very clearly written paper, and a pleasure to read. It combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L. The model is trained by a policy gradient algorithm. While the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments. ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention). The best method finally outperforms the lea-3d baseline for summarization. What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE.  It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail. Some minor points that I wonder about:  - The heuristic against repeating trigrams seems quite crude.  Is there a more sophisticated method that can avoid redundancy without this heuristic? - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)? If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well. - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward? Do you have any insights are speculations regarding this?",14,284,25.818181818181817,5.377777777777778,168,2,282,0.0070921985815602,0.0137931034482758,0.9907,77,41,47,18,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 11, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 6, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 2}",0,0,11,0,0,10,6,2,0,0,0,3,1,0,0,0,5,0,0,0,9,0,2,0.4322055550866844,0.3385822744767227,0.2200138432123179
ICLR2018-HkAClQgA--R3,Accept,"The paper is generally well-written and the intuition is very clear.  It combines the advanced attention mechanism, pointer networks and REINFORCE learning signal to train a sequence-to-sequence model for text summarization. The experimental results show that the model is able to achieve the state-of-the-art performance on CNN/Daily Main and New York Times datasets. It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us. I would like to see the model ablation w.r.t. repetition avoidance trick by muting the second trigram at test time. Intuitively, if the repetition issue is prominent to having decent summarization performance, it might affect our judgement on the significance of using intra-attention or combined RL signal. Another thought on this: is it possible to integrate the trigram occurrence with summarization reward? so that the recurrent neural networks with attention could capture the learning signal to avoid the repetition issue and the heuristic function in the test time can be removed. In addition, as the encoder-decoder structure gradually becomes the standard choice of sequence prediction, I would suggest the authors to add the sum of parameters into model ablation for reference. Suggested References: Bahdanau et al. (2016) An Actor-critic Algorithm for Sequence Prediction. (actor-critic on machine translation) Miao & Blunsom (2016) Language as a Latent Variable: Discrete Generative Models for Sentence Compression. (mixture pointer mechanism + REINFORCE)",13,238,18.307692307692307,5.597402597402597,145,1,237,0.0042194092827004,0.0248962655601659,0.7681,89,29,30,6,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 10, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,0,10,1,1,6,4,0,0,0,0,4,2,0,0,1,1,0,0,0,4,1,1,0.5023825884944942,0.5574214278600844,0.3147841356291088
ICLR2018-HkCnm-bAb-R1,Reject,"The paper presents Erdos-Selfridge-Spencer games as environments for investigating deep reinforcement learning algorithms. The proposed games are interesting and clearly challenging, but I am not sure what they tell us about the algorithms chosen to test them. There are some clarity issues with the justification and evaluation which undermine the message the authors are trying to make. In particular, I have the following concerns:    u2022 these games have optimal policies that are expressible as a linear model, meaning that if the architecture or updating of the learning algorithm is such that there is a bias towards exploring these parts of policy space, then they will perform better than more general algorithms. What does this tell us about the relative merits of each approach? The authors could do more to formally motivate these games as difficult for any deep learning architecture if possible. u2022 the authors compare linear models with non-linear models at some point for attacker policies, but it is unclear whether these linear models are able to express the optimal policy. In fact, there is a level of non-determinism in how the attacker policies are encoded which means that an optimal policy cannot be (even up to soft-max) expressed by the agent (as I read things the number of pieces chosen in level l is always chosen uniformly randomly). u2022 As the authors state, this paper is an empirical evaluation, and the theorems presented are derived from earlier work. There is possibly too much focus on the proofs of these theorems. u2022 There are a number of ambiguities and errors which places difficulties on the interpretation (and potential replication) of the experiments. As this is an empirical study, this is the yardstick by which the paper should be judged. In particular, this relates to:     u25e6 The architecture of each of the tested Deep RL methods.  u25e6 What is done to select appropriate tuning parameters of the tested Deep RL methods, if anything. u25e6 It is unclear whether 'incorrect actions' in the supervised learning evaluations, refer to non-optimal actions, or simply actions that do not preserve the dominance of the defender, e.g. both partitions may have potential >0.5     u25e6 Fig 4. right looks like a reward signal, but is labelled Proportion correct. The text is not clear enough to be sure which it is. u25e6 Fig 4. left and right has 4 methods: rl rewards, rl correct actions, sup rewards, and sup correct actions. The specifics of how these methods are constructed is unclear from the paper. u25e6 What parts of the evaluation explores how well these methods are able to represent the states (feature/representation learning) and what parts are evaluating the propagation of sparse rewards (the reinforcment learning core)? The authors could be clearer and more targetted with respect to this question. There is value in this work, but in its current state I do not think it is ready for publicaiton. # Detailed notes  [p4, end of sec 3] The authors say that the difficulty of the games can be varied with continuous changes in potential, but the potential is derived from the discrete initial game state, so these values are not continuously varying (even though it is possible to adjust them by non-integer amounts). [p4, sec 4.1] strategy unevenly partitions the occupied levels...with the proportional difference between the two sets being sampled randomly What is meant by this? The proportional difference between the two sets is discussed as if it is a continuous property, but must be chosen from the discrete set of all available partitions. If one partition one is chosen uniformly randomly from all possibly sets A, B (and the potential proportion calculated) then I don't know why it would be written in this way. That suggests that proportions that are closer to 1:1 are chosen more often than extreme partitions, but how? This feels a little under-justified. very different states A, B (uneven potential, disjoint occupied levels) Are these states really very different, or at least for the reasons indicated. Later on (Theorem 3) we see how an optimal partition is generated. This chooses a partition where one part contains all pieces in layer (l+1) and above and one part with all pieces in layer (l-1) and below, with layer l being distributed between the two parts. The first part will typically have a slightly lower potential than the other and all layers other than layer l will be disjoint. [p6, Fig 4] The right plot y-limits vary between -1 and 1 so it cannot represent a proportion of correct actions. Also, in the text the authors say:   >> The results, shown in Figure 4 are surprising. Reinforcement learning    >> is better at playing the game, but does worse at predicting optimal moves. I am not sure which plot shows the playing of the game. Is this the right hand plot? In which case are we looking at rewards? In fact, I am a little confused as to what is being shown here. Is sup rewards a supervised learning method trained on rewards, or evaluated on rewards, or both? And how is this done. The text is just not clear enough. [p7 Fig 6 and text] Here the authors are comparing how well agents select the optimal actions as compared to how close they are to the end of the game. This relates to the surprising fact that Reinforcement learning is better at playing the game, but does worse at predicting optimal moves.. I think an important point here is how many training/test examples there are in each bin. If there are more in the range 3-7 moves from the end of the game, than there are outside this range, then the supervised learner will  [p8 proof of theorem 3]  u03c6(A l+1 ) < 0.5 and u03c6(A l ) > 0.5. Is it true that both these inequalities are strict? Since A l only contains pieces from levels K to l + 1 In fact this should read from levels K to l. we can move k < m u2212 n pieces from A l+1 to A l Do the authors mean that we can define a partition A, B where A   A_{l+1} plus some (but not all) elements in level l (A_{l}setminus A_{l+1})? ...such that the potential of the new set equals 0.5  It will equal exactly 0.5 as suggested, but the authors could make it more precise as to why (there is a value n+k < l (maybe < l) such that (n+k)*2^{-(K-l+1)} 0.5 (guaranteed). They should also indicate why this then justifies their proof (namely that phi(S0)-0.5 >  0.5). [p8 paramterising action space] A comment: this doesn't give as much control as the authors suggest. Perhaps the agent should also chose the proportion of elements in layer l to set A. For instance, if there are a large number of elements in l, and or phi(A_{l+1}) is very close to 0.5 (or phi(A_l) is very close to 0.5) then this doesn't give the attacker the opportunity to fine tune the policy to select very good partitions.  It is unclear expected level of control that agents have under various conditions (K and starting states).  [p9 Fig 8] As the defender's score is functionally determined by the attackers score, it doesn't help to include this on the plot. It just distracts from the signal. ",55,1209,23.705882352941178,4.839572192513369,392,9,1200,0.0075,0.0384615384615384,0.9991,309,144,228,72,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 5, 'DAT': 0, 'MET': 43, 'EXP': 15, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 40, 'SUB': 0, 'CLA': 3}",0,0,0,5,0,43,15,2,2,0,0,3,0,0,0,0,0,0,0,1,40,0,3,0.4406932753512462,0.3577519434339173,0.22310433180709394
ICLR2018-HkCnm-bAb-R2,Reject,"This paper presents a study of reinforcement learning methods applied to Erdos-Selfridge-Spencer games, a particular type of two-agent, zero-sum game.  The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. This is used as a sort of ground truth that enables study of the behavior of certain reinforcement learning algorithms (for just one or to both players).  An empirical study is performed, measuring the performance of both agents, tuning the difficulty of the game for each agent by changing the starting position of the game. - The comparison of supervised learning vs RL performance is interesting. Is the supervised algorithm only able to implement Markovian policies?  Is the RL agent able to find policies with longer-term dependence that it can follow?  Is that what is meant by the sentence on page 6 We conjecture that reinforcement learning is learning to focus most on moves that matter for winning? - Why do you think the defender trained as part of a multiagent setting generalizes better than the single agent defender?  Is there something different about the distribution of policies seen by each defender?  Quality: The method appears to be technically correct, clearly written, and easy to read. Originality: I believe this is the first use of ESS games to study RL algorithms. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. Impact: I think this is an interesting and creative contribution to studying RL, particularly the use of an easy-to-analyze game in an RL setting. ",14,279,27.9,5.156133828996283,146,3,276,0.0108695652173913,0.03125,0.9806,78,31,52,10,3,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 12, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,0,3,0,12,1,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,0.2171564272610571,0.1167087280246978,0.08701466649270556
ICLR2018-HkCnm-bAb-R3,Reject,"This paper presents an adversarial combinatorial game: Erdos-Selfridge-Spencer attacker-defender game, with the goal to use it as a benchmark for reinforcement learning. It first compares PPO, A2C, DQN on the task of defending vs. an epsilon-sub-optimal attacker, with varying levels of difficulty. Secondly it compared RL and supervised learning (as they know the optimal actiona at all times). Then it trains (RL) the attacker, and finally trains the attacker and the defender (each a separate model) jointly/concurrently. Various points:  - The explanation of the Erdos-Selfridge-Spencer attacker-defender game is clear. - As noted by the authors in section 5, with this featurization, the network only has to learn the weight to multiply (the multiplication is already the inner product) the feature x_i to be 2^{-(K-i)}, K is fixed for an experiment, and i is the index of the feature, thus can be matched by the index of the weight (vector or diagonal matrix).  The defender network has to do this to the features of A and of B, and compare the values; the attacker (with the action space following theorem 3) has to do this for (at most) K progressive partitions. All of this leads me to think that a linear baseline is a must-have in most of the plots, not just Figure 15 in the appendix on one task, moreso as the environment (game) is new. A linear baseline also allows for easy interpretation of what is learned (is it the exact formula of phi(S)?), and can be parametrized to work with varying values of K. - In the experimental section, it seems (due transparent coloring in plots, that I understand to be the minimum and maximum values as said in the text in section 4.1, or is that a confidence interval or standard deviation(s)? In ny case:) that 3 random seeds are sometimes not enough to derive strong conclusions, in particular in Figure 9. - Everything leads me to believe that, up to 6.2, the game is only dealt with as a fixed MDP to be overfit by the model through RL:    - there is no generalization from K k (train) to K > k (test). - sections 6.2, 6.3 and the appendix are more promising but there is only one experiment with potential 1.0 (which is the most interesting operating point for multiagent training) in Figure 8, and potential 0.999 in the appendix. There is no study of the dynamics of attacks/defenses (best responses or Nash equilibrium). Nits:  - in Figure 8, there is no need to plot both the attacker and defender rewards. - Figure 3 overwrites x axis of top figures. - Figure 4 right y-axis should be average rewards. It seems the game is easy from a reinforcement learning standpoint, and this is not necessarily a bad thing, but then the experimental study should be more rigorous in term of convergences, error bars, and baselines.",18,470,26.11111111111111,4.889655172413793,208,4,466,0.0085836909871244,0.0205761316872428,0.9496,131,51,68,21,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 8, 'EXP': 5, 'RES': 0, 'TNF': 4, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 0}",0,0,1,4,0,8,5,0,4,1,0,0,0,0,0,0,0,0,0,0,10,2,0,0.4309153500024431,0.2279265156441304,0.195051192756166
ICLR2018-HkCsm6lRb-R1,Accept,"The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified,  and when some are left undefined, creating a more abstract generation task. Pros: + The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing . Both quantitative and qualitative results are provided. + The paper is fairly clea r.  Cons: - It is unclear how to judge diversity qualitatively, e.g. in Fig. 4(b) . - Fig. 5 could be more convincing; bushy eyebrows is a difficult attribute to judge , and in the abstract generation when that is the only attribute specified, it is not clear how good the results are. ",8,117,10.636363636363637,5.462962962962963,69,1,116,0.0086206896551724,0.0314960629921259,0.2739,30,17,24,6,7,4,"{'ABS': 2, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 3}",2,1,3,2,0,3,0,3,2,0,0,0,0,0,0,1,0,0,0,0,1,1,3,0.5008568718989725,0.4446067145861526,0.2785036115176948
ICLR2018-HkCsm6lRb-R2,Accept,"This paper presented a multi-modal extension of variational autoencoder (VAE) for the task visually grounded imagination.   In this task,  the model learns a joint embedding of the images and the attributes . The proposed model is novel but incremental comparing to existing frameworks. The author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality. Pros: 1. The paper is well-written, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community.  2. The discussion comparing the related work/baseline methods is insightful. 3. The proposed model addresses many important problems, such as attribute learning, disentanged representation learning, learning with missing values, and proper evaluation methods. Cons/questions: 1. The motivation of the model choice of q is not clear. Comparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q. BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. q(z|x), q(z|y), and q(z|x,y). How does such model choice affect the final performance? 2. Baselines are not necessarily sufficient .  The paper compared the vanilla version of BiVCCA but not the one with factorized representation version . In the original VAECCA paper, the extension of using factorized representation (private and shared) improved the performance] . The author should also compare this extension of VAECCA. 3. Some details are not clear. a) How to set/learn the scaling parameter lambda_y and beta_y ? If it is set as hyper-parameter, how does the performance change concerning them?  b) Discussion of the experimental results is not sufficient . For example, why JMVAE performs much better than the proposed model when all attributes are given. What is the conclusion from Figure 4(b)? The JMVAE seems to generate more diverse (better coverage) results which are not consistent with the claims in the related work. The same applies to figure 5. ",23,312,11.555555555555555,5.575757575757576,156,1,311,0.0032154340836012,0.0307692307692307,0.9725,107,33,51,14,9,5,"{'ABS': 0, 'INT': 3, 'RWK': 17, 'PDI': 7, 'DAT': 4, 'MET': 14, 'EXP': 6, 'RES': 4, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 4, 'CMP': 6, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 3}",0,3,17,7,4,14,6,4,2,0,0,1,0,0,0,1,4,6,0,0,11,0,3,0.6485339386213116,0.562667322776987,0.4061052767908904
ICLR2018-HkCsm6lRb-R3,Accept,"The paper proposes a method for generating images from attributes. The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples , and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples. During training the auto-encoder is trained on paired data (image, attribute) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa. The authors propose handling missing data using a product of experts where the product is taken over available attributes, and it sharpens the prior distribution. The authors evaluate their method using correctness i.e. if the generated images have the desired attributes, coverage i.e. if the generated images sample unspecified attributes well, and compositionality i.e. if  images can be generated from unseen attributes. Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided, it outperforms when some of the attributes are missing (Figure 4a). It also outperforms existing methods in terms of coverage and compositionality. Major comments:  The paper is well written, and summarizes its contribution succinctly. I did not fully understand the 'retrofitting' idea.  If I understood correctly, the authors first train theta and phi and then fix theta to train phi_x and phi_y. If that is true, then is calL(theta, phi, phi_x, phi_y) are right cost function since one does not maximize all three ELBO terms when optimizing theta? Please clarify? Minor comments:  - 'in order of increasing abstraction', does the order of gender-> smiling or not -> hair color matter? Or, is male, *, blackhair a valid option?  -  what are the image sizes for the CelebA dataset  - page 5: double the   - Which multi-label classifier is used to classify images in attributes?",17,296,21.142857142857142,5.395833333333333,157,0,296,0.0,0.0064102564102564,0.9127,97,23,68,14,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 10, 'DAT': 7, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 1}",0,1,6,10,7,8,2,2,1,0,0,0,0,0,0,0,2,1,0,0,10,1,1,0.5744744966174649,0.5612012091175902,0.3680602533515977
ICLR2018-HkCvZXbC--R1,Reject,"Summary: This paper studied the conditional image generation with two-stream generative adversarial networks. More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label. During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation. An auxiliary ""label difference cost"" was further introduced to encourage class information captured by the foreground generator. Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline.    Novelty/Significance    Controllable image generation is an important task in representation learning and computer vision. I also like the unsupervised learning through gating function and label difference cost . However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs).    Detailed comments    The proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region. Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability . For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet. Given the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable.  However, I didn't see such detailed analysis as in the other papers on controllable image generation. In Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp. It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better) . Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned.  Is it possible to use all the binary attributes in the dataset. Also, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks) .  Overall, I believe the paper is interesting but not ready for publication. I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation . Hopefully, the suggested studies will improve the quality of the paper in the future submission.    Presentation    The paper is readable but not well polished .   -- In Figure 1, the ""G1"" on the right should be ""G2""; -- Section 2.2.1, ""X_f"" should be ""x_f""; -- the motivation of having ""z_v"" should be introduced earlier; -- Section 2.2.4, please use either ""alpha"" or ""alpha"" but not both; -- Section 3.3, the dataset information is incorrect: ""20599 images"" should be ""202599 images"";  Missing reference: - - Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017. -- Domain Separation Networks, Bousmalis et al. In NIPS 2016. n-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017. ",26,518,17.266666666666666,5.814432989690721,244,0,518,0.0,0.0161001788908765,0.9724,171,80,76,26,12,8,"{'ABS': 0, 'INT': 4, 'RWK': 19, 'PDI': 4, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 1, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 4, 'EXT': 1}","{'APR': 3, 'NOV': 2, 'IMP': 2, 'CMP': 1, 'PNF': 3, 'REC': 2, 'EMP': 11, 'SUB': 0, 'CLA': 2}",0,4,19,4,1,8,4,1,3,2,0,1,4,1,3,2,2,1,3,2,11,0,2,0.8609645222474426,0.8954450126828468,0.7631069043979747
ICLR2018-HkCvZXbC--R2,Reject,"[Overview]  This paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner. In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents. To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations. After generating both parts in parallel, they are combined using alpha blending to compose the final image. This generated image is then sent to the discriminator. The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA. The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately. [Strenghts]  This paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part. Corresponding to these two parts are two generators. To ensure this, the authors introduced three strategies:  1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images. As usual, the label information was added as the input of generator and discriminator as well. 2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2. Based on this, outputs of Gc should not change much when flipping the input labels. 3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other. Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2. Decomposing the semantic part from the context part in an image based on a generative model is an interesting problem. However, to my opinion, completing it without any supervision is challenging and meaningless. In this paper, the authors proposed a conditional way to generate images compositionally. It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017). [Weaknesses]  This paper proposed an interesting and intuitive image generation model. However, there are several weaknesses existed:  1. There is no quantitative evaluation and comparisons. From the limited qualitative results shown in Fig.2-10, we can hardly get a comprehensive sense about the model performance. The authors should present some quantitative evaluations in the paper, which are more persuasive than a number of examples. To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance. Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation. Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images. This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier. 2. The authors should try more complicated datasets, like CIFAR-10. Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs. It is easy to train since its low resolution, but also means a lot since it a relative complicated scene. I would suggest the authors also run the experiments on CIFAR-10. 3. The authors did not perform any ablation study. Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models. As such, I can hardly get a sense of the effects of different losses and know about the relative performance in the whole GAN spectrum.  I strongly suggest the authors add some ablation studies. The authors should at least compare with one-layer conditional GAN.  4. The proposed model merely showed two-layer generation results. There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6). The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration. 5. Please show f1, m1, f2, m2 separately, instead of showing the blending results in Fig3, Fig4, Fig6, Fig7, Fig9, and Fig10. I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017). 6. I did not understand very well the label difference loss in (5). Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image. G2 takes all the duty  can still obtain a lower L_ld. From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information. 7. Minor typos and textual errors. In Fig.1, should the right generator be G2 rather than G1? In 2.1.3 and 2.2.1, please add numbers to the equations. [Summary]  This paper proposed an interesting way of generating images, called 3C-GAN. It generates images in a layer-wise manner. To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties.  In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA. However, as I pointed out above, the paper missed quantitative evaluation and comparison, and ablation study. Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted.",53,938,14.88888888888889,5.2594718714121695,326,3,935,0.0032085561497326,0.0221052631578947,0.9633,278,110,157,40,12,5,"{'ABS': 0, 'INT': 1, 'RWK': 42, 'PDI': 12, 'DAT': 6, 'MET': 13, 'EXP': 13, 'RES': 4, 'TNF': 2, 'ANA': 4, 'FWK': 0, 'OAL': 9, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 9, 'SUB': 9, 'CLA': 0}",0,1,42,12,6,13,13,4,2,4,0,9,2,1,0,0,5,4,2,0,9,9,0,0.865952573436327,0.5619898302900359,0.546598598309434
ICLR2018-HkCvZXbC--R3,Reject,"- Paper summary  The paper proposes a label-conditional GAN generator architecture and a GAN training objective for the image modeling task. The proposed GAN generator consists of two components where one focuses on generating foreground while the other focuses on generating background. The GAN training objective function utilizing 3 conditional classifier. It is shown that through combining the generator architecture and the GAN training objective function, one can learn a foreground--background decomposed generative model in an unsupervised manner . The paper shows results on the MNIST, SVHN, and Celebrity Faces datasets. - Poor experimental validation  While it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised manner,  it is clear how this capability can help practical applications, especially no such examples are shown in the paper. The paper also fails to provide any quantitative evaluation of the proposed method. For example, the paper will be more interesting if inception scores were shown for various challenging datasets. In additional, there is no ablation study analyzing impacts of each design choices . As a result, the paper carries very little scientific value.",11,183,18.3,5.853107344632768,108,0,183,0.0,0.0105263157894736,0.5244,55,23,38,4,9,5,"{'ABS': 1, 'INT': 1, 'RWK': 6, 'PDI': 3, 'DAT': 2, 'MET': 6, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 1}",1,1,6,3,2,6,2,2,0,4,0,0,0,0,0,0,5,1,0,0,7,1,1,0.6448812892278581,0.559479446758405,0.41076422295777765
ICLR2018-HkCy2uqQM-R1,,"The paper intends to show that complex and real valued neural network are different and lead to different results on similar tasks, the complex valued network being more appropriate to 'difficult' problems and datasets. The work seems to have been written in a rush leading to a big number of typos and quickly filled experiment tables (7 and 8 are full of zeros ?). The only valid conclusion is that real and complex valued neural network cannot be directly compared using the same number of parameters. Some theoretical aspect or at least some intuition should be more in depth detailed to understand when one should be better than the other. Concerning the novelty the paper is in the same spirit as https://arxiv.org/abs/1705.09792 but with weaker experiments, theoretical justifications and no valid conclusion.",5,131,26.2,5.1015625,82,1,130,0.0076923076923076,0.0303030303030303,-0.4464,30,26,25,4,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,2,2,0,0,1,2,1,1,0,1,0,0,0,0,1,0,2,0,0,2,0,1,0.5001922026234081,0.4451835644364774,0.2786267197433519
ICLR2018-HkCy2uqQM-R2,,"This work re-evaluates complex-valued neural networks: complex weights, complex activation functions. The paper acknowledges that complex networks are not new, and that the findings of previous authors is that complex networks perform less well than real-valued alternatives. The paper reports a comparison of real-valued and complex-valued neural networks, controlling for storage capacity (with an interesting discussion of controlling for capacity in terms of computational inference). The paper concludes with Overall the complex-valued neural networks do not perform as well as expected . I didn't understand this conclusion, because previous work found complex-valued neural networks to be inferior, which is consistent with the results reported here. I did not see support in this paper for the claim in the abstract that special architectures make complex networks work better, or that they are well suited to particular data sets .  The empirical results are only presented in table-of-numbers format (graphical comparisons would be easier to understand), and tables 5-8 are all zero, which doesn't make sense for these classification tasks.",7,166,23.714285714285715,5.833333333333333,93,0,166,0.0,0.0059171597633136,0.8039,46,28,31,12,8,4,"{'ABS': 1, 'INT': 4, 'RWK': 4, 'PDI': 0, 'DAT': 1, 'MET': 3, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",1,4,4,0,1,3,0,2,1,0,0,1,0,0,0,0,3,2,0,0,2,1,0,0.5723026780439485,0.4452796377333732,0.32669613457639035
ICLR2018-HkCy2uqQM-R3,,"The authors show how techniques typically applied to real-valued networks (e.g. with real-valued inputs and parameters) can be straighforwardly generalized to complex-valued networks (e.g. with complex-valued inputs and parameters). The authors then provide several evaluations of complex-valued networks on some standard ML benchmark tasks. They find that the complex-valued networks do not in general perform better than real-valued networks.                      Clarity:  I found the paper clear and easy to understand.  In a number of places there are clear signs of sloppiness (e.g. undefined citations).  I found the undefined citations in the middle of page 2 frustrating, since I'd have liked to follow up on those citations as comparison points for this work .   Quality: The mathematical formulas describing basic complex analysis ideas (e.g.  derivatives of complex functions, definitions of complex versions of standard activation functions) seem reasonable to me .   The general approach of assigning a parameter budget to ensure fairness in comparison between complex and real-valued networks seems reasonable -- - although of course, since all results should be reported on cross-validated testing subsets anyhow, parameter equalization is not the only approach to fair evaluation .     Originality:  It seems very unclear to me what is added in this paper in comparison to works like (e.g.) Trabelsi (2017). That and other recent work have provided some systematic evaluations of complex-valued networks, and shown their utility in a number of cases.  The current paper's authors talk about previous work not being well-controlled for number of parameters. However, at least in some key cases in the recent literature, parameter numbers *were* controlled (see e.g. Table 4 of Trabelsi (2017)). So I'm not really sure what is being added here. Significance:   The paper does not make a great case for caring about complex-valued networks. Of course, negative results are of value, but it doesn't seem like much is at stake in this work to begin with. It's not like people expected complex-valued networks to somehow be extremely effective for the tasks discussed here - - so the failure to be better than the real-valued alternatives seems unremarkable. The paper also doesn't illustrate any novel results on tasks for which it would be reasonable to assume that complex-valued inputs would be particularly important.   (The authors reference some signal processing tasks in the introduction , but don't actually show any results on such tasks.)      ",22,380,16.52173913043478,5.464864864864865,188,3,377,0.0079575596816976,0.0438799076212471,0.7909,106,60,62,26,10,5,"{'ABS': 0, 'INT': 3, 'RWK': 15, 'PDI': 7, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 3}","{'APR': 0, 'NOV': 1, 'IMP': 8, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,3,15,7,0,4,1,3,0,5,0,1,2,3,0,1,8,1,0,0,7,0,2,0.7168112887392449,0.5597046917746027,0.44170286040844126
ICLR2018-HkGJUXb0--R1,Reject,"The paper addresses the problem of tensor decomposition which is relevant and interesting. The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method. TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices.  The proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order. I think such generalization is interesting but the innovation seems to be very limited. The paper develops three different kinds of solvers for TR decomposition, i.e., SVD, ALS and SGD. All of these are well known methods. Finally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images).  I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods. Please refer to below for more comments and questions. -- The rating has been updated. Pros: 1. The topic is interesting. 2. The generalization over TT makes sense. Cons: 1. The writing of the paper could be improved and more clear: the conclusions on inner product and F-norm can be integrated into Theorem 5. And those theorems in section 4 are just some properties from previous definitions; they are not theorems. 2. The property of TR decomposition is that the tensors can be shifted (circular invariance). This is an interesting property and it seems to be the major strength of TR over TT.  I think the paper could be significantly improved by providing more applications of this property in both theory and experiments. 3. As the number of latent tensors increase, the ALS method becomes much worse approximation of the original optimization. Any insights or results on the optimization performance vs. the number of latent tensors? 4. Also, the paper mentions Eq. 5 (ALS) is optimized by solving d subproblems alternatively.  I think this only contains a single round of optimization. Should ALS be applied repeated (each round solves d problems) until convergence? 5. What is the memory consumption for different solvers? 6. SGD also needs to update at least d times for all d latent tensors. Why is the complexity O(r^3) independent of the parameter d? 7. The ALS is so slow (if looking at the results in section 5.1), which becomes not practical. The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers. 8. What does iteration mean in experimental results such as table 2? Different algorithms have different cost for each iteration so comparing that seems not fair. The results could make more sense by providing total time consumptions and time cost per iteration. also applies to table 4. 9. Why is the epsion in table 3 not consistent? Why not choose epsion   9e-4 and epsilon 2e-15 for tensorization? 10. Also, table 3 could be greatly improved by providing more ablations such as results for (n 16, d 8), (n 4, d 4), etc. That could help readers to better understand the effect of TR. 11. Section 5.3 could be improved by providing a curve (compression vs. error) instead of just providing a table of sampled operating points. 12. The paper mentions the application of image representation but only experiment on 32x32 images. How does the proposed method handle large images? Otherwise, it does not seem to be a practical application. 13. Figure 5: Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images? Minor: - Typo: Page 4 Line 7 Note that this algorithm use the similar strategy: use -> uses",42,608,11.92156862745098,5.175724637681159,245,7,601,0.0116472545757071,0.0145867098865478,0.9977,171,73,103,33,11,8,"{'ABS': 0, 'INT': 0, 'RWK': 27, 'PDI': 7, 'DAT': 1, 'MET': 8, 'EXP': 6, 'RES': 6, 'TNF': 7, 'ANA': 1, 'FWK': 0, 'OAL': 7, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 9, 'CMP': 1, 'PNF': 3, 'REC': 1, 'EMP': 14, 'SUB': 1, 'CLA': 2}",0,0,27,7,1,8,6,6,7,1,0,7,1,1,0,1,9,1,3,1,14,1,2,0.7910359041785303,0.8975629016478914,0.7036770741066993
ICLR2018-HkGJUXb0--R2,Reject,"This paper proposes a tensor train decomposition with a ring structure for function approximation and data compression. Most of the techniques used are well-known in the tensor community (outside of machine learning). The main contribution of the paper is the introduce such techniques to the ML community and presents experimental results for support. The paper is rather preliminary in its examination. For example, it is claimed that the proposed decomposition provides enhanced representation ability, but this is not justified rigorously either via more comprehensive experimentation or via a theoretical justification. Furthermore, the paper lacks in novelty aspect, as it is uses mostly well-known techniques.",6,104,17.333333333333332,5.900990099009901,70,0,104,0.0,0.0096153846153846,0.4582,36,10,14,6,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 4, 'DAT': 1, 'MET': 2, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,0,3,4,1,2,3,1,0,0,0,1,0,0,0,1,1,0,0,0,3,2,0,0.5007614590875372,0.4457950358224518,0.28494879838532544
ICLR2018-HkGJUXb0--R3,Reject,"This paper presents a tensor decomposition method called tensor ring (TR) decomposition. The proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensors. This is in contrast with another popular approach based on tensor train (TT) decomposition which requires several constraints on the core tensors (such as the rank of the first and last core tensor to be 1). To learn TR representations, the paper presents a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm. To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented, and an SGD algorithm is also presented to make the model scalable. The proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results. This is an interesting work.  TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations.  This seems to be a good direction. The experimental results are somewhat limited but the overall framework looks appealing.",10,183,18.3,5.429378531073446,108,1,182,0.0054945054945054,0.0108108108108108,0.7748,58,27,33,3,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,4,3,0,6,3,2,0,0,0,2,0,0,0,0,3,0,0,0,4,0,0,0.5018845362839641,0.2241841678236469,0.22854735753585237
ICLR2018-HkGcX--0--R1,Reject,"Summary:  This paper attempts to solve the problem of meaningfully combining variational autoencoders (VAEs) and PixelCNNs. It proposes to do this by simultaneously optimizing a VAE with PixelCNN++ decoder, and a VAE with factorial decoder. The model is evaluated in terms of log-likelihood (with no improvement over a PixelCNN++) and the visual appearance of samples and reconstructions. Review:  Combining density networks (like VAEs) and autoregressive models is an unsolved problem and potentially very useful. To me, the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels (below Equation 7). Unfortunately the authors were unable to make any good use of this insight, and I will explain below why I don't see any evidence of an improved generative model in this paper. As the paper is written now, it is not clear what the goal of the authors is. Is it density estimation? Then the addition of the VAE had no measurable effect on the PixelCNN++'s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability. Is it representation learning? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement. Is it image synthesis (not a real application by itself), then the paper should have demonstrated the usefulness of the model on a real task and probably involve human subjects in a quantitative evaluation. Much of the authors' analysis is based on a qualitative evaluation of samples. However, samples can be very misleading. A lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the low-level statistics of natural images.  In contrast to the authors, I fail to see a meaningful difference between the groups of samples in Figure 1. The VAE samples in Figure 3b) look quite smooth. Was independent Gaussian noise added to the VAE samples or are those (as is sometimes done) sampled means? If the former, what was sigma and how was it chosen? On page 7, the authors conclude that ""the pixelCNN clearly takes into account the output of the VAE decoder"" based on the samples. Being a mixture model, a PixelCNN++ could easily represent the following mixture:  p(x | z)   0.01 prod_i p(x_i | x_{<i}) + 0.99 prod_i p(x_i | z) The first term is just like a regular PixelCNN++, ignoring the latent variables. The second term is just like a variational autoencoder with factorial decoder. The samples in this case would be dominated by the VAE, which depends on the latent state. The log-likelihood would be dominated by the first term and would be minimally effected (see Theis et al., 2016). Note that I am not saying that this is exactly what the model has learned. I am merely providing a possible counter example to the notion that the PixelCNN++ has learned to use of the latent representation in a meaningful way. What happens if the KL term is simply downweighted but the factorial decoder is not included? This seems like it would be a useful control to include. The paper is well written and clear.",30,547,22.791666666666668,5.025291828793774,242,4,543,0.007366482504604,0.0323159784560143,0.985,161,54,88,34,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 6, 'DAT': 3, 'MET': 17, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 21, 'SUB': 0, 'CLA': 2}",0,0,0,6,3,17,1,0,2,0,0,2,0,0,0,0,0,0,1,0,21,0,2,0.4330118999498061,0.3458536171010469,0.21795810138443925
ICLR2018-HkGcX--0--R2,Reject,"The proposed approach is straight forward, experimental results are good, but don't really push the state of the art. But the empirical analysis (e.g. decomposition of different cost terms) is detailed and very interesting.   ",3,34,8.5,5.294117647058823,28,0,34,0.0,0.027027027027027,0.7498,8,8,6,4,3,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,2,1,0,0.2142857142857142,0.3339552907681763,0.10715094076575937
ICLR2018-HkGcX--0--R3,Reject,"The authors present Auxiliary Guided Autoregressive Variational autoEncoders (AGAVE), a hybrid approach that combines the strengths of variational autoencoders (global statistics) and autorregressive models (local statistics) for improved image modeling. This is done by controlling the capacity of the autorregressive component within an auxiliary loss function. The proposed approach is a straightforward combination of VAE and PixelCNN that although empirically better than PixelCNN, and presumably VAE, does not outperform PixelCNN++. Provided that the authors use PixelCNN++ in their approach, quantitively speaking, it is difficult to defend the value of adding a VAE component to the model. The authors do not describe how lambda was selected, which is critical for performance, provided the results in Figure 4. That being said, the contribution from the VAE is likely to be negligible given the performance of PixelCNN++ alone. - The KL divergence in (3) does more than simply preventing the approximation q() from becoming a point mass distribution.",7,154,22.0,5.891156462585034,92,2,152,0.0131578947368421,0.0193548387096774,0.3892,44,19,30,7,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,5,0,2,1,0,0,0,0,0,0,0,0,2,0,0,3,0,0,0.2868300559239896,0.2235832996490981,0.12942717421453528
ICLR2018-HkJ1rgbCb-R1,Reject,"The paper proposes a feature learning technique for molecular prediction using reinforcement learning. The predictive model is an interesting two-step approach where important atoms of the molecule are added one-by-one with a reward given by a second Q-network that learns how well we can solve the prediction problem with the given set of atoms. The overall scheme is intuitive, but   The model is experimented on two small datasets of few thousand of molecules, and compared to a state-of-the-art DeepTox, and also to some basic baselines (RF/SVM/logreg). In the Tox21 dataset the proposed sparse RL-CNN method is less accurate than DeepTox or full CNN. In the hERG dataset RL-CNN is again weaker than the full CNN, but also seems to be beaten by several baseline methods. Overall the results are surprisingly weak, since e.g. with LASSO one often improves by using less features in complex problems. Both datasets should be compared to LASSO as well. It's somewhat odd that the test performance in table 2 is often better than CV performance.  This feels suspicious, especially with 79.0 vs 84.3. The table 2 does not seem reliable result, and should use more folds and more randomizations, etc. The key problem of the method is its seeming inabability to find the correct number of atoms to use. In both datasets the number of atoms were globally fixed, which is counter-intuitive. The authors should at least provide learning curves where different number of atoms are used; but ideally the method should learn the number of atoms to use for each molecule. The proposed Q+P network is interesting, but its unclear how well it works in general. There should be experiments that compare the the Q+P model with incresing number of atoms against a full CNN, to see whether the Q+P can converge to maximal performance. Overall the method is interesting and has a clear impact for molecular prediction, however the paper has limited appeal to the broader audience. Its difficult to assess how useful the Q/P-network is in general. The inability to choose the optimal number of atoms is a major drawback of the method, and the experimental section could be improved. This paper also would probably be more suitable for a chemoinformatics journal, where the rationale learning would be highly appreciated. ",19,378,18.0,5.040983606557377,188,4,374,0.0106951871657754,0.0340314136125654,0.9611,92,52,66,21,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 5, 'MET': 9, 'EXP': 2, 'RES': 4, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 2, 'CLA': 0}",0,0,2,3,5,9,2,4,1,0,0,0,0,0,0,0,0,4,0,0,12,2,0,0.5025982889657837,0.3406330292964974,0.2512360251221295
ICLR2018-HkJ1rgbCb-R2,Reject,"This paper presents an interesting approach to identify substructural features of molecular graphs contributing to the target task (e.g. predicting toxicity). The algorithm first builds two conv nets for molecular graphs, one is for searching relevant substructures (policy improvement), and another for evaluating the contribution of selected substructures to the target task (policy evaluation). These two phases are iterated in a reinforcement learning manner as policy iterations. Both parts are based on conv nets for molecular graphs, and this framework is a kind of 'self-supervised' scheme compared to the standard situations that the environment provides rewards. The experimental validations demonstrate that this model can learn a competitive-performed conv nets only dependent on the highlighted substructures, as well as reporting some case study on the inhibition assay for hERG proteins. Technically speaking, the proposed self-supervised scheme with two conv nets is very interesting. This demonstrates how we can perform progressive substructure selections over molecular graphs to highlight relevant substructures as well as maximizing the prediction performance. Given that conv nets for molecular graphs are not trivially interpretable, this would provides a useful approach to use conv nets for more explicit interpretations of how the task can be performed by neural nets. However, at the same time, I had one big question about the purpose and usage of this approach. As the paper states in Introduction, the target problem is 'hard selection' of substructures, rather than 'soft selection' that neural nets (with attention, for example) or neural-net fingerprints usually provide.  Then, the problem would become a combinatorial search problem, which has been long studied in the data mining and machine learning community. There would exist many exact methods such as LEAP, CORK, and graphSig under the name of 'contrast/emerging/discriminative' pattern mining exactly developed for this task. Also, it is widely known that we can even perform a wrapper approach for supervised learning from graphs simultaneously with searching all relevant subgraphs as seen in Kudo+ NIPS 2004, Tsuda ICML 2007, Saigo+ Machine Learning 2009, etc. It would be unconvincing that the proposed neural nets approach fits to this hard combinatorial task rather than these existing (mostly exact) methods. In addition to the above point, several technical points below would also be unclear. - A simple heuristic by adding 'selected or not' variables to the atom features works as intended? Because this is fed to the conv net, it seems we can ignore this elements of features by tweaking the weight parameters accordingly. If the conv net performs the best when we use the entire structure, then learning might be forced to ignore the selection. Can we guarantee in some sense this would not happen? - Zeroing out the atom features also sounds quite simple and a bit groundless. Confusingly, the P network also has an attention mechanism, and it is a bit unclear to me what was actually worked. - In the experiments, the baseline is based on LR, but this would not be fair because usually we cannot expect any linear relationship for molecular fingerprints. It's highly correlated due to the inclusion relationships between subgraphs. At least, any nonlinear baseline (e.g. Random forest or something?) should be presented for discussing the results. Pros: - interesting self-supervised framework provided for highlighting relevant substructures for a given prediction task - the hard selection setting is encoded in input graph featurization Cons: - it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks). At least one of the typical ones should be compared or discussed. - I'm still not quite sure whether or not some heuristic parts work as intended. ",29,604,21.571428571428573,5.491496598639456,289,4,600,0.0066666666666666,0.0212071778140293,0.49,156,79,108,45,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 5, 'DAT': 0, 'MET': 20, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 24, 'SUB': 0, 'CLA': 0}",0,1,0,5,0,20,4,1,0,0,0,0,0,0,0,0,1,0,0,0,24,0,0,0.3623922941273148,0.2365272432236106,0.1653471785708636
ICLR2018-HkJ1rgbCb-R3,Reject,"In this manuscript, the authors propose an interesting deep reinforcement learning approach via CNNs to learn the rationales associated to target chemical properties. The paper has merit, but in its current form does not match the acceptance criteria for ICLR. In particular, the main issue lies in the poor performance reached by the systems, both overall and in comparison with baseline methods, which at the moment hardly justifies the effort required in setting up the DL framework. Moreover, the fact that test performances are sometimes (much) better than training results are quite suspicious in methodological terms. Finally, the experimental part is quite limited (two small datasets), making it hard to evaluate the scalability (in all sense) of the proposed solution to much larger data. ",5,124,20.666666666666668,5.290322580645161,98,1,123,0.008130081300813,0.016,-0.8401,35,18,20,7,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,0,1,1,0,1,1,3,0,0,0,1,0,0,1,0,0,0,0,0,3,1,0,0.4286828323568475,0.3345772482030192,0.2168692365114856
ICLR2018-HkL7n1-0b-R1,Accept,"This paper satisfies the following necessary conditions for acceptance. The writing is clear and I was able to understand the presented method (and its motivation) despite not being too familiar with the relevant literature.  Explicitly writing the auto-encoder(s) as pseudo-code algorithms was particular helpful. I found no technical errors. The problem addressed is one worth solving - building a generative model of observed data. There is some empirical testing which show the presented method in a good light. The authors are careful to relate the presented method with existing ones, most notably VAE and AAE. I suppose one could argue that the close connection to existing methods means that this paper is not innovative enough. I think that would be unfair - most new methods have close relations with existing ones - it is just that sometimes the authors do not flag this up as they should. WAE is a bit oversold. The authors state that WAE generates samples of better quality (than VAE) without any condition being put on when it does this. There is no proof that it is always better, and I can't see how there could be. Any method of inferring a generative model from data must make some 'inductive' assumptions. Surely one could devise situations where VAE outperforms WAE. I think this issue should have been examined in more depth. I found no typo or grammatical errors which is unusual - good careful job",16,235,14.6875,5.080357142857143,137,3,232,0.0129310344827586,0.0625,0.9435,60,30,50,14,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 4, 'DAT': 2, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 9, 'SUB': 4, 'CLA': 2}",0,0,9,4,2,6,0,0,0,0,0,3,0,0,0,0,0,0,1,1,9,4,2,0.3593425488724143,0.5609323796301178,0.21761087446896663
ICLR2018-HkL7n1-0b-R2,Accept,"This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). While I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y. This work contains plenty of novel material, which is clearly compared to previous work: - The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution . - Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max - The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival The experiments are very convincing, both numerically and visually. Source of confusion: in algorithm 1 and 2, tilde{z} is sampled from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.",13,275,25.0,4.774436090225564,149,1,274,0.0036496350364963,0.028673835125448,0.9833,67,27,54,23,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 3, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 2}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 3}",0,0,7,3,0,3,2,0,0,0,0,3,1,2,0,2,2,0,1,0,7,1,3,0.5012882728415516,0.6706538256503987,0.35043012236711074
ICLR2018-HkL7n1-0b-R3,Accept,"This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric. By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework. In my opinion, this is one of the crucial contributions of this paper. While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture. The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons. In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations. The closest work to this paper is the adversarial variational bayes framework by Mescheder et.al. which also attempts at unifying VAEs and GANs. While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section.",9,190,21.11111111111111,5.835106382978723,124,1,189,0.0052910052910052,0.0209424083769633,0.836,54,24,28,7,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,5,1,0,7,3,3,0,0,0,3,1,0,0,0,2,2,0,0,3,1,0,0.5736263087057527,0.4458535585197682,0.32275363889576947
ICLR2018-HkMCybx0--R1,Reject,"This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU). Experiments show that ISRLU is promising compared to competitors like ReLU and ELU. Pros: (1) The paper is clearly written. (2) The proposed ISRLU function has similar curves with ELU and has a learnable parameter alpha (although only fixed value is used in the experiments) to control the negative saturation zone. Cons: (1) Authors claim that ISRLU is faster than ELU, while still achieves ELU's performance. However, they only show the reduction of computation complexity for convolution, and speed comparison between ReLU, ISRLU and ELU on high-end CPU . As far as I know, even though modern CNNs have reduced convolution's computation complexity, the computation cost of activation function is still only a very small part (less than 1%) in the overall running time of training/inference .   (2) Authors only experimented with two very simple CNN architectures and with three different nonlinear activation functions, i.e., ISRLU/ELU/ReLU and showed their accuracies on MNIST . They did not provide the comparison of running time which I believe is important here as the efficiency is emphasized a lot throughout the paper .  (3) For ISRLU of CNN, experiments on larger scale dataset such as CIFAR or ImageNet would be more convincing . Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results. Overall, I think the current version of the paper is not ready for ICLR conference. As I suggested above, authors need more experiments to show the effectiveness of their approach. ",14,261,18.642857142857142,5.3562753036437245,150,1,260,0.0038461538461538,0.018450184501845,0.5076,87,29,42,20,10,7,"{'ABS': 0, 'INT': 3, 'RWK': 7, 'PDI': 4, 'DAT': 1, 'MET': 7, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 3, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 6, 'CLA': 1}",0,3,7,4,1,7,6,1,0,4,0,2,0,1,1,0,3,3,0,1,7,6,1,0.7170492877353671,0.7823733033397188,0.5662681326849914
ICLR2018-HkMCybx0--R2,Reject,"Summary: - The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function. Contributions: - The paper proposes a cheaper activation and validates it with an MNIST experiment. The paper also shows major speedup compared to ELU and TANH (unit-wise speedup) .  Pros: - The proposed function has similar behavior as ELU but 4x cheaper. - The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future. - The paper is clearly written and key contributions are well present. Cons: - Clearly, the proposed function is not faster than ReLU. In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN). But the authors also need to justify that ISRLU (or ELU) doesn't need BN . In fact, in a recent study of ELU-ResNet (Shah et al., 2016) finds that ELU without BN leads to gradient explosion. To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks. At inference time, all of ReLU, ELU, and ISRLU can fuse BN weights into convolution weights, so again ISRLU will not be faster than ReLU. The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU? I couldn't find it based on the results presented here . - The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted. - The speedup is only measured on CPU. For practical usage, especially in computer vision, GPU speedup is needed to show an impact .  Conclusion: - Based on the comments above, I recommend weak reject. References: - Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit. In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16).",20,343,16.333333333333332,4.936555891238671,181,1,342,0.0029239766081871,0.0083565459610027,0.9738,119,34,56,24,11,7,"{'ABS': 0, 'INT': 2, 'RWK': 13, 'PDI': 6, 'DAT': 1, 'MET': 11, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 3, 'PNF': 0, 'REC': 1, 'EMP': 13, 'SUB': 1, 'CLA': 2}",0,2,13,6,1,11,4,2,0,1,1,0,2,1,0,2,4,3,0,1,13,1,2,0.7898307621920072,0.7857459447109894,0.6322014517899569
ICLR2018-HkMCybx0--R3,Reject,"Summary: The contribution of this paper is an alternative activation function which is faster to compute than the Exponential Linear Unit, yet has similar characteristics. The paper first presents the mathematical form of the proposed activation function (ISRLU), and then shows the similarities to ELU graphically. It then argues that speeding up the activation function may be important since the convolution operations in CNNs are becoming heavily optimized and may form a lesser fraction of the overall computation. The ISRLU is then reported to be 2.6x faster compared to ELU using AVX2 instructions.  The possibility of computing a faster approximation of ISRLU is also mentioned . Preliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU .  Quality and significance: The paper proposes an interesting direction for optimizing the computational cost of training and inference using neural networks . However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice. The paper does not present clear benchmarks showing a) what is the fraction of CPU cycles spent in evaluating the activation function in any reasonably practical neural network, b) and what is the percentage of cycles saved by employing the ISRLU. The presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU. The effect of using the faster approximation on performance also remains to be investigated. Clarity: The content of the paper is unclear in certain areas. - It is not clear what Table 2 is showing. What is performance measured in? In general the Table captions need to be clearer and more descriptive . The acronym pkeep in later Tables should be clarified. - Why is the final Cross-Entropy Loss so high even though the accuracy is >99% for the MNIST experiments ? It looks like the loss at initialization was reported instead?",17,329,20.5625,5.313664596273292,165,2,327,0.0061162079510703,0.0355029585798816,0.3352,83,42,67,23,9,6,"{'ABS': 0, 'INT': 3, 'RWK': 12, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 6, 'RES': 4, 'TNF': 2, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 7, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 3}",0,3,12,2,0,7,6,4,2,3,0,1,0,0,0,0,7,1,1,0,8,1,3,0.6460248215451723,0.6714708587429629,0.4489025501471713
ICLR2018-HkMhoDITb-R1,Reject,"Summary: The paper demonstrates the use of a quantum annealing machine to solve a free-energy based reinforcement learning problem. Experimental results are demonstrated on a toy gridworld task, where if I understand correctly it does better than a DQN and a method based on RBM-free-energy approximation (Sallans and Hinton, 2004) Clarity: The paper is very hard to read. It seems to be targeted towards a physics/quantum hardware crowd rather than a machine learning audience. I think most readers, even those very familiar with probabilistic models and RL, would find reading the paper difficult due to jargon/terminology and poorly explained concepts. The paper would need a major rewrite to be of interest to the ML community. Relevance: RL, probabilistic models, and function approximators are all relevant topics. However, the focus of the paper seems to be on parts (like hardware aspects) that are not particularly relevant to the ML community. I have a hard time imagining follow-up work on this, given that the experiments are run on a toy task and require specialized hardware (so they would be extremely difficult to reproduce/improve upon). Soundness: I can't judge the technical soundness as it is mostly outside my expertise. I wonder if the main algorithm the work is based on (Crawford et al, 2016) has been peer-reviewed (citation appears to be a preprint, and couldn't find a conference/journal version). Significance: It's good to know that the the quantum annealing machine can be used for RL. However, the method of choice (free-energy based Q-function approximation) seems a bit exotic, and the experimental results are extremely underwhelming (5x3 gridworld).",13,264,22.0,5.493827160493828,144,5,259,0.0193050193050193,0.0303030303030303,0.7791,76,31,54,15,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 2}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 4}",0,0,0,1,0,3,2,1,0,0,0,5,0,2,1,0,1,0,0,0,5,0,4,0.4293490118910659,0.4471756793963785,0.2396411952282923
ICLR2018-HkMhoDITb-R2,Reject,"The paper is easy to read  for a physicist, but I am not sure how useful it would be for ICLR... it is not clear for me it there is an interest for quantum problems in this conference. This is something I will let to the Area Chair to deceede. Other than this, the paper is interesting, certainly correct, and provides a nice perspective on the future of learning with quantum computers. I like the  quantum boltzmann machine problems. I feel, however, but it might be a bit far from the main interest of the conference. Comments:  * What the authors called Free energy-based reinforcement learning seems to me just the minimization / maximiation of the free energy. This is simply maximum likelihood applied to the free energy and I think that calling it reinforcement learning is not only wrong, but also is very confusing, given this is usually reserved to an entirely different learning process. * While i liked the introduction of the quantum Boltzmann machine, I would be happy to learn what they can do? Are these useful, for instance, to study correlated fermions/bosons? The paper does not explain why one should be concerns with these devices. * The fact that the simulation on a classical computer agrees with the one on a quantum computer is promising,  but I would say that this shows that, so far, there is not yet a clear advantage in using a quantum computer. This might change, but in the mean time, what is the benefits for the ICLR community? ",13,253,23.0,4.823529411764706,127,5,248,0.0201612903225806,0.0648854961832061,0.9944,60,26,49,18,6,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 4, 'NOV': 0, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,2,0,1,0,5,0,0,0,1,1,5,0,0,4,0,2,0,0,0,5,1,0,0.4297793911880849,0.4470093085651687,0.2316889746395632
ICLR2018-HkMhoDITb-R3,Reject,"There is no scientific consensus on whether quantum annealers such as the D-Wave 2000Q that use the transverse-field Ising models yield any gains over classical methods (c.f. https://arxiv.org/abs/1703.00622). However, it is an exciting research area and this paper is an interesting demonstration of the feasibility of using quantum annealers for reinforcement learning. This paper builds on Crawford et al. (2016), an unpublished preprint, who develop a quantum Boltzmann machine reinforcement learning algorithm (QBM-RL). A QBM consists of adding a transverse field term to the RBM Hamiltonian (negative log likelihood), but the benefits of this for unsupervised tasks are unclear (c.f. https://arxiv.org/abs/1601.02036, another unpublished preprint). QBM-RL consists of using a QBM to model the state-action variables: it is an undirected graphical model whose visible nodes are clamped to observed state-action pairs. The hidden nodes model dependencies between states and actions, and the weights of the model are updated to maximize the free energy or Q function (value of the state-action pair). The authors extend QBM-RL to work with quantum annealers such as the D-Wave 2000Q, which has a specific bipartite graph structure and requires special consideration because it can only yield samples of hidden variables in a fixed basis. To overcome this, the authors develop a Suzuki-Trotter expansion and call it 'replica stacking', where a classical Hamiltonian in one dimension higher is used to approximate the quantum Hamiltonian. This enables the use of quantum annealers. The authors compare their method to standard baselines in a grid world environment. Overall, I do not want to criticize the work. It is an interesting proof of concept. But given the high price of quantum annealers, limited applicability of the technique, and unclear benefits of the authors' method, I do not think it is relevant to this specific conference. It may be better suited to a workshop specific to quantum machine learning methods.                                         + please add an algorithm box for your method. It deviates significantly from QBM-RL.  For example, something like: (1) init weights of boltzmann machine randomly (2) sample c_eff ~ C from the pool of configurations sampled from the transverse-field Ising model using a quantum annealer with chimera graph (3) using the samples, calculate effective classical hamiltonian used to approximate the quantum system (4) use the weight update rules derived from Bellman equations (spell out the rules). + moving the details of sampling into the appendix would help; they are not important for understanding the main ingredients of your method n There are so many moving parts in your system, and someone without a physics background will struggle to understand it. Clarifying the algorithm in terms familiar to machine learning researchers will go a long way toward helping people understand your method. + the benefits of your method is unclear - it looks like the method works, but doesn't outperform the others. this is fine, but it is better to be straightforward about this and bill it as a 'proof of concept'  + perhaps consider rebranding the paper as something like 'RL using replica stacking for sampling from quantum boltzmann machines with quantum annealers'. Elucidating why replica stacking is a crucial contribution of your work would be helpful, and could be of broad interest in the machine learning community. Right now it is too dense to be useful for the average person without a physics background: what difficulties are intrinsic to a quantum Hamiltonian?  What is the intuition behind the Suzuki-Trotter decomposition you develop? What is the 'quantum' Boltzmann machine in machine learning terms (hidden-hidden connections in an undirected graphical model!)?  What is replica-stacking in graphical model terms (this would be a great ML contribution in its own right!)?  Really spelling these things out in detail (or in the appendix) would help                                            1) eq 14 is malformed 2) references are not well-formatted 3) need factor of 1/2 to avoid double counting in sums over nearest neighbors (please be precise)",35,640,24.615384615384617,5.41845140032949,286,3,637,0.0047095761381475,0.0149863760217983,0.9971,193,80,113,18,8,6,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 5, 'DAT': 0, 'MET': 25, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 1, 'EXT': 2}","{'APR': 2, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 18, 'SUB': 2, 'CLA': 0}",0,0,2,5,0,25,2,0,0,0,1,3,1,2,2,0,1,1,1,0,18,2,0,0.5779372309851772,0.6773562854782869,0.41233460810337064
ICLR2018-HkMvEOlAb-R1,Accept,"This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation.  Pipeline: -Data are augmented with domain-specific transformations . For instance, in the case of MNIST, rotations with different degrees are applied . All data are then labelled as original or transformed by ...(specific transformation). -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels. -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion. -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer. Detailed Comments: (*) Pros -The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST. -Use of ACOL and GAR is interesting, also the idea to make labeled data from unlabelled ones by using data augmentation. (*) Cons -minor: in the title, I find the expression unsupervised clustering uselessly redundant since clustering is by definition unsupervised. -Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits . This is not a very challenging task. And just because something works on MNIST, does not mean it works in general. What are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)? -This is not clear what is novel here since ACOL and GAR already exist. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. My main problem  was about the lack of novelty.  The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv .  The other issue concerned the validation of the approach on databases other than MNIST. The author also addressed this point, and I changed my scores accordingly. ",19,312,15.6,5.618892508143323,168,1,311,0.0032154340836012,0.0247678018575851,0.83,93,36,61,21,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 7, 'DAT': 10, 'MET': 12, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 1}",0,1,7,7,10,12,5,2,0,1,0,1,0,0,0,4,2,0,0,0,9,0,1,0.6471732844127981,0.4496034633251905,0.3641077162561213
ICLR2018-HkMvEOlAb-R2,Accept,"This paper utilizes ACOL algorithm for unsupervised learning. ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9) . Given that in many applications such parent-class supervised information is not available , the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning . The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. The authors use multiple data sets to study different aspects of the proposed approach .  I updated my scores based on the reviewers responses. It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv! Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR) . So my main problem with this paper, lack of novelty, is addressed and my score has changed. Thanks to the reviewer for clarifying this. ",11,201,20.1,5.180412371134021,118,0,201,0.0,0.0432692307692307,0.2797,67,20,34,10,8,2,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 3, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 3}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,5,3,1,6,1,0,0,0,0,0,1,3,0,1,0,0,0,0,3,0,0,0.5731387394670564,0.2234661370919081,0.25247815293537657
ICLR2018-HkMvEOlAb-R3,Accept,"The paper is well written and clear . The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task . The idea is to introduce the notion of pseudo labelling. Pseudo labelling can be obtained by transformations of original input data . The key point is the definition of the transformations .  Only whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task . Since it is not known in advance what might be a good set of transformations, it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters.",8,123,17.571428571428573,5.132231404958677,66,2,121,0.0165289256198347,0.0852713178294573,0.8487,36,11,21,5,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 3, 'DAT': 3, 'MET': 5, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,1,4,3,3,5,4,0,0,0,0,0,0,0,0,0,3,0,0,0,7,0,2,0.4302630293123437,0.3372422863101409,0.20364284226395918
ICLR2018-HkNGsseC--R1,Accept,"The paper studies the expressive power provided by overlap in convolution layers of DNNs.  Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling. Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs. n For these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >  filter width).  The key part of the proof is showing a lower bound on the rank for networks with overlap .  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a generic tensor. The results are interesting overall, but the paper has many caveats: 1.  the results are only for ConvACs,  which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important). n2.  it's not clear if the importance of overlap is too surprising (or is a pressing question to understand, as in the case of depth) . 3.  the rank of the tensor being high does not preclude approximation (to a very good accuracy) by tensors of much smaller rank. That said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets , as it gives some intuition about how much overlap might 'suffice'.    I recommend weak accept.",12,259,18.5,5.264227642276423,150,1,258,0.003875968992248,0.0218978102189781,0.9492,74,36,40,12,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 10, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 3, 'RES': 4, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,0,10,0,0,5,3,4,0,5,0,1,0,0,0,0,3,1,0,1,7,0,1,0.4307349629320339,0.5593833734615091,0.27123308107806604
ICLR2018-HkNGsseC--R2,Accept,"The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields. To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs. The grid tensors are matricized and the ranks of the resultant matrices are compared.  The paper obtains a lower bound on the rank of the resultant grid tensors , and uses them to show that an exponentially large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs. Assuming that the result carries over to ConvNets, I find this result to be very interesting. While overlapped convolutional layers are almost universally used, there has been very little theoretical justification for the same . This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts. ",8,143,17.875,5.807142857142857,83,0,143,0.0,0.0068027210884353,0.6353,35,21,30,7,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 6, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,8,0,0,6,3,2,0,6,0,0,0,0,0,0,2,4,0,0,5,0,0,0.3593089234076765,0.336220687392723,0.18375447575565956
ICLR2018-HkNGsseC--R3,Accept,"The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size; the so called overlapping convolutional architectures . The main object of study is to quantify the benefits of overlap in convolutional architectures. The main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e., there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture; whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures. The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network. (1) The results of the paper hold only for product pooling and linear activation function except for the representation layer, which allows general functions. It is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special case . That this is the case should be made clear in the title and abstract . The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case , but since it is left as future work, the paper should make it clear throughout. (2) The experiment is minimal and even the given experiment is not described well. What data augmentation was used for the CIFAR-10 dataset? It is only mentioned that the data is augmented with translations and horizontal flips. What is the factor of augmentation ? How much translation ? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily. Indeed, this simple intuition seems to be why the authors chose to make the problem by introducing translations and flips. (3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations (a) why are non-overlapping architectures so common? (b) why only slight overlap is used in practice? The paper seems to claim that since overlapping architectures have higher expressivity that answers (a) . It appears that the paper does not answer (b) well: it points out that since there is exponential increase, there is no reason to increase it beyond a particular point. It seems the right resolution will be to show that after the overlap is set to a certain small value , there will be *only* linear increase with increasing overlap; i.e., the paper should show that small overlap networks are efficient with respect to *large* overlap networks; a comparison that does not seem to be made in the paper. (4) Small typo: the dimensions seem to be wrong in the line below the equation in page 3 .   The paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper) . However, the writing is hurried and the high-level conclusions are not fully supported by theory and experiments. ",29,518,27.26315789473684,5.305835010060362,220,5,513,0.0097465886939571,0.0282485875706214,0.9791,128,64,99,24,11,6,"{'ABS': 1, 'INT': 0, 'RWK': 23, 'PDI': 4, 'DAT': 6, 'MET': 8, 'EXP': 14, 'RES': 7, 'TNF': 1, 'ANA': 4, 'FWK': 1, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 8, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 2, 'CLA': 5}",1,0,23,4,6,8,14,7,1,4,1,0,2,0,0,0,8,2,0,1,10,2,5,0.7911360254434163,0.6731489194683163,0.5546682855160245
ICLR2018-HkOhuyA6--R1,Reject,"The authors propose to use 2D CNNs for graph classification by transforming graphs to an image-like representation from its node embedding. The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms. Essentially the authors propose an approach to use a node embedding to achieve graph classification. In my opinion there are several weak points:  1) The approach to obtain the image-like representation is not well motivated .ther approaches how to  aggregate the set of node embeddings for graph classification are known, see, e.g., Representation Learning on Graphs: Methods and Applications, William L. Hamilton, Rex Ying, Jure Leskovec, 2017. The authors should compare to such methods as a baseline. 2) The experimental evaluation is not convincing: - the selection of competing methods is not sufficient. I would like to suggest to add an approach similar to Duvenaud et al., Convolutional networks on graphs for learning molecular fingerprints, NIPS 2015. n- the accuracy results are taken from other publications and it is not clear that this is an authoritative comparison; the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method, cf., e.g., Kriege et al.,  On Valid Optimal Assignment Kernels and Applications to Graph Classification, NIPS 2016. n- it would be interesting to apply the approach to graphs with discrete and continuous labels .  3) The authors argue that their method is preferable to graph kernels in terms of time complexity. This argument is questionable. Most graph kernels compute explicit feature maps and can therefore be used with efficient linear SVMs (unfortunately most publications use a kernelized SVM) . Moreover, the running of computing the node embedding must be emphasized: On page 2 the authors claim a constant time complexity at the instance level , which is not true when also considering the running time of node2vec. Moreover, I do not think that node2vec is more efficient than, e.g., Weisfeiler-Lehman refinement used by graph kernels. In summary: Since the technical contribution is limited, the approach needs to be justified by an authoritative experimental comparison . This is not yet achieved with the results presented in the submitted paper.  Therefore, it should not be accepted in its current form.",21,371,20.61111111111111,5.448863636363637,181,1,370,0.0027027027027027,0.0367454068241469,-0.5809,104,45,79,18,11,7,"{'ABS': 0, 'INT': 1, 'RWK': 19, 'PDI': 7, 'DAT': 1, 'MET': 11, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 4, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 4, 'CMP': 4, 'PNF': 0, 'REC': 2, 'EMP': 13, 'SUB': 2, 'CLA': 1}",0,1,19,7,1,11,4,2,0,1,1,0,4,1,0,2,4,4,0,2,13,2,1,0.7903722565485259,0.7859084491778118,0.6150275648218904
ICLR2018-HkOhuyA6--R2,Reject,"The paper introduces a method for learning graph representations (i.e., vector representations for graphs). An existing node embedding method is used to learn vector representations for the nodes . The node embeddings are then projected into a 2-dimensional space by PCA. The 2-dimensional space is binned using an imposed grid structure. The value for a bin is the (normalized) number of nodes falling into the corresponding region. The idea is simple and easily explained in a few minutes . That is an advantage. Also, the experimental results look quite promising . It seems that the methods outperforms existing methods for learning graph representations .   The problem with the approach is that it is very ad-ho c. There are several (existing) ideas of how to combine node representations into a representation for the entire graph . For instance, averaging the node embeddings is something that has shown promising results in previous work . Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing , it is especially important to compare your method more thoroughly to simpler methods. Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives.  The experimental results are also not explained thoroughly enough. For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization) , you will have to run node2vec several times to reduce the variance of your resulting discretized density maps. How many times did you run node2vec on each graph?  ",19,254,14.941176470588236,5.425101214574899,133,1,253,0.0039525691699604,0.0074074074074074,0.7956,70,29,53,17,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 11, 'PDI': 4, 'DAT': 1, 'MET': 11, 'EXP': 8, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,2,11,4,1,11,8,4,0,0,0,0,0,0,0,0,5,1,0,0,11,1,1,0.5042806948966583,0.5619672764977769,0.3230131118786038
ICLR2018-HkOhuyA6--R3,Reject,"The paper presents a novel representation of graphs as multi-channel image-like structures . These structures are extrapolated  by  1) mapping the graph nodes into an embedding using an algorithm like node2vec 2) compressing the embedding space using pca 3) and extracting 2D slices from the compressed space and computing 2D histograms per slice. he resulting multi-channel image-like structures are then feed into vanilla 2D CNN. The papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures . Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures. The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.",6,126,21.0,5.905172413793103,77,0,126,0.0,0.0076923076923076,0.9001,34,17,21,3,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 2, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,3,2,2,4,1,0,0,0,0,0,0,0,0,1,1,1,0,0,4,0,1,0.4295783713192998,0.5574214278600844,0.27383707589241685
ICLR2018-HkPCrEZ0Z-R1,Reject,"The paper studies a combination of model-based and model-free RL. The idea is to train a forward predictive model which provides multi-step estimates to facilitate model-free policy learning. Some parts of the paper lack clarity and the empirical results need improvement to support the claims (see details below).  Clarity  - The main idea of the proposed method is clear. - Some notations and equations are broken. For example:  (1) The definition of bar{A} in Section 4 is broken. (2) The overall objective in Section 5 is broken. (3) The computation of w in Algorithm 2 is problematic. - Some details of the experiments/methods are confusing. For example:  (1) The step number k is dynamically determined by a short line search as in Section 4 ``Dynamic Rollout'', but later in the experiments (Section 6) the value of k is set to be 2 uniformly. (2) Only the policy and value networks specified. The forward models are not specified. (3) In algorithm 1, what exact method is used in determining if mu is converged or not? Originality The proposed method can be viewed as a multi-step version of the stochastic value gradient algorithm. An empirical comparison could be helpful but not provided. The idea of the proposed method is related to the classic Dyna methods from Sutton. A discussion on the difference would be helpful. Significance - The paper could compare against other relevant baselines that combine model-based and model-free RL methods, such as SVG (stochastic value gradient). - To make a fair comparison, the results in Table 1 should consider the amount of data used in pre-training the forward models. Current results in Table 1 only compare the amount of data in policy learning. - Figure 3 is plotted for just one random starting state. The Figure could have been more informative if it was averaged over different starting states. The same issue is found in Figure 2. It would be helpful if the plots of other domains are provided. - In Figure 2, even though the diff norm fluctuates, the cosine similarity remains almost constant. Does it suggest the cosine similarity is not effective in measuring the state similarity? - Figure 1, 4 and 5 need confidence intervals or standard errors. Pros: - The research direction in combining model-based and model-free RL is interesting. - The main idea of the proposed method is clear. Cons: - Parts of the paper are unclear and some details are missing. - The paper needs more discussion and comparison to relevant baseline methods. - The empirical results need improvement to support the paper's claims.",32,417,13.9,5.214470284237726,179,0,417,0.0,0.02073732718894,0.9907,128,44,77,14,12,6,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 10, 'DAT': 2, 'MET': 12, 'EXP': 6, 'RES': 5, 'TNF': 7, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 6, 'CLA': 4}",0,1,9,10,2,12,6,5,7,0,1,2,1,1,0,0,4,4,1,0,9,6,4,0.8619476562118271,0.6729147115164936,0.608728240680118
ICLR2018-HkPCrEZ0Z-R2,Reject,"The main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts, and reduce the variance of those control variates using the reparameterization trick. This is laid out primarily in Equations 1-5, and seems like a nice idea, although I must admit I had some trouble following the maths in Equation 5. They include results showing that their method has better sample efficiency than TRPO (which their method also uses under the hood to update value function parameters). My main issue with this paper is that the empirical section is a bit weak, for instance only one run seems to be shown for both methods, there is no mention of hyper-parameter selection, and the measure used for generating Table 1 seems pretty arbitrary to me (how were those thresholds chosen?). In addition, one thing I would have liked to get out of this paper is a better understanding of how much each component helps. This could have been done via empirical work, for instance: - Explore the effect of the planning horizon, and implicitly compare to SVG(1), which as the authors point out is the same as their method with a horizon of 1. - Show the effect of the reparameterization trick on estimator variance. - Compare the bias and variance of TRPO estimates vs the proposed method.",7,223,27.875,4.971698113207547,129,3,220,0.0136363636363636,0.0398230088495575,0.9565,67,14,44,7,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,0,2,3,0,4,3,1,0,0,0,2,0,0,0,0,2,1,0,0,6,0,1,0.429738902930346,0.4476022682671072,0.23768757023833603
ICLR2018-HkPCrEZ0Z-R3,Reject,"This paper presents a model-based approach to variance reduction in policy gradient methods. The basic idea is to use a multi-step dynamics model as a baseline (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased. The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks. This paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission. But right now (at least as of the version I'm reviewing), the paper reads as being half-finished.  Several terms are introduced without being properly defined, and one of the key formalisms presented in the paper (the idea of embedding an imaginary trajectory remains completely opaque to me. Further, the paper seems to simply leave out some portions: the introduction claims that one of the contributions is we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates, but I see literally no section that hints at anything like this (no mention of dynamic unfolding or latent space trajectory embedding ever occurs later in the paper). In a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term. But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick. But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about. The paper frequently refers to embedding imaginary trajectories into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me). I also don't really understand why something like this would be needed given the understanding above, but it's likely I'm just missing something here. But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper. Finally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently. The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach. And the experiments are described with very little detail or discussion about the experimental setup. Nor are either of these issues simply due to space constraints: the paper is 2 pages under the soft ICLR limit, with no appendix.  Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking. My honest impression is simply that this is still work in progress and that the write up was done rather hastily. I think it will eventually become a good paper, but it is not ready yet.",20,614,30.7,5.035532994923858,284,9,605,0.0148760330578512,0.0357142857142857,0.994,148,71,101,54,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 8, 'DAT': 0, 'MET': 6, 'EXP': 6, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 7, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 9, 'CLA': 3}",0,1,9,8,0,6,6,1,0,0,0,7,1,1,0,0,1,0,1,0,9,9,3,0.6458705974849751,0.5615468972425792,0.40199183672343564
ICLR2018-HkSZyinVG-R1,,"The authors propose using piecewise linear activation functions with contraints to make it continous. They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article. Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron. Given a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required. Authors present no clear explanation on why the shift should result in improved performance.",5,101,20.2,5.32,75,0,101,0.0,0.0588235294117647,0.5293,34,12,18,0,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,4,3,4,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0.3582329307457568,0.1111111111111111,0.13886066232750233
ICLR2018-HkSZyinVG-R2,,"The paper proposes a piecewise linear activation function that is build on ELU. In general it was an OK paper and there are many to be improved.  + Novelty seems minor. In my sense, the authors do not provide any evidence theoretically or analysis on why the shifted version of ELU (which does not pass the origin) is more favorable. The idea proposed in the paper is just a stack of better experiments. Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape? Section 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated. In  section 3, the shifted version, delta, is abruptly proposed only based on results presented in 4.1 could improve learning. This is not a professional ML paper looks like. + Experiment not strong to support the idea. Experiments are only conducted in CIFAR100. This is obviously not enough. In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins). And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%. Also, the paper needs to be re-organized in a better way (eg., state clearly the difference from previous methods, how to formulate the story, etc.) For now, I don't think it is ready to ICLR. ",14,254,15.875,4.686192468619247,140,4,250,0.016,0.0386100386100386,0.9808,63,28,51,22,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 5, 'EXP': 5, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 0}",0,1,3,2,1,5,5,1,1,1,0,3,0,0,1,1,0,3,1,0,6,3,0,0.7159423791904125,0.6702241319719043,0.49532483570705416
ICLR2018-HkSZyinVG-R3,,"This is an emergency review, as the replacement of an overdue review. ------------------------------------------------------------------------  This paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit. This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor. However, the motivation above is not justified well. No theoretic results are present to support this design. Figure 4 shows the only experimental results to ""support"" the motivation. However, it is a bit weird that 1) 100% tuned results are not shown, and 2) the learned activation goes up as the input goes negative, which is not the shape of ELU. As a result, the motivation does not seem clear. The shift variables seem only useful when they are not shared for different pixels. Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used). The question is if it is worth adding so many pixel-wise parameters. Moreover, the proposed formulation does not seem useful for the fully connected layer at any time. The experiments are limited. Only the basic LeNet and another network are considered on Cifar-100. The results are not as good as the state-of-the-art. More importantly, the proposed activation functions reduce the errors only a bit (<0.5%). Stronger results on more datasets are necessary to justify the usefulness of the proposed method. ",17,247,13.72222222222222,5.076271186440678,130,1,246,0.0040650406504065,0.044,0.5822,63,32,48,20,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 2, 'MET': 5, 'EXP': 1, 'RES': 6, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 0}",0,1,1,3,2,5,1,6,1,0,0,0,0,0,0,0,0,1,1,0,8,3,0,0.5728625398108623,0.4490114995049882,0.3159731658617461
ICLR2018-HkTEFfZRb-R1,Accept,"This paper starts by gently going over the concept of adversarial attacks on neural networks (black box vs white box, reactive vs proactive, transfer of attacks, linearity hypothesis), as well as low-precision nets and their deployement advantages. Adversarial examples are introduced as a norm-measurable deviation from natural inputs to a system. We are reminded of adversarial training, and of the fact that binarized nets are highly non linear due to the nature of their weights and activations. This paper then proposes to examine the robustness of binarized neural networks to adversarial attacks on MNIST and CIFAR-10. The quantization scheme used here is v32 Conv2D -> ReLU -> BNorm -> sign -> bit Conv2D -> ReLU -> Scalar -> BNorm -> sign, but sign is really done with the stochastic quantization method of Courbariaux et al, even at test time (in order to make it more robust). What the experimental results show: - High capacity BNNs are usually more robust to white-box attacks than normal networks, probably because the gradient information that an adversary would use becomes very poor as training progresses. - BNNs are harder to properly train with adversarial examples because of the polarized weight distribution that they induce - Against black-box attacks, it seems there is little difference between NNs and BNNs. Some comments and questions: - In figure 1 I'm not sure what Scalar refers to, and it is not explained in the paper (nor could I find it explained in Papernot et al 2017a). - Do you adopt the Shift based Batch Normalizing Transform of Courbariaux et al? If not, why? - It might be worth at least _quickly_ explaining what the 'Carlini-Wagner L2 from CleverHans' is rather than simply offering a citation with no explanation. Idem for 'smooth substitute model black-box misclassificiation attack'. We often assume our readers know most of what we know, but I find this is often not the case and can discourage the many newcomers of our field. - Running these attacks to 1000 iterations [...], therefore we believe this targeted attack represents a fairly substantial level of effort on behalf of the adversary.  while true for us researchers, computational difficulty will not be a criterion to stop for state actor or multinational tech companies, unless it can be proven that e.g. the number of iterations needs to grow exponentially (or in some other unreasonable way) in order to get reliable attacks. - MLP with binary units 3 better, 'as in Fig.' is missing before '3', or something of the sort. - You say We postpone a formal explanation of this outlier for the discussion.  but really you're explaining it in the next paragraph (unless there's also another explanation I'm missing). Training BNNs with adversarial examples is hard. - You compare stochastic BNNs with deterministic NNs, but not with stochastic NNs. What do you think would happen? Some of arguments that you make in favour of BNNs could also maybe be applied to stochastic NNs.  My opinions on this paper: - Novelty: it certainly seems to be the first time someone has tackled BNNs and adversarial examples - Relevance: BNNs can be a huge deal when deploying applications, it makes sense to study their vulnerabilities - Ease of understanding: To me the paper was mostly easy to understand, yet, considering there is no page limit in this conference, I would have buffed up the appendix, e.g. to include more details about the attacks used and how various hyperparameters affect things. - Clarity: I feel like some details are lacking that would hinder reproducing and extending the work presented here. Mostly, it isn't always clear why the chosen prodecures and hyperparameters were chosen (wrt the model being a BNN) - Method: I'm concerned by the use of MNIST to study such a problem. MNIST is almost linearly separable, has few examples, and given the current computational landscape, much better alternatives are available (SVHN for example if you wish to stay in the digits domain). Concerning black-box attacks, it seems that BNNs less beneficial in a way; trying more types of attacks and/or delving a bit deeping into that would have been nice. The CIFAR-10 results are barely discussed. Overall I think this paper is interesting and relevant to ICLR. It could have stronger results both in terms of the datasets used and the variety of attacks tested, as well as some more details concerning how to perform adversarial training with BNNs (or why that's not a good idea).",31,722,24.89655172413793,5.081661891117479,360,12,710,0.0169014084507042,0.0293724966622162,-0.9893,198,93,126,46,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 6, 'MET': 9, 'EXP': 14, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 22, 'SUB': 1, 'CLA': 0}",0,0,0,3,6,9,14,0,1,0,0,2,0,0,1,0,0,1,0,0,22,1,0,0.4320379830051482,0.4575055505761469,0.24387274733038064
ICLR2018-HkTEFfZRb-R2,Accept,"1) Summary This paper proposes a study on the robustness of one low-precision neural networks class - binarized neural networks (BNN) - against adversarial attacks. Specifically, the authors show that these low precision networks are not just efficient in terms of memory consumption and forward computation, but also more immune to adversarial attacks than their high-precision counterparts. In experiments, they show the advantage of BNNs by conducting experiments based on black-box and white-box adversarial attacks without the need to artificially mask gradients. 2) Pros: + Introduced, studied, and supported the novel idea that BNNs are robust to adversarial attacks. + Showed that BNNs are robust to the Fast Gradient Sign Method (FGSM) and Carlini-Wagner attacks in white-box adversarial attacks by presenting evidence that BNNs either outperform or perform similar to the high-precision baseline against the attacks. + Insightful analysis and discussion of the advantages of using BNNs against adversarial attacks. 3) Cons: Missing full-precision model trained with PGD in section 3.2: The authors mention that the full-precision model would also likely improve with PGD training, but do not have the numbers. It would be useful to have such numbers to make a better evaluation of the BNN performance in the black-box attack setting. Additional comments: Can the authors provide additional analysis on why BNNs perform worse than full-precision networks against black-box adversarial attacks? This could be insightful information that this paper could provide if possible. 4) Conclusion: Overall, this paper proposes great insightful information about BNNs that shows the additional benefit of using them besides less memory consumption and efficient computation. This paper shows that the used architecture for BBNs makes them less susceptible to known white-box adversarial attack techniques. ",13,275,22.916666666666668,5.843283582089552,140,1,274,0.0036496350364963,0.0142348754448398,-0.982,90,47,35,8,4,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 7, 'RES': 0, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 0, 'CLA': 0}",0,0,0,2,0,2,7,0,0,2,0,0,0,0,0,0,0,0,0,0,11,0,0,0.2865656718207945,0.1173306854595408,0.11687015601831639
ICLR2018-HkTEFfZRb-R3,Accept,"his work presents an empirical study demonstrating that binarized networks are more robust to adversarial examples. The authors follow the stochastic binarization procedure proposed by Courbariaux et al. The robustness is tested with various attacks such as the fast gradient sign method and the projected gradient method on MNIST and CIFAR. The experimental results validate the main claims of the paper on some datasets. While reducing the precision can intuitively improve the robustness, It remains unclear if this method would work on higher dimensional inputs such as Imagenet. Indeed:   (1) state of the art architectures on Imagenet such as Residual networks are known to be very fragile to precision reduction. Therefore, reducing the precision can also reduce the robustness as it is positively correlated with accuracy. (2) Compressing reduces the size of the hypothesis space explored. Therefore, larger models may be needed to make this method work for higher dimensional inputs. The paper is well written overall and the main idea is simple and elegant.  I am less convinced by the experiments.  ",11,172,14.333333333333334,5.396449704142012,107,1,171,0.0058479532163742,0.0225988700564971,0.8869,48,25,32,9,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 3, 'MET': 3, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,0,2,3,3,3,2,2,0,0,0,1,0,0,0,0,0,1,0,0,5,0,1,0.5009135510178699,0.3358211630727052,0.2520571446664346
ICLR2018-HkUR_y-RZ-R1,Accept,"This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN. Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model. The underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique. I am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset. As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset. For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component. This reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains. Overall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights. (extremely minor typo: One popular possibility from L2S is go the full reduction route down to binary classification)",9,240,26.666666666666668,5.291845493562231,147,1,239,0.00418410041841,0.0207468879668049,0.9732,69,28,46,12,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 5, 'EXP': 1, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 1}",0,1,0,2,2,5,1,4,0,1,0,0,0,0,0,1,0,0,0,0,4,1,1,0.5012632647350163,0.4463103167489733,0.28222224696977927
ICLR2018-HkUR_y-RZ-R2,Accept,"This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text. In order to do so, they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore. Pros: - Good literature review. But the future work on bandits is already happening: Paper accepted at ACL 2017: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler. Cons: - The key argument of the paper is that SEARNN is a better IL-inspired algorithm than the previously proposed ones. However there is no direct comparison either theoretical or empirical against them. In the examples on spelling using the dataset of Bahdanau et al. 2017, no comparison is made against their actor-critic method. Furthermore, given its simplicity, I would expect a comparison against scheduled sampling. - A lot of important experimental details are in the appendices and they differ among experiments. For example, while mixed rollins are used in most experiments, reference rollins are used in MT, which is odd since it is a bad option theoretically. Also,  no details are given on how the mixing in the rollouts was tuned. Finally, in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work, this is not the case eventually, as it is acknowledged at least in the case of MIXER. I would have expected the same encoder-decoder architecture to have been used for all the methods considered. - the two losses introduced are not really new. The log-loss is just MLE, only assuming that instead of a fixed expert that always returns the same target, we have a dynamic one. Note that the notion of dynamic expert is present in the SEARN paper too. Goldberg and Nivre just adapted it to transition-based dependency parsing. Similarly, since the KL loss is the same as XENT, why give it a new name? - the top-k sampling method is essentially the same as the targeted exploration of Goodman et al. (2016) which the authors cite. Thus it is not a novel contribution. - Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7. They both could be mini-batched similarly. Also, not sure I see why SEARNN can be used on any task, in comparison to other methods. They all seem to be equally capable. Minor comments: - Figure 1: what is the difference between cost-sensitive loss and just loss? - local vs sequence-level losses: the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise (BLEU/ROUGE) do not decompose over the the predictions of the RNNs. - Can't see why SEARNN can help with the vanishing gradient problem. Seem to be rather orthogonal. ",29,461,15.366666666666667,4.961711711711712,244,1,460,0.0021739130434782,0.0211416490486257,-0.971,125,51,81,31,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 0, 'DAT': 1, 'MET': 18, 'EXP': 3, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 2, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 11, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 3, 'CLA': 0}",0,1,7,0,1,18,3,3,1,0,2,0,0,0,0,2,2,11,1,0,9,3,0,0.5764526911476633,0.6731204489669191,0.40702396837726873
ICLR2018-HkUR_y-RZ-R3,Accept,"The paper proposes new RNN training method based on the SEARN learning to search (L2S) algorithm and named as SeaRnn. It proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by L2S. It can consider different classifiers and loss functions, and a sampling strategy for making the optimization problem scalable is proposed. SeaRnn improves the results obtained by MLE training in three different problems, including a large-vocabulary machine translation. In summary, a very nice paper. Quality: SeaRnn is a well rooted and successful application of the L2S strategy to the RNN training that combines at the same time global optimization and scalable complexity. Clarity: The paper is well structured and written, with a nice and well-founded literature review. Originality: the paper presents a new algorithm for training RNN based on the L2S methodology, and it has been proven to be competitive in both toy and real-world problems. Significance: although the application of L2S to RNN training is not new,",9,167,18.55555555555556,5.41875,91,0,167,0.0,0.0119760479041916,0.9286,54,21,28,4,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,1,2,0,6,3,2,0,0,0,2,0,0,0,2,1,0,0,0,2,0,1,0.430155201286385,0.4451115094638055,0.24402764995253715
ICLR2018-HkZy-bW0--R1,Accept,"The principal problem that the paper addresses is how to integrate error-backpropagation learning in a network of spiking neurons that use a form of sigma-delta coding. The main observation is that static sigma-delta coding as proposed in OConnor and Welling (2016b), is not correct when the weights change during training, as past activations are taken into account with the old rather than the new weights. The solution proposed in this work is to have past activations decay exponentially, to reduce this problem. The coding scheme then mimics the proporitional-integral-derivative idea from control-theory. The result, spikes having an exponentially decaying effect on the postsynaptic neuron, is similar to that observed in biological spiking neurons. The authors show how spike-based learning can be implemented with spiking neurons using such coding, and demonstrate the results on an MLP with one hidden layer applied to the temporal MNIST dataset, and to the Youtube-BB dataset. This approach is original and significant, though the presented results are a bit on the thin side. As presented, the spiking networks are not exactly deep: I am puzzled by the statement that in the youtube-bb dataset only the top 3 layers are spiking. The network for the MNIST dataset is similarly only 3 layers deep (input, hidden, output). Is there a particular reason for this? The presentation right now suggests that the scheme does in practise not work for deep networks... With regard to the learning rule: while the rule is formulated in terms of spikes, it should be noted that for neuron with many inputs and outputs, this update will have to be computed very very often, even for networks with low average firing rates. The paper is clear in most points, with some parts that could use further elucidation. In particular, in Sec 2.5 the feedback pass for weight updating is computed. It is unclear from the text that this is an ongoing process, in parallel to the feedforward pass. In Sec 2.6 e_t is termed the postsynaptic (pre-nonlinearity) activation, which is confusing as the computation is going the other way (post-to-pre). These two sections would benefit from a more careful layout of the process, what is going on in a forward pass, a backward pass, how does this interact. Section 2.7 tries to relate the spike-based learning rule to the biologically observed STDP phenomenon. While the formulation in terms of pre-post spike-times is interesting, the result is clearly different from STDP, and ignores the fact that e_t refers to the backpropagating error (which presumably would be conveyed by a feedback network): applying the plotted pre-post spike-time rule in the same setting as where STDP is observed will not achieve error-backpropagation. The shorthand notation in the paper is hard to follow in the first place btw, perhaps this could be elaborated/remedied in an appendix, there is also some rather colloquial writing in places:obscene wast of energy (abstract), There's aren't (2.6, p5). The correspondence of spiking neurons to sigma-delta modulation is incorrectly attributed to Zambrano and Bohte (2016), but is rather presented in Yoon (2017/2016, check original date of publication!).   ",22,512,24.38095238095238,5.194726166328601,242,4,508,0.0078740157480314,0.0174757281553398,0.4359,140,60,90,28,8,4,"{'ABS': 1, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 14, 'EXP': 0, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 2}",1,1,2,1,2,14,0,6,0,0,0,2,0,0,0,0,0,1,1,0,8,0,2,0.5751215758216474,0.4488792815591992,0.32570270194711076
ICLR2018-HkZy-bW0--R2,Accept,"This paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This approach extends the method presented on Arxiv on Sigma delta quantized networks (Peter O'Connor and Max Welling. Sigma delta quantized networks. arXiv preprint arXiv:1611.02024, 2016b.). Overall, the paper is interesting and promising; only a few works tackle the problem of learning with spikes showing the potential advantages of such form of computing. The paper, however, is not flawless. The authors demonstrate the method on just two datasets, and effectively they show results of training only for Feed-Forward Neural Nets (the authors claim that ""the entire spiking network end-to-end works"" referring to their pre-trained VGG19, but this paper presents only training for the three top layers). Furthermore, even if suitable datasets are not available, the authors could have chosen to train different architectures. The first dataset is the well-known benchmark MNIST also presented in a customized Temporal-MNIST. Although it is a common base-line, some choices are not clear: why using a FFNN instead that a CNN which performs better on this dataset; how data is presented in terms of temporal series u2013 this applies to the Temporal MNIST too; why performances for Temporal MNIST u2013 which should be a more suitable dataset u2014 are worse than for the standard MNIST; what is the meaning of the right column of Figure 5 since it's just a linear combination of the GOps results. For the second dataset, some points are not clear too: why the labels and the pictures seem not to match (in appendix E); why there are more training iterations with spikes w.r.t. the not-spiking case. Overall, the paper is mathematically sound, except for the ""future updates"" meaning which probably deserves a clearer explanation. Moreover, I don't see why the learning rule equations (14-15) are described in the appendix, while they are referred constantly in the main text. The final impression is that the problem of the dynamical range of the hidden layer activations is not fully resolved by the empirical solution described in Appendix D: perhaps this problem affects CCNs more than FFN. Finally, there are some minor issues here and there (the authors show quite some lack of attention for just 7 pages): -tTwo times ""get"" in ""we get get a decoding scheme"" in the introduction; -tTwo times ""update"" in ""our true update update as"" in Sec. 2.6; -tPag3 correct the capital S in 2.3.1 n-tPag4 Figure 1 increase font size (also for Figure2); close bracket after Equation 3; N (number of spikes) is not defined -tPag5 ""one-hot"" or ""onehot""; -tin the inline equation the sum goes from n 1 to S, while in eq.(8) it goes from n 1 to N; -tEq(10)(11)(12) and some lines have a typo (a cdot) just before some of the ws; -tPag6 k_{beta} is not defined in the main text; -tPag7 there are two ""so that"" in 3.1; capital letter ""It used 32x10^12..""; beside, here, why do not report the difference in computation w.r.t. not-spiking nets? -tPag7 in 3.2 ""discussed in 1"" is section 1? -tPag14 Appendix E, why the labels don't match the pictures; -tPag14 Appendix F, explain better the architecture used for this experiment.",27,540,30.0,4.997995991983968,252,2,538,0.0037174721189591,0.0296296296296296,0.893,153,64,88,41,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 5, 'MET': 11, 'EXP': 6, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 5, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 2}",0,1,1,0,5,11,6,0,2,0,0,2,0,0,0,1,0,0,5,0,11,1,2,0.5031459798247212,0.5621025406700526,0.31524599084286864
ICLR2018-HkZy-bW0--R3,Accept,"This paper applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network. Whereas neither of these components are new, to my knowledge, nobody has combined all three of them previously. The paper is generally clearly written and represents a valuable contribution. The authors may want to consider the following comments:  1. I did not really understand the analogy with STDP in neuroscience because it relies on the assumption that spiking of the post-synaptic neuron encodes the backpropagating error signal. I am not aware of any evidence for this. Given that the authors' algorithm does not reproduce the sign-flip in the STDP rule I would suggest revise the corresponding part of the paper. Certainly, the claim in the Discussion ""show these to be equivalent to a form of STDP u2013 a learning rule first observed in neuroscience."" is inappropriate. 2.  If the authors' encoding scheme really works I feel that they could beef up their experimental results to demonstrate its unqualified advantage. 3. The paper could benefit greatly from better integration with the existing literature. a. Sigma-Delta model of spiking neurons has a long history in neuroscience starting with the work of Shin. Please note that these papers are much older than the ones you cite:  Shin, J., Adaptive noise shaping neural spike encoding and decoding. Neurocomputing, 2001. 38-40: p. 369-381.  Shin, J.,The noise shaping neural coding hypothesis: a brief history and physiological implications. Neurocomputing, 2002. 44: p. 167-175. Shin, J.H., Adaptation in spiking neurons based on the noise shaping neural coding hypothesis. Neural Networks, 2001. 14(6-7): p. 907-919. More recently, the noise-shaping hypothesis has been tested with physiological data: Chklovskii, D. B., & Soudry, D. (2012). Neuronal spike generation mechanism as an oversampling, noise-shaping a-to-d converter. In Advances in Neural Information Processing Systems (pp. 503-511). (see Figure 5A for the circuit implementing a Predictive Sigma-Delta encoder discussed by you)  b. It is more appropriate to refer to encoding a combination of the current value and the increment as a version of predictive coding in signal processing rather than the proportional derivative scheme in control theory because the objective here is encoding, not control. Also, predictive coding has been commonly used in neuroscience: Srinivasan MV, Laughlin SB, Dubs A (1982) Predictive coding: a fresh view of inhibition in the retina. Proc R Soc Lond B Biol Sci 216: 427u2013459. pmid:6129637 Using leaky neurons for encoding and decoding is standard, see e.g.: Bharioke, Arjun, and Dmitri B. Chklovskii. Automatic adaptation to fast input changes in a time-invariant neural circuit.  PLoS computational biology 11.8 (2015): e1004315. For the application of these ideas to spiking neurons including learning please see a recent paper: Denu00e8ve, Sophie, Alireza Alemi, and Ralph Bourdoukan. The brain as an efficient and robust adaptive learner. Neuron 94.5 (2017): 969-977. Minor: Penultimate paragraph of the introduction section: ""get get"" -> get First paragraph of the experiments section: ""so that so that"" -> so that ",24,490,11.136363636363637,5.613793103448276,243,1,489,0.0020449897750511,0.024,0.9719,155,70,78,21,8,2,"{'ABS': 0, 'INT': 2, 'RWK': 10, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 4, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,2,10,0,0,3,4,1,0,0,0,1,4,2,0,0,0,0,0,0,7,0,1,0.572983960164707,0.22595396683128,0.2579618460406687
ICLR2018-Hk__kGbCW-R1,Reject,"The article proposes to use dense skip-connections on the vertical (between-layers) connections of recurrent networks. Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to. The experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks. I am not convinced by the presented results for the following reasons: 1) the paper introduces two concepts - the dense skip-connections and the multi-head attention. Experiments only show their joint impact, yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attention? 2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines. What happens for deeper and narrower baselines that have a similar number of parameters? 3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the jumps in the training curves as signs of learning rate anneal). Maybe this is what helps? I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated? Quality: Good  Clarity: The paper is clearly written. Originality: The addition of dense connections to recurrent networks is trivial. Pros&cons + the proposed additions (dense skip connections) and multi-head attentions yield performance improvements - the impact of the two contributions is not disentangled in the paper - the two contributions are fairly obvious",13,258,28.666666666666668,5.569721115537849,137,5,253,0.0197628458498023,0.0416666666666666,0.9684,81,23,51,14,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 2, 'DAT': 0, 'MET': 9, 'EXP': 8, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 2, 'CLA': 1}",0,0,7,2,0,9,8,2,0,1,0,1,0,0,0,1,1,0,0,0,7,2,1,0.5032028366211072,0.5593939766729349,0.3106667059733004
ICLR2018-Hk__kGbCW-R2,Reject,"This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer.  In addition, attention context is computed for each layer, then, combined together as a single context. Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters. Motivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable. However I have some concerns to this paper. - It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from. I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms. - It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers. Do you have experiments by fixing the number of layers and varying the hidden size? Other comment:  - Section 3: sequence-to sequence -> sequence-to-sequence  - It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem?",10,209,20.9,5.45273631840796,117,0,209,0.0,0.0368663594470046,0.7276,64,28,30,13,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 6, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 4}",0,0,3,3,0,7,6,0,0,1,0,1,0,1,0,0,1,1,0,0,5,0,4,0.5022117071697308,0.4471756793963785,0.2784757767897118
ICLR2018-Hk__kGbCW-R3,Reject,"This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se). Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs. The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise. Overall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak. The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?) for which the reason could be a multitude of issues probably related to hyper-parameter tuning. What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation? Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2. Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos.",9,253,36.142857142857146,5.0,139,4,249,0.0160642570281124,0.0354330708661417,0.894,60,29,41,22,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 5, 'EXP': 4, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,1,4,2,0,5,4,3,1,1,0,2,0,0,0,1,0,0,0,0,5,1,1,0.6445740848498848,0.4469322741838163,0.3578410768518762
ICLR2018-HkanP0lRW-R1,Reject,"Authors propose a greedy scheme to select a subset of (highly correlated) spectral features in a classification task. The selection criterion used is the average magnitude with which this feature contributes to the activation of a next-layer perceptron. Once validation accuracy drops too much, the pruned network is retrained, etc. Pro:  - Method works well on a single data set and solves the problem - Paper is clearly written  - Good use of standard tricks Con:  - Little novelty This paper could be a good fit for an applied conference such as the International Symposium on Biomedical Imaging.  ",7,94,18.8,5.431818181818182,74,0,94,0.0,0.0194174757281553,0.8271,30,15,14,5,4,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,0,3,1,2,0,0,0,0,0,3,0,0,1,1,0,0,0,0,2,0,1,0.286173915167883,0.4450664018792874,0.16113598682599709
ICLR2018-HkanP0lRW-R2,Reject,"This paper explores the use of neural networks for classification and segmentation of hypersepctral imaging (HSI) of cells.  The basic set-up of the method and results seem correct and will be useful to the specific application described. While the narrow scope might limit this work's significance, my main issue with the paper is that while the authors describe prior art in terms of HSI for biological preps, there is a very rich literature addressing HSI images in other domains: in particular for remote sensing. I think that this work can (1) be a lot clearer as to the novelty and (2) have a much bigger impact if this literature is addressed. In particular, there are a number of methods of supervised and unsupervised feature extraction used for classification purposes (e.g. endmember extraction or dictionary learning). It would be great to know how the features extracted from the neural network compare to these methods, as well as how the classification performance compares to typical methods, such as performing SVM classification in the feature space. The comparison with the random forests is nice, but that is not a standard method. Putting the presented work in context for these other methods would help make there results more general, and hopefully increase the applicability to more general HSI data (thus increasing significance).  An additional place where this comparison to the larger HSI literature would be useful is in the section where the authors describe the use of the network weights to isolate sub-bands that are more informative than others. Given the high correlation in many spectra, typically something like random sampling might be sufficient (as in compressive sensing). This type of compression which can be applied at the sensor -- a benefit the authors mention of their band-wise sub-sampling. It would be good to acknowledge this prior work and to understand if the features from the network are superior to the random sampling scheme. For these comments, I suggest the authors look at the following papers (and especially the references therein): [1] Li, Chengbo, et al. A compressive sensing and unmixing scheme for hyperspectral data processing. IEEE Transactions on Image Processing 21.3 (2012): 1200-1210. [2] Bioucas-Dias, Josu00e9 M., et al. Hyperspectral unmixing overview: Geometrical, statistical, and sparse regression-based approaches. IEEE journal of selected topics in applied earth observations and remote sensing 5.2 (2012): 354-379. [3] Charles, Adam S., Bruno A. Olshausen, and Christopher J. Rozell. Learning sparse codes for hyperspectral imagery. IEEE Journal of Selected Topics in Signal Processing 5.5 (2011): 963-978.  ",14,417,16.68,5.425641025641026,207,3,414,0.0072463768115942,0.0189573459715639,0.991,133,65,50,12,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 6, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,2,3,0,9,0,1,0,0,0,1,0,0,0,1,1,6,0,0,3,0,0,0.3593434816792299,0.4462741721000803,0.2026733420861175
ICLR2018-HkanP0lRW-R3,Reject,"In this paper, the authors proposed a framework to classify cells and implement cell segmentation based on the deep learning  techniques. Using the classification results to guide the feature selection method, the method can achieve comparable performance even 90% of the input features are reduced. In general, the paper addresses an interesting problem, but the technical contribution is rather incremental. The authors seem to apply some well-define methods to realize a new task. The authors are expected to clarify their technical contributions or model improvement to address the specific problem. Moreover, there also exist some recent progress on image segmentation, such as FCN or mask R-CNN. The authors are expected to demonstrate the results by improving these advanced models. In general, this is an interesting paper, but would be more fit to MICCAI or ISBI.     ",9,135,15.0,5.409090909090909,88,0,135,0.0,0.0283687943262411,0.9558,40,16,27,4,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,0,3,0,6,0,1,0,0,0,2,0,0,1,1,0,0,0,0,5,0,0,0.2871292648149711,0.3358211630727052,0.14365073229125405
ICLR2018-HkbJTYyAb-R1,Reject,"In this paper, the authors propose a type of Normalizing Flows (Rezende and Mohamed, 2015) for Variational Autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) they call Convolutional Normalizing Flows. More particularly, it aims at extending on the Planar Flow scheme proposed in Rezende and Mohamed (2015). The authors notice an improvement through their method over Normalizing Flows, IWAE with diagonal gaussian approximation, and standard Variational Autoencoders. As noted by AnonReviewer3, several baselines are missing. But the authors partly address that issue in the comment section for the MNIST dataset. The requirement of h being bijective seems wrong. For example, if h was a rectifier nonlinearity in the zero-derivative regime, the Jacobian determinant of the ConvFlow would be 1. More importantly, the main issue is that this paper might need to highlight the fundamental difference between their proposed method and Inverse Autoregressive Flow (Kingma et al., 2016).  The proposed connectivity pattern proposed for the convolution in order to make the Jacobian determinant computation is exactly the same as Inverse Autoregressive Flow and the authors seems to be aware of the order dependence of their architecture which is every similar to autoregressive models.  This presentation of the paper can be misleading concerning the true innovation in the model trained. Proposing ConvFlow as a type of Inverse Autoregressive Flow would be more accurate and would allow to highlight better the innovation of the work. Since this work does not offer additional significant insight over Inverse Autoregressive Flow, its value should be on demonstrating the efficiency of the proposed method. MNIST and Omniglot seems insufficient for that purpose given currently published work. In the current state, I can't recommend the paper for acceptance. Danilo Jimenez Rezende, Shakir Mohamed: Variational Inference with Normalizing Flows. ICML 2015 Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra: Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models. ICML 2014 Diederik P. Kingma, Max Welling: Auto-Encoding Variational Bayes. ICLR 2014 Diederik P. Kingma, Tim Salimans, Rafal Ju00f3zefowicz, Xi Chen, Ilya Sutskever, Max Welling: Improving Variational Autoencoders with Inverse Autoregressive Flow. NIPS 2016",17,344,16.38095238095238,5.846625766871166,170,4,340,0.0117647058823529,0.0202312138728323,0.9896,113,57,50,7,6,7,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 4, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,0,1,3,4,5,0,0,0,0,0,2,3,0,0,1,1,2,1,1,5,1,0,0.4298692737876842,0.7803827700743396,0.33819839955874065
ICLR2018-HkbJTYyAb-R2,Reject,"The paper proposes to increase the expressivity of the variational approximation in VAEs using a new convolutional parameterization of normalizing flows. Starting from the planar flow proposed in Rezende & Mohammed 2015 using a vector inner product followed by a nonliniarity+element-wise scaling the authors suggests to replace inner product with a shifted 1-D convolution. This reduces the number of parameters used from 2*d to k + d and importantly still maintains the linear time computation of the determinant. This approach feels so straightforward that i'm surprised that it have not been tried before. The authors present results on a synthetic task as well as MNIST and OMNIGLOT. Please find some more detailed comments/questions below   Q1) I feel that section 3 could be more detailed about how the convolution normalizing flow relate to normalizing flow, inverse autoregressive flow and the masked-convolution used in real NVP? Especifically a) is it correct that convolutional normalizing flow trades global connectivity for more expressivity locally? b) Can convolutional flow be seen as faster but u00b4more restricted version of the LSTM implemented inverse autoregressive flow (full lower triangular jacobian vs k off diagonal elements per row in convolutional normalizing flow) Q2) I miss some more baselines in the experimental section. Did the authors compare the convolutional normalizing flow with e.g. Inverse Autoregressive flow or Auxiliary latent variables? Q3) Albeit the MNIST results seems convincing - and to a lesser degree the OMNIGLOT ones - I miss results on larger natural image benchmark datasets like cifar10 and ImageNet or preferably other modalities like text? Would it be possible to include results on any of these datasets? Overall i think the idea is nice and potentially useful due to the ease of implementation and speed of convolutional operations.  However I think the authors needs to 1) better describe how their method differs from prior work and 2) compare their method to more baselines for the experiments to fully convincing ",14,317,35.22222222222222,5.727891156462585,174,4,313,0.0127795527156549,0.0276923076923076,0.9836,95,57,47,15,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 3, 'MET': 7, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,1,4,3,7,2,2,0,0,0,0,0,0,0,0,0,1,0,0,10,0,0,0.4304645600039088,0.2278198391358089,0.1934448635653676
ICLR2018-HkbJTYyAb-R3,Reject,"The authors propose a new method for improving the flexibility of the encoder in VAEs, called ConvFlow. If I understand correctly (please correct me if not) the proposed method is a simplification of Inverse Autoregressive Flow as proposed by Kingma et al. Both of these methods use causal convolution to construct a normalizing flow with tractable Jacobian determinant. The difference is that Kingma et al. used 2d convolution (as well a fully connected architectures) where the authors of this paper propose to use 1d convolution.",3,85,17.0,5.5,55,0,85,0.0,0.0588235294117647,0.8316,26,7,15,5,1,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,1,0,0,0,0,2,0,0,0.0719289556561407,0.2228441796570651,0.031177589614084334
ICLR2018-Hkbd5xZRb-R1,Accept,"Summary:  The paper proposes a framework for constructing spherical convolutional networks (ConvNets) based on a novel synthesis of several existing concepts. The goal is to detect patterns in spherical signals irrespective of how they are rotated on the sphere. The key is to make the convolutional architecture rotation equivariant. Pros:  + novel/original proposal justified both theoretically and empirically + well written, easy to follow n+ limited evaluation on a classification and regression task is suggestive of the proposed approach's potential + efficient implementation Cons:  - related work, in particular the first paragraph, should compare and contrast with the closest extant work rather than merely list them - evaluation is limited; granted this is the nature of the target domain Presentation:  While the paper is generally written well, the paper appears to conflate the definition of the convolutional and correlation operators? This point should be clarified in a revised manuscript. In Section 5 (Experiments), there are several references to S^2CNN. This naming of the proposed approach should be made clear earlier in the manuscript. As an aside, this appears a little confusing since convolution is performed first on S^2 and then SO(3). Evaluation:  What are the timings of the forward/backward pass and space considerations for the Spherical ConvNets presented in the evaluation section? Please provide specific numbers for the various tasks presented. How many layers (parameters) are used in the baselines in Table 2?  If indeed there are much less parameters used in the proposed approach, this would strengthen the argument for the approach. On the other hand, was there an attempt to add additional layers to the proposed approach for the shape recognition experiment in Sec. 5.3 to improve performance? Minor Points:  - some references are missing their source, e.g., Maslen 1998 and Kostolec, Rockmore, 2007, and Ravanbakhsh, et al. 2016. - some sources for the references are presented inconsistency, e.g., Cohen and Welling, 2017 and Dieleman, et al. 2017  - some references include the first name of the authors, others use the initial - in references to et al. or not, appears inconsistent  - Eqns 4, 5, 6, and 8 require punctuation - Section 4 line 2, period missing before Since the FFT  - coulomb matrix --> Coulomb matrix - Figure 5, caption: The red dot correcpond to --> The red dot corresponds to   Final remarks:  Based on the novelty of the approach, and the sufficient evaluation, I recommend the paper be accepted.  ",26,389,24.3125,5.5,201,3,386,0.0077720207253886,0.0238663484486873,0.9752,119,42,66,17,7,7,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 12, 'EXP': 7, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 7, 'SUB': 5, 'CLA': 5}",0,0,1,4,0,12,7,0,2,0,0,5,3,0,0,1,0,1,2,1,7,5,5,0.503676946790034,0.782322337627341,0.3955950445063613
ICLR2018-Hkbd5xZRb-R2,Accept,"The focus of the paper is how to extend convolutional neural networks to have built-in spherical invariance. Such a requirement naturally emerges when working with omnidirectional vision (autonomous cars, drones, ...). To get invariance on the sphere (S^2), the idea is to consider the group of rotations on S^2 [SO(3)] and spherical convolution [Eq. (4)]. To be able to compute this convolution efficiently, a generalized Fourier theorem is useful. In order to achieve this goal, the authors adapt tools from non-Abelian [SO(3)] harmonic analysis. The validity of the idea is illustrated on 3D shape recognition and atomization energy prediction. The paper is nicely organized and clearly written; it fits to the focus of ICLR and can be applicable on many other domains as well. ",8,123,13.666666666666666,5.205128205128205,79,0,123,0.0,0.008,0.9578,42,14,21,7,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 0, 'CLA': 1}",0,0,0,4,0,3,0,0,0,1,0,2,0,0,1,0,0,0,0,0,0,0,1,0.2864379883354728,0.2222222222222222,0.13054843265270236
ICLR2018-Hkbd5xZRb-R3,Accept,"First off, this paper was a delight to read.  The authors develop an (actually) novel scheme for representing spherical data from the ground up, and test it on three wildly different empirical tasks: Spherical MNIST, 3D-object recognition, and atomization energies from molecular geometries. They achieve near state-of-the-art performance against other special-purpose networks that aren't nearly as general as their new framework.  The paper was also exceptionally clear and well written. The only con (which is more a suggestion than anything)--it would be nice if the authors compared the training time/# of parameters of their model versus the closest competitors for the latter two empirical examples. This can sometimes be an apples-to-oranges comparison, but it's nice to fully contextualize the comparative advantage of this new scheme over others.  That is, does it perform as well and train just as fast?  Does it need fewer parameters?  etc. I strongly endorse acceptance.",9,149,18.625,5.424657534246576,107,1,148,0.0067567567567567,0.0129870129870129,0.9774,33,26,20,19,5,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 0, 'CLA': 2}",0,0,1,1,0,1,4,0,0,0,0,3,0,0,0,1,0,1,0,1,4,0,2,0.3574922821340843,0.5575025629309385,0.22670040032882777
ICLR2018-HkbmWqxCZ-R1,Reject,"Summary  This paper proposes a penalized VAE training objection for the purpose of increasing the information between the data x and latent code z. Ideally, optimization would consist of maximizing log p(x) - | I(x,z) - M |, where M is the user-specified target mutual information (MI) and I(x,z) is the model's current MI value, but I(x,z) is intractable, necessitating the use of an auxiliary model r(z|x).  Optimization, then, consists of alternating gradient ascent on the VAE parameters and r's parameters. Experiments on simulations and text data are reported, showing that increasing M has the desired effect of allowing more deviation from the prior. Specifically, this is shown through text generation where the sampled sentences become more varied as M is decreased and better reconstructed as M is increased. Evaluation  Pros:  I like how this paper formalizes failure in representation learning as information loss in z---although the formulation is not particularly novel, i.e. [Zhao et al., ArXiv 2017]), and constructs an explicit, penalized objective to allow the user to specify the amount of information retained in z.  In my opinion, the proposed objective is more transparent than the objectives proposed by related work.  For instance, Chen et al.'s (2017) Lossy VAE, while aiming to solve essentially the same problem, does so by parameterizing the prior and using a windowed decoder, but there is no explicit control mechanism as far as I'm aware (except for how many parameters / window size). Perhaps the Beta-VAE's [Higgins et al., ICLR 2017] KLD weight is similarly interpretable (as beta increases, less information is retained), but I like that M has the clear meaning of mutual information---whereas the beta in the Beta-VAE is just a Lagrangian. In terms of experiments, I like the first simulation; it's a convincing sanity check. As for the second, I like the spirit of it, but I have some criticisms, as I'll explain below. Cons:  The method requires training an auxiliary model r(z|x) to estimate I(x,z). While I don't find the introduction of r(z|x) problematic, I do wish there was more discussion and analysis of how well the mutual information is being approximated during training, especially given some of the simplifying assumptions, such as r(z|x) p(z|x). If the MI estimate is way off, that detracts from the method and makes an alternative like the Beta-VAE---which doesn't require an auxiliary model---more appealing, since what makes the MAE superior---its principled targeting of MI---does not hold in practice. As for the movie review experiment, I find the sentence samples a bit anecdotal. Was the seed sentence (""there are many great scenes of course"") randomly chosen or hand picked? Was this interpolation behavior typical?  I ask these questions because I find the plot in Figure 3 all but meaningless. It's good that we see reconstruction quality go up as M increases, as expected, but the baseline VAE model is a strawman. How does reconstruction percentage look for the Bowman et al. (2015) VAE? What about the Beta-VAE?  Or Lossy VAE? Figure 3 would be okay if there were more experiments, but as it is the only quantitative result, more work should have gone in to it. For instance, a compelling result would be if we see one or more of the models above plateau in reconstruction percentage and the MAE surpass that plateau. Conclusions  While I found aspects of this paper interesting, I recommend rejection primarily for two reasons.  The first is that I would like to see how well the mutual information is being estimated during training. If the estimate is way off, this makes the method less appealing as what I like about it---the interpretable MI target---is not really a 'target', in practice, and rather, is a rough hyperparameter similar to the Beta-VAE's beta term (which has the added benefit of no auxiliary model). The second reason is the paper's weak experimental section.  The only quantitative result is Figure 3, and while it shows reconstruction percentage increases with M, there is no way to contextualize the number as the only comparison model is a weak VAE, which gives ~ 0%.  Questions I would like to see answered: How good is the MI estimate? How close is the converged VAE to the target?  How does the model compare to the Bowman et al. VAE or the Beta-VAE?  (It would be quite compelling to show similar or improved performance without the training tricks used by Bowman et al.) Can we somehow estimate the appropriate M directly from data (such as based on the entropy of training or validation set) in order to set the target rigorously?    1.  S. Zhao, J. Song, and S. Ermon.  ""InfoVAE: Information Maximizing Variational Autoencoders.""  ArXiv 2017. 2.  X. Chen, D. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Shulman, I. Sutskever, and P. Abbeel.  ""Variational Lossy Autoencoder.""  ICLR 2017. 3.  I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. ""Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.""  ICLR 2017 4.  S. Bowman, L. Vilnis, O. Vinyas,  A. Dai, R. Jozefowicz, and S. Bengio.  ""Generating Sentences from a Continuous Space.""  CoNLL 2016.",38,846,13.428571428571429,5.185378590078329,361,1,845,0.0011834319526627,0.0034013605442176,0.994,304,107,137,36,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 16, 'EXP': 11, 'RES': 1, 'TNF': 5, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 4, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 21, 'SUB': 4, 'CLA': 0}",0,0,6,2,0,16,11,1,5,0,0,2,4,0,0,0,0,2,0,1,21,4,0,0.5766809400915928,0.4573207852234582,0.3340225730919252
ICLR2018-HkbmWqxCZ-R2,Reject,"This paper presents mutual autoencoders (MAE). MAE aims to address the limitation of regular variational autoencoders (VAE) for latent representation learning u2014 VAE sometimes simply ignores the latent code z, especially with a powerful decoding distribution. The idea of MAE is to optimize the VAE objective subject to a constraint on the mutual information between the data x and latent code z: setting the mutual information constraints larger will force the latent code z to learn a meaningful representation of the data. An approximation strategy is employed to approximate the intractable mutual information. Experimental results on both synthetic data and movie review data demonstrate the effectiveness of the MAEs. Overall, the paper is well-written. The problem that VAEs fail to learn a meaningful representation is a well-known issue. This paper presents a simple, yet principled modification to the VAE objective to address this problem. I do, however, have two major concerns about the paper: 1. The proposed idea to add a mutual information constraint between the data x and latent code z is a very natural fix to the failure of regular VAEs. However, mutual information itself is not a quantity that is easy to comprehend and specify. This is not like, e.g., l2 regularization parameter, for which there exists a relatively clear way to specify and tune. For mutual information, at least it is not clear to me, how much mutual information is ""enough"" and I am pretty sure it is model/data-dependent. To make it worse, there exist no metrics in representation learning for us to easily tune this mutual information constraint. It seems the only way to select the mutual information constraint is to qualitative inspect the model fits. This makes the method less practical. 2. The approximation to the mutual information seems rather loose. If I understand correctly, the optimization of MAE is similar to that of a regular VAE, with an additional parametric model r_w(z|x) which is used to approximate the infomax bound. (And this also adds an additional term to the gradient wrt theta). r_w(z|x) is updated at the same time as theta, which means r_w(z|x) is quite far from being an optimal r* as it is intended, especially early during the optimization. Further more, all the derivation following Eq (12-13) are based on r* being optimal, while in reality, it is probably not even close. This makes the whole approximation quite hand-waving. Related to 2, the discussion in Section 6 deserves more elaboration. It seems that having a flexible encoder is quite important, yet the authors only mention lightly that they use the approximate posterior from Cremer et al. (2017). Will MAE not work without this?  How will VAE (without the mutual information constraint) work with this? A lot of the details seem to be glossed over. Furthermore, this work is also related to the deep variational information bottleneck of Alemi et al. 2017 (especially in the appendix they derived the VAE objective using information bottleneck principle). My intuition is that using a larger mutual information constraint in MAE is somewhat similar to setting the regularization beta to be smaller than 1 u2014 both are making the approximating posterior more concentrated. I wonder if the authors have explored this idea. Minor comments:  1. It would be more informative to include the running time in the presented results. 2. Since the goal of r_w(z | x) is to approximate the posterior p(z | x), what about directly using q(z | x) to approximate it? 3. In Algorithm 1, should line 14 and 15 be swapped? It seems samples are required in line 14 as well. 4. Nitpicking: technically the model in Eq (1) is not a hierarchical model.   ",34,610,15.64102564102564,5.2196428571428575,249,6,604,0.009933774834437,0.029126213592233,0.9259,162,85,109,43,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 10, 'DAT': 0, 'MET': 18, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 21, 'SUB': 1, 'CLA': 1}",0,0,2,10,0,18,1,2,0,0,0,2,0,0,0,0,0,1,1,0,21,1,1,0.4335400018656136,0.5679947042524149,0.27337720891534795
ICLR2018-HkbmWqxCZ-R3,Reject,"The authors propose a variational autoencoder constrained in such a way that the mutual information between the observed variables and their latent representation is constant and user specified. To do so, they leverage the penalty function method as a relaxation of the original problem, and a variational bound (infomax) to approximate the mutual information term in their objective. I really enjoyed reading the paper, the proposed approach is well motivated and clearly described. However, the experiments section is very weak. Although I like the illustrative toy problem, in that it clearly highlights how the method works, the experiment on real data is not very convincing. Further, the authors do not consider a more rigorous benchmark including additional datasets and state-of-the-art modelling approaches for text. - {cal Z} in (1) not defined, same for Theta.",7,133,19.0,5.67741935483871,87,0,133,0.0,0.0298507462686567,0.6632,35,18,22,13,7,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 2, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 1}",0,0,1,2,1,2,2,1,0,0,0,1,0,0,0,0,0,0,0,0,5,0,1,0.5003956877673491,0.2247100519615941,0.22368939646445735
ICLR2018-Hkc-TeZ0W-R1,Accept,"The paper seems clear enough and original enough. The idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit. Where the paper falls short is motivating the problem setting.  Traditionally, for determining optimal execution plans, one may resort to cost-based optimization (e.g., database management systems). This paper's introduction provides precisely 1 statement to suggest that may not work for deep learning. Here's the relevant phrase: the cost function is typically non-stationary due to the interactions between multiple devices . Unfortunately, this statement raises more questions than it answers . Why are the cost functions non-stationary? What exactly makes them dynamic? Are we talking about a multi-tenancy setting where multiple processes execute on the same device ? Unlikely, because GPUs are involved. Without a proper motivation, its difficult to appreciate the methods devised. Pros: - Jointly optimizing forming of groups and placing these seems to have merit n- Experiments show improvements over placement by human experts  - Targets an important problem  Cons: - Related work seems inadequately referenced . There exist other linear/tensor algebra engines/systems that perform such optimization including placing operations on devices in a distributed setting.  This paper should at least cite those papers and qualitatively compare against those approaches. Here's one reference (others should be easy to find): SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs by Boehm et al, IEEE Data Engineering Bulletin, 2014. - The methods are not well motivated.  There are many approaches to devising optimal execution plans, e.g., rule-based, cost-based, learning-based. In particular, what makes cost-based optimization inapplicable? Also, please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic, optimally forming groups and placing them is learn-able. - The template seems off . I don't see the usual two lines under the title (Anonymous authors, Paper under double-blind review). - The title seems misleading.  .... Device Placement seems to suggest that one is placing devices when in fact, the operators are being placed.",26,324,15.428571428571429,5.813880126182966,200,11,313,0.0351437699680511,0.0555555555555555,0.982,93,43,71,21,10,7,"{'ABS': 0, 'INT': 4, 'RWK': 12, 'PDI': 20, 'DAT': 1, 'MET': 9, 'EXP': 8, 'RES': 0, 'TNF': 1, 'ANA': 6, 'FWK': 0, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 2, 'NOV': 1, 'IMP': 5, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 15, 'SUB': 0, 'CLA': 1}",0,4,12,20,1,9,8,0,1,6,0,2,2,0,2,1,5,1,2,0,15,0,1,0.7191606515433511,0.7867485632941424,0.5496478376535026
ICLR2018-Hkc-TeZ0W-R2,Accept,"In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model's runtime. However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step. In particular, hand-crafted features are fed to the FCN and the output is the prediction of group id of this operation. Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder. Overall speaking, this work is quite interesting. However, it also has several limitations, as explained below. n First, the computational cost of the proposed method seems very high.  It may take more than one day on 320-640 GPUs for training (I did not find enough details in this paper, but the training complexity will be no less than the in [1]). This makes it very hard to reproduce the experimental results (in order to verify it), and its practical value becomes quite restrictive (very few organizations can afford such a cost). Second, as the author mentioned, it's hard to compare the experimental results in this paper wit those in [1] because different hardware devices and software versions were used. However, this is not a very sound excuse. I would encourage the authors to implement colocRL [1] on their own hardware and software systems, and make direct comparison.  Otherwise, it is very hard to tell whether there is improvement, and how significant the improvement is . In addition, it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the placements .   [1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. https://arxiv.org/pdf/1706.04972.pdf  ",15,297,16.5,5.110714285714286,171,2,295,0.0067796610169491,0.0163934426229508,0.9764,95,37,43,20,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 10, 'DAT': 2, 'MET': 6, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 5, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 5, 'CLA': 0}",0,1,9,10,2,6,5,3,0,2,0,1,0,1,0,0,5,2,0,0,7,5,0,0.7172312739031746,0.4489122042377697,0.4066354598125822
ICLR2018-Hkc-TeZ0W-R3,Accept,"This paper proposes a device placement algorithm to place operations of tensorflow on devices. Pros:  1. It is a novel approach which trains the placement end to end . 2. The experiments are solid to demonstrate this method works very well . 3. The writing is easy to follow. 4. This would be a very useful tool for the community if open sourced. Cons:  1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models . The latter would be more exciting. The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph? However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case. 2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs. n3. It is not clear how the adjacency information is used. ",11,175,9.72222222222222,4.777777777777778,97,1,174,0.0057471264367816,0.0497237569060773,0.9209,44,16,35,12,8,5,"{'ABS': 0, 'INT': 3, 'RWK': 5, 'PDI': 5, 'DAT': 2, 'MET': 5, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 5, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 4}",0,3,5,5,2,5,4,1,0,1,0,0,0,0,0,1,5,2,0,0,7,0,4,0.5733342069143195,0.5598400145281571,0.3512805045068058
ICLR2018-HkcTe-bR--R1,Reject,"The paper proposes a set of benchmarks for molecular design, and compares different deep models against them.  The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks. The paper does not seem to include any method development. The paper suffers from a lack of focus . Several existing models are discussed to some length, while the benchmarks are introduced quite shortly. The dataset is not very clearly defined: it seems that there are 1.2 million training instance, does this apply for all benchmarks? The paper's title also does not seem to fit: this feels like a survey paper, which is not reflected in the title . Biologically lots of important atoms are excluded from the dataset, for instance natrium, calcium and kalium . I don't see any reason to exlude these . What does biological activities on 11538 targets mean? The paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used. Are the test molecules somehow generated in a directed or undirected fashion? Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion?  There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties. Currently it's unclear which type of problems are solved here. Table 1 lists 5 models, while fig 3 contains 7, why the discrepancy? In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83).  Variances should be added, and preferably more than 3 initialisations used. Overall this is an interesting paper , but does not have any methodological contribution, and there is also few insightful results about the compared methods, nor is there meaningful analysis of the problem domain of molecules either",20,323,21.53333333333333,5.415841584158416,173,1,322,0.0031055900621118,0.0332326283987915,-0.7967,85,34,63,22,11,8,"{'ABS': 0, 'INT': 5, 'RWK': 9, 'PDI': 10, 'DAT': 4, 'MET': 7, 'EXP': 3, 'RES': 3, 'TNF': 2, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 5, 'CMP': 3, 'PNF': 1, 'REC': 1, 'EMP': 7, 'SUB': 5, 'CLA': 3}",0,5,9,10,4,7,3,3,2,2,0,3,0,1,0,1,5,3,1,1,7,5,3,0.7890260386359699,0.8936360813811122,0.6862230875412482
ICLR2018-HkcTe-bR--R2,Reject,"Summary: This work is about model evaluation for molecule generation and design. 19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design. on the positive side: The paper is well written, quality and clarity of the work are good. The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation. It is investigated how several RL strategies perform on a large, standardized data set. Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed. An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym. on the negative side: There is no new novel contribution on the methods side. minor comments:  Section 2.1.  see Fig.2 u2014> see Fig.1 page 4just before equation 8: the the",11,153,15.3,5.089655172413793,89,0,153,0.0,0.0129032258064516,0.8942,52,25,22,2,10,5,"{'ABS': 1, 'INT': 1, 'RWK': 7, 'PDI': 3, 'DAT': 2, 'MET': 6, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 3, 'CLA': 1}",1,1,7,3,2,6,3,0,0,4,0,1,0,1,0,1,2,0,0,0,5,3,1,0.7164208424578128,0.5583047749600183,0.4547143011813188
ICLR2018-HkcTe-bR--R3,Reject,"Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules.  The experiments seem extensive, using many recently proposed RL methods,  and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception .    Originality and significance: The conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically.  The paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniques .  Clarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up.",7,127,21.166666666666668,5.936,93,2,125,0.016,0.0294117647058823,0.8338,42,20,20,11,8,7,"{'ABS': 0, 'INT': 3, 'RWK': 6, 'PDI': 2, 'DAT': 1, 'MET': 4, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 4, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,3,6,2,1,4,3,0,0,0,1,0,0,1,0,1,4,1,0,1,6,1,1,0.5728643165857494,0.7810316748973363,0.4468704823973749
ICLR2018-HkeJVllRW-R1,Reject,"This paper presented interesting ideas to reduce the redundancy in convolution kernels. They are very close to existing algorithms .  (1)tThe SW-SC kernel (Figure 2 (a)) is an extension of the existing shaped kernel (Figure 1 (c)). (2)tThe CW-SC kernel (Figure 2 (c)) is very similar to interleaved group convolutions. The CW-SC kernel can be regarded as a redundant version of interleaved group convolutions [1]. I would like to see more discussions on the relation to these methods and more strong arguments for convincing reviewers to accept this paper. [1] Interleaved Group Convolutions. Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. ICCV 2017.  http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Interleaved_Group_Convolutions_ICCV_2017_paper.pdf",8,104,10.4,6.09375,67,0,104,0.0,0.0093457943925233,0.8783,41,13,18,3,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,5,1,0,2,0,0,2,0,0,0,2,0,0,0,1,0,0,1,2,0,0,0.4291792632602931,0.3339552907681763,0.2184340615308776
ICLR2018-HkeJVllRW-R2,Reject,"This paper introduces a new design of kernels in convolutional neural networks. The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels. Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels. Strengths: - The complementary kernels come at no loss compare to standard ones - The resulting wider networks can achieve better accuracies than the original ones  Weaknesses: - The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions - The improvement over the baseline is not very impressive - There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)  Detailed comments: - The separation into + and x patterns is quite clear for 3x3 kernels. However, two such patterns would not be sufficient for 5x5 or 7x7 kernels. This idea would have more impact if it generalized to arbitrary kernel dimensions. - The improvement over the original models are of the order of less than 1 percent. I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches. It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment. - Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper. To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors). What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS. - In Section 2, the authors review ideas of so-called random kernel sparsity. Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead. They also do not require pre-training and re-training, but just a single training procedure. Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model. - In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks. - I am not entirely convinced by the discussion of the grouped sparsity method in Section 3.1. In fact, the order of the channels is arbitrary, since the kernels are learnt. Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this? - Is there a particular reason why the central points appears in both complementary kernels (+ and x)? - Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation? Did the baseline (original model) reported here also use 50k?  What would the results be with 45k? - Fig. 5 is not entirely clear to me. What was the width of each layer?  The original one or the modified one? - It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model. - In Table 4, I find it surprising that there is an actual speedup for the model with larger width. I would have expected the same runtime. How do the authors explain this?",30,616,22.0,4.96,248,5,611,0.0081833060556464,0.0405616224648986,0.9861,159,79,82,41,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 24, 'PDI': 6, 'DAT': 0, 'MET': 8, 'EXP': 4, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 2}",0,1,24,6,0,8,4,1,2,0,0,1,2,0,0,0,0,2,1,0,6,1,2,0.647050487058416,0.5588636403578144,0.3997048268782675
ICLR2018-HkeJVllRW-R3,Reject,"Summary: This paper proposed a sparse-complementary convolution as an alternative to the convolution operation in deep networks.  In this method, two new types of kernels are developed, namely the spatial-wise and channel-wise sparse-complementary kernels. The authors argue that the proposed kernels are able to cover the same receptive field as the regular convolution with almost half the parameters. By adding more filters or layers in the model while keeping the same FLOPs and parameters, the models with the proposed method outperform the regular convolution models. The paper is easy to follow and the idea is interesting. However, the novelty of the paper is limited and the experiments are not sufficient. Strengths: 1. The authors proposed the sparse-complementary convolution to cover the same receptive field as the regular convolution.  2. The authors implement the proposed sparse-complementary convolution on NVIDIA GPU and achieved competitive speed under the same computational load to regular convolution. 3. The authors demonstrated that, given the same resource budget, the wider networks with the proposed method are more efficient than the deeper networks due to the nature of GPU parallel mechanism. Weak points:  1. The novelty of this paper is limited. The main idea is to design complementary kernels that cover the same receptive field as the regular convolution. However, the performance improvement is marginal and may come from the benefit of wide networks rather than the proposed complementary kernels. Moreover, the experiments are not sufficient to support the arguments. For example, how is the performance of a model containing SW-SC or CW-SC without deepening or widening the networks? Without such experiment, it is unclear whether the improved performance comes from the sparse-complementary kernels or the increased number of kernels. 2. The relationship between the proposed spatial-wise kernels and the channel-wise kernels is not very clear. Which kernel is better and how to choose between them in a deep network? There is no experimental proof in the paper. 3. The proposed two kernels introduce sparsity in the spatial and channel dimension, respectively.  The two methods are used separately. Is it possible to combine them together? 4. The proposed method only considers the ""+-shape"" and ""x-shape"" sparse pattern. Given the same receptive field with multiple complementary kernels, is the kernel shape important for the training? There is no experimental result to verify this. 5. As mentioned in the paper, there are many methods which introduce sparsity in the convolution layer, such as ""random kernels"", ""low-rank approximated kernels"" and ""mixed-shape kernels"". However, there is no experimental comparison with these methods. 6. In the paper, the author mentioned another sparse-complementary baseline (sc-seq), which applies sparse kernels sequentially. It yields smaller receptive field than the proposed method when the model depth is very small. Indeed, when the model goes deeper, the receptive field becomes very close to that of the proposed method. In the experiments, it is strange that this method can also achieve comparable or better results. So, what is the advantage of the proposed ""sc"" method compared to the ""sc-seq"" method? 8. Figure 5 is hard to understand. This figure only shows that training shallower networks is more effective than training the deeper networks on GPU. However, it does not mean training the wider networks is more efficient than training the deeper ones.",34,543,13.923076923076923,5.455576559546314,202,1,542,0.0018450184501845,0.0201096892138939,0.9085,135,87,85,27,10,5,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 16, 'DAT': 1, 'MET': 21, 'EXP': 17, 'RES': 3, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 6, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 5, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 0, 'CLA': 3}",0,0,9,16,1,21,17,3,2,1,0,6,3,0,0,2,5,3,0,0,15,0,3,0.7225886721717075,0.5648968090777549,0.4583473312930454
ICLR2018-HkepKG-Rb-R1,Reject,"SUMMARY   The paper proposes a new form of regularization utilizing logical constraints. The semantic loss function is built on the exploitation of symbolic knowledge extracted from data and connecting the logical constraints to the outputs of a neural network. The use of Boolean logic as a constraint provides a secondary regularization term to prevent over-fitting and improve predictions. The benefit of using the function is found primarily with semi-supervised tasks where data is partially unlabelled. The logical constraints provided by the semantic loss function allow for improved classification of unlabeled data. Output constraints for the semantic loss function are represented with one-hot encoding, prefer- ence rankings, and paths in a grid. These three different output constraints are designed to explore different learning purposes. The semantic function was tested on both semi-supervised classifica- tion tasks as well as structure learning. The paper primarily focuses on the one-hot encoding constraint as it is viewed as a capable technique for multi-class classification. POSITIVES   In terms of structure, the paper was written very well. Sufficient background information was con- veyed which helped in understanding the proposed semantic loss function. A thorough breakdown is also carried out on the semantic loss function itself by explaining its axioms which help explain how the outputs of a neural network match a given constraint. As a scientific contribution, I would say results from the experiments were able to justify the proposal of the semantic loss function. The function was able to perform better than most other implementations for semi-supervised learning tasks, and the function was tested on multiple datasets. The paper also made use of testing the function against other notable machine learning approaches, and in most cases the function performed better, but this usually was confined to semi-supervised learning tasks. During supervised learning tasks the function did not perform markedly better than older implementations. Given that, the semantic loss function did prove to be a seemingly simple approach to improving semi-supervised classification tasks. u2022 The background section covers the knowledge required in understanding the semantic loss function. The paper also clearly explains the meaning for some of the notation used in the definitions. u2022 Experiments which clearly show the benefit of using the semantic loss function. Multiple experiment types were done as well which showed evidence of the broad applicability of the function. u2022 In depth description of the definitions, axioms, and propositions of the semantic loss function. u2022 A large number of experiments exploring the usefulness of the function for multiple learning tasks, and on multiple datasets. NEGATIVES   I was not clear if the logical constraints are to be instantiated before learning, i.e. they are defined by hand prior to being implemented in the neural network. This is a pretty important question and drastically changes the nature of the learning process. Beyond that complaint, the paper did not suffer from any critical issues. There were some issues with spelling, and the section titled 'Algorithm' fails to clearly define a complete algorithm using the semantic loss function.  It would have helped to have two algorithms. One defining the pipeline for the semantic loss function, and another showing the implementation of the function in a machine learning framework. The semantic loss function found success only in cases were the learning task was semi-supervised, and not in cases of total supervised learning. This is not a true negative, but an observation on the effectiveness of the function. - A few typos in the paper. - The axioms for the semantic loss function where defined but there seemed to be a lack of a clear algorithm provided showing the pipeline implementation of the semantic loss function. - While the semantic loss function does improve learning performance in most cases, the im- provements are confined to semi-supervised learning tasks, and with the MNIST dataset another methodology, Ladder Nets, was able to outperform the semantic loss function. RELATED WORK  The paper proposed that logic constraints applied to the output of neural networks have the capacity to improve semi-supervised classification tasks as well as finding the shortest path. In the introduction, the paper lists Zhiting Hu et al. paper titled Harnessing Deep Neural Networks with Logic Rules as an example of a similar approach. Hu et al. paper utilized logic constraints in conjunction with neural nets as well. A key difference was that Hu et al. applied their network architecture to supervised classification tasks. Since the performance of the current papers semantic loss function with supervised tasks did not improve upon other methods, it may benefit to utilize the research by Hu et al. as a means of direct comparison for supervised learning tasks, and possibly incorporate their methods with the semantic loss function in order to improve upon supervised learning tasks. CONCLUSION  Given the success of the semantic loss function with semi-supervised tasks, I would accept this paper. The semantic loss was able to improve learning with respect to the tested datasets, and the paper clearly described the properties of the functions. The paper would benefit by including a more concrete algorithm describing the flow of data through a given neural net to the semantic loss function, as well as the process by which the semantic loss function constrains the data based on propositional logic, but in general this complaint is more nit picking. The semantic loss function and the experiments which tested the function showed clearly that there is a benefit to this research and there are areas for it to improve.",42,908,18.916666666666668,5.4971493728620295,310,4,904,0.0044247787610619,0.0097826086956521,0.9926,279,124,143,42,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 7, 'DAT': 4, 'MET': 28, 'EXP': 9, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 1, 'EMP': 18, 'SUB': 1, 'CLA': 4}",0,1,2,7,4,28,9,2,0,0,0,6,0,0,0,0,0,3,1,1,18,1,4,0.5797006800605881,0.6777176733859394,0.41590088146416215
ICLR2018-HkepKG-Rb-R2,Reject,"The authors propose a new loss function that is directed to take into account Boolean constraints involving the variables of a classification problem. This is a nice idea, and certainly relevant. The authors clearly describe their problem, and overall the paper is well presented.  The contributions are a loss function derived from a set of axioms, and experiments indicating that this loss function captures some valuable elements of the input. This is a valid contribution, and the paper certainly has some significant strengths. Concerning the loss function, I find the whole derivation a bit distracting and unnecessary. Here we have some axioms, that are not simple when taken together, and that collectively imply a loss function that makes intuitive sense by itself. Well, why not just open the paper with Definition 1, and try to justify this definition on the basis of its properties. The discussion of axioms is just something that will create debate over questionable assumptions. Also it is frustrating to see some axioms in the main text, and some axioms in the appendix (why this division?). After presenting the loss function, the authors consider some applications. They are nicely presented; overall the gains are promising but not that great when compared to the state of the art --- they suggest that the proposed semantic loss makes sense. However I find that the proposal is still in search of a killer app. Overall, I find that the whole proposal seems a bit premature and in need of more work on applications (the work on axiomatics is fine as long as it has something to add). Concerning the text, a few questions/suggestions: - Before Lemma 3, this allows... is the this including the other axioms in the appendix? - In Section 4, line 3: I suppose that the constraint is just creating a problem with a class containing several labels, not really a multi-label classification problem (?). - The beginning of Section 4.1 is not very clear. By reading it, I feel that the best way to handle the unlabeled data would be to add a direct penalty term forcing the unlabeled points to receive a label. Is this fair? - Page 6: a mor methodological... should it be a more methodical? - There are problems with capitalization in the references. Also some references miss page numbers and some do not even indicate what they are (journal papers, conference papers, arxiv, etc). ",23,396,18.0,5.138964577656676,191,1,395,0.0025316455696202,0.037037037037037,-0.9441,104,38,71,28,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 5, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 5, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 3}",0,0,1,5,0,9,0,0,0,0,0,3,2,1,0,0,1,1,5,0,6,0,3,0.4309133955500673,0.5590738885666918,0.2671857162388474
ICLR2018-HkepKG-Rb-R3,Reject,"This paper suggest a method for including symbolic knowledge into the learning process. The symbolic knowledge is given as logical constraints which characterize the space of legal solutions.  This knowledge is injected into the learning process by augmenting the loss function with a symbolic-loss term that, in addition to the traditional loss, increases the probability of legal states (which also includes incorrect, yet legal, predictions). Overall the idea is interesting, but the paper does not seem ready for publication. The  idea of semantic-loss function is appealing and is nicely motivated in the paper, however the practical aspects of how it can be applied are extremely vague and hard to understand. Specifically, the authors define it over all assignments to the output variables that satisfy the constraints. For any non-trivial prediction problem, this would be at least computationally challenging. The author discuss it briefly mentioning a method by Darwiche-2003, but do not offer much intuition or analysis beyond that.  Their experiments focus on multiclass classification, which implicitly has a one-vs.-all constraint, although it's not clear why defining a formal loss function is needed (instead of just taking the argmax of the multiclass net), and even beyond that - why would it result in such significant improvements (when there are a few annotated data points)? The more interesting case is where the loss needs to decompose over the parts of a structural decision, where symbolic knowledge can help  constrain the output space. This has been addressed in the literature (e.g., [1], [2]) it's not clear why the authors don't compare to these models, or even attempt any meaningful evaluation. [1] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. ACL, 2016.  [2]Posterior Regularization for Structured Latent Variable Models.  Ganchev et-al 2010.",13,297,19.8,5.433566433566433,179,0,297,0.0,0.0263157894736842,0.8844,78,42,56,19,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,0,1,3,0,7,2,1,0,0,0,1,1,0,0,0,0,1,1,1,6,1,0,0.5017059481981282,0.5586653427297704,0.3143049650231325
ICLR2018-HkfXMz-Ab-R1,Accept,"The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language. In this setting, they propose an algorithm based on sketches- abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names.  Conditioned on information such as type specification or keywords of a method they generate the method's body from the trained sketches.  u00a0 Positives: u00a0 tu2022tNovel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposed tu2022tVery good abstract: It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results. tu2022tExcellently structured and presented paper u00a0 tu2022t Motivation given in form of relevant applications and mention that it is relatively unstudied tu2022t The hypothesis/ the papers goal is clearly stated. It is introduced with 'We ask' followed by two well formulated lines that make up the hypothesis. It is repeated multiple times throughout the paper. Every mention introduces either a new argument on why this is necessary or sets it in contrast to other learners, clearly stating discrepancies. tu2022tExplanations are exceptionally well done: terms that might not be familiar to the reader are explained. This is true for mathematical aspects as well as program generating specific terms . Examples are given where appropriate in a clear and coherent manner tu2022t Problem statement well defined mathematically and understandable for a broad audience tu2022t Mentioning of failures and limitations demonstrates a realistic  view on the project tu2022t Complexity and time analysis provided tu2022t Paper written so that it's easy for a reader to implement the methods tu2022t Detailed descriptions of all instantiations even parameters and comparison methods tu2022tSystem specified tu2022tValidation method specified tu2022tData and repository, as well as cleaning process provided tu2022tEvery figure and plot is well explained and interpreted tu2022t Large successful evaluation section provided tu2022t Many different evaluation measures defined to measure different properties of the project tu2022t Different observability modes tu2022tEvaluation against most compatible methods from other sources  tu2022t Results are in line with hypothesis tu2022tThorough appendix clearing any open questions  u00a0  It would have been good to have a summary/conclusion/future work section u00a0 SUMMARY: ACCEPT. The authors present a very intriguing novel approach that  in a clear and coherent way.  The approach is thoroughly explained for a large audience. The task itself is interesting and novel. The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising. Even though no conclusive section is provided, the paper is not missing any information. ",30,447,26.294117647058822,5.688940092165899,235,1,446,0.0022421524663677,0.0087527352297593,0.9976,134,70,73,28,11,7,"{'ABS': 0, 'INT': 3, 'RWK': 10, 'PDI': 18, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 2, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 2}","{'APR': 2, 'NOV': 3, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 12, 'SUB': 4, 'CLA': 4}",0,3,10,18,0,7,1,2,1,3,0,4,1,2,2,3,0,1,1,0,12,4,4,0.789249690174879,0.7852826253785815,0.6311935968233238
ICLR2018-HkfXMz-Ab-R2,Accept,"This paper aims to synthesize programs in a Java-like language from a task description (X) that includes some names and types of the components that should be used in the program.  The paper argues that it is too difficult to map directly from the description to a full program, so it instead formulates the synthesis in two parts. First, the description is mapped to a sketch (Y) containing high level program structure but no concrete details about, e.g., variable names. Afterwards, the sketch is converted into a full program (Prog) by stochastically filling in the abstract parts of the sketch with concrete instantiations. The paper presents an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs. Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST. The data is derived from an online repository of ~1500 Android apps, and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale. This is one of the strongest points of the paper. One point I found confusing is how exactly the Combinatorial Concretization step works. Am I correct in understanding that this step depends only on Y, and that given Y, Prog is conditionally independent of X? If this is correct, how many Progs are consistent with a typical Y? Some additional discussion of why no learning is required for the P(Prog | Y) step would be appreciated. I'm also curious whether using a stochastic latent variable (Z) is necessary . Would the approach work as well using a more standard encoder-decoder model with determinstic Z? Some discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate. Overall, I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found in the wild. While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel, I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion.",20,397,28.357142857142858,5.374316939890711,191,2,395,0.0050632911392405,0.0225,0.9581,116,45,65,23,8,5,"{'ABS': 0, 'INT': 4, 'RWK': 11, 'PDI': 4, 'DAT': 2, 'MET': 4, 'EXP': 12, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 3, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,4,11,4,2,4,12,0,0,0,0,3,1,0,1,3,1,0,0,0,10,1,0,0.574313565232069,0.5612433876381786,0.3620994564597628
ICLR2018-HkfXMz-Ab-R3,Accept,"This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved. This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps. The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously. This paper has many strengths: 1) The writing is clear, and the paper is well-motivated 2) The proposed algorithm is described in excellent detail, which is essential to reproducibility 3) As stated previously, the approach is validated with a large number of real Android projects 4) The fact that the language generated is non-trivial (Java-like) is a substantial plus 5) Good discussion of limitations   Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.",8,168,42.0,5.664516129032258,98,0,168,0.0,0.0117647058823529,0.9789,42,26,34,5,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 6, 'DAT': 2, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 2}",0,1,4,6,2,3,2,0,0,1,0,2,0,0,0,2,1,1,1,0,4,2,2,0.5726346240122242,0.7798765692460004,0.45411920164188174
ICLR2018-Hkfmn5n6W-R1,Reject,"This is a theory paper. The authors consider networks with single hidden layer. They assume gaussian input and binary labels. Compared to some of the existing literature, they study a more realistic model that allows for mild overparametrization and approximately speaking d_0 d_1 sqrt(N). The main result is that volume of suboptimal local minima exponentially decreases in comparison to global minima. In my opinion, paper has multiple drawbacks. 1) Lack of surprise factor: There are already multiple papers essentially saying similar things. I am not sure if this contributes substantially on top of existing literature. 2) Lack of algorithmic results: While the volume of suboptimal DLM being small is an interesting result, it doesn't provide substantial algorithmic insight. Recent literature contains results that states not only all locals are global but also gradient descent provably converges to the global with a good rate. See Soltanolkotabi et al. 3) Mean squared error for classification problem (discrete labels) does not sound reasonable to me. I believe there are already some zero error results for continuous labels. Logistic loss would have made a more compelling story. Minor comments: i) Results are limited to single hidden layer whereas the title states multilayer. While single hidden layer is multilayer, stating single hidden layer upfront might be more informative for the reader. ii) Theorem 10 and Theorem 6 essentially has the same bound on the right hand side but Theorem 10 additionally divides local volume by global which decreases by exp(-2Nlog N). So it appears to me that Thm 10 is missing an additional exp(2Nlog N) factor on the right hand side. Revision (response to authors): I appreciate the authors' response and clarification. I do agree that my comparison to Soltanolkotabi missed the fact that his result only applies to quadratic activations for global convergence (also many thanks to Jason for clarification). Additionally, this paper appeared earlier on arXiv. In this sense, this paper has novel technical contribution compared to prior literature. On the other hand, I still think the main message is mostly covered by existing works. I do agree that squared-loss can be used for classification but it makes the setup less realistic. Finally, while introduction discusses the last two layers, I don't see a technical result proving that the results extends to the last two layers of a deeper network. At least one of the assumptions require Gaussian data and the input to the last two layers will not be Gaussian even if all previous layers are fixed. Consequently, the multilayer title is somewhat misleading.",26,422,15.62962962962963,5.396984924623116,214,3,419,0.0071599045346062,0.0213270142180094,0.9365,116,69,73,32,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 6, 'PDI': 3, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 12, 'SUB': 1, 'CLA': 0}",0,2,6,3,0,8,0,5,0,0,0,4,0,1,0,3,0,3,1,0,12,1,0,0.502656966956429,0.5627216276222444,0.3177619687692908
ICLR2018-Hkfmn5n6W-R2,Reject,"This paper studies the question: Why does SGD on deep network is often successful, despite the fact that the objective induces bad local minima? The approach in this paper is to study a standard MNN with one hidden layer. They show that in an overparametrized regime, where the number of parameters is logarithmically larger than the number of parameters in the input, the ratio between the number of (bad) local minima to the number of global minima decays exponentially. They show this for a piecewise linear activation function, and input drawn from a standard Normal distribution. Their improvement over previous work is that the required overparameterization is fairly moderate, and that the network that they considered is similar to ones used in practice. This result seems interesting, although it is clearly not sufficient to explain even the success on the setting studied in this paper, since the number of minima of a certain type does not correspond to the probability of the SGD ending in one: to estimate the latter, the size of each basin of attraction should be taken into account. The authors are aware of this point and mention it as a disadvantage. However, since this question in general is a difficult one, any progress might be considered interesting. Hopefully, in future work it would be possible to also bound the probability of starting in one of the basins of attraction of bad local minima. The paper is well written and well presented, and the limitations of the approach, as well as its advantages over previous work, are clearly explained. As I am not an expert on the previous works in this field, my judgment relies mostly on this work and its representation of previous work. I did not verify the proofs in the appendix.  ",13,297,24.75,4.913494809688581,145,4,293,0.0136518771331058,0.0334448160535117,0.9741,75,32,41,19,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,0,2,3,0,5,0,3,0,0,1,0,0,3,0,0,1,1,0,0,3,0,1,0.4299013445743957,0.4456883593141303,0.23884129516684743
ICLR2018-Hkfmn5n6W-R3,Reject,"## Summary This paper aims to tackle the question: why does standard SGD based algorithms on neural network converge to 'good' solutions?    Pros:  Authors ask the question of convergence of optimization (ignoring generalization error): how likely is that an over-parameterized (d1d0 > N) single hidden layer binary classifier find a good (possibly over-fitted) local minimum. They make a set of assumptions (A1-A3) which are weaker (d1 > N^{1/2}) than the ones used earlier works. Previous works needed a wide hidden layer (d1 > N).  Assumptions (d0 input dim, d1 hidden dim, N n of datapoints, X datapoints matrix): A1. Datapoints X come from a Gaussian distribution  A2. N^{1/2} < d0  < N A3. N polylog(N) < d0d1 (approximate n of. parameters)  and d1  < N This paper proves that total angular volume of regions (defined with respect to the piecewise linear regions of neuron activations) with differentiable bad-local minima are exponentially small when compared with to the total angular volume of regions containing only differentiable global-minimal. The proof boils down to counting arguments and concentration inequality. Cons:  Non-differentiable stationary points are left as a challenging future work on this paper.  Non-differentiability aside, authors show a possible way by which shallow neural networks might be over-fitting the data. But this is only half the story and does not completely answer the question.  First, exponentially vanishing (in N) volume of the regions containing bad-local minima doesn't mean that the number of bad local minima are exponentially small when compared to number global minima. Secondly, as the authors aptly pointed out in the discussion section, this results doesn't mean neural networks will converge to good local minima because these bad local minimas can have a large basins of attraction. Lastly, appropriate comparisons with the existing literature is lacking. It is hinted that this paper is more general as the assumptions are more realistic. However, it comes at a cost of losing sharpness in the theoretical results. It is not well motivated why one should study the angular volume of the global and local minima. ## Questions and comments 1. How critical is Gaussian-datapoints assumption (A1)? Which part of the proof fails to generalize? 2. Can the proof be extended to scalar regression? It seems hard to generalize to vector output neural networks. What about deep neural networks? 3. Can you relate the results to other more recent works like: https://arxiv.org/pdf/1707.04926.pdf. 4. Piecewise linear and positively homogeneous (https://arxiv.org/pdf/1506.07540.pdf) activation seem to be important assumption of the paper. It should probably be mentioned explicitly. 5. In the experiments section, it is mentioned that ...inputs to the hidden neurons converge to a distinctly non-zero value.  This indicates we converged to DLMs.  How can you guarantee that it is a local minimum and not a saddle point? ",27,451,15.033333333333331,5.492822966507177,226,5,446,0.0112107623318385,0.0336842105263157,-0.4507,133,76,79,26,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 4, 'DAT': 1, 'MET': 10, 'EXP': 4, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 0}",0,0,3,4,1,10,4,4,0,0,1,1,0,1,0,0,1,3,1,0,14,1,0,0.6458734403247944,0.5638753273228941,0.4037495211685208
ICLR2018-HkgNdt26Z-R1,Accept,"This paper deals with improving language models on mobile equipments based on small portion of text that the user has ever input. For this purpose, authors employed a linearly interpolated objectives between user specific text and general English, and investigated which method (learning without forgetting and random reheasal) and which interepolation works better. Moreover, authors also look into privacy analysis to guarantee some level of differential privacy is preserved. Basically the motivation and method is good, the drawback of this paper is its narrow scope and lack of necessary explanations. Reading the paper, many questions arise in mind:  - The paper implicitly assumes that the statistics from all the users must   be collected to improve general English. Why is this necessary? Why not   just using better enough basic English and the text of the target user? - To achieve the goal above, huge data (not the portion of the general English) should be communicated over the network. Is this really worth doing?  If only   the portion of general English must be communicated, why is it validated? - For measuring performance, authors employ keystroke saving rate. For the   purpose of mobile input, this is ok: but the use of language models will   cover much different situation where keystrokes are not necessarily    available, such as speech recognition or machine translation. Since this    paper is concerned with a general methodology of language modeling,    perplexity improvement (or other criteria generally applicable) is also   important. - There are huge number of previous work on context dependent language models,   let alone a mixture of general English and specific models. Are there any   comparison with these previous efforts? Finally, this research only relates to ICLR in that the language model employed is LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so. I would like to advise the authors to submit this work to such conferences where it will be reviewed by more NLP experts. Minor: - t of $G_t$ in page 2 is not defined so far. - What is gr in Section 2.2",18,341,24.357142857142858,5.155688622754491,187,0,341,0.0,0.0133689839572192,0.9831,93,45,61,25,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 16, 'PDI': 4, 'DAT': 1, 'MET': 4, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 2, 'EMP': 8, 'SUB': 1, 'CLA': 1}",0,0,16,4,1,4,2,1,0,0,0,1,0,0,0,0,1,2,0,2,8,1,1,0.5022219902543897,0.6711573317399225,0.35667698967741895
ICLR2018-HkgNdt26Z-R2,Accept,"my main concern is the relevance of this paper to ICLR. This paper is much related not to representation learning but to user-interface. The paper is NOT well organized and so the technical novelty of the method is unclear. For example, the existing method and proposed method seems to be mixed in Section 2. You should clearly divide the existing study and your work. The experimental setting is also unclear. KSS seems to need the user study. But I do not catch the details of the user study, e.g., the number of users.",8,93,11.625,4.527472527472527,54,2,91,0.0219780219780219,0.064516129032258,-0.3007,24,6,21,8,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 2}",0,0,4,1,0,3,1,0,0,0,0,1,0,1,0,1,0,0,2,0,4,1,2,0.4293132765061498,0.5575641318547419,0.2585877005465494
ICLR2018-HkgNdt26Z-R3,Accept,"This paper discusses the application of word prediction for software keyboards. The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements. The authors propose a simple method of mixing the global model with user specific data. Collecting the user specific models and averaging them to form the next global model. The proposal is practical. However, I am not convinced that this is novel enough for publication at ICLR. One major question. The authors assume that the global model will depict general english. However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English. It is not clear to me as how this mechanism guarantees that it will not over-fit or that there will be no catastrophic forgetting.",10,155,15.5,5.131578947368421,91,1,154,0.0064935064935064,0.0580645161290322,0.1245,38,23,24,9,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 5, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 1, 'CLA': 2}",0,0,5,5,1,4,1,0,0,0,0,2,0,0,0,1,1,0,0,1,7,1,2,0.4299265747777921,0.6704795463465785,0.2917885709737744
ICLR2018-Hki-ZlbA--R1,Reject,"Summary: The paper proposes a method to compute adversarial examples with minimum distance to the original inputs, and to use the method to do two things: Show how well heuristic methods do in finding optimal/minimal adversarial examples (how close the come to the minimal change that flips the label) and to assess how a method that is designed to make the model more robust to adversarial examples actually works. Pros:  I like the idea and the proposed applications. It is certainly highly relevant, both in terms of assessing models for critical use cases as well as a tool to better understand the phenomenon. Some of the suggested insights in the analysis of defense techniques are interesting. Cons:  The is not much technical novelty. The method boils down to applying Reluplex (Katz et al. 2017b) in a binary search (although I acknowledge the extension to L1 as distance metric). The practical application of the method is very limited since the search is very slow and is only feasible at all for relatively small models. State-of-the-art practical models that achieve accuracy rates that make them interesting for deployment in potentially safety critical applications are out of reach for this analysis.  The network analysed here does not reach the state-of-the-art on MNIST from almost two decades ago. The analysis also has to be done for each sample. The long runtime does not permit to analyse large amounts of input samples, which makes the analysis in terms of the increase in robustness rather weak. The statement can only be made for the very limited set of tested samples. It is also unclear whether it is possible to include distance metrics that capture more sophisticated attacks that fool network even under various transformations of the input. The paper does not consider the more recent and highly relevant Moosavi-Dezfooli et al. ""Universal Adversarial Perturbations"" CVPR 2017. The distance metrics that are considered are only L_inf and L1, whereas it would be interesting to see more relevant ""perceptual losses"" such as those used in style transfer and domain adaptation with GANs. Minor details: * I would consider calling them ""minimal adversarial samples"" instead of ""ground-truth"". * I don't know if the notation in the Equation in the paragraph describing Carlini & Wagner comes from the original paper, but the inner max would be easier to read as max_{i  eq t} {Z(x')_i } * Page 3 ""Neural network verification"": I dont agree with the statement that neural networks commonly are trained on ""a small set of inputs"". * Algorithm 1 is essentially only a description of binary search, which should not be necessary. * What is the timeout for the computation, mentioned in Sec 4? * Page 7, second paragraph: I wouldn't say the observation is in line with Carlini & Wagner, because they take a random step, not necessarily one in the direction of the optimum?  That's also the conclusion two paragraphs below, no? * I don't fully agree with the conclusion that the defense of Madry does not overfit to the specific method of creating adversarial examples. Those were not created with the CW attack, but are related because CW was used to initialize the search.  ",24,520,22.60869565217392,5.100603621730382,254,0,520,0.0,0.0260707635009311,-0.7811,140,67,93,45,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 3, 'DAT': 3, 'MET': 15, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 8, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 17, 'SUB': 4, 'CLA': 0}",0,0,4,3,3,15,3,2,0,8,0,0,0,0,0,1,0,2,0,0,17,4,0,0.5043360636440765,0.4548329554840864,0.2848620507957177
ICLR2018-Hki-ZlbA--R2,Reject,"The authors propose to employ provably minimal-distance examples as a tool to evaluate the robustness of a trained network. This is demonstrated on a small-scale network using the MNIST data set. First of all, I find it striking that a trained network with 97% accuracy (as claimed by the authors) seems extremely brittle -- considering the fact that all the adversarial examples in Figure 1 are hardly borderline examples at all, at least to my eyes. This does reinforce the (well-known?) weakness of neural networks in general. I therefore find the authors' statement on page 3 disturbing: ... they are trained over a small set of inputs, and can then perform well, in general, on previously-unseen inputs -- which seems false (with high probability over all possible worlds). Secondly, the term ground truth example seems very misleading to me. Perhaps closest misclassified examples? Finally, while the idea of closest misclassified examples seems interesting, I am not convinced that they are the right way to go when it comes to both building and evaluating robustness. All such examples shown in the paper are indeed within-class examples that are misclassified. But we could equally consider another extreme, where the trained network is over-regularized in the sense that the closest misclassified examples are indeed from another class, and therefore correctly misclassified. Adding these as adversarial examples could seriously degrade the accuracy. Also, for building robustness, one could argue that adding misclassified examples that are furthest (i.e. closest to the true decision boundary) is a much more efficient training approach, since a few of these can possibly subsume a large number of close examples.  ",12,267,19.071428571428573,5.486166007905139,145,6,261,0.0229885057471264,0.0404411764705882,-0.7715,61,39,44,21,5,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 9, 'EXP': 0, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,0,2,1,9,0,2,2,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0.3592826937684062,0.1148428557201689,0.14238166101628036
ICLR2018-Hki-ZlbA--R3,Reject,"The paper describes a method for generating so called ground truth adversarial examples: adversaries that have minimal (L1 or L_inf) distance to the training example used to generate them. The technique uses the recently developed reluplex, which can be used to verify certian properties of deep neural networks that use ReLU activations. The authors show how the L1 distance can be formulated using a ReLU and therefore extend the reluplex also work with L1 distances. The experiments on MNIST suggest that the C&W attack produces close to optimal adversarial examples, although it is not clear if these findings would transfer to larger more complex networks. The evaluation also suggests that training with iterative adversarial examples does not overfit and does indeed harden the network to attacks in many cases. In general, this is a nice idea, but it seems like the inherent computational cost will limit the applicability of this approach to small networks and datasets for the time being. Incidentally, it would have been useful if the authors provided indicative information on the computational cost (e.g. in the form of time on a standard GPU) for generating these ground truths and carrying out experiments. The experiments are quite small scale, which I expect is due to the computational cost of generating the adversarial examples. It is difficult to say how far the findings can be generalized from MNIST to more realistic situations. Tests on another dataset would have been welcomed. Also, while interesting, are adversarial examples that have minimal L_p distance from training examples really that useful in practice?  Of course, it's nice that we can find these, but it could be argued that L_p norms are not a good way of judging the similarity of an adversarial example to a true example. I think it would be more useful to investigate attacks that are perceptually insignificant, or attacks that operate in the physical world, as these are more likely to be a concern for real world systems. In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone. ",14,374,24.933333333333334,5.108333333333333,180,6,368,0.0163043478260869,0.0452127659574468,-0.601,84,53,73,26,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 6, 'MET': 8, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,0,0,2,6,8,3,0,0,0,1,1,0,0,0,0,1,0,0,0,4,2,0,0.4307349185126617,0.3353058821461837,0.21178804330784584
ICLR2018-HkinqfbAb-R1,Reject,"Approach is interesting however my main reservation is with the data set used for experiments and making general (!) conclusions. MNIST, CIFAR-10 are too simple tasks perhaps suitable for debugging but not for a comprehensive validation of quantization/compression techniques. Looking at the results, I see a horrific degradation of 25-43% relative to DC baseline despite being told about only a minimal loss in accuracy. A number of general statements is made based on MNIST data, such as on page 3 when comparing GMM and k-means priors, on page 7 and 8 when claiming that parameter tying and sparsity do not act strongly to improve generalization. In addition, by making a list of all hyper parameters you tuned I am not confident that your claim that this approach requires less tuning. Additional comments:  (a) you did not mention student-teacher training (b) reference to previously not introduced K-means prior at the end of section 1 (c) what is that special version of 1-D K-means? (d) Beginning of section 4.1 is hard to follow as you are referring to some experiments not shown in the paper. (e) Where is 8th cluster hiding in Figure 1b? (f) Any comparison to a classic compression technique would be beneficial. (g) You are referring to a sparsity at the end of page 8 without formally defining it. (h) Can you label each subfigure in Figure 3 so I do not need to refer to the caption? Can you discuss this diagram in the main text, otherwise what is the point of dumping it in the appendix? (i) I do not understand Figure 4 without explanation. ",15,267,26.7,5.042016806722689,150,1,266,0.0037593984962406,0.0148148148148148,-0.9475,74,26,48,18,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 5, 'EXP': 3, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 5, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,0,0,0,2,5,3,1,4,0,0,0,0,0,0,0,0,1,5,0,6,2,0,0.3584220018034265,0.4479071838221941,0.20332454315598444
ICLR2018-HkinqfbAb-R2,Reject,"This is yet another paper on parameter tying and compression of DNNs/CNNs. The key idea here is a soft parameter tying under the K-means regularization on top of which an L1 regularization is further imposed for promoting sparsity. This strategy seems to help the hard tying in a later stage while keeping decent performance. The idea is sort of interesting and the reported experimental results appear to be supportive. However, I have following concerns/comments. 1.  The roles played by K-means and L1 regularization are a little confusing from the paper.  In Eq.3, it appears that the L1 regularization is always used in optimization. However, in Eq.4,  the L1 norm is not included.  So the question is,  in the soft-tying step, is L1 regularization always used?  Or a more general question,  how important is it to regularize the cross-entropy with both K-means and L1? 2. A follow-up question on K-means and L1. If no L1 regularization, does the K-means soft-tying followed by a hard-tying work as well as using the L1 regularization throughout? 3. It would be helpful to say a few words on the storage of the model parameters. 4. It would be helpful to show if the proposed technique work well on sequential models like LSTMs.",14,207,13.8,5.096774193548387,108,2,205,0.0097560975609756,0.02803738317757,0.971,57,21,36,12,3,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 6, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,0,6,0,8,0,1,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0.2163335583914857,0.1148428557201689,0.08680176372813964
ICLR2018-HkinqfbAb-R3,Reject,"As the authors mentioned, weight-sharing and pruning are not new to neural network compression. The proposed method resembles a lot with the deep compression work (Han et. al. 2016), with the distinction of clustering across different layers and a Lasso regularizer to encourage sparsity of the weights.  Even though the change seems minimal, the authors has demonstrated the effectiveness on the benchmark. But the description of the optimization strategy in Section 3 needs some refinement. In the soft-tying stage, why only the regularizer (1) is considered, not the sparsity one? In the hard-tying stage, would the clustering change in each iteration? If not, this has reduced to the constrained problem as in the Hashed Compression work (Chen et. al. 2015) where the regularizer (1) has no effect since the clustering is fixed and all the weights in the same cluster are equal. Even though it is claimed that the proposed method does not require a pre-trained model to initialize, the soft-tying stage seems to take the responsibility to pre-train the model. The experiment section is a weak point. It is much less convincing with no comparison result of compression on large neural networks and large datasets. The only compression result on large neural network (VGG-11) comes with no baseline comparisons. But it already tells something: 1) what is the classification result for reference network without compression? 2) the compression ratio has significantly reduced comparing with those for MNIST. It is hard to say if the compression performance could generalize to large networks. Also, it would be good to have an ablation test on different parts of the objective function and the two optimization stages to show the importance of each part, especially the removal of the soft-tying stage and the L1 regularizer versus a simple pruning technique after each iteration.  This maybe a minor issue, but would be interesting to know: what would the compression performance be if the classification accuracy maintains the same level as that of the deep compression. As discussed in the paper, it is a trade-off between accuracy and compression. The network could be compressed to very small size but with significant accuracy loss. Some minor issues: - In Section 1, the authors discussed a bunch of pitfalls of existing compression techniques, such as large number of parameters, local minimum issues and layer-wise approaches. It would be clearer if the authors could explicitly and succinctly discuss which pitfalls are resolved and how by the proposed method towards the end of the Introduction section. - In Section 4.2, the authors discussed the insensitivity of the proposed method to switching frequency. But there is no quantitative results shown to support the claims. - What is the threshold for pruning zero weight used in Table 2? - There are many references and comparisons missing: Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations in NIPS 17 for instance. This paper also considers quantization for compression which is related to this work.",25,487,19.48,5.349036402569594,215,3,484,0.006198347107438,0.024340770791075,0.4707,137,49,76,18,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 2, 'MET': 14, 'EXP': 7, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 0}",0,1,5,1,2,14,7,1,1,0,0,0,1,0,0,0,0,2,0,0,19,1,0,0.646984857436025,0.3446457297176968,0.3278734952849494
ICLR2018-HkjL6MiTb-R1,Reject,"This paper introduces siamese neural networks to the competing risks framework of Fine and Gray. The authors optimize for the c-index by minimizing a loss function driven by the cumulative risk of competing risk m and correct ordering of comparable pairs. While the idea of optimizing directly for the c-index directly is a good one (with an approximation and with useful complementary loss function terms), the paper leaves something to be desired in quality and clarity. Related works: - For your consideration: is multi-task survival analysis effectively a competing risks model, except that these models also estimate risk after the first competing event (i.e. in a competing risks model the rates for other events simply go to 0 or near-zero)? Please discuss. Also, if the claim is that there are not deep learning survival analyses, please see, e.g. Jing and Smola. - It would be helpful to define t_k explicitly to alleviate determining whether it is the interval time between ordered events or the absolute time since t_0 (it's the latter). Consider calling k a time index instead of t_k a time interval (subject x experiences cause m occurs [sic] in a time interval t_k) - Line after eq 8: do you mean accuracy term? - I would not call Reg a regularization term since it is not shrinking the coefficients . It is a term to minimize a risk not a parameter . - You claim to adjust for event imbalance and time interval imbalance but this is not mathematically shown nor documented in the experiments. - The results show only one form of comparison, and the results have confidence intervals that overlap with at least one competing method in all tasks.",12,275,22.916666666666668,5.081712062256809,153,0,275,0.0,0.0070671378091872,0.8912,81,28,53,15,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 2, 'CLA': 1}",0,1,10,1,0,4,4,1,0,2,0,0,1,0,0,0,2,2,0,1,7,2,1,0.5731868456471236,0.6706702869896838,0.40945004101840643
ICLR2018-HkjL6MiTb-R2,Reject,"The authors tackle the problem of estimating risk in a survival analysis setting with competing risks. They propose directly optimizing the time-dependent discrimination index using a siamese survival network. Experiments on several real-world dataset reveal modest gains in comparison with the state of the art. - The authors should clearly highlight what is their main technical contribution. For example, Eqs. 1-6 appear to be background material since the time-dependent discrimination index is taken from the literature, as the authors point out earlier . However, this is unclear from the writing.  - One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks. It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines. In other words, the authors oversell their own work, specially in comparison with the state of the art. - The authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty there. The application/setting may be novel, but not the architecture of choice. - From Eq. 4 to Eq. 5, the authors argue that the denominator does not depend on the model parameters and can be ignored. However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values. This could be problematic if the risks are unbalanced. - The competitive gain of the authors method in comparison with other competing methods is minor. - The authors introduce F(t, D | x) as cumulative incidence function (CDF) at the beginning of section 2, however, afterwards they use R^m(t, x), which they define as risk of the subject experiencing event m before t. Is the latter a proxy for the former? How are they related?",18,302,15.1,5.380622837370242,151,1,301,0.0033222591362126,0.0385852090032154,-0.782,88,37,51,14,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 15, 'PDI': 4, 'DAT': 1, 'MET': 5, 'EXP': 8, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 2}",0,0,15,4,1,5,8,0,0,1,0,0,0,1,0,3,1,3,0,0,12,0,2,0.5029088692160425,0.5628027626930986,0.3125068949755295
ICLR2018-HkjL6MiTb-R3,Reject,"The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis. The author follow the reasoning that '... these ideas were not explored in the context of survival analysis', thereby disregarding the significant published literature based on the Concordance Index (CI).  Besides this deficit, the paper does not present a proper statistical setup (e.g. 'Is censoring assumed to be at random? ...) , and numerical results are only referring to some standard implementations, thereby again neglecting the state-of-the-art solution.  That being said, this particular use of deep learning in this context might be novel.",4,101,16.833333333333332,5.595959595959596,72,1,100,0.01,0.0188679245283018,-0.3237,28,14,19,6,7,3,"{'ABS': 1, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",1,0,2,1,0,1,0,1,0,1,0,2,0,0,0,1,2,0,0,0,1,0,0,0.5001259067104346,0.3333813699817812,0.2519859483893066
ICLR2018-HklZOfW0W-R1,Reject,"Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper. A projected sub-gradient descent algorithm is used. The UPS optimizer by itself is not new. Graph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter. First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points.  There are some concerns about this network that need to be clarified: 1. sigma is never clarified in the main context or experiments 2. the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random. 3. another explanation about the weights as the rescaling to matrix A needs to further clarified. As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction. What is the disagreement between L1 penalty and prediction quality? Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A? 4. Authors stated that the last step is to build a mapping from the GPS features into the response Y. They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features. However, no detailed information is given in the paper. In the experiments, authors only stated that ""we fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph"", and then the graph is used to train existing models as the input of the graph. Which architecture is used for building the mapping ? In the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified. For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on. More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm.",19,347,17.35,5.024767801857585,160,1,346,0.0028901734104046,0.0114613180515759,0.9527,98,34,60,13,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 14, 'PDI': 7, 'DAT': 0, 'MET': 9, 'EXP': 6, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 11, 'SUB': 3, 'CLA': 2}",0,0,14,7,0,9,6,1,2,0,0,0,0,0,0,0,0,1,0,0,11,3,2,0.4324293843030822,0.4509585068803711,0.24688341683486353
ICLR2018-HklZOfW0W-R2,Reject,"There are many language issues rendering the text hard to understand, e.g., -- in the abstract: several convolution on graphs architectures -- in the definitions: Let data with N observation (no verb, no plural, etc). -- in the computational section: Training size is 9924 and testing is 6695.   so part of my negative impression may be pure mis-understanding of what the authors had to say. Still, the authors clearly utilise basic concepts (c.f. utilize eigenvector  basis of the graph Laplacian to do filtering in the Fourier domain) in ways that do not seem to have any sensible interpretation whatsoever, even allowing for the mis-understanding due to grammar. There are no clear insight,  no theorems, and an empirical evaluation on an ill-defined problem in  time-series forecasting. (How does it relate to graphs? What is the graph  in the time series or among the multiple time series? How do the authors implement the other graph-related approaches in this problem featuring time series?) My impression is hence that the only possible outcome is  rejection.",9,168,24.0,5.078787878787879,108,1,167,0.0059880239520958,0.0337078651685393,-0.9331,48,21,29,5,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 3, 'DAT': 0, 'MET': 0, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 2}",0,0,5,3,0,0,1,0,0,0,0,1,1,1,0,0,1,0,0,0,4,0,2,0.429011979904676,0.3352803407087163,0.21337630973165084
ICLR2018-HklZOfW0W-R3,Reject,"The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph. Using this scheme, they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problems. Pros:  -Authors provide some empirical evidence for the benefits of using their technique. -Authors are fairly upfront about how, overall, it seems their technique isn't doing *too* much--null results are still results, and it would be interesting to better understand *why* learning a better graph for these networks doesn't help very much. Cons:  -The grammar in the paper is pretty bad. It could use a couple more passes with an editor. -For a, more or less, entirely empirical paper, the choices of experiments are...somewhat befuddling. Considerably more details on implementation, training time/test time, and even just *more* experiment domains would do this paper a tremendous amount of good. -While I mentioned it as a pro, it also seems to be that this technique simply doesn't buy you very much as a practitioner. If this is true--that learning better graph representations really doesn't help very much, that would be good to know, and publishable, but actually *establishing* that requires considerably more experiments. Ultimately, I will have to suggest rejection, unless the authors considerably beef up their manuscript with more experiments, more details, and improve the grammar considerably.",11,236,21.454545454545453,5.388392857142857,133,3,233,0.0128755364806866,0.0210084033613445,0.7415,61,31,46,33,6,7,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 4, 'DAT': 0, 'MET': 0, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 7, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 7, 'SUB': 5, 'CLA': 1}",0,0,2,4,0,0,5,2,0,0,0,7,0,1,0,1,1,0,1,1,7,5,1,0.4295028139672274,0.7819362284201214,0.33817249927765525
ICLR2018-HklpCzC6--R1,Reject,"The paper proposes an image segmentation method which iteratively refines the semantic segmentation mask obtained from a deep net. To this end the authors investigate a denoising auto-encoder (DAE). Its purpose is to provide a semantic segmentation which improves upon its input in terms of the log-likelihood. More specifically, the authors `propose to condition the autoencoder with an additional input' (page 1). To this end they use features obtained from the deep net. Instead of training the DAE with ground truth y, the authors found usage of the deep net prediction to yield better results. The proposed approach is evaluated on the CamVid dataset. Summary: u2014u2014 I think the paper discusses a very interesting topic and presents an elegant approach. A few points are missing which would provide significantly more value to a reader. Specifically, an evaluation on the classical Pascal VOC dataset, details regarding the training protocol of the baseline (which are omitted right now), an assessment regarding stability of the proposed approach (not discussed right now), and a clear focus of the paper on segmentation or conditioning. See comments below for details and other points. Comments: u2014u2014 1. When training the DAE, a combination of squared loss and categorical cross-entropy loss is used. What's the effect of the squared error loss and would the categorical cross-entropy on its own be sufficient?  This question remains open when reading the submission. 2. The proposed approach is evaluated on the CamVid dataset which is used less compared to the standard and larger Pascal VOC dataset. I conjecture that the proposed approach wouldn't work too well on Pascal VOC. On Pascal VOC, images are distinctly different from each other whereas subsequent frames are similar in CamVid, i.e., the road is always located at the bottom center of the image. The proposed architecture is able to take advantage of this dataset bias, but would fail to do so on Pascal VOC, which has a much more intricate bias. It would be great if the authors could check this hypothesis and report quantitative results similar to Tab. 1 and Fig. 4 for Pascal VOC. 3. The authors mention a grid-search for the stepsize and the number of iterations. What values were selected in the end on the CamVid and hopefully the Pascal VOC dataset? 4. Was the dense CRF applied out of the box, or were its parameters adjusted for good performance on the CamVid validation dataset? While parameters such as the number of iterations and epsilon are tuned for the proposed approach on the CamVid validation set, the submission doesn't specify whether a similar procedure was performed for the CRF baseline. 5. Fig. 4 seems to indicate that the proposed approach doesn't converge. Hence an appropriate stepsize and a reasonable number of iterations need to be chosen on a validation set. Choosing those parameters guarantees that the method performs well on average, but individual results could potentially be entirely wrong, particularly if large step sizes are chosen. I suspect this effect to be more pronounced on the Pascal VOC dataset (hence my conjecture in point 2). To further investigate this property, as a reader, I'd be curious to get to know the standard deviation/variance of the accuracy in addition to the mean IoU. Again, it would be great if the authors could check this hypothesis and report those results. 6. I find the experimental section to be slightly disconnected from the initial description. Specifically, the paper `proposes to condition the autoencoder with an additional input' (page 1). No experiments are conducted to validate this proposal. Hence the main focus of the paper (image segmentation or DAE conditioning) remains vague. If the authors choose to focus on image segmentation, a comparison to state-of-the-art should be provided on classical datasets such as Pascal VOC, if DAE conditioning is the focus, some experiments in this direction should be included in addition to the Pascal VOC results. Minor comment: u2014u2014 - I find it surprising that the authors choose not to cite some related work on combining deep nets with structured prediction.",34,674,16.4390243902439,5.2168486739469575,268,2,672,0.0029761904761904,0.0251479289940828,0.9802,185,81,121,34,9,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 7, 'DAT': 8, 'MET': 13, 'EXP': 7, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 0}",0,0,2,7,8,13,7,3,1,0,0,1,0,3,0,0,0,0,1,0,19,1,0,0.6471956051473169,0.3445285671605068,0.3276772772897108
ICLR2018-HklpCzC6--R2,Reject,"I am a returning reviewer for this paper, from a previous conference. Much of the paper remains unchanged from the time of my previous review. I have revised my review according to the updates in the paper: Summary of the paper: This work proposes a neural network based alternative to standard CRF post-processing techniques that are generally used on top semantic segmentation CNNs. As an alternative to CRF, this work proposes to iteratively refine the predicted segmentation with a denoising auto encoder (DAE).  Results on CamVid semantic segmentation dataset showed better improvements over base CNN predictions in comparison to popular DenseCRF technique. Paper Strengths: - A neat technique for incorporating CRF-like pixel label relations into semantic segmentation via neural networks (auto encoders). - Promising results on CamVid segmentation dataset with reliable improvements over baseline techniques and minor improvements when used in conjunction with recent models. Major Weaknesses: I have two main concerns for this work: - One is related to the novelty as the existing work of Xie et al. ECCV'16 also proposed similar technique with very similar aim.  I think, conceptual or empirical comparisons are required to assess the importance of the proposed approach with respect to existing ones. Mere citation and short discussion is not enough. Moreover, Xie et al. seem to have demonstrated their technique on two different tasks and on three different datasets. - Another concern is related to experiments. Authors experimented with only one dataset and with one problem. But, I would either expect some demonstration of generality (more datasets or tasks) or strong empirical performance (state-of-the-art on CamVid) to assess the empirical usefulness with respect to existing techniques. Both of these aspects are missing in experiments. Minor Weaknesses: - Negligible improvements with respect to CRF techniques on modern deep architectures. - Runtime comparison is missing with respect to baseline techniques. Applying the proposed DAE 40-50 times seems very time consuming for each image. - By back-propagating through CRF-like techniques [Zheng et al. ICCV'15, Gadde et al. ECCV'16, Chandra et al. ECCV'16 etc.], one could refine the base segmentation CNN as well. It seems this is also possible with the proposed architecture. Is that correct? Or, are there any problems with the end-to-end fine-tuning as the input distribution to DAE constantly changes? Did authors try this? Suggestions: - Only Gaussian noise corruption is used for training DAE. Did authors experiment with any other noise types? Probably, more structured noise would help in learning better contextual relations across pixel labels? Clarifications: What is the motivation to add Euclidean loss to the standard cross-entropy loss for segmentation in Eq-3? Review summary: The use of denoising auto encoders (DAEs) for capturing pixel label relations and then using them to iteratively refine the segmentation predictions is interesting. But, incomplete comparisons with similar existing work and limited experiments makes this a weak paper.",31,465,16.607142857142858,5.610132158590308,225,4,461,0.0086767895878524,0.0168421052631578,0.9817,163,63,74,20,9,3,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 2, 'DAT': 5, 'MET': 15, 'EXP': 7, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 4, 'CLA': 0}",0,0,5,2,5,15,7,2,0,0,0,1,1,3,0,0,0,2,0,0,19,4,0,0.6474769796603695,0.3449657592426612,0.3306984372098615
ICLR2018-HklpCzC6--R3,Reject,"This paper proposes an iterative procedure on top of a standard image semantic segmentation networks. The submission proposes a change to the training procedure of stacking a denoising auto-encoder for image segmentation. The technical contribution of this paper is small. The paper aims to answer a single question: When using a DAE network on top of a segmentation network output, should one condition on the predicted, or the ground truth segmentation? (why not on both?) The answer is conditioning on the predicted image for a second round of inference is a bit better. The method also performs a bit better (no statistical significance tests) than other post-processing methods (Dense-CRF, CRF-RNNs) Experimental results are available only on a small dataset and for two different networks. This may be sufficient for a first proof-of-concept but a comparison against standard benchmark methods and datasets for semantic segmentation is missing. It is unlikely that in the current state of this submission is a contribution to image segmentation, evidence is weak and several improvements are suggested. - The experimental evidence is insufficient. The improvements are small, statistical tests are not available. The CamVid dataset is the smallest of the image segmentation datasets used these days, more compelling would be MSCOCO or Cityscapes, better most of them. The question whether this network effect is tied to small-dataset and low-resolution is not answered. Will a similar effect be observed when compared to networks trained on way more data (e.g., CityScapes)? - The most important baseline is missing: auto-context [Tu08]. Training the same network the DAE uses in an auto-context way. That is, take the output of the first model, then train another network using both input and prediction again for semantic segmentation (and not Eq.3). This is easy to do, practically almost always achieves better performance and I would assume the resulting network is faster and performs similar to the method presented in this submission on (guessing, I have not tried). In any case, to me this is the most obvious baseline. - I am in favour of probabilistic methods, but the availability of an approximation of p(y) (or the nearest mode) is not used (as is most often the case). - Runtimes are absent. This is a practical consideration which is important especially if there is little technological improvement. The DAE model of this submission compares to simple filtering methods as Kru00e4henbu00fchl&Koltun DenseCRF which are fast and performance results are comparable. The question wether this is practically relevant is missing, judging from the construction I guess this does not fare well.  Also training time is significantly more, please comment. - The related work is very well written, thanks. This proposal is conceptually very similar to auto-context [Tu08] and this reference missing (this is also the most important baseline) [Tu08] Tu, ""Auto-context and its application to high-level vision tasks"", CVPR 2008   ",26,469,19.541666666666668,5.412026726057906,210,3,466,0.0064377682403433,0.0355648535564853,0.9892,137,67,83,33,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 4, 'DAT': 2, 'MET': 9, 'EXP': 8, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 5, 'CLA': 1}",0,0,3,4,2,9,8,2,0,1,0,3,0,0,0,0,0,2,0,0,13,5,1,0.5745549845198488,0.452451802253036,0.3206925095563165
ICLR2018-HkmaTz-0W-R1,Reject,"This paper provides visualizations of different deep network loss surfaces using 2D contour plots, both at minima and along optimization trajectories. They mention some subtle details that must be taken into account, such as scaling the plot axes by the filter magnitudes, in order to obtain correctly scaled plots. Overall, I think there is potential with this work but it feels preliminary. The visualizations are interesting and provide some general intuition, but they don't yield any clear novel insights that could be used in practice. Also, several parts of the paper spend too much time on describing other work or on implementation details which could be moved to the appendix. General Comments: - I think Sections 2, 3, 4 are too long, we only start getting to the results section at the end of page 4. I suggest shortening Section 2, and it should be possible to combine Sections 3 and 4 into a page at most. 1D interpolations and 2D contour plots can be described in a few sentences each. - I think Section 5 can be put in the Appendix - it's essentially an illustration of why the weight scaling is important.  Once these details are done correctly, the experiments support the relatively well-accepted hypothesis that flat minima generalize better.  - The plots in Section 6 are interesting, it would be nice if the authors had an explanation of why the loss surface changes the way it does when skip connections are added. - In Section 7, it's less useful to spend time describing what happens when the visualization is done wrong (i.e. projecting along random directions rather than PCA vectors) -  this can be put in the Appendix. I would suggest just including the visualizations of the optimization trajectories which are done correctly and focus on deriving interesting/useful conclusions from them.",13,298,21.285714285714285,5.205035971223022,164,3,295,0.0101694915254237,0.0358306188925081,0.9635,75,29,64,15,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 4, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,8,0,0,1,5,1,0,0,0,5,3,0,0,1,4,0,0,0,5,0,0,0.429686088296828,0.3359652730180489,0.21340453055766118
ICLR2018-HkmaTz-0W-R2,Reject,"The main concern of this submission is the novelty. Proposed method to visualize the loss function sounds too incremental from existing works. One of the main distinctions is using filter-wise normalization, but it is somehow trivial. In experiments, no comparisons against existing works is performed (at least on toy/controlled environments). Some findings in this submission indeed look interesting, but it is not clear if those results are something difficult to find with other existing standard ways, or even how reliable they are since the effectiveness has not been evaluated. Minor comments:  In introduction, parameter with zero training error doesn't mean it's a global minimizer In section 2, it is not clear that visualizing loss function is helpful in see the reasons of generalization given minima. In figure 2, why do we have solutions at 0 for small batch size and 1 for large batch size case? (why should they be different?)",10,151,21.571428571428573,5.219178082191781,105,0,151,0.0,0.0460526315789473,-0.8739,37,20,32,8,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 9, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 3, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 2}",0,0,9,1,0,1,3,1,1,0,0,1,0,0,0,1,3,1,0,0,4,1,2,0.5008162948024892,0.6687097473389455,0.34340017997532923
ICLR2018-HkmaTz-0W-R3,Reject,"* In the flat vs sharp dilemma, the experiments display that the dilemma, if any, is subtle. Table 1 does not necessarily contradict this view.  It would be a good idea to put the test results directly on Fig. 4 as it does not ease reading currently (and postpone ResNet-56 in the appendix). How was Figure 5 computed ? It is said that *a* random direction was used from each minimiser to plot the loss, so how the 2D directions obtained ? * On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets. The difference VGG - ResNets is also interesting, but it would have been interesting to see how this affects the current state of the art in understanding deep learning, something that was done for the flat vs sharp dilemma, but is lacking here. For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ? * On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted. There is however a phenomenon I would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller effective d', you only have to figure out a generating system for this subspace and carry out optimisation inside). Can this be related to the flat vs sharp dilemma ? I would suppose that flatness tends to increase the variability captured by leading eigenvectors ? Typoes:  Legend of Figure 2: red lines are error -> red lines are accuracy Table 1: test accuracy -> test error Before 6.2: architecture effects -> architecture affects",13,318,31.8,5.01,161,2,316,0.0063291139240506,0.0180722891566265,0.9788,90,34,62,17,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 12, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 7, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,0,12,0,0,7,7,1,4,0,0,0,1,0,0,1,3,0,1,0,6,1,1,0.4315450168127324,0.6698725271377772,0.304652587312534
ICLR2018-Hkn7CBaTW-R1,Accept,"summary of article:  This paper organizes existing methods for understanding and explaining deep neural networks into three categories based on what they reveal about a network: functions, signals, or attribution. ""The function extracts the signal from the data by removing the distractor. The attribution of output values to input dimensions shows how much an individual component of the signal contributes to the output..."" (p. 5).  The authors propose a novel quality criterion for signal estimators, inspired by the analysis of linear models. They also propose two new explanatory methods, PatternNet (for signal estimation) and PatternAttribution (for relevance attribution), based on optimizing their new quality criterion. They present quantitative and qualitative analyses comparing PatternNet and PatternAttribution to several existing explanation methods on VGG-19. * Quality: The claims of the paper are well supported by quantitative results and qualitative visualizations. * Clarity: Overall the paper is clear and well organized. There are a few points that could benefit from clarification. * Originality: The paper puts forth an original framing of the problem of explaining deep neural networks. Related work is appropriately cited and compared. The authors's quality criterion for signal estimators allows them to do a quantitative analysis for a problem that is often hard to quantify. * Significance: This paper justifies PatternNet and PatternAttribution as good methods to explain predictions made by neural networks. These methods may now serve as an important tool for future work which may lead to new insights about how neural networks work. Pros: * Helps to organize existing methods for understanding neural networks in terms of the types of descriptions they provide: functions, signals or attribution. * Creative quantitative analyses that evaluate their signal estimator at the level of single units and entire networks. Cons: * Experiments consider only the pre-trained VGG-19 model trained on ImageNet. Results may not generalize to other architectures/datasets. * Limited visualizations are provided. Comments: * Most of the paper is dedicated to explaining these signal estimators and quality criterion in case of a linear model. Only one paragraph is given to explain how they are used to estimate the signal at each layer in VGG-19. On first reading, there are some ambiguities about how the estimators scale up to deep networks. It would help to clarify if you included the expression for the two-component estimator and maybe your quality criterion for an arbitrary hidden unit. * The concept of signal is somewhat unclear.  Is the signal      * (a) the part of the input image that led to a particular classification, as described in the introduction and suggested by the visualizations, in which case there is one signal per image for a given trained network?   *  (b) the part of the input that led to activation of a particular unit, as your unit wise signal estimators are applied, in which case there is one signal for every unit of a trained network? You might benefit from two terms to separate the unit-level signal (what caused the activation of a particular unit?) from the total signal (what caused all activations in this network?). * Assuming definition (b) I think the visualizations would be more convincing if you showed the signal for several output units. One would like to see that the signal estimation is doing more than separating foreground from background but is actually semantically specific. For instance, for the mailbox image, what does the signal look like if you propagate back from only the output unit for umbrella compared to the output unit for mailbox?  * Do you have any intuition about why your two-component estimator doesn't seem to be working as well in the convolutional layers?  Do you think it is related to the fact that you are averaging within feature maps? Is it strictly necessary to do this averaging? Can you imagine a signal estimator more specifically designed for convolutional layers? Minor issues:  * The label Figure 4 is missing. Only subcaptions (a) and (b) are present. * Color scheme of figures: Why two oranges? It's hard to see the difference.",38,655,21.12903225806452,5.38267716535433,273,8,647,0.0123647604327666,0.018978102189781,0.9739,189,82,119,24,10,7,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 5, 'DAT': 2, 'MET': 22, 'EXP': 4, 'RES': 0, 'TNF': 4, 'ANA': 5, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 2, 'CMP': 2, 'PNF': 5, 'REC': 0, 'EMP': 16, 'SUB': 1, 'CLA': 3}",0,1,1,5,2,22,4,0,4,5,1,3,0,0,0,2,2,2,5,0,16,1,3,0.7203312352583209,0.7877259919274998,0.5716112028249675
ICLR2018-Hkn7CBaTW-R2,Accept,"The authors analyze show theoretical shortcomings in previous methods of explaining neural networks and propose an elegant way to remove these shortcomings in their methods PatternNet and PatternAttribution. The quest of visualizing neural network decision is now a very active field with many contributions. The contribution made by the authors stands out due to its elegant combination of theoretical insights and improved performance in application. The work is very detailed and reads very well. I am missing at least one figure with comparison with more state-of-the-art methods (e.g. I would love to see results from the method by Zintgraf et al. 2017 which unlike all included prior methods seems to produce much crisper visualizations and also is very related because it learns from the data, too). Minor questions and comments: * Fig 3: Why is the random method so good at removing correlation from fc6?  And the S_w even better? Something seems special about fc6. * Fig 4: Why is the identical estimator better than the weights estimator and that one better than S_a? * It would be nice to compare the image degradation experiment with using the ranking provided by the work from Zintgraf which should by definition function as a kind of gold standard * Figure 5, 4th row (mailbox): It looks like the umbrella significantly contributes to the network decision to classify the image as mailbox which doesn't make too much sense. Is is a problem of the visualization  (maybe there is next to no weight on the umbrella), of PatternAttribution or a strange but interesting a artifact of the analyzed network? * page 8 ... closed form solutions (Eq (4) and Eq. (7)) The first reference seems to be wrong. I guess Eq 4. should instead reference the unnumbered equation after Eq. 3. n Update 2018-01-12: Upgraded Rating from 7 to 8 (see comment below)",15,303,18.9375,5.148936170212766,171,5,298,0.0167785234899328,0.0353697749196141,0.9425,80,42,46,20,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 0, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 9, 'SUB': 1, 'CLA': 2}",0,0,1,3,0,7,2,0,2,1,0,2,0,0,0,0,0,0,1,1,9,1,2,0.5017746649668854,0.5606123501051533,0.31212186969379235
ICLR2018-Hkn7CBaTW-R3,Accept,"I found this paper an interesting read for two reasons: First, interpretability is an increasingly important problem as machine learning models grow more and more complicated. Second, the paper aims at generalization of previous work on confounded linear model interpretation in neuroimaging (the so-called filter versus patterns problem). The problem is relevant for discriminative problems: If the objective is really to visualize the generative process,  the filters learned by the discriminative process need to be transformed to correct for spatial correlated noise. Given the focus on extracting visualization of the generative process, it would have been meaningful to place the discussion in a greater frame of generative model deep learning (VAEs, GANs etc etc). At present the state of the art discussion appears quite narrow, being confined to recent methods for visualization of discriminative deep models. The authors convincingly demonstrate for the linear case, that their PatternNet mechanism can produce the generative process (i.e. discard spatially correlated distractors). The PatternNet is generalized to multi-layer ReLu networks by construction of node-specific pattern vectors and back-propagating these through the network. The proof (eqs. 4-6) is sketchy and involves uncontrolled approximations. The back-propagation mechanism is very briefly introduced and depicted in figure 1. Yet, the results are rather convincing. Both the anecdotal/qualitative examples and the more quantitative patch elimination experiment figure 4a (?number missing)  I do not understand the remark: However, our method has the advantage that it is not only applicable to image models but is a generalization of the theory commonly used in neuroimaging Haufe et al. (2014).   what ??  Overall, I appreciate the general idea. However, the contribution could have been much stronger based on a detailed derivation with testable assumptions/approximations, and if based on a clear declaration of the aim.  ",14,290,17.058823529411764,5.85,179,1,289,0.0034602076124567,0.0067114093959731,0.8966,85,45,45,19,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 6, 'DAT': 0, 'MET': 8, 'EXP': 0, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 1, 'CLA': 0}",0,0,2,6,0,8,0,1,2,0,0,1,0,0,0,0,0,1,1,0,11,1,0,0.4307230585402906,0.4506640187928741,0.24700731149454416
ICLR2018-HknbyQbC--R1,Reject,"I thank the authors for the thoughtful response and rebuttal.  The authors have substantially updated their manuscript and improved the presentation. Re: Speed. I brought up this point because this was a bulleted item in the Introduction in the earlier version of the manuscript. In the revised manuscript, this bullet point is now removed. I will take this point to be moot. Re: High resolution. The authors point to recent GAN literature that provides some first results with high resolution GANs but I do not see quantitative evidence in the high resolution setting for this paper. (Figure 4 provides qualitative examples from ImageNet but no quantitative assessment. )  Because the authors improved the manuscript, I upwardly revised my score to 'Ok but not good enough - rejection'. I am not able to accept this paper because of the latter point.                             The authors present an interesting new method for generating adversarial examples.  Namely, the author train a generative adversarial network (GAN) to adversarial examples for a target network. The authors demonstrate that the network works well in the semi-white box and black box settings. The authors wrote a clear paper with great references and clear descriptions. My primary concern is that this work has limited practical benefit in a realistic setting. Addressing each and every concern is quite important:  1) Speed. The authors suggest that training a GAN provides a speed benefit with respect to other attack techniques. The FGSM method (Goodfellow et al, 2015) is basically 1 inference operation and 1 backward operation. The GAN is 1 forward operation. Granted this results in a small difference in timing 0.06s versus 0.01s, however it would seem that avoiding a backward pass is a somewhat small speed gain. Furthermore, I would want to question the practical usage of having an 'even faster' method for generating adversarial examples. What is the reason that we need to run adversarial attacks 'even faster'? I am not aware of any use-cases, but if there are some, the authors should describe the rationales at length in their paper. 2) High spatial resolution images.  Previous methods, e.g. FGSM, may work on arbitrarily sized images. At best, GANs generate reasonable images that are lower resolutions (e.g. < 128x128). Building GAN's that operate above-and-beyond moderate spatial resolution is an open research topic. The best GAN models for generating high resolution images are  difficult to train and it is not clear if they would work in this setting. Furthermore, images with even higher resolutions, e.g. 512x512, which is quite common in ImageNet, are difficult to synthesizes using current techniques. 3) Controlling the amount of distortion. A feature of previous optimization based methods is that a user may specify the amount of perturbation (epsilon). This is a key feature if not requirement in an adversarial perturbation because a user might want to examine the performance of a given model as a function of epsilon. Performing such an analysis with this model is challenging (i.e. retraining a GAN) and it is not clear if a given image generated by a GAN will always achieve a given epsilon perturbation/  On a more minor note, the authors suggest that generating a *diversity* of adversarial images is of practical import.  I do not see the utility of being able to generate a diversity of adversarial images. The authors need to provide more justification for this motivation.",35,557,14.282051282051285,5.326848249027237,239,3,554,0.0054151624548736,0.035234899328859,0.7637,164,73,97,26,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 31, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 8, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 6, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 2}",0,0,31,1,0,11,8,1,1,0,0,4,0,0,0,0,6,0,1,0,13,1,2,0.5056561851754787,0.5633403630867648,0.31543808736368933
ICLR2018-HknbyQbC--R2,Reject,"This paper describes AdvGAN, a conditional GAN plus adversarial loss. AdvGAN is able to generate adversarial samples by running a forward pass on generator. The authors evaluate AdvGAN on semi-white box and black box setting. AdvGAN is a simple and neat solution to for generating adversary samples. The author also reports state-of-art results. Comment:  1. For MNIST samples, we can easily find the generated sample is a mixture of two digitals. Eg, for digital 7 there is a light gray 3 overlap.  I am wondering this method is trying to mixture several samples into one to generate adversary samples. For real color samples, it is harder to figure out the mixture. 2. Based on mixture assumption, I suggest the author add one more comparison to other method, which is relative change from original image, to see whether AdvGAN is the most efficient model to generate the adversary sample (makes minimal change to original image).    ",10,154,11.846153846153848,5.181818181818182,90,0,154,0.0,0.025,0.5709,42,28,29,5,7,6,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 5, 'DAT': 5, 'MET': 6, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 2, 'EMP': 5, 'SUB': 2, 'CLA': 1}",0,2,3,5,5,6,1,1,0,0,0,0,0,0,0,1,0,2,0,2,5,2,1,0.5018190177100037,0.6693981359437151,0.35165045903982367
ICLR2018-HknbyQbC--R3,Reject,"The paper proposes a way of generating adversarial examples that fool classification systems. They formulate it for a blackbox and a semi-blackbox setting (semi being, needed for training their own network, but not to generate new samples). n The model is a residual gan formulation, where the generator generates an image mask M, and (Input + M) is the adversarial example. nThe paper is generally easy to understand and clear in their results. I am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist . From this paper's literature survey, they dont exist. So this paper is innovative in two parts: - it applies GANs to adversarial example generation n- the method is a simple feed-forward network, so it is very fast to compute n The experiments are pretty robust, and they show that their method is better than the proposed baselines . I am not sure if these are complete baselines or if the baselines need to cover other methods (again, not fully familiar with all literature here). ",10,172,19.11111111111111,5.125,102,0,172,0.0,0.0621468926553672,0.8576,41,22,35,15,7,3,"{'ABS': 0, 'INT': 3, 'RWK': 5, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 6, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,3,5,2,0,7,2,3,0,0,0,0,0,1,0,0,6,0,0,0,6,0,1,0.5021310415898582,0.3366833037497876,0.2430816841802324
ICLR2018-Hko85plCW-R1,Accept,"This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention. The paper is very well written and easy to follow . The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger. The first set of questions is about the monotonic attention. Training the monotonic attention with expected context vectors is intuitive, but can this be justified further? For example, how far does using the expected context vector deviate from marginalizing the monotonic attention?  The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention . How does the greedy step affect training and decoding? It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding . These questions should really be answered in [1] . Since the authors are extending their work and since these issues might cause training difficulties, it might be useful to look into these design choices .  The second question is about the window size $w$ . Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper, why not attend to the entire segment, i.e., from the current boundary to the previous boundary ?  It is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2. (The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.) How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW ?  For the experiments, it is intriguing to see that $w 2$ works best for speech recognition . If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention?  The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information. Would the special cases lead to worse performance and if so why is there a difference? n [1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017",23,376,26.857142857142858,4.991525423728813,184,3,373,0.0080428954423592,0.0179487179487179,0.9825,104,43,63,19,11,6,"{'ABS': 0, 'INT': 2, 'RWK': 16, 'PDI': 3, 'DAT': 1, 'MET': 11, 'EXP': 8, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 4, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 13, 'SUB': 3, 'CLA': 3}",0,2,16,3,1,11,8,1,1,1,0,0,1,1,0,0,4,0,1,1,13,3,3,0.7901710145828799,0.6746498889884771,0.5605680066292413
ICLR2018-Hko85plCW-R2,Accept,"The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position. Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model. In terms of the model this is a relatively small extention of Raffel et al 2017. Results show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model.  Is the offline attention baseline unidirectional or bidirectional? In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model. My concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model. Especially given that w 2 works best for speech recognition, it not clear that the model extension is actually helping. My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer. While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement. For document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this. If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper. Sentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here? I like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model. ---  The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices). While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper.",16,406,29.0,5.061068702290076,180,2,404,0.0049504950495049,0.0391198044009779,0.9942,109,52,62,27,8,4,"{'ABS': 0, 'INT': 0, 'RWK': 11, 'PDI': 6, 'DAT': 0, 'MET': 7, 'EXP': 6, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 0, 'CLA': 2}",0,0,11,6,0,7,6,3,0,1,0,1,3,0,0,0,3,0,0,1,7,0,2,0.5745982711980349,0.4483533974212521,0.3182714805052609
ICLR2018-Hko85plCW-R3,Accept,"This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window). Pros. - the paper is clearly written. n- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments) . Cons. - in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al), but it shows significant gains from it. - in terms of considering a monotonic alignment, Hori et al, Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM,  in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods. The paper should also discuss this method in Section 4. n Comments: - Eq. (16): $j$ in the denominator should be $t_j$. ",9,148,13.454545454545457,5.517985611510792,88,0,148,0.0,0.0193548387096774,0.872,46,25,20,5,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,2,6,2,0,6,1,0,0,1,0,0,1,0,0,1,1,1,0,0,7,0,1,0.5017382188720144,0.5592873001646134,0.3167177405219233
ICLR2018-Hkp3uhxCW-R1,Reject,"*Summary*  The paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs). The paper first considers the Bayes by Backprop approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model. Several experiments demonstrate the quality of the prediction and the uncertainty over dropout.  *Originality + significance*  To my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs. However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application.  Nevertheless, the parameterisation of the conditional variational distribution q(theta | phi, (x, y)) using recognition model is interesting and could be useful in other models. However, this has not been tested or concretely shown in this paper. The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example.  *Clarity*  The paper is, in general, well-written. However, the presentation in 4 is hard to follow. I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain phi, a variational approximation over both theta and phi is needed, and a q that couples theta, phi and and the gradient of the log likelihood term wrt phi is chosen. Additional comments:  Why is the variational approximation called sharpened?  At test time, normal VI just uses the fixed q(theta) after training. It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(theta | phi, x) in eqs. 19-20 parameterised? The first paragraph of page 5 uses q(theta | phi, (x, y)), but y is not known at test time. What is C in eq. 9? This comment variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty... is not precise. EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs. In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational. On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the Two problems with variational EM...  paper by Turner and Sahani (2010). There are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy? Was there any KL reweighting scheduling as done in the original BBB paper? What is the significance of the difference between BBB and BBB with sharpening in the language modelling task? Was sharpening used in the image caption generation task? What is the computational complexity of BBB with posterior sharpening? Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer? Would be interesting to see the time/accuracy frontier.",25,520,22.60869565217392,5.238476953907815,241,0,520,0.0,0.0167910447761194,0.9362,153,68,94,27,9,7,"{'ABS': 0, 'INT': 1, 'RWK': 12, 'PDI': 11, 'DAT': 0, 'MET': 10, 'EXP': 11, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 4, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 4, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 2}",0,1,12,11,0,10,11,0,0,1,0,2,4,1,0,1,4,1,2,0,6,2,2,0.6475317931656354,0.7812810554003151,0.511658752806845
ICLR2018-Hkp3uhxCW-R2,Reject,This paper proposes an interesting variational posterior approximation for the weights of an RNN. The paper also proposes a scheme for assessing the uncertainty of the predictions of an RNN. pros: --I liked the posterior sharpening idea. It was well motivated from a computational cost perspective hence the use of a hierarchical prior. --I liked the uncertainty analysis. There are many works on Bayesian neural networks but they never present an analysis of the uncertainty introduced in the weights. These works can benefit from the uncertainty analysis scheme introduced in this paper. --The experiments were well carried through. cons: --Change the title! the title is too vague.  Bayesian recurrent neural networks already exist and is rather vague for what is being described in this paper. --There were a lot of unanswered questions:  (1) how does sharpening lead to lower variance? This was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment section (2) how is the level of uncertainty related to performance? It would have been insightful to see effect of sigma_0 on the performance rather than report the best result.  (3) what was the actual computational cost for the BBB RNN and the baselines? --There were very minor typos and some unclear connotations.  For example there is no such thing as a variational Bayes model. I am willing to adjust my rating when the questions and remarks above get addressed.,18,244,17.428571428571427,5.181034482758621,128,0,244,0.0,0.0120967741935483,-0.6101,69,28,39,9,10,7,"{'ABS': 1, 'INT': 1, 'RWK': 8, 'PDI': 3, 'DAT': 0, 'MET': 5, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 2}","{'APR': 1, 'NOV': 0, 'IMP': 6, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 2}",1,1,8,3,0,5,2,2,0,2,0,4,0,2,1,0,6,1,1,0,8,1,2,0.7162806993385955,0.7824527981347721,0.5693117369807694
ICLR2018-Hkp3uhxCW-R3,Reject,"The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop (BBB) algorithm. In particular, the authors propose a new framework to sharpen the posterior. In particular, the hierarchical prior in (6) and (7) frame an interesting modification to directly learning a multivariate normal variational approximation. In the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why. In particular, how does the variational posterior change as a result of the hierarchical prior?  It seems that (7) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior; however, with the two layers in the prior it's unclear what actually is happening.  Carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it.  As a final point, the authors state, as long as the improvement along the gradient is great than the KL loss incurred...this method is guaranteed to make progress towards optimizing L.   Do the authors mean that the negative log-likelihood will be improved in this case? Or the actual optimization? Improving the negative log-likelihood seems straightforward, but I am confused by what the authors mean by optimization. The new evaluation metric proposed in Section 6.1.1 is confusing, and I do not understand what the metric is trying to capture.  This needs significantly more detail and explanation. Also, it is unclear to me what would happen when you input data examples that are opposite to the original input sequence; in particular, for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs. It's completely feasible that these outputs would just be highly uncertain, and I'm not sure how you can ascribe meaning to them. The authors should not compare to the uniform prior as a baseline for entropy. It's much more revealing to compare it to the empirical likelihoods of the words.",17,349,24.928571428571427,5.270833333333333,173,3,346,0.0086705202312138,0.0478873239436619,0.9124,73,49,65,20,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 15, 'DAT': 1, 'MET': 8, 'EXP': 8, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 6}",0,0,1,15,1,8,8,1,0,0,0,5,0,0,0,0,0,1,0,0,13,1,6,0.5033665886366362,0.4523136090168304,0.2774789277231787
ICLR2018-HkpRBFxRb-R1,Reject,"SUMMARY The major contribution of the paper is a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.  These CARs are used in the A3C algorithm. The weights are based on the confidence of the value function of the n-step return. Even though this idea in not new the authors propose a simple and robust approach for doing it by using the value function estimation network of A3C. During experiments, the autodidactic returns perform better only half of the time as compared to lambda returns. COMMENTS The j-step returns TD error is not written correctly In Figure 1 it is not obvious how the confidence of the values is estimated. Figure 1 is unreadable. A lighter version of Algorithm 1 in Appendix F should be moved in the text, since this is the novelty of the paper. ",10,151,16.77777777777778,4.9375,88,0,151,0.0,0.0326797385620915,0.9403,46,17,23,6,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 0, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,0,1,0,3,1,0,2,0,0,1,0,0,0,1,0,0,2,0,4,0,1,0.4290951107597044,0.4463718856727767,0.23873896177017934
ICLR2018-HkpRBFxRb-R2,Reject,"The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state. The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture.  However, the method by which these confidence estimates are refined could be better described. The authors describe these confidences variously as: some notion of confidence that the agent has in the value function estimate and weighing the returns based on a notion of confidence has been explored earlier (White & White, 2016; Thomas et al., 2015). But the exact method is difficult to piece together from what is written. I believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c parameters. This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5. If so, this should be stated explicitly. There is another theoretical point that could be clearer. The variation in an autodidactic update of a value function (Equation (4)) depends on a few things, the in variation future value function estimates themselves being just one factor. Another two sources of variation are: the uncertainty over how likely each path is to be taken, and the uncertainty in immediate rewards accumulated as part of some n-step return. In my opinion, the quality of the paper would be much improved by a brief discussion of this, and some reflection on what aspects of these variation contribute to the confidence vectors and what isn't captured. Nonetheless, I believe that the paper represents an interesting and worthy submission to the conference. I would strongly urge the authors to improve the method description in the camera read version though. A few additional comments are as follows:    u2022 The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach (CAR) over the A3C (TD(0) estimates) and LRA3C (truncated TD(lambda) estimates) approaches. However, the way the results are presented/plotted, namely the linear plot of the (shifted) relative performance of CAR (and LRA3C) versus A3C, visually inflates the importance of tasks on which CAR (and LRA3C) perform better than A3C, and diminishes the importance of those tasks on which A3C performs better.  It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting.  u2022 On page 3, when Gt is first mentioned, Gt should really be described first, before the reader is told what it is often replaced with.  u2022 On page 3, where delta_t is defined (the j step return TD error, I think the middle term should be $gamma^j V(S_{t+j})$  u2022 On page 4 and 5, when describing the gradient for the actor and critic, it would be better if these were given their own terminology, but if not, then use of the word respectively in each case would help. ",19,521,26.05,4.98582995951417,237,4,517,0.0077369439071566,0.0131826741996233,0.9974,148,46,88,27,7,5,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 15, 'EXP': 0, 'RES': 1, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 3, 'REC': 1, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,2,0,3,0,15,0,1,3,1,0,2,0,0,0,0,0,2,3,1,3,2,0,0.5037388229754761,0.5571464473383596,0.31926507010469024
ICLR2018-HkpRBFxRb-R3,Reject,"This paper revisits the idea of exponentially weighted lambda-returns at the heart of TD algorithms. The basic idea is that instead of geometrically weighting the n-step returns we should instead weight them according to the agent's own estimate of its confidence in it's learned value function. The paper empirically evaluates this idea on Atari games with deep non-linear state representations, compared to state-of-the-art baselines. This paper is below the threshold because there are issues with the : 1) motivation, 2) the technical details, and (3) the empirical results. The paper begins by stating that the exponential weighting of lambda returns is ad-hoc and unjustified. I would say the idea is well justified in several ways. First the lambda return definition lends itself to online approximations that achieve a fully incremental online form with linear computation and nearly as good performance of the off-line version. Second, decades of empirical results illustrating good performance of TD compared with MC methods.  And an extensive literature of theoretical results. The paper claims that the exponential has been noted to be ad-hoc, please provide a reference for this. There have been several works that have noted that lambda can and perhaps should be changed as a function of state (Sutton and Barto, White and White [1], TD-Gammon). In fact, such works even not that lambda should be related to confidence. The paper should work harder to motivate why adapting lambda as a function of state---which has been studied---is not sufficient. I don't completely understand the objective. Returns with higher confidence should be weighted higher, according to the confidence estimate around the value function estimate as a function of state?  With longer returns, n>>1, the role of the value function in the target is down-weighted by gamma^n---meaning its accuracy is of little relevance to the target. How does your formalism take this into account? The basic idea of the lambda return assumes TD targets are better than MC targets due to variance, which place more weight on shorter returns. I addition I don't understand how learning confidence of the value function has a realizable target. We do not get supervised targets of the confidence of our value estimates. What is your network updating toward? The work of Konidaris et al [1] is a more appropriate reference for this work (rather than the Thomas reference provided). Your paper does not very clearly different itself from Konidaris's work here. Please expand on this. The experiments have some issues. One issue is that basic baselines could more clearly illustrate what is going on. There are two such baselines: random fixed weightings of the n-step returns, and persisting with the usual weighting but changing lambda on each time step (either randomly or according to some decay schedule). The first baseline is a sanity check to ensure that you are not observing some random effect. The second checks to see if your alternative weighting is simply approximating the benefits of changing lambda with time or state. I would say the current results indicate the conventional approach to TD is working well if not better than the new one. Looking at fig 3, its clear the kangaroo is skewing the results, and that overall the new method is performing worse. This is further conflated by fig7 which attempts to illustrate the quality of the learned value functions. In Kangaroo, the domain where your method does best, the l2 error is worse. On the other hand in sea quest and space invaders, where your method does worse, the l2 error is better. These results seem conflicting, or at least raise more questions than they answer. [1] A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning . Adam White and Martha White. Autonomous Agents and Multi-agent Systems (AAMAS), 2016 [2] G. D. Konidaris, S. Niekum, and P. S. Thomas. TDu03b3: Re-evaluating complex backups in temporal difference learning. In Advances in Neural Information Processing Systems 24, pages 2402u20132410. 2011. ",40,655,14.886363636363637,5.236714975845411,286,2,653,0.003062787136294,0.0257575757575757,0.9753,190,89,115,33,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 10, 'PDI': 3, 'DAT': 0, 'MET': 14, 'EXP': 2, 'RES': 12, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 5, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 9, 'PNF': 0, 'REC': 0, 'EMP': 19, 'SUB': 1, 'CLA': 0}",0,1,10,3,0,14,2,12,2,1,0,0,5,0,0,0,0,9,0,0,19,1,0,0.6477244177730792,0.3454658676180266,0.3280524868743823
ICLR2018-HkpYwMZRb-R1,Reject,"Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur. Review - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect.  It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower. While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions. The authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. Unfortunately, all tables with the experimental results are left to the appendix. As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors' overall points. While the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement. There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10).  At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative. This paper has a lot of content, but not all of it appears to be relevant to the authors' central points. Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement. My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme. Pros -  * This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. * At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points. * The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions. Cons -  * The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read. For example, the authors' first contribution is hidden among the text presentation of section 2. * The paper relies heavily on the supplement to make their central points. * It is nearly double the recommended page length with a nearly 30 page supplement Minor issues -  * Use one style for introducing and defining terms either use italics or single quotes. The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved. * The citation style of Authors (YEAR) at times leads to awkward sentence parsing. * Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters. * The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction.  * All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations. * In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text).  - - -  Response to the authors  During the initial reviewing period, I was unable to distill the significance of the authors' contributions from the current literature in large part due to the nature of the writing style. After reading the authors responses and consulting the differences between the versions of the paper, my review remains the same. It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer. Consulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar. Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6. However, I did notice that the majority of these changes were superficial re-orderings of the original text. Given the limited substantive changes to the main text, I did not deeply re-read the text of the paper beyond the introduction.",34,823,22.86111111111111,5.104458598726115,310,6,817,0.00734394124847,0.0199530516431924,0.6664,228,83,139,38,10,6,"{'ABS': 1, 'INT': 4, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 4, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 16, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 13, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 10}",1,4,1,2,0,4,3,4,3,0,0,16,0,1,0,0,1,0,13,1,6,3,10,0.7162395225805878,0.6714588495808509,0.5054860480080356
ICLR2018-HkpYwMZRb-R2,Reject,"Paper Summary: This is a very long paper (55 pages), and I did not read it in its entirety. The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed. To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers. The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function. (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?) They focus on linear MLPs in the paper for computational simplicity. With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5). They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks. They then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i). Expanding out, each term is a product of some of the r_i and some of the identities I. If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth. This is described in Veit et al, 2016. While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function.  Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100. They also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm. Main Comments: This could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance. In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text. Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis. For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it easy to train. But typically easy to train does not involve a specific preprocessing of gradients. Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them. I think the assumptions made in the results should also be clearer. (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?) I also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100? This doesn't really look anything like networks that are trained in practice. Calling these popular architectures is misleading. In summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims. ",27,614,21.928571428571427,5.036458333333333,276,3,611,0.0049099836333878,0.0193861066235864,0.9902,138,69,124,37,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 11, 'EXP': 7, 'RES': 5, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 1, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,1,1,3,0,11,7,5,0,5,0,4,0,0,0,0,0,0,3,1,9,1,1,0.5750242085578362,0.5606543528819059,0.36346404836942225
ICLR2018-HkpYwMZRb-R3,Reject,"The paper makes some bold claims. In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate. This is either a very important paper or the analysis is incorrect but it's not my area of expertise. Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion. It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail. Regardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside. As such it's not even clear if this is proper for a conference. The authors have already provided several pages worth of additional comments on the website on further related work. I view this as an issue in and of itself. Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow. While the authors may claim it's necessary to use that much space to make their point I will argue that this length is uncalibrated to standards. I've seen many papers that need to go through much more complicated derivations and theory and remain within a 8-10 page limit by being precise and strictly to the point. Perhaps Godel could be a good inspiration here, with a 21 page PhD thesis that fundamentally changed mathematics. In addition to being quite bold in claims, it is also somewhat confrontational in style. I understand the authors are trying to make a very serious claim about much of the common wisdom, but again, having reviewed papers for many years, this is highly unusual and it is questionable whether it is necessary.  So, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field. ",16,420,24.705882352941178,4.91183879093199,224,5,415,0.0120481927710843,0.0448113207547169,0.9762,98,42,75,31,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 0, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 9, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 5, 'REC': 1, 'EMP': 3, 'SUB': 5, 'CLA': 0}",0,0,3,1,0,0,3,2,0,1,0,9,0,1,1,0,0,1,5,1,3,5,0,0.5007524197452994,0.6685835632648518,0.3437394815160651
ICLR2018-HksxTdiWz-R1,,"This article tackles the extraction of sentiments at a fine-grained level. Thus, the authors insist on context modeling to obtain a relevant analysis of a word's meaning. The authors propose a first modeling called ASC. Identifying some weakness in the formulation, the authors propose 5 solutions. The authors apply their different models on a small dataset (semeval 2014), they compare basic memory network implementations with their approaches.     The authors do not put into perspective their approach. Given the literature in topics / sentiment modeling, it is a real weakness of this article. To improve readability, the authors should propose a diagram of the network, summarizing all notations Dimension of c_i / o ? d  Definition of u (eq. 3)  > v? At the beginning of section 3, the discussion about the independence of the terms in the decomposition (6) is not completely relevant: alpha terms embedded the relation between the target and the context i Among the different solutions, IT and CI are very redundant. A kind of matching between the context and the target is already considered in the definition of alpha -with metric learning in M- : why not using those terms instead of ai <di,dt>? We see that the authors build models that become more and more complex, but their motivation in combining attention and IT/CI is not clear: they learn the relation between context and target twice without any factorization. In section 5, the authors mention briefly some works from the RNN domain & the classical use case of memory networks. They claim that: The above studies cannot be directly applied to the ASC task as they are not capable of mining finer-grained aspect dependent sentiments.  I do not agree with them: latent representations from RNN are fine-grained & context dependent; latent representations from Socher et al. are also fine-grained & target dependent: the position in the latent space -modeling context- has an impact on the estimated sentiment. In the experimental section, there is no discussion about the ration between the dataset size and the number of parameters to estimate. However: there is a strong risk of overfitting in the current situation and we will always wonder if the given figures correspond to lucky trial. It is true that the authors use strong regularization techniques (drop out, external knowledge of words embedding...). However, the validation procedure is not clear in the article. The main weakness of the experimental section resides in the lack of comparison with classical approach in sentiment analysis: none of the state-of-the-art approaches are implemented here (RNN, basic models on W2V aggregations, ...). That makes  the contribution very difficult to evaluate. The analysis of the results are interesting, both from the quantitative & qualitative point of view.",24,444,21.142857142857142,5.35308056872038,210,0,444,0.0,0.0303030303030303,-0.5751,147,49,58,23,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 2, 'MET': 13, 'EXP': 1, 'RES': 4, 'TNF': 2, 'ANA': 4, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,1,4,2,2,13,1,4,2,4,0,1,0,0,0,0,0,2,1,1,13,0,0,0.7178894357407152,0.45202509621975,0.4026378947701421
ICLR2018-HksxTdiWz-R2,,"This paper considers the task of aspect sentiment classification, which entails categorizing texts with respect to the sentiment expressed concerning particular aspects (e.g., television resolution or price). Toward this end, the authors adopt a memory-network based approach. The idea is to explicitly model interactions between aspects and words expressing sentiment about them. This is achieved via an attention mechanism. Overall, this paper does seem to identify a concrete problem, and I liked the use of explicit aspect embeddings for sentiment analysis. However, the contribution is relatively minor here. Furthermore, the approaches did not seem particularly well motivated in my view; inconsistent and seemingly erroneous notation in places complicate the picture. Moreover, the experimental evaluation is small, considering only two datasets. My feeling is that this work is a bit preliminary at present, but could be expanded into a nice contribution eventually.  Specific comments --- - I had some trouble following the notation in places, and I think this is due to a bit of sloppiness. Specifically:      1. Target aspect vectors live in R^V, but it's not clear to me what V represents; this is the number of distinct aspects? This should be clarified, ideally with concrete examples. 2. Are the x_1 ... x_n here word embeddings or one-hot encodings? I believe the latter, but this implies that the vocabulary dimension (V) is the same as the number of aspects, since A is apparently shared. This seems a bit surprising, or at least would seem to warrant further explanation. In Eq. 2, below, it actually seems words are embedded separately via C, although again the dimensions are not provided. Regardless, more discussion regarding what the Ax_i embeddings are meant to capture (in contrast to the Cx_i vectors) would be appreciated. 3. What is u is equation 3? As far as I can tell, this term is undefined. In Equation 5 this is mysteriously replaced with v_t, which is the target aspect embedding and so may have been the intention for u all along? 4. In Equation 6, on the RHS in the expansion, the alpha_1 W c_i should be alpha_1 W c_1. The mistake is repeated for the following two terms.   - Equations 3 and 4 suggest that despite their ordinal structure, sentiment labels are treated as unstructured at predict time. This seems like a missed opportunity to capitalize on structure to bias predictions (neutral sentiment is closer to positive than is negative, after all, but the model does not know this as currently specified). - The authors write alpha_i W c_i does not explicitly depend on the target word t.  I'm not sure I follow though, because the alpha terms do indeed depend on the word $t$, as per equation (1), which includes v_t, a vector representation of the target aspect. I think what the authors intend to note here is that the W parameters are independent of this, or as expounded upon in the concrete example that follows, that context words and weights W both are. In any case, this statement should be clarified. - It would seem to me at first glance that the most natural means of overcoming the problem discussed at length toward the end of Section 3 would be to add an additional layer, which would facilitate interactions between the attention-weighted word embedding (alpha_i c_i) and aspect embedding (v_t). However, the authors do not seem to have considered this straight-forward approach. Why?   - Surely d_t   Dt should be d_t   D v_t in the Interaction Term (IT) subsection? - The authors write diWdt generates slightly better results and we adopt it in our experiments.  -- please clarify; better results on a development set, I hope? - The evaluation is small. The authors use only two datasets comprising only a few thousand data points (and hence the test sets comprise 500-1000 instances). It is therefore hard to draw anything conclusive from these results, especially given how many moving parts there are here (from model initializations to training procedures). Regardless, the authors should report the micro-F1 in addition to the macro-F1. Smaller comments --- - As a stylistic thing, I would suggest not pluralizing attention (i.e., remove Attentions). - In Eq 3, please be explicit as to whether the '+' is here a concatentation or an actual (element-wise) sum? Eq. 5 clarifies this implicitly, but would be good to state outright. ",40,709,17.29268292682927,5.199088145896656,333,7,702,0.0099715099715099,0.0472972972972973,0.9913,190,74,132,50,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 4, 'DAT': 2, 'MET': 27, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 18, 'SUB': 8, 'CLA': 0}",0,1,0,4,2,27,3,2,0,2,1,4,0,0,0,0,1,0,2,0,18,8,0,0.6499651974218997,0.4558260253188286,0.35976374317386084
ICLR2018-HksxTdiWz-R3,,"Overall strength: In this paper, the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attentions. This work has a well-established motivation: traditional attention for target-dependent sentiment classification cannot model the interaction between target term and context words when making predictions. To solve this problem, the authors proposed five formulations in the final prediction layer. The illustration about the problem is clear, as well as the explanation for the formulations. Major concerns: 1.tThis work brings some modifications to the prediction layer, which is a bit trivial. Although the effect has been shown, the model is too specific to a narrow area, and is not general to be applied in a broad sense. It could have more contribution if the authors model the interactions within the attention model itself, instead of a simple prediction layer, which is problem-dependent. 2.tThe experiments are insufficient to show the effectiveness. It would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in Table 4. Only two examples are not convincing. 3.tIn section 3, the authors claimed that (5) models the target and context independently. However, in section 4, in (7), the authors claimed the target vector v_t will affect the context shifting their representation to c'_i. This should also work for (5). 4.tThere are too many typos in the paper, e.g., alpha is replaced by a, etc. Other concerns: 1.tIt seems that one needs to train at least three embedding matrices: A, C, D which represent input embeddings, output embeddings, and interactive embeddings, respectively. I wonder if this brings redundant parameters that do not guarantee convergence. Why not use one matrix instead? Did the authors try experiments with less embedding matrices? 2.tThere is another work that also considers the target-context interaction using interactive attention model. Please refer to this paper ""Interactive Attention Networks for Aspect-Level Sentiment Classification"". A comparison is needed. 3.tIt is better to provide results in terms of accuracy for both datasets, as previous methods usually use accuracy for comparison. How's the score of the proposed model compared with the above paper as well as [Tang et al. 2016]? ",23,369,16.772727272727273,5.53125,188,2,367,0.0054495912806539,0.0135135135135135,0.9553,111,36,64,21,9,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 11, 'EXP': 2, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 1}",0,0,3,2,1,11,2,2,1,1,0,3,0,0,0,0,0,1,0,0,8,3,1,0.6458120749621326,0.4490114995049882,0.3672366788969528
ICLR2018-HktJec1RZ-R1,Accept,"The paper introduces a neural translation model that automatically discovers phrases. This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way. However, the clarity of the paper could be improved. The local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others? Are all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments? In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?",6,121,30.25,5.145299145299146,80,0,121,0.0,0.0082644628099173,0.9175,34,9,26,8,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,0,2,0,3,2,0,1,0,0,1,0,0,0,0,0,0,0,0,4,0,1,0.4292173084525623,0.2240880945267511,0.1959357832179797
ICLR2018-HktJec1RZ-R2,Accept,"Authors proposed a new neural-network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source information. The model is based on the SWAN architecture which is previously proposed, and an additional local reordering layer to reshuffle source information to adjust those positions to the target sentence. Using the SWAN architecture looks more reasonable than the conventional attention mechanism when the ground-truth word alignment is monotone. Also, the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks. The window size of the local reordering layer looks like the distortion limit used in traditional phrase-based statistical machine translation methods, and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model; small window sizes may drop information about long dependency. For example, verbs in German sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sentence. Since reordering windows restrict the context of each position to a limited number of neighbors, it may not capture distant information enough. I expected that some observations about this point will be unveiled in the paper, but unfortunately, the paper described only a few BLEU scores with different window sizes which have not enough information about it. It is useful for all followers of this paper to provide some observations about this point. In addition, it could be very meaningful to provide some experimental results on linguistically distant language pairs, such as Japanese and English, or simply reversing word orders in either source or target sentences (this might work to simulate the case of distant reordering). Authors argued some differences between conventional attention mechanism and the local reordering mechanism, but it is somewhat unclear that which ones are the definite difference between those approaches. A super interesting and mysterious point of the proposed method is that it achieves better BLEU than conventional methods despite no any global language models (Table 1 row 8), and the language model options (Table 1 row 9 and footnote 4) may reduce the model accuracy as well as it works not so effectively. This phenomenon definitely goes against the intuitions about developing most of the conventional machine translation models. Specifically, it is unclear how the model correctly treats word connections between segments without any global language model. Authors should pay attention to explain more detailed analysis about this point in the paper. Eq. (1) is incorrect . According to Fig. 2, the conditional probability in the product operator should be revised to p(a_t | x_{1:t}, a_{1:t-1}), and the independence approximation to remove a_{1:t-1} from the conditions should also be noted in the paper. Nevertheless, the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN.  ",19,486,23.142857142857142,5.550847457627119,227,6,480,0.0125,0.0244897959183673,0.9569,152,56,67,28,10,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 12, 'EXP': 2, 'RES': 3, 'TNF': 2, 'ANA': 3, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 5, 'CLA': 0}",0,1,2,2,1,12,2,3,2,3,1,0,0,0,0,0,1,2,0,0,10,5,0,0.7174479516006521,0.4505859299485071,0.4041542199662519
ICLR2018-HktJec1RZ-R3,Accept,"This paper introduces a new architecture for end to end neural machine translation. Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right. This kind of approach is more related to ngram based machine translation than conventional phrase based one. The idea is nice. The proposed approach does not rely on attention based model. This opens nice perpectives for better and faster inference. My first concern is about the architecture description. For instance, the swan part is not really stand alone. For reader who does not already know this net, I'm not sure this is really clear. Moreover, there is no link between notations used for the swan part and the ones used in the reordering part. Then, one question arises. Why don't you consider the reordering of the whole source sentence. Maybe you could motivate your choice at this point. This is the main contribution of the paper, since swan already exists. Finally, the experimental part shows nice improvements  but: 1/ you must provide baseline results with a well tuned phrase based mt system; 2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. ",16,233,14.5625,5.022123893805309,134,1,232,0.0043103448275862,0.0338983050847457,0.9168,67,22,46,21,6,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 11, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,1,1,1,11,3,0,0,0,0,0,0,0,0,1,0,1,2,0,6,2,0,0.4312457412926926,0.5588335881618952,0.26816792660949496
ICLR2018-HktRlUlAZ-R1,Accept,"This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling. The method is combination of a spatial transformer module that predicts a focal point, around which a log-polar transform is performed.  The resulting log-polar image is analyzed by a conventional CNN. I find the basic idea quite compelling. Although this is not mentioned in the article, the proposed approach is quite similar to human vision in that people choose where to focus their eyes, and have an approximately log-polar sampling grid in the retina. Furthermore, dealing well with variations in scale is a long-standing and difficult problem in computer vision, and using a log-spaced sampling grid seems like a sensible approach to deal with it. One fundamental limitation of the proposed approach is that although it is invariant to global translations, it does not have the built-in equivariance to local translations that a ConvNet has. Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance. The use of the heatmap centroid as the prediction for the focal point is potentially problematic as well. It would not work if the heatmap is multimodal, e.g. when there are multiple instances in the same image or when there is a lot of clutter.  There is a minor conceptual confusion on page 4, where it is written that Group-convolution requires integrability over a group and identification of the appropriate measure dg. We ignore this detail as implementation requires application of the sum instead of integral.  When approximating an integral by a sum, one should generally use quadrature weights that depend on the measure, so the measure cannot be ignored. Fortunately, in the chosen parameterization, the Haar measure is equal to the standard Lebesque measure, and so when using equally-spaced sampling points in this parameterization, the quadrature weights should be one. (Please double-check this - I'm only expressing my mathematical intuition but have not actually proven this). It does not make sense to say that The above convolution requires computation of the orbit which is feasible with respect to the finite rotation group, but not for general rotation-dilations, and then proceed to do exactly that (in canonical coordinates). Since the rotation-dilation group is 2D, just like the 2D translation group used in ConvNets, this is entirely feasible. The use of canonical coordinates is certainly a sensible choice (for the reason given above), but it does not make an infeasible computation feasible. The authors may want to consider citing - Warped Convolutions: Efficient Invariance to Spatial Transformations, Henriques & Vedaldi. This paper also uses a log-polar transform, but lacks the focal point prediction / STN. Likewise, although the paper makes a good effort to rewiev the literature on equivariance / steerability, it missed several recent works in this area: - Steerable CNNs, Cohen & Welling - Dynamic Steerable Blocks in Deep Residual Networks, Jacobsen et al. - Learning Steerable Filters for Rotation Equivariant CNNs, Weiler et al. The last paper reports 0.71% error on MNIST-rot, which is slightly better than the PTN-CNN-B++ reported on in this paper. The experimental results presented in this paper are quite good, but both MNIST and ModelNet40 seem like simple / toyish datasets. For reasons outlined above, I am not convinced that this approach in its current form would work very well on more complicated problems. If the authors can show that it does (either in its current form or after improving it, e.g. with multiple saccades, or other improvements) I would recommend this paper for publication. Minor issues & typos - Section 3.1, psi_gh   psi_g psi_h. I suppose you use psi for L and L', but this is not very clear. - L_h f   f(h^{-1}), p. 4 - coordiantes, p. 5",32,635,19.24242424242424,5.334442595673877,307,4,631,0.0063391442155309,0.0365296803652968,0.9907,176,80,106,41,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 3, 'MET': 15, 'EXP': 3, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 2, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 17, 'SUB': 7, 'CLA': 3}",0,1,4,2,3,15,3,4,0,0,0,3,2,2,0,0,0,5,1,0,17,7,3,0.7186134048781355,0.56677785393344,0.4492464853370034
ICLR2018-HktRlUlAZ-R2,Accept,"This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision. The idea is to jointly train - a network predicting a polar origin, - a module transforming the image into a log-polar representation according to the predicted origin, - a final classifier performing the desired classification task. A (not too large) translation of the input image therefore does not change the log-polar representation. Rotation and scale from the polar origin result in translation of the log-polar representation. As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image. Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge. (+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs. (-) The evaluation, however, is quite limited. The approach is evaluated on:  1) several variants of MNIST. The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST. This variant is of course very well suited to the proposed method, and a bit artificial. 2) 3d voxel occupancy grids with a small resolution. The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation. (-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale? Then the input image could be rectified to a canonical orientation and scale, without needing equivariance. My intuition is that this simpler approach would work better. It should at least be evaluated. Despite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs. ",21,280,15.555555555555555,5.481060606060606,137,2,278,0.0071942446043165,0.0173611111111111,0.9607,87,28,47,13,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 3, 'MET': 19, 'EXP': 7, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 0}",0,1,0,1,3,19,7,1,0,0,1,0,0,0,0,2,1,0,0,0,8,2,0,0.505092814278163,0.4489499305811849,0.2860970309988553
ICLR2018-HktRlUlAZ-R3,Accept,"The authors introduce the Polar Transformer, a special case of the Spatial Transformer (Jaderberg et al. 2015) that achieves rotation and scale equivariance by using a log-polar sampling grid. The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST. A weakness of the paper is that it does not attempt to solve a real-world problem. However, I think because it is a conceptually novel and potentially very influential idea, it is a valuable contribution as it stands. Issues:  - The clutter in SIM2MNIST is so small that predicting the polar origin is essentially trivially solved by a low-pass filter. Although this criticism also applies to most previous work using 'cluttered' variants of MNIST, I still think it needs to be considered. What happens if predicting the polar origin is not trivial and prone to errors? These presumably lead to catastrophic failure of the post-transformer network, which is likely to be a problem in any real-world scenario. - I'm not sure if Section 5.5 strengthens the paper. Unlike the rest of the paper, it feels very 'quick & dirty' and not very principled. It doesn't live up to the promise of rotation and scale equivariance in 3D. If I understand it correctly, it's simply a polar transformer in (x,y) with z maintained as a linear axis and assumed to be parallel to the axis of rotation. This means that the promise of rotation and scale equivariance holds up only along (x,y). I guess it's not possible to build full 3D rotation/scale equivariance with the authors' approach (spherical coordinates probably don't do the job), but at least the scale equivariance could presumably have been achieved by using log-spaced samples along z and predicting the origin in 3D. So instead of showing a quick 'hack', I would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provided. ",15,322,20.125,5.148148148148148,168,8,314,0.0254777070063694,0.0489296636085626,0.2317,71,42,58,36,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 3, 'MET': 6, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,1,1,3,3,6,0,4,0,1,0,2,0,0,0,1,0,1,1,0,9,1,1,0.5730828376871722,0.6716423261454104,0.3929737977253857
ICLR2018-HktXuGb0--R1,Reject,"The authors propose to solve the inverse reinforcement learning problem of inferring the reward function from observations of a behaving agent, i.e. trajectories, albeit without observing state-action pairs as is common in IRL but only with the state sequences. This is an interesting problem setting. But, apparently, this is not the problem the authors actually solve, according to eq. 1-5. Particularly eq. 1 is rather peculiar. The main idea of RL in MDPs is that agents do not maximize immediate rewards but instead long term rewards. I am not sure how this greedy action should result in maximizing the total discounted reward along a trajectory.  Equation 3 seems to be a cost function penalizing differences between predicted and observed states. As such, it implements a sort of policy imitation, but that is quite different from the notion of reward in RL and IRL. Similarly, equation 4 penalizes differences between predicted and observed state transitions. Essentially, the current manuscript does not learn the reward function of an MDP in the RL setting, but it learns some sort of a shaping reward function to do policy imitation, i.e. copy the behavior of the demonstrator as closely as possible. This is not learning the underlying reward function. So, in my view, the manuscript does a nice job at policy fitting, but this is not reward estimation. The manuscript has to be rewritten that way. One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say: C. A. Rothkopf, C. Dimitrakakis. Preference elicitation and inverse reinforcement learning. ECML 2011 Overall the manuscript leverages on deep learning's power of function approximation and the simulation results are nice, but in terms of the soundness of the underlying RL and IRL theory there is some work to do.",15,299,13.0,5.204946996466431,150,1,298,0.0033557046979865,0.03,0.982,97,25,49,19,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 11, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 0, 'CLA': 1}",0,0,0,4,0,11,0,1,0,2,0,0,0,0,0,0,0,0,0,0,12,0,1,0.2884188258183159,0.2290637540054949,0.12947365628155683
ICLR2018-HktXuGb0--R2,Reject,"This paper uses inverse reinforcement learning to infer additional shaping rewards from demonstrated expert trajectories.  The key distinction from many previous works in this area is that the expert's actions are assumed to not be available, and the inferred reward on a transition is assumed to be a function of the previous and subsequent state. The expert trajectories are first used to train either a generative model or an LSTM on next state prediction.  The inferred reward for a newly experienced transition is then defined from the negative error between the predicted and actual next state.  The method is tested on several reacher tasks (low dimensional continuous control), as well as on two video games (Super Mario Bros and Flappy Bird).  The results are positive, though they are often below the performance of behavioral cloning (which only trains from the expert data but also uses the expert's actions).  The proposed methods perform competitively with hand-designed dense shaping rewards for each task. The main weakness of the proposed approach is that the addition of extra rewards from the expert trajectories seems to skew the system's asymptotic behavior away from the objective provided by the actual environment reward.  One way to address this would be to use the expert trajectories to infer not only a reward function, but also an initial state value function (trained on the expert trajectories with the inferred reward). This initial value function could be added to the learned value function and would not limit asymptotic performance (unlike the addition of inferred rewards as proposed here). This connection between reward shaping and initial Q values was described by Wiewirora in 2003 (""Potential-based Shaping and Q-Value Initialization are Equivalent""). I am also uncertain of the robustness of the proposed approach when the learning agent goes beyond the distribution of states provided by the expert (where the inferred reward model has support).  Will the inferred reward function in these situations go towards zero? Will the inferred reward skew the learning algorithm to a worse policy? How does one automatically balance the reward scale provided by the environment with the the reward scaling provided by psi, or is this also assumed to be manually crafted for each domain?  These questions make me uncertain of the utility of the proposed method. ",15,379,27.071428571428573,5.283783783783784,178,2,377,0.0053050397877984,0.0103092783505154,0.9964,110,44,61,22,5,1,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 11, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,0,1,4,0,11,1,1,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0.359822677866271,0.1154648131550119,0.14704914982167916
ICLR2018-HktXuGb0--R3,Reject,"To speed up RL algorithms, the authors propose a simple method based on utilizing expert demonstrations. The proposed method consists in explicitly learning a prediction function that maps each time-step into a state. This function is learned from expert demonstrations. The cost of visiting a state is then defined as the distance between that state and the predicted state according to the learned function. This reward is then used in standard RL algorithms to learn to stick close to the expert's demonstrations. An on-loop variante of this method consists of learning a function that maps each state into a next state according to the expert, instead of the off-loop function that maps time-steps into states. While the experiments clearly show the advantage of this method, this is hardly surprising or novel. The concept of encoding the demonstration explicitly in the form of a reward has been around for over a decade. This is the most basic form of teaching by demonstration. Previous works had used other models for generalizing demonstrations (GMMs, GPs, Kernel methods, neural nets etc..). This paper uses a three layered fully connected auto-encoder (which is not that deep of a model, btw) for the same purpose. The idea of using this model as a reward instead of directly cloning the demonstrations is pretty straightforward.  Other comments: - Most IRL methods would work just fine by defining rewards on states only and ignoring actions all together. If you know the transition function, you can choose actions that lead to highly rewarding states, so you don't need to know the expert's executed actions. - We assume that maximizing likelihood of next step prediction in equation 1 will be globally optimized in RL. Could you elaborate more on this assumption? Your model finds rewards based on local state features, where a greedy (one-step planning) policy would reproduce the expert's demonstrations (if the system is deterministic). It does not compare the global performance of the expert to alternative policies (as is typically done in IRL). - Related to the previous point: a reward function that makes every step of the expert optimal may not be always exist. The expert may choose to go to terrible states with the hope of getting to a highly rewarding state in the future.  Therefore, the objective functions set in this paper may not be the right ones, unless your state description contains features related to future states so that you can incorporate future rewards in the current state (like in the reacher task, where a single image contains all the information about the problem). What you need is actually features that can capture the value function (like in DQN) and not just the immediate reward (as is done in IRL methods). - What if in two different trajectories, the expert chooses opposite actions for the same state appearing in both trajectories? For example, there are two shortest paths to a goal, one starts with going left and another starts with going right. If you try to generate a state that minimizes the sum of distances to the two states (left and right ones), then you may choose to remain in the middle, which is suboptimal. You wouldn't have this issue with regular IRL techniques, because you can explain both behaviors with future rewards instead of trying to explain every action of the expert using only local state description. ",26,559,22.36,5.044362292051756,250,5,554,0.0090252707581227,0.0106007067137809,0.9953,148,46,102,36,6,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 5, 'DAT': 1, 'MET': 16, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,0,5,1,16,3,0,0,0,1,0,0,3,0,0,0,0,0,0,10,0,0,0.432752557445353,0.1167087280246978,0.17702201884697932
ICLR2018-HkuGJ3kCb-R1,Accept,"This paper provides theoretical and empirical motivations for removing the top few principle components of commonly-used word embeddings. The paper is well-written and I enjoyed reading it. However, it does not explain how significant this result is beyond that of (Bullinaria and Levy, 2012), who also removed the top N dimensions when benchmarking SVD-factorized word embeddings. From what I can see, this paper provides a more detailed explanation of the phenomenon (why it works), which is supported with both theoretical results and a series of empirical analyses, as well as updating the benchmarks and methods from the pre-neural era. Although this contribution is relatively incremental, I find the depth of this work very interesting, and I think future work could perhaps rely on these insights to create better embedding algorithms that directly enforce isotropy. I have two concerns regarding the empirical section, which may be resolvable fairly quickly: 1) Are the embedding vectors L2 normalized before using them in each task? This is known to significantly affect performance. I am curious whether removing the top PCs is redundant or not given L2 normalization. 2) Most of the benchmarks used in this paper are toy tasks. As Schnabel et al (2015) and Tsvetkov et al (2015) showed, there is often little correlation between success on these benchmarks and improvement of downstream NLP tasks. I would like to measure the change in performance on a major NLP task that heavily relies on pre-trained word embeddings such as SQuAD. Minor Comments: * The last sentence in the first paragraph (The success comes from the geometry of the representations...) is not true; the success stems from the ability to capture lexical similarity. Levy and Goldberg (2014) showed that searching for the closest word vector to (king - man + woman) is equivalent to optimizing a linear combination of 3 similarity terms [+(x,king), -(x,man), +(x, woman)]. This explanation was further demonstrated by Linzen (2016) who showed that even when removing the negative term (x, man), many analogies can still be solved, i.e. by looking for a word that is similar both to king and to woman. Add to that the fact that the analogy trick works best when the vectors are L2 normalized; if they are all on the unit sphere, what is the geometric interpretation of (king - man + woman), which is not on the unit sphere? I suggest removing this sentence and other references to linguistic regularities from this paper, since they are controversial at best, and distract from the main findings. * This is also related to Bullinaria and Levy's (2012) finding that downweighting the eigenvalue matrix in SVD-based methods improves their performance.  Levy et al (2015) showed that keeping the original eigenvalues can actually degenerate SVD-based embeddings. Perhaps there is a connection to the findings in this paper? ",19,462,25.666666666666668,5.268518518518518,231,6,456,0.0131578947368421,0.0276595744680851,0.9914,123,58,86,27,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 1}",0,0,5,1,1,5,2,3,0,1,1,3,0,0,0,0,1,1,0,0,10,1,1,0.6444683001150462,0.5611531724691423,0.40420381120545895
ICLR2018-HkuGJ3kCb-R2,Accept,"This paper proposes that sets of word embeddings can be improved by subtracting the common mean vector and reducing the effect of dominant components of variation. Comments:  Reference to 'energy' and 'isotropic' in the first paragraph come without any explanation. Can plain terms be used instead to express the same ideas? This would make understanding easier (I have a degree in maths, but never studied physics, and I had to look them up). Otherwise, I like the simple explanation of the method given in the intro.   The experiments conducted in the paper are comprehensive. It is very positive that the improvements appear to be quite consistent across well-known tasks. As well as proposing a simple trick to produce these improvements, the authors aim to provide theoretical insight and (implicitly, at least) pursue a better understanding of semantic word spaces. This has the potential to be a major contribution, as such spaces, and semantic representation in neural nets in general, is poorly understood. However (perhaps because I'm not familiar with Arora et al.) I found the mathematical analysis e.g. S2.2 dense, without any clearly-stated intuitive motivation or conclusions (as per the introduction section) about what is going on semantically. E.g. it is not clear to me why isotropy is something desirable in a word embedding space. I understand that the discarded components tend to encode frequency, and this is very interesting and somewhat indicative f why the method might work. However, Figure 2 is particularly hard to interpret? The correlations, and the distribution of high-frequency words) seems to be quite different for each of the three models?! In general, I don't think the authors should rely on readers having read Arora et al. - anything that builds on that work needs to reintroduce their findings in pain terms in the current paper. Another concern is the novelty in relation to related work. I have not read Arora et al. but the authors say that they 'null away the first principal component', and Sahlgren et al centre the mean. Taken together, this seems very similar to what the authors propose here (please clarify). More generally, these sorts of tricks have often been applied by deep learning researchers and passed around anecdotally (e.g. initialise transition matrix in RNNs with orthonormal noise) as ways to improve training. It is important to share and verify these things, but such a contribution feels more appropriate for a workshop than the main conference. This makes the work that the authors do in interpreting and understanding why these tricks work particularly important. As is, however, I thing that the conclusions from this analysis are unclear and opaque. Can they be better communicated, or is it the case that the results of the analysis are in fact inconclusive? The vast amount of work included in the appendix is impressive. What particularly caught my eye was appendix B, where the authors try to understand if their method can simply be 'learned' by any network that uses pre-trained word embeddings. This is a really nice experiment, and I think it could easily be part of the main paper (perhaps swapping with the stuff in section 2.2).  The conclusion would be a good place for summarising the main findings in plain terms, but that doesn't really happen (unless the finding about frequency is the only finding). Instead, there is a vague connection to population genetics and language evolution.  This may be an interesting future direction, but the connection is tenuous, so that this reader, at least, was left a little baffled.  [REVISED following response]  Thanks for your thorough response which did a good job of addressing most of my concerns. I have changed the score accordingly.      ",31,613,18.57575757575757,5.13481228668942,312,10,603,0.0165837479270315,0.0318979266347687,0.9966,155,75,111,46,10,7,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 3, 'DAT': 0, 'MET': 9, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 3, 'IMP': 0, 'CMP': 2, 'PNF': 2, 'REC': 1, 'EMP': 15, 'SUB': 0, 'CLA': 1}",0,1,5,3,0,9,2,1,1,2,0,4,0,1,1,3,0,2,2,1,15,0,1,0.7169749741257156,0.786754128515609,0.5610988752896633
ICLR2018-HkuGJ3kCb-R3,Accept,"This paper proposes a simple post-processing technique for word representations designed to improve representational quality and performance on downstream tasks. The procedure involves mean subtraction followed by projecting out the first D principle directions and is motivated by improving isotropy of the partition function. Extensive empirical analysis supports the efficacy of the approach. The idea of post-processing word embeddings to improve their performance is not new, but I believe the specific procedure and its connection to the concept of isotropy has not been investigated previously. Relative to other post-processing techniques, this method has a fair amount of theoretical justification, particularly as described in Appendix A. I think the experiments are reasonably comprehensive. All told, I think this is a good paper, but I do have some comments and questions that I think should be addressed before publication. 1) I think it is useful to analyze the distribution of singular values of the matrix of word vectors. However, I did not find the heuristic analysis based on the visual appearance of these distributions to be convincing. For example, in Fig. 1, it is not clear to me that there exists a separation between regimes of exponential decay and rough constancy. It would be ideal if a more quantitative metric is established that captures the main qualitative behavior alluded to here. Furthermore, the vocabulary size is likely to have a strong effect on the shape of the distributions. Are the plots in Fig. 4 for the same vocabulary size? Related to this, the dimensionality of the representation will have a strong effect on the shape, and this should be controlled for in Fig. 8. One way to do this would be to instead plot the density of singular values. Finally, for the Gaussian matrix simulations, in the asymptotic limit, the density of singular values depends only on the ratio of dimensions, i.e. the vector dimension to the vocabulary size. Fig. 4/8 might be more revealing if this ratio were controlled for. 2) It would be useful to describe why isotropy of the partition function is the goal, as opposed to isotropy of the vectors themselves. This may be argued in Arora et al. (2016), but summarizing that argument in this paper would be helpful. In fact, an additional experiment that would be very valuable would be to investigate empirically which form of isotropy is more effective in governing performance. One way to do this would be to enforce approximate isotropy of the partition function without also enforcing isotropy of the vectors themselves. Practically speaking, one might imagine doing this by requiring I   1 to second order without also requiring that the mean vanish. I think this would allow for sigma_max > sigma_min while still satisfying I   1 to second order.  (But this is just off the top of my head -- there may be better ways to conduct this experiment). It is not clear to me why the experiment leading to Table 2 is a good proxy for the exact computation of I. It would be great if there were some mathematical justification for this approximation. Why does Fig. 3 use D 10, 20 when much smaller D are considered elsewhere? Also I think a log scale on the x-axis might be more informative. 3) It would be good to mention other forms of post-processing, especially in the context of word similarity. For example, in the original paper, GloVe advocates averaging the target and context vector representations, and normalizing across the feature dimension before computing cosine similarity. 4) I think it's likely that there is a strong connection between the optimal value of D and the frequency distribution of words in the evaluation dataset. While the paper does mention that D may depend on specifics of the dataset, etc., I would expect frequency-dependence to be the main factor, and it might be worth exploring this effect explicitly. ",32,643,16.92105263157895,5.2324414715719065,270,16,627,0.0255183413078149,0.0491551459293394,0.9982,155,75,115,34,10,4,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 3, 'DAT': 2, 'MET': 12, 'EXP': 6, 'RES': 1, 'TNF': 7, 'ANA': 3, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 6, 'REC': 0, 'EMP': 18, 'SUB': 2, 'CLA': 0}",0,0,1,3,2,12,6,1,7,3,0,3,0,1,0,1,0,0,6,0,18,2,0,0.7179035611010675,0.4554322419641131,0.3964747433467111
ICLR2018-HkwBEMWCZ-R1,Accept,"The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights. They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning. The analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims. I have no major criticisms. One suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy. The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance.",6,160,26.666666666666668,5.532051282051282,99,0,160,0.0,0.01875,-0.3287,48,18,27,10,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,1,0,1,1,3,0,0,0,1,1,1,0,0,0,0,0,0,0,0,3,2,0,0.5005003842275694,0.2235728136002296,0.2259765380720256
ICLR2018-HkwBEMWCZ-R2,Accept,"Paper examines the use of skip connections (including residual layers) in deep networks as a way of alleviating two perceived difficulties in training: 1) when a neuron does not contain any information, and 2) when two neurons in a layer compute the same function. Both of these cases lead to singularities in the Hessian matrix, and this work includes a number of experiments showing the effect of skip connections on the Hessian during training. This is a significant and timely topic. While I may not be the best one to judge the originality of this work, I appreciated how the authors presented clear and concise arguments with experiments to back up their claims.  ",5,113,22.6,5.144230769230769,73,1,112,0.0089285714285714,0.017391304347826,-0.069,32,13,14,4,2,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,2,3,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0.1432797265543448,0.2222222222222222,0.06469149675622522
ICLR2018-HkwBEMWCZ-R3,Accept,"This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function. The discussion is largely based on a sequence of experiments, some of which are interesting and insightful. The discussion here can be useful for other researchers. My main concern is that the result here is purely empirical, with no concrete theoretical justification. What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation. Therefore, i found many of the discussion to be questionable. I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper. Do you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues? Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper. All the conclusions are based on one or two datasets. Could you consider testing the result on more different datasets to verify if the results are generalizable? ",11,196,19.6,5.18848167539267,113,0,196,0.0,0.0203045685279187,0.7498,50,21,36,8,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 1, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 0}",0,1,0,1,2,1,4,3,0,1,1,0,0,0,0,0,1,0,0,0,4,3,0,0.5718346533290098,0.3354125586545052,0.28828739218651267
ICLR2018-HkwVAXyCW-R1,Accept,"UPDATE: Following the author's response I've increased my score from 5 to 6. The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5]). The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function.  Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates. Pros: - Task of reducing computation by skipping inputs is interesting  - Model is novel and interesting - Experiments on multiple tasks and datasets confirm the efficacy of the method - Skipping behavior can be controlled via an auxiliary loss term - Paper is clearly written Cons: - Missing comparison to prior work on sequential MNIST - Low performance on Charades dataset, no comparison to prior work  - No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification The task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy. However, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work. On permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task. For Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on ""fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge"", and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don't expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else. In a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks. .  Though experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results. On the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author's own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments. References  [1] Le et al, ""A Simple Way to Initialize Recurrent Networks of Rectified Linear Units"", arXiv 2015 [2] Arjovsky et al, ""Unitary Evolution Recurrent Neural Networks"", ICML 2016 [3] Cooijmans et al, ""Recurrent Batch Normalization"", ICLR 2017 [4] Zhang et al, ""Architectural Complexity Measures of Recurrent Neural Networks"", NIPS 2016 [5] Sigurdsson et al, ""Hollywood in homes: Crowdsourcing data collection for activity understanding"", ECCV 2016 [6] Sigurdsson et al, ""Asynchronous temporal fields for action recognition"", CVPR 2017",33,774,30.96,5.56198347107438,322,4,770,0.0051948051948051,0.0177664974619289,0.9737,248,101,124,33,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 13, 'PDI': 0, 'DAT': 4, 'MET': 16, 'EXP': 6, 'RES': 10, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 10, 'PNF': 0, 'REC': 2, 'EMP': 13, 'SUB': 2, 'CLA': 2}",0,0,13,0,4,16,6,10,1,2,0,2,1,0,0,2,0,10,0,2,13,2,2,0.6486865857937963,0.6754373385353509,0.4606417023522769
ICLR2018-HkwVAXyCW-R2,Accept,"The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps. The proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time steps. The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks. Strength: - The experimental results on the simple skip RNNs have shown a good improvement over the previous results. Weakness: - Although the paper shows that skip RNN worked well, I found the appropriate baseline is lacking here. Comparable baselines, I believe, are regular LSTM/GRU whose inputs are randomly dropped out during training. - Most of the experiments in the main paper are on toy tasks with small LSTMs. I thought the main selling point of the method is the computational gain. Would it make more sense to show that on large RNNs with thousands of hidden units? After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry-picked, and it will be good to include more NLP tasks.",10,197,21.88888888888889,5.126984126984127,118,0,197,0.0,0.02,0.9578,60,32,35,5,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 0, 'DAT': 0, 'MET': 6, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,0,4,0,0,6,4,1,0,0,0,0,0,0,0,0,0,4,0,0,6,1,0,0.287465297365487,0.3367946081791181,0.14516873511637485
ICLR2018-HkwVAXyCW-R3,Accept,"This paper proposes an idea to do faster RNN inference via skip RNN state updates. I like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance. But the experiments are not convincing enough. First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task. I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters. Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim. Minor --  Cite Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation by Yoshua Bengio, Nicholas Leonard and Aaron Courville for straight-through estimator.",7,131,18.714285714285715,5.173228346456693,91,1,130,0.0076923076923076,0.037037037037037,-0.1747,42,13,23,5,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,0,3,0,2,1,0,0,0,0,0,1,0,0,0,0,0,0,0,3,0,0,0.3575116489803533,0.112355025980797,0.14180968825007884
ICLR2018-HkwZSG-CZ-R1,Accept,"The authors has addressed my concerns, so I raised my rating. The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting. There are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. [1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning. Results on this data might be more convincing. The results of MOS is very good, but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15. This will make it less usable, I think it's necessary to provide the training time comparison. Finally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER. [1] Melis, Gu00e1bor, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models.  arXiv preprint arXiv:1707.05589 (2017). [2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342 [3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017 ",15,248,13.052631578947368,5.2727272727272725,155,3,245,0.0122448979591836,0.0199203187250996,0.6994,83,39,33,13,8,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 5, 'MET': 3, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 5, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 6, 'CLA': 0}",0,0,0,1,5,3,2,5,0,1,0,0,5,1,0,0,0,0,0,0,5,6,0,0.5724318939976102,0.2252434345032014,0.2585830403637373
ICLR2018-HkwZSG-CZ-R2,Accept,"The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language. As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15). This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores. The question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention. This makes a lot of the questions asked in this paper extremely relevant to the field. However, it is unclear that the rank of the logit matrix is the right quantity to consider. For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices. Further, the first two observations in Section 2.2 would be more accurately described as intuitions of the authors. As they write themselves there is no evidence showing that semantic meanings are fully linearly correlated.  Why then try to link meanings to basis vectors for the rows of A? To be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084   sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant. Unfortunately, this is only tested on relatively small data sets, up to 2M tokens and a vocabulary of size 30K for language modeling. They do constitute a good starting place to test a model, but given the importance of regularization on those specific tasks, it is difficult to predict how the MoS would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting. Another important missing experiment would consist in varying the number of mixture components (this could very well be done on WikiText2). This could help validate the hypothesis: how does the estimated rank vary with the number of components? How about the performance and pairwise KL divergence? This paper offers a promising direction for language modeling research, but would require more justification, or at least a more developed experimental section. Pros: - Important starting question - Thought-provoking approach - Experimental gains on small data sets Cons: - The link between the intuition and reality of the gains is not obvious - Experiments limited to small data sets, some obvious questions remain",24,475,29.6875,5.216035634743875,239,0,475,0.0,0.03099173553719,0.9787,131,46,74,34,10,5,"{'ABS': 0, 'INT': 3, 'RWK': 1, 'PDI': 2, 'DAT': 4, 'MET': 10, 'EXP': 5, 'RES': 7, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 5, 'CLA': 0}",0,3,1,2,4,10,5,7,0,2,1,0,0,2,0,0,1,1,1,0,13,5,0,0.7174690952218082,0.5634457508069571,0.4495215923421674
ICLR2018-HkwZSG-CZ-R3,Accept,"Language models are important components to many NLP tasks. The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state. This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes. The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization. Pros: --The paper is very well written and easy to follow. The ideas build up on each other in an intuitive way. --The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know. --The maths is very rigorous. --The experiment section is thorough. Cons: --To claim SOTA all models need to be given the same capacity (same number of parameters). In Table 2 the baselines have a lower capacity. This is an unfair comparison --I suspect the proposed approach is slower than the baselines. There is no mention of computational cost. Reporting that would help interpret the numbers. The SOTA claim might not hold if baselines are given the same capacity. But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR.",17,222,13.875,5.09478672985782,130,1,221,0.004524886877828,0.027027027027027,0.8746,59,35,42,12,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 4, 'DAT': 0, 'MET': 3, 'EXP': 4, 'RES': 0, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 1}",0,1,5,4,0,3,4,0,1,2,0,3,0,2,0,1,0,4,0,1,4,2,1,0.644240850719816,0.668990703151087,0.4503550736185905
ICLR2018-HkwrqtlR--R1,Reject,"The main take-away messages of this paper seem to be:  1. GANs don't really match the target distribution. Some previous theory supports this, and some experiments are provided here demonstrating that the failure seems to be largely in under-sampling the tails, and sometimes perhaps in introducing spurious modes. 2. Even if GANs don't exactly match the target distribution, their outputs might still be useful for some tasks. (I wouldn't be surprised if you disagree with what the main takeaways are; I found the flow of the paper somewhat disjointed, and had something of a hard time identifying what the point was.) Mode-dropping being a primary failure mode of GANs is already a fairly accepted hypothesis in the community (see, e.g. Mode Regularized GANs, Che et al ICLR 2017, among others), though some extra empirical evidence is provided here. The second point is, in my opinion, simultaneously (i) an important point that more GAN research should take to heart, (ii) relatively obvious, and (iii) barely explored in this paper. The only example in the paper of using a GAN for something other than directly matching the target distribution is PassGAN, and even that is barely explored beyond saying that some of the spurious modes seem like reasonable-ish passwords. Thus though this paper has some interesting aspects to it, I do not think its contributions rise to the level required for an ICLR paper. Some more specifics:  Section 2.1 discusses four previous theoretical results about the convergence of GANs to the true density. This overview is mostly reasonable, and the discussion of Arora et al. (2017) and Liu et al. (2017) do at least vaguely support the conclusion in the last section of this paragraph. But this section is glaringly missing an important paper in this area: Arjovsky and Bottou (2017), cited here only in passing in the introduction, who proved that typical GAN architectures *cannot* exactly match the data distribution. Thus the question of metrics for convergence is of central importance, which it seems should be important to the topic of the present paper. (Figure 3 of Danihelka et al. https://arxiv.org/abs/1705.05263 gives a particularly vivid example of how optimizing different metrics can lead to very different results.) Presumably different metrics lead to models that are useful for different final tasks. Also, although they do not quite fit into the framing of this section, Nowozin et al.'s local convergence proof and especially the convergence to a Nash equilibrium argument of Heusel et al. (NIPS 2017, https://arxiv.org/abs/1706.08500) should probably be mentioned here. The two sample testing section of this paper, discussed in Section 2.2 and then implemented in Section 3.1.1, seems to be essentially a special case of what was previously done by Sutherland et al. (2017), except that it was run on CIFAR-10 as well. However, the bottom half of Table 1 demonstrates that something is seriously wrong with the implementation of your tests: using 1000 bootstrap samples, you should reject H_0 at approximately the nominal rate of 5%, not about 50%! To double-check, I ran a median-heuristic RBF kernel MMD myself on the MNIST test set with N_test   100, repeating 1000 times, and rejected the null 4.8% of the time. My code is available at https://gist.github.com/anonymous/2993a16fbc28a424a0e79b1c8ff31d24 if you want to use it to help find the difference from what you did. Although Table 1 does indicate that the GAN distribution is more different from the test set than the test set is from itself, the apparent serious flaw in your procedure makes those results questionable. (Also, it seems that your entry labeled MMD in the table is probably n * MMD_b^2, which is what is computed by the code linked to in footnote 2.) The appendix gives a further study of what went wrong with the MNIST GAN model, arguing based on nearest-neighbors that the GAN model is over-representing modes and under-representing the tails. This is fairly interesting; certainly more interesting than the rehash of running MMD tests on GAN outputs, in my opinion. Minor:  In 3.1.1, you say ideally the null hypothesis H0 should never be rejected u2013 it should be rejected at most an alpha portion of the time. In the description of section 3.2, you should clarify whether the train-test split was done such that unique passwords were assigned to a single fold or not: did 123456 appear in both folds? (It is not entirely clear whether it should or not; both schemes have possible advantages for evaluation.)",27,739,25.48275862068965,5.021428571428571,332,13,726,0.0179063360881542,0.0402684563758389,0.8558,186,89,119,62,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 3, 'MET': 11, 'EXP': 3, 'RES': 3, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 3}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 0}",0,1,4,0,3,11,3,3,3,0,0,2,0,3,1,0,0,2,1,0,13,1,0,0.6460671754165427,0.5631362073308612,0.40092374055599794
ICLR2018-HkwrqtlR--R2,Reject,"This paper considers the question of how well GANs capture the true data distribution. The train GAN models on MNIST, CIFAR and a pass word dataset and then use two-kernel ample tests to assess how well the models have modeled the data distribution. They find that in most cases GANs don't match the true distribution. It is unclear to me what the contribution of this paper is. The authors appear to simple perform experiments done elsewhere in different papers. I have not learned anything new by reading this work. Neither the method nor the results are novel contributions to the study of GANs.  The paper is also written in a very informal manner with several typos throughout. I would recommend the authors try to rewrite the work as perhaps more of a literature review + throughout experimentations of GAN evaluation techniques. In its current form I don't think it should be accepted. Additional comments: - The authors claim GANs are able to perform well even when data is limited. Could the authors provide some examples to back up this claim. As far as I understand GANs require lots of data to properly train. - on page 3 the authors claim that using human assessments of GAN generated images is bad because humans have a hard time performing the density estimation (they might ignore tails of the distribution for example) .. I think this is missing up a bunch of different ideas.. First, a key questions is *what do we want our GANs for?* Density estimation is only one of those answers. If the goal is density estimation then of course human evaluation is an inappropriate measure of performance. But if the goal is realistic synthesis of thats then human perceptual measures are more appropriate. Using humans can be ban in other ways of course since they would have a hard time assessing generalizability (i.e. you could just sample training images and humans would think the samples looked great!).  ",20,324,15.428571428571429,4.932475884244373,178,5,319,0.0156739811912225,0.0422960725075528,0.1719,95,36,66,23,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 3, 'MET': 6, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 11, 'SUB': 1, 'CLA': 1}",0,0,0,4,3,6,5,1,0,0,0,5,0,0,0,2,1,0,0,1,11,1,1,0.4305989286047431,0.6729313485996146,0.29761847258006696
ICLR2018-HkwrqtlR--R3,Reject,"This paper tried to tell us something else about GANs except for their implicit generation power. The conclusion is GANs can capture some structure of the data in high dimensional space. To me, the paper seems a survey paper instead of a research one. The introduction part described the involving of generative models and some related work about GANs. However, the author did not claim what the main contributions are. Even in Section 2, I can see nothing new but all the others' work. The experimental section included some simulation results, which are weird for me since they are not quite related to previous content. Moreover, the 3.1.1 KERNEL TWO-SAMPLE TEST is something which has been done in other paper [Li et al., 2017, Guo et al., 2017].  It is suggested that the author should delete some of the parts describing their work and make clear claims about the main contributions of the paper. Meanwhile, the experimental results should support the claims.   ",9,162,14.727272727272728,4.909677419354839,96,1,161,0.0062111801242236,0.0301204819277108,0.7096,47,15,26,8,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 1}",0,1,1,2,0,1,2,2,0,0,0,2,0,0,1,1,0,1,0,0,1,1,1,0.5002466163543244,0.6666666666666666,0.3478068838475733
ICLR2018-HkxF5RgC--R1,Accept,"The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference. Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper). It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal. Pros: - Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs - Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well Cons: - This work may be better reviewed at a more GPU-centric conference - Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity",8,204,40.8,5.01015228426396,117,4,200,0.02,0.0287081339712918,0.8533,55,23,40,13,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 1, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 2, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,0,0,2,0,1,4,1,0,0,0,2,0,1,2,0,0,0,0,0,4,1,0,0.4289347346164609,0.3352088715488304,0.2143918705294028
ICLR2018-HkxF5RgC--R2,Accept,"This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip. The paper describes the use additional mechanisms for synchronization and memory loading. The evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity). With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads. Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise. Storing weights persistent on chip should give a sharp benefit when all weights fit on the chip. One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those. To summarize, the paper adds the ability to support pruning over persistent RNNs. However, Narang et. al., 2017 already explore this idea, although briefly. Furthermore, the gains from the sparsity appear rather limited over real applications. I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads). Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial. However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized.",13,243,16.2,5.343347639484978,135,1,242,0.0041322314049586,0.037037037037037,0.9781,68,27,42,18,6,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,2,1,1,0,10,0,2,0,0,0,1,0,0,0,0,0,1,1,0,7,1,0,0.4309043784175154,0.4481761890535022,0.2391343092995473
ICLR2018-HkxF5RgC--R3,Accept,"The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels. The authors further propose model-pruning optimizations which are aware of the persistent implementation. It's not clear if the paper is relevant to the ICLR audience due to its emphasize on low-level optimization which has little insight in learning representations. The exposition in the paper is also not well-suited for people without a systems background, although I'll admit I'm mostly using myself as a proxy for the average machine learning researcher here. For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation. Modulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs. The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations.",7,151,21.571428571428573,5.605442176870748,94,0,151,0.0,0.0397350993377483,0.9327,45,24,23,7,5,6,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,2,0,1,0,1,1,0,0,0,0,5,0,0,2,1,1,1,0,0,1,0,1,0.3573500513043748,0.6666763325776349,0.24608009453351168
ICLR2018-Hy1d-ebAb-R1,Reject,"The paper introduces a generative model for graphs. The three main decision functions in the sequential process are computed with neural nets. The neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step T. The paper is well written but, in my opinion, a description of the learning framework should be given in the paper. Also, a summary of the hyperparameters used in the proposed system should be given. It is claimed that all possible types of graphs can be learned which seems rather optimistic. For instance, when learning trees, the system is tweaked for generating trees. Also, it is not clear whether models for large graphs can be learned. The paper contain many interesting contributions but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs. Therefore, I am not convinced that the paper is ready for publication at ICLR'18. * Introduction. I am not convinced by the discussion on graph grammars in the second paragraph. It is known that there does not exist a definition of regular grammars in graph (see Courcelle and Engelfriet, graph structure and monadic second-order logic ...). Moreover, many problems are known to be undecidable. For weighted automata, the reference Droste and Gastin considers weighted word automata and weighted logic for words. Therefore I does not seem pertinent here. A more complete reference is handbook of weighted automata by Droste. Also, many decision problems for wighted automata are known to be undecidable. I am not sure that the paragraph is useful for the paper. A discussion on learning as in footnote 1 shoud me more interesting. * Related work. I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs. * Section 3.1. Constraints can be introduced to impose structural properties of the generated graphs. This leads to the question of cheating in the learning process. * Section 3.2. The functions f_m and g_m for defining graph embedding are left undefined. As the graph embedding is used in the generating process and for learning, the functions must be defined and their choice explained and justified. * Section 3. As said before, a general description of the learning framework should be given. Also, it is not clear to me how the node and graph embeddings are initialized and how they evolve along the learning process. Therefore, it is not clear to me why the proposed updating framework for the embeddings allow to generate decision functions adapted to the graphs to be learned. Consequently, it is difficult to see the influence of T. Also, it should be said whether the node embeddings and graph embeddings for the output graph can be useful. * Section 3. A summary of all the hyperparameters should be given. * Section 4.1. The number of steps is not given. Do you present the same graph multiple times. Why T 2 and not 1 or 10 ? * Section 4.2. From table 2, it seems that all permutations are used for training which is rather large for molecules of size 20. Do you use tweaks in the generation process. * Section 4.3. The generation process is adapted for generating trees which seems to be cheating. Again the choice of T seems ad hoc and based on computational burden. * Section 5 should contain a discussion on complexity issues because it is not clear how the model can learn large graphs. * Section 5. The discussion on the difficulty of training shoud be emphasized and connected to the --missing-- description of the model architecture and its hyperparameters. * acronyms should be expansed at their first use",43,617,12.591836734693878,5.034305317324185,217,5,612,0.0081699346405228,0.0729001584786053,-0.9708,167,69,127,29,11,4,"{'ABS': 0, 'INT': 2, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 16, 'EXP': 1, 'RES': 2, 'TNF': 12, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 2, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 16, 'SUB': 8, 'CLA': 1}",0,2,4,2,0,16,1,2,12,1,0,4,2,1,0,0,0,0,0,1,16,8,1,0.7902540788088501,0.4545205415253393,0.4258021899314714
ICLR2018-Hy1d-ebAb-R2,Reject,"The authors proposed a graph neural network based architecture for learning generative models of graphs. Compared with traditional learners such as LSTM, the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data. The representation is clear with detailed empirical studies. I support its acceptance. The draft does need some improvements and here is my suggestions. 1. Figure 1 could be improved using a concrete example like in Figure 6. If space allowed, an example of different ordering leads to the same graph will also help. 2. More details on how node embedding vectors are initialized. How does different initializations affect results? Why is nodes at different stages with the same initialization problematic? 3. More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation. 4. The sequence ordering is important. While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment. I suggest the authors at least discuss the empirical over-fitting problem with respect to ordering. 5. In Section 4.1, the choice of ER random graph as a baseline is too simplistic. It does not provide a meaningful comparison. A better generative model for cycles and trees could help. 6. When comparing training curves with LSTM, it might be helpful to also include the complexity comparison of each iteration.",18,231,10.5,5.61214953271028,137,1,230,0.0043478260869565,0.0129870129870129,0.9836,69,31,41,10,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 2, 'RES': 2, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 1, 'EMP': 9, 'SUB': 4, 'CLA': 0}",0,1,2,0,0,7,2,2,3,2,0,4,0,0,0,0,0,1,3,1,9,4,0,0.5733596814242627,0.5609743824068704,0.36485521566443224
ICLR2018-Hy1d-ebAb-R3,Reject,"The authors introduce a sequential/recurrent model for generation of small graphs. The recurrent model takes the form of a graph neural network. Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden state. The paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field. The proposed model has several interesting novelties (mainly in terms of new applications/experiments, and being fully auto-regressive), yet also shares many similarities with the generative component of the model introduced in [1] (not cited): Both models make use of (recurrent) graph neural networks to learn intermediate node representations, from which they predict whether new nodes/edges should be added or not. [1] speeds this process up by predicting multiple nodes and edges at once, whereas in this paper, such a multi-step process is left for future work. Training the generative model with fixed ground-truth ordering was similarly performed in [1] (""strong supervision"") and is thus not particularly novel. Eqs.1-3: Why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop (h_v -> h_v')? Have the authors experimented with other variants (dropping the weight sharing in either or both of these steps)? Ordering problem: A solution for the ordering problem was proposed in [2]: learning a matching function between the orderings of model output and ground truth. A short discussion of this result would make the paper stronger. For chemical molecule generation, a direct comparison to some more recent work (e.g. the generator of the grammar VAE [3]) would be insightful. Other minor points: - In the definition of f_nodes: What is p(y)? It would be good to explicitly state that (boldface) s is a vector of scores s_u (or score vectors, in case of multiple edge types) for all u in V. u2028 - The following statement is unclear to me: ""but building a varying set of objects is challenging in the first place, and the graph model provides a way to do it. "" Maybe this can be substantiated by experimental results (e.g. a comparison against Pointer Networks [4])? - Typos in this sentence: ""Lastly, when compared using the genaric graph generation decision sequence, the Graph architecture outperforms LSTM in NLL as well. ""  Overall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well-written, well-presented and addresses an important problem. [1] D.D. Johnson, Learning Graphical State Transitions, ICLR 2017 [2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016 [3] M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017 [4] O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, NIPS 2015",20,481,17.178571428571427,5.470588235294118,263,1,480,0.0020833333333333,0.0163934426229508,0.8763,164,77,71,26,10,8,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 5, 'RES': 0, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 8, 'SUB': 1, 'CLA': 4}",0,2,3,2,0,10,5,0,1,1,1,2,2,0,0,2,1,2,1,1,8,1,4,0.7171858328854379,0.8936482662870601,0.6343429963368252
ICLR2018-Hy3MvSlRW-R1,Reject,"The paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent (which is called narrator by the authors) that obfuscates the document, i.e. changing words in the document. The authors mention that word dropout can be considered as its special case which randomly drops words without any prior. Then the authors claim that smartly choosing the words to drop can make a stronger adversarial agent, which in turn would improve the performance of the reader as well. Hence the adversarial agent is trained and is architecturally similar to the reader but just has a different last layer, which predicts the word that would make the reader fail if the word is obfuscated.. I think the idea is interesting and novel.. While there have been numerous GAN-like approaches for language understanding, very few, if any, have shown worthy results. So if this works, it could be an impactful achievement.. However, I am concerned with the experimental results. First, CBT: NE and CN numbers are too low. Even a pure LSTM achieves (no attention, no memory) 44% and 45%, respectively (Yu et al., 2017). These are 9% and 6% higher than the reported numbers for adversarial GMemN2N.  So it is very difficult to determine if the model is appropriate for the dataset in the first place, and whether the gain from the non-adversarial setting is due to the adversarial setup or not. Second, Cambridge dialogs: the dataset's metric is not accuracy-based (while the paper reports accuracy), so I assume some preprocessing and altering have been done on the dataset. So there is no baseline to compare. Though I understand that the point of the paper is the improvement via the adversarial setting, it is hard to gauge how good the numbers are. Third, TripAdvisor: the dataset paper by Wang et al. (2010) is not evaluated on accuracy (rather on ranking, etc.). Did you also make changes to the dataset? Again, this makes the paper less strong because there is no baseline to compare. In short, the only comparable dataset is CBT, which has too low accuracy compared to a very simple baseline. In order to improve the paper, I recommend the authors to evaluate on more common datasets and/or use more appropriate reading models. ---  Typos: page 1 first para: One the first hand -> On the first hand page 1 first para: minimize to probability -> minimize the probability page 3 first para: compensate -> compensated page 3 last para: softmaxis -> softmax is page 4 sec 2.4: similar to the reader -> similarly to the reader page 4 sec 2.4: unknow -> unknown page 4 sec 3 first para: missing reference at a given dialog page 5 first para: Concretly -> Concretely Table 1: GMenN2N -> GMemN2N Table 1: what is difference between mean and average? page 8 last para: missing reference at Iterative Attentive Reader page 9 sec 6.2 last para: several citations missing, e.g. which paper is by Tesauro? [Yu et al. 2017] Adams Wei Yu, Hongrae Kim, and Quoc V. Le. Learning to Skim Text. ACL 2017  ",23,510,18.88888888888889,4.892631578947368,238,1,509,0.0019646365422396,0.0133843212237093,0.9065,153,78,83,32,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 5, 'DAT': 5, 'MET': 4, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,0,7,5,5,4,4,2,0,0,1,2,2,0,0,0,1,4,0,0,9,1,1,0.644844310100521,0.5608827027058693,0.41014357833307274
ICLR2018-Hy3MvSlRW-R2,Reject,"Summary:  This paper proposes an adversarial learning framework for machine comprehension task. Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task. Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N. My Comments:  This paper is a direct application of adversarial learning to the task of reading comprehension. It is a reasonable idea and authors indeed show that it works. 1. The paper needs a lot of editing. Please check the minor comments. 2. Why is the adversary called narrator network? It is bit confusing because the job of that network is to obfuscate the passage. 3. Why do you motivate the learning method using self-play? This is just using the idea of adversarial learning (like GAN) and it is not related to self-play. 4 In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting. How is this happening? Can you elaborate more? 5. The learning framework is not explained in a precise way. What do you mean by re-initializing and retraining the narrator? Isn't it costly to reinitialize the network and retrain it for every turn? How many such epochs are done? You say that test set also contains obfuscated documents. Is it only for the validation set? Can you please explain if you use obfuscation when you report the final test performance too? It would be more clear if you can provide a complete pseudo-code of the learning procedure. 6. How does the narrator choose which word to obfuscate? Do you run the narrator model with all possible obfuscations and pick the best choice? 7. Why don't you treat number of hops as a hyper-parameter and choose it based on validation set? I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set. 8. In figure 2, how are rounds constructed? Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement? This will be clear if you provide the pseudo-code for learning. 9. I do not understand author's' justification for figure-3. Is it the case that the model learns to attend to last sentences for all the questions? Or where it attends varies across examples?  10. Are you willing to release the code for reproducing the results?  Minor comments:  Page 1, ""exploit his own decision"" should be ""exploit its own decision "" In page 2, section 2.1, sentence starting with ""Indeed, a too low percentage ..."" needs to be fixed. Page 3, ""forgetting is compensate"" should be ""forgetting is compensated"". Page 4, ""for one sentences"" needs to be fixed. Page 4, ""unknow"" should be ""unknown"". Page 4, ""?? "" needs to be fixed. Page 5, ""for the two first datasets"" needs to be fixed. Table 1, ""GMenN2N"" should be ""GMemN2N"". In caption, is it mean accuracy or maximum accuracy? Page 6, ""dataset was achieves"" needs to be fixed. Page 7, ""document by obfuscated this word"" needs to be fixed. nPage 7, ""overall aspect of the two first readers"" needs to be fixed. Page 8, last para, references needs to be fixed. Page 9, first sentence, please check grammar. Section 6.2, last sentence is irrelevant. ",45,574,14.717948717948715,4.94172932330827,218,0,574,0.0,0.0222602739726027,0.8346,154,45,127,18,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 3, 'MET': 17, 'EXP': 3, 'RES': 3, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 22, 'SUB': 1, 'CLA': 1}",0,1,0,3,3,17,3,3,4,0,0,3,1,0,0,0,0,0,3,0,22,1,1,0.6474955247482532,0.4576286884237535,0.3681462568909116
ICLR2018-Hy3MvSlRW-R3,Reject,"The main idea of this paper is to automate the construction of adversarial reading comprehension problems in the spirit of Jia and Liang, EMNLP 2017. In that work a distractor sentence is manually added to a passage to superficially, but not logically, support an incorrect answer. It was shown that these distractor sentences largely fool existing reading comprehension systems although they do not fool human readers. This paper replaces the manual addition of a distractor sentence with a single word replacement where a  arrator is trained adversarially to select a replacement to fool the question answering system. This idea seems interesting but very difficult to evaluate. An adversarial word replacement my in fact destroy the factual information needed to answer the question and there is no control for this. The performance of the question answering system in the presence of this adversarial narrator is of unclear significance and the empirical results in the paper are very difficult to interpret. No comparisons with previous work are given (and perhaps cannot be given). A better model would be the addition of a distractor sentence as this preserves the information in the original passage. A language model could probably be used to generate a compelling distractor. But we want that the corrupted passage has the same correct answer as the uncorrupted passage and this difficult to guarantee. A trained  arrator could learn to actually change the correct answer.",12,235,19.58333333333333,5.443946188340807,117,3,232,0.0129310344827586,0.0337552742616033,-0.969,61,23,43,13,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 4, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 0}",0,0,2,4,0,7,1,1,0,0,0,0,0,0,0,0,0,1,0,0,7,0,0,0.3589023973135163,0.22595396683128,0.16001205726728202
ICLR2018-Hy6GHpkCW-R1,Accept,"This paper introduces a neural network architecture for generating sketch drawings. The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts I agree. The contribution of this paper of this paper is two-fold. I Firstly, the paper introduces a large sketch dataset that future papers can rely on. Secondly, the paper introduces the model for generating sketch drawings. I  The model is inspired by the variational autoencoder. I However, the proposed method departs from the theory that justifies the variational autoencoder. I believe the following things would be interesting points to discuss / follow up: - The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood. It seems to have a negative impact for the range of values that are discussed. However, I would expect there to be an optimum. Does the KL term help prevent overfitting at some stage? Answering this question may help understand what influence variational inference has on this model. - The decoder model has randomness injected in it at every stage of the RNN. Because of this, the latent state actually encodes a distribution over drawings, rather than a single drawing. It seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high KL regularisation term. Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic? This definitely would bring it closer to the way the VAE was originally introduced. - The unconditional generative model *only* relies on the injected randomness for generating drawings, as the initial state is initialised to 0. This also is not in the spirit of the original VAE, where unconditional generation involves sampling from the prior over the latent space. I believe the design choices made by the authors to be valid in order to get things to work. But it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well (or whether it works better). This would help interesting applications inform what is wrong with current theoretical views. Overall, I would argue that this paper is a clear accept.",24,364,17.333333333333332,5.3130434782608695,176,4,360,0.0111111111111111,0.035230352303523,0.9739,93,41,72,23,10,5,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 16, 'EXP': 1, 'RES': 5, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 1, 'EMP': 14, 'SUB': 3, 'CLA': 0}",0,2,1,1,2,16,1,5,0,1,1,2,0,0,0,0,1,1,0,1,14,3,0,0.7183684320405815,0.5638543552251571,0.4483645265552015
ICLR2018-Hy6GHpkCW-R2,Accept,"The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder. Each node is represented with (dx, dy) along with one-hot representation of three different drawing status. A bi-directional LSTM is used to encode latent space in the training stage. Auto-regressive VAE is used for decoding. Similar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term. Pros: - Good solution to an interesting problem. - Very interesting dataset to be released. - Intensive experiments to validate the performance. Cons: - I am wondering whether the dataset contains biases regarding (dx, dy). In the data collection stage, how were the points lists generated from pen strokes?  Did each points are sampled from same travelling distance or according to the same time interval?  Are there any other potential biases brought because the data collection tools? - Is log-likelihood a good loss here? Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke. How do you deal with this case? - Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.? ",16,204,17.0,5.345,134,1,203,0.0049261083743842,0.0234741784037558,0.6962,65,25,44,7,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 9, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,0,0,3,9,4,2,2,0,0,1,0,0,0,0,0,1,0,0,0,10,1,0,0.4298712282400601,0.33893095024692,0.2167928884059191
ICLR2018-Hy6GHpkCW-R3,Accept,"The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches. + new and large dataset + novel algorithm + well written - no evaluation of dataset - virtually no evaluation of algorithm - no baselines or comparison The paper is well written, and easy to follow. The presented algorithm sketch-rnn seems novel and significantly different from prior work. In addition, the authors collected the largest sketch dataset, I know of. This is exciting as it could significantly push the state of the art in sketch understanding and generation. Unfortunately the evaluation falls short. If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short. For ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop. Remove the variational component, and phrasing it simply as an auto-encoder. Table 1 is good, but not sufficient. Training loss alone likely does not capture the quality of a sketch. A comparison the Graves 2013 is absolutely required, more comparisons are desired. Finally, it would be nice to see where the algorithm falls short, and where there is room for improvement. If the authors wish to push their dataset, it would help to first evaluate the quality of the dataset. For example, how well do humans classify these sketches? How diverse are the sketches? Are there any obvious modes? Does the discretization into strokes matter? Additionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on? Are there any good automated metrics, and how well do they correspond to human judgement? In summary, I'm both excited about the dataset and new architecture, but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset. I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation.",30,359,22.4375,5.217391304347826,170,2,357,0.0056022408963585,0.0164383561643835,0.9921,91,45,69,19,9,7,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 7, 'MET': 16, 'EXP': 6, 'RES': 1, 'TNF': 2, 'ANA': 1, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 1, 'CMP': 5, 'PNF': 2, 'REC': 0, 'EMP': 12, 'SUB': 9, 'CLA': 2}",0,0,3,0,7,16,6,1,2,1,1,2,0,0,0,3,1,5,2,0,12,9,2,0.6474872405353423,0.7861742910200759,0.5143145393211851
ICLR2018-Hy7fDog0b-R1,Accept,"Quick summary: This paper shows how to train a GAN in the case where the dataset is corrupted by some measurement noise process. They propose to introduce the noise process into the generation pipeline such that the GAN generates a clean image, corrupts its own output and feeds that into the discriminator. The discriminator then needs to decide whether this is a real corrupted measurement or a generated one. The method is demonstrated to the generate better results than the baseline on a variety of datasets and noise processes. Quality: I found this to be a nice paper - it has an important setting to begin with and the proposed method is clean and elegant albeit a bit simple. Originality: I'm pretty sure this is the first paper to tackle this problem directly in general. Significance: This is an important research direction as it is not uncommon to get noisy measurements in the real world under different circumstances. Pros: * Important problem * elegant and simple solution * nice results and decent experiments (but see below) Cons: * The assumption that the measurement process *and* parameters are known is quite a strong one. Though it is quite common in the literature to assume this, it would have been interesting to see if there's a way to handle the case where it is unknown (either the process, parameters or both). * The baseline experiments are a bit limited - it's clear that such baselines would never produce samples which are any better than the fixed version which is fed into them. I can't however, think of other baselines other than ignore so I guess that is acceptable. * I wish the authors would show that they get a *useful* model eventually - for example, can this be used to denoise other images from the dataset? Summary: This is a nice paper which deals with an important problem, has some nice results and while not groundbreaking, certainly merits a publication.",16,319,26.58333333333333,5.013245033112582,165,2,317,0.0063091482649842,0.0365853658536585,0.9877,86,46,59,12,8,8,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 3, 'DAT': 2, 'MET': 6, 'EXP': 5, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 9, 'SUB': 2, 'CLA': 1}",0,2,3,3,2,6,5,4,0,0,0,4,0,0,0,1,1,1,1,1,9,2,1,0.5736688624642979,0.8939712248759543,0.49742171837239935
ICLR2018-Hy7fDog0b-R2,Accept,"The paper explores GAN training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus noise. The paper explores in detail several practically useful versions of the linear measurement model, such as blurring, linear projection, masking etc. and establishes identifiability conditions/theorems for the underlying models. The AmbientGAN approach advocated in the paper amounts to learning end-to-end differentiable Generator/Discriminator networks that operate in the measurement space. The experimental results in the paper show that this works much better than reasonable baselines, such as trying to invert the measurement model for each individual training sample, followed by standard GAN training. The theoretical analysis is satisfactory. However, it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of AmbientGAN training. For example, if the condition number for the linear measurement model is high, one would expect that recovering the target real distribution is more difficult. The condition in Theorem 5.4 is a step in this direction, showing that the required number of samples for correct recovery increases with the probability of missing data. It would be great if Theorems 5.2 and 5.3 also came with similar quantitative bounds.",9,222,22.2,5.672897196261682,122,0,222,0.0,0.0135135135135135,0.9255,61,37,32,8,6,1,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 0}",0,2,0,1,0,7,3,3,0,1,0,0,0,0,0,0,0,0,0,0,6,0,0,0.4303818955522883,0.1142208982853259,0.1751040429665762
ICLR2018-Hy7fDog0b-R3,Accept,"The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves. Broadly, the approach is to have a generator produce the full real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples. By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements. The paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing. The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement). However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative. I would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.). Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training). I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality).",9,310,34.44444444444444,5.486486486486487,170,5,305,0.0163934426229508,0.0288461538461538,0.9686,81,52,46,21,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,0,1,0,5,4,2,0,2,0,1,0,0,0,0,0,0,0,0,6,1,1,0.5013397771035905,0.3364431205075482,0.2508564932749166
ICLR2018-Hy8hkYeRb-R1,Reject,"Quality  The authors introduce a deep network for predictive coding. It is unclear how the approach improves on the original predictive coding formulation of Rao and Ballard, who also use a hierarchy of transformations. The results seem to indicate that all layers are basically performing the same. No insight is provided about the kinds of filters that are learned. n Clarity  In its present form it is hard to assess if there are benefits to the current formulation compared to already existing formulations. . The paper should be checked for typos. Originality  There exist alternative deep predictive coding models such as https://arxiv.org/abs/1605.08104. This work should be discussed and compared. Significance   It is hard to see how the present paper improves on classical or alternative (deep) predictive coding results. Pros  Relevant attempt to develop new predictive coding architectures Cons  Unclear what is gained compared to existing work.",11,145,13.181818181818182,5.398601398601398,89,0,145,0.0,0.0522875816993464,0.872,38,22,34,4,4,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,0,2,0,0,5,0,4,0,0,0,1,0,0,0,1,0,1,0,0,6,1,1,0.2869626477499367,0.5586653427297704,0.1751652512308663
ICLR2018-Hy8hkYeRb-R2,Reject,"The paper A Deep Predictive Coding Network for Learning Latent Representations considers learning of a generative neural network. The network learns unsupervised using a predictive coding setup. A subset of the CIFAR-10 image database (1000 images horses and ships) are used for training. Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown. It is then claimed that the generated images show that the network has learned good latent representations. I have some concerns about the paper, maybe most notably about the experimental result and the conclusions drawn from them. The numerical experiments are motivated as a way to understand the capacity of the network with regards to modeling the external environment (abstract). And it is concluded in the final three sentences of the paper that the presented network can infer effective latent representations for images of other objects (i.e., of objects that have not been used for training); and further, that in this regards, the network is better than most existing algorithms [...]. I expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggested. Also some at least quantifiable (if not benchmarked) outcomes should have been presented given the rather strong claims/conclusions in abstract and discussion/conclusion sections. As a matter of fact, all images shown (including those in the appendix) are blurred versions of the original images, except of one single image: Fig. 4 last row, 2nd image (and that is not commented on). If these are the generated images, then some reconstruction is done by the network, fine, but also not unsurprising as the network was told to do so by the used objective function. What precisely do we learn here? I would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network. How can the reader conclude any functioning from these images? Using the right objective function, reconstructions can also be obtained using random (not learned) generative fields and relatively basic models. The fact that image reconstruction for shifted images or new images is evidence for a sophisticated latent representations is, to my mind, not at all shown here. What would be a good measure for an effective latent representation that substantiates the claims made? The reconstruction of unseen images is claimed central but as far as I could see, Figures 2, 3, and 4 are not even referred to in the text, nor is there any objective measure discussed. Studying the relation between predictive coding and deep learning makes sense, but I do not come to the same (strong) conclusions as the author(s) by considering the experimental results - and I do not see evidence for a sophisticated latent representation learned by the network. I am not saying that there is none, but I do not see how the presented experimental results show evidence for this. Furthermore, the authors stress that a main distinguishing feature of their approach (top of page 3) is that in their network information flows from latent space to observed space (e.g. in contrast to CNNs). That is a true statement but also one which is true for basically all generative models, e.g., of standard directed graphical models such as wake-sleep approaches (Hinton et al., 1995), deep SBNs and more recent generative models used in GANs (Goodfellow et al, 2014). Any of these references would have made a lot of sense. With my evaluation I do not want to be discouraging about the general approach. But I can not at all give a good evaluation given the current experimental results (unless substantial new evidence which make me evaluate these results differently is provided in a discussion). Minor:  - no legend for Fig. 1  -notes -> noted  have focused     ",27,647,23.96296296296296,5.265472312703583,263,1,646,0.001547987616099,0.0091047040971168,0.9915,171,81,117,39,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 7, 'RES': 10, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 12, 'SUB': 6, 'CLA': 0}",0,1,1,1,1,10,7,10,3,0,0,0,1,0,0,0,0,1,3,0,12,6,0,0.6461739595872552,0.4519424966169311,0.37014934055939896
ICLR2018-Hy8hkYeRb-R3,Reject,"The paper attempts to extend the predictive coding model to a multilayer network. The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images. The overall idea and approach being pursued here is a good one, but the model needs further development.",4,49,16.333333333333332,5.304347826086956,37,0,49,0.0,0.0204081632653061,0.3182,15,5,10,1,4,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 1, 'CLA': 0}",0,1,0,1,1,3,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0.286214669941855,0.3333333333333333,0.14397897666493223
ICLR2018-HyB9Np6WG-R1,,"This paper provides a new method for learning representations of prepositions. The basic idea is to count word pairs which co-occur with a preposition, rather than single words which co-occur, as in standard word vector models such as word2vec. This seems to work quite well, and I speculate that it is because prepositions often function to indicate grammatical relations between different arguments, rather than being content-bearing words themselves. The paper counts up these word-pair co-occurrences in a tensor, then applies a tensor decomposition and low-rank approximation method to produce word and preposition representations. Experiments show that the method helps to find paraphrases of phrasal verbs, as well as improve downstream performance on preposition selection and preposition attachment disambiguation. This paper was quite interesting and clearly written for the most part. I enjoyed reading it and the various evaluations that it described that target both the use of prepositions inside phrasal verbs as well as in its role in indicating grammatical relationships between different elements in a sentence. I think that this work could be quite useful to the field, but that a number of frustrating weaknesses prevent me from recommending it without qualifications. The main weaknesses of the paper are in the soundness of some of its qualitative analyses and claims. First, I found the cosine similarity scores in Table 1 largely uninterpretable. The claim is that different prepositions should have representations that are sufficiently distinct from each other. Even if we accept this premise (and why should we? They are of the same syntactic category after all), using the similarity scores to make this argument is not reasonable, as there is no absolute interpretation or calibration of the similarity scores that can be applied across models. Is 0.22 in similarity high or low? The other evaluation decision that is confusing is the paraphrase evaluation of the phrasal verbs. This was not done systematically, but a broad general claim that the tensor multiplication models does the best cannot be verified. To me, the word2vec addition paraphrases also look quite good. It seems to me that a human subject experiment to somehow compare the two methods is required. I wonder whether this approach could be generalized to other classes of words or morphemes. For example, you could imagine that in a morphologically rich language, this method would work well to learn the representation of certain morphemes such as case endings or verbal conjugation.",19,402,22.33333333333333,5.487046632124352,210,4,398,0.0100502512562814,0.0323383084577114,0.9476,102,48,66,24,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 11, 'EXP': 2, 'RES': 7, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 14, 'SUB': 1, 'CLA': 1}",0,1,0,4,0,11,2,7,1,1,0,2,0,0,0,0,0,0,0,1,14,1,1,0.5745742181080012,0.452529891097403,0.32176480483529735
ICLR2018-HyB9Np6WG-R2,,"This paper introduces a method for learning representations for prepositions. They first take co-occurrence counts counts of pairs of words in a local window of each preposition, and then factorize the matrix to find low dimensional word representations. The main difference from previous work is restricting the context to be close to a preposition. The authors report improved paraphrasing of phrasal verbs, and state-of-the-art accuracy in correcting grammatical errors involving prepositions, and good results on prepositional phrase attachment. - The paper frequently overclaims. For one example, we're told that ""Preposition selection [is] a major area of study in both syntactic and semantic computational linguistics"", but at best it's quite a specialized niche. The paper would be much improved if it was generally toned down. - The authors claim their method is ""vastly"" better at paraphrasing phrasal verbs than baselines, based on qualitative comparison. However, I couldn't find any details on how the phrasal verbs were chosen, or what (if any) held out data was used for tuning the models. Even assuming this is a meaningful task, surely the natural baseline would be to treat these phrasal verbs as non-compositional (e.g. extend the vocab with words like ""sparked_off"")  and train Word2Vec. - The other experiments are lacking important details. For example, we're just told some values that hyperparameters were fixed at for both tasks - how were these chosen (including for the baselines)? Was the model architecture tuned based on the proposed representations?  Were the word representations fixed, or fine tuned during training? - Despite the author's expectations that their representations will be 'widely used', I am struggling to think of cases where they would be useful, outside of the very specific tasks involving prepositions that they use. That is because almost all tasks require good representations for all words, not just prepositions. The authors should add more justification for the where/how these representations will be useful. Overall, I think the technical contributions of the paper are quite limited, and the experiments are not well enough described for publication.",18,333,20.8125,5.533950617283951,183,2,331,0.0060422960725075,0.0117647058823529,0.993,91,32,69,25,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 1, 'MET': 10, 'EXP': 4, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 4, 'CLA': 0}",0,1,4,2,1,10,4,2,0,0,0,2,0,0,0,0,0,3,0,0,9,4,0,0.5743407720975272,0.3388633474514215,0.2930436404211606
ICLR2018-HyB9Np6WG-R3,,"This paper proposes to learn vector representations of prepositions by learning them as tensor decompositions of a triple of a left word (maybe head), the preposition, and right word (maybe dependent).  This is an interesting idea with linguistic validity, and practically possible because of the commonness and promiscuity of prepositions, reflecting their primary grammatical and relational roles (as function words not content words). The resulting representations are show to be useful u2013 they produce SOTA results on preposition selection by a decent margin (on a practically useful and recently studied tasks that is arguably the task that best reflects on the quality of the learned representations) and good (but not quite SOTA) results, without further linguistic features beyond POS tags, on the much more studied task of preposition attachment disambiguation. Overall, I really liked this paper. Despite this enthusiasm, I am doubtful whether this is a good paper for ICLR. And I write this as a card-carrying computational linguist. This is partly because of the writing. It is very hard not to see the content of the introduction as addressing a linguistics/computational linguistics audience rather than the mainstream of the ICLR audience (you get this impression rather strongly from the start of each of the first 3 paragraphs of the introduction...). More profoundly, this impression comes from the nature of the investigation and results. While this paper makes a contribution to representation learning in suggesting a good way to learn a representation for prepositions, it does not make any contributions to methods of representation learning. Indeed, it is basically an application of the orthogonalized alternating least squares method of Sharan and Valiant (2017) and more generally of the tensor decomposition ideas of numerous papers of Anandkumar and colleagues. There aren't any new technical ideas here. While the learned representations are successful for the two main performance tasks discussed above, the ancillary evidence provided from the paraphrasing of prepositional phrase seems highly uncompelling to me. That is, the task seems a completely valid one u2013 one would like to be able to show that sparked off is a synonym of provoked, but the actually results provided on this task seem quite uncompelling. Among other things, the example used in the text in section 3 seems bad to me. It isn't really the case that split off something means divided something.  (Sally split off a sliver of wood does not mean Sally divided a sliver of wood. separated would be much closer.  Indeed, of the examples in Table 2, the first 3 look bad, the fourth isn't generally  true but valid in certain contexts, the 5th is again wrong  and only the 6th is really good. Similar remarks for the many more examples in the supplementary materials. The most intriguing question is the one raised in the first paragraph of the conclusion: While prepositions are natural for modeling via word triples and indeed their high frequency and small number of types makes this quite practical, the kind of concerns raised here are also applicable to a whole bunch of word types, and it would be natural to want to extend the method to them. E.g., we would also like to learn synonymy with light verb like take note or pay attention means roughly  otice or observe; or the widely studied SVO triples like <rock,sank,ship> would also seem to cry out for a tensor decomposition. It would be interesting to think about what further might be done here. Minor comments:  - Abstract: saying that word2vec and GloVe treat prepositions as content words seems slightly wrong; really they treat them just as words since all words are treated the same u2013 though one can argue that most words are content words and the method of modeling word meaning is generally much more appropriate for content words. - p.2: folklore within the NLP community. I'm not sure whether this is true or not; while pairwise counts have been the method of choice in recent word vector learning methods, it wasn't true of older methods (Collobert and Weston or Bengio's NPLM) and n-gram counts for n > 2 are widespread in pre-neural NLP. At any rate, it is rather unconvincing when the only evidence you cite is the paper by Sharan and Valiant, where AFAIK, neither author has ever published an NLP paper or attended  an NLP conference....  - p.5 FEC should be FCE; Ng et al. (2014) should be (Ng et al., 2014)   ",27,734,25.310344827586206,5.098430813124108,328,9,725,0.0124137931034482,0.0333333333333333,0.9813,189,100,108,70,10,6,"{'ABS': 1, 'INT': 2, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 6, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 2, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 16, 'SUB': 1, 'CLA': 2}",1,2,3,2,0,10,0,6,1,0,1,4,1,0,2,0,1,1,0,0,16,1,2,0.7171980037934144,0.6760868291711335,0.5017517393471048
ICLR2018-HyBbjW-RW-R1,Reject,"This paper considers hyperparameter searches in which all of the candidate points are selected in advance. The most common approaches are uniform random search and grid search, but more recently low-discrepancy sequences have sometimes been used to try to achieve better coverage of the space.  This paper proposes using a variant of the determinantal point process, the k-DPP to select these points. The idea is that the DPP provides an alternative form of diversity to low-discrepancy sequences. Some issues I have with this paper:  1. Why a DPP? It's pretty heavyweight. Why not use any of the other (potentially cheaper) repulsive point processes that also achieve diversity?  Is there anything special about it that justifies this work? 2. What about all of the literature on space-filling designs, e.g., latin hypercube designs? Statisticians have thought about this for a long time. 3. The motivation for not using low-discrepancy sequences was discrete hyperparameters. In practice, people just chop up the space or round. Is a simple kernel with one length scale on a one-hot coding adding value? In this setup, each parameter can only contribute same or different to the diversity assessment.  In any case, the evaluations didn't have any discrete parameters.  Given that the discrete setting was the motivation for the DPP over LDS, it seems strange not to even look at that case. 4. How do you propose handling ordinal variables? They're a common case of discrete variables but it wouldn't be sensible to use a one-hot coding. 5. Why no low discrepancy sequence in the experimental evaluation of section 5? Since there's no discrete parameters, I don't see what the limitation is. 6. Why not evaluate any other low discrepancy sequences than Sobol? 7. I didn't understand the novelty of the MCMC method relative to vanilla M-H updates. It seems out of place. 8. The figures really need error bars --- Figure 3 in particular.  Are these differences statistically significant? ",25,320,12.8,5.246666666666667,180,3,317,0.0094637223974763,0.0182926829268292,0.9102,92,36,52,21,5,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 20, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 3}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 0}",0,0,0,1,0,20,1,0,1,0,0,0,0,3,0,1,1,0,0,0,14,1,0,0.3619151634410799,0.452529891097403,0.2055413103625431
ICLR2018-HyBbjW-RW-R2,Reject,"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al. (2011). The k-DPP sampling algorithm and the concept of k-DPP-RBF over hyperparameters are not new, so the main contribution here is the empirical study. The first experiment by the authors shows that k-DPP-RBF gives better star discrepancy than uniform random search while being comparable to low-discrepancy Sobol sequences in other metrics such as distance from the center or an arbitrary corner (Fig. 1). The second experiment shows surprisingly that for the hard learning rate range, k-DPP-RBF performs better than uniform random search, and moreover, both of these outperform BO-TPE (Fig. 2, column 1). I have a few reservations. First, I do not find these outcomes very surprising or informative, except for the second experiment (Fig. 2, column 1). Second, their study only applies to a small number like 3-6 hyperparameters with a small k 20. The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al. (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al. (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.     ",9,231,14.4375,5.576576576576577,135,0,231,0.0,0.0127118644067796,0.9259,74,53,19,12,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 7, 'EXP': 5, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,0,1,2,0,7,5,1,2,0,0,0,0,1,0,0,0,1,0,0,5,1,0,0.5019285336721051,0.3358211630727052,0.2550774042102097
ICLR2018-HyBbjW-RW-R3,Reject,"In this paper, the authors consider non-sequential (in the sense that many hyperparameter evaluations are done simultaneously) and uninformed (in the sense that the hyperparameter evaluations are chosen independent of the validation errors observed) hyperparameter search using determinantal point processes (DPPs). DPPs are probability distributions over subsets of a ground set with the property that subsets with more diverse elements have higher probability.  Diverse here is defined using some similarity metric, often a kernel. Under the RBF kernel, the more diverse a set of vectors is, the closer the kernel matrix becomes to the identity matrix, and thus the larger the determinant (and therefore probability under the DPP) grows. The authors propose to do hyperparameter tuning by sampling a set of hyperparameter evaluations from a DPP with the RBF kernel. Overall, I have a couple of concerns about novelty as well as the experimental evaluation for the authors to address. As the authors rightly point out, sampling hyperparameter values from a DPP is equivalent to sampling proportional to the posterior uncertainy of a Gaussian process, effectively leading to a pure exploration algorithm. As the authors additionally point out, such methods have been considered before, including methods that directly propose to batch Bayesian optimization by choosing a single exploitative point and sampling the remainder of the batch from a DPP (e.g., [Kathuria et al., 2016]). The default procedure for parallel BayesOpt used by SMAC [R2] is (I believe) also to choose a purely explorative batch. I am unconvinced by the argument that while this can lead to easy parallelization within one iteration of Bayesian optimization, the overall algorithms are still sequential.  These methods can typically be expanded to arbitrarily large batches and fully utilize all parallel hardware. Most implementations of batch Bayesian optimization in practice (SMAC and Spearmint as examples) will even start new jobs immediately as jobs finish -- these implementations do not wait for the entire batch to finish typically. Additionally, while there has been some work extending GP-based BayesOpt to tree-based parameters [R3], at a minimum SMAC in particular is known well suited to the tree-based parameter search considered by the authors. I am not sure that I agree that TPE is state-of-the-art on these problems: SMAC typically does much better in my experience. Ultimately, my concern is that--considering these tools are open source and relatively stable software at this point--if DPP-only based hyperparameter optimization is truly better than the parallelization approach of SMAC, it should be straightforward enough to download SMAC and demonstrate this. If the argument that BayesOpt is somehow still sequential is true, then k-DPP-RBF should outperform these tools in terms of wall clock time to perform optimization, correct? [R1] Kathuria, Tarun and Deshpande, Amit and Kohli, Pushmeet. Batched Gaussian Process Bandit Optimization via Determinantal Point Processes, 2016.  [R2] Several papers, see: http://www.cs.ubc.ca/labs/beta/Projects/SMAC/  [R3] Jenatton, R., Archambeau, C., Gonzu00e1lez, J. and Seeger, M., 2017, July. Bayesian Optimization with Tree-structured Dependencies. In International Conference on Machine Learning (pp. 1655-1664).",15,492,22.363636363636363,5.778017241379311,249,1,491,0.0020366598778004,0.0181086519114688,0.9833,160,68,81,40,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 4, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,2,4,0,8,2,0,0,0,0,0,1,0,0,0,0,3,0,0,5,0,0,0.3592387852190097,0.224944377075974,0.16252356919155067
ICLR2018-HyDAQl-AW-R1,Reject,"Summary: This paper explores how to handle two practical issues in reinforcement learning. The first is including time remaining in the state, for domains where episodes are cut-off before a terminal state is reached in the usual way. The second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration. The ideas are illustrated through several well-worked micro-world experiments. Overall the paper is well written and polished. They slowly worked through a simple set of ideas trying to convey a better understanding to the reader, with a focus on performance of RL in practice. My main issue with the paper is that these two topics are actually not new and are well covered by the existing RL formalisms. That is not to say that an empirical exploration of the practical implications is not of value, but that the paper would be much stronger if it was better positioned in the literature that exists. The first idea of the paper is to include time-remaining in the state. This is of course always possible in the MDP formalism. If it was not done, as in your examples, the state would not be Markov and thus it would not be an MDP at all. In addition, the technical term for this is finite horizon MDPs (in many cases the horizon is taken to be a constant, H). It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate. The paper should make this connection to the literature more clear and discuss what is missing in our existing understanding of this case, to motivate your work. See Dynamic Programming and Optimal Control and references too it. The second idea is that episodes may terminate due to time out, but we should include the discounted value of the time-out termination state in the return. I could not tell from the text but I assume, the next transition to the start state is fully discounted to zero, otherwise the value function would link the values of S_T and the next state, which I assume you do not want. The impact of this choice is S_T is no longer a termination state, and there is a direct fully discounted transition to the start states. This is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic RL frameworks (e.g., RL glue). If we treat the value of S_T as zero or consider gamma on the transition into the time-out state as zero, then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior. The literature might not be totally clear about this, but it is very well discussed in a recent ICML paper: White 2017 [1]  Another way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the Horde paper [2]. In this setting the behavior policy can have terminations and episodes in the classic sense (perhaps due to time outs). However, the agent's continuation function (gamma : S -> [0,1]) can specify weightings on states representing complex terminations (or not), completely independent of the behavior policy or actual state transition dynamics of the underlying MDP. To clearly establish your contributions, the authors must do a better job of relating their work to [1] and [2]. [1] White. Unifying task specification in reinforcement learning. Martha White. International Conference on Machine Learning (ICML), 2017. [2] Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., & Precup, D. (2011). Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems: 2, 761--768. Small comments that did not impact paper scoring: 1) eq 1 we usually don't use the superscript gamma 2) eq2, usually we talk about truncated n-step returns include the value of the last state to correct the return. You should mention this 3) Last paragraph of page 2 should not be in the intro 4) in section 2.2 why is the behavior policy random instead of epsilon greedy? 5) It would be useful to discuss the average reward setting and how it relates to your work. 6) Fig 5. What does good performance look like in this domain. I have no reference point to understand these graphs 7) page 9, second par outlines alternative approaches but they are not presented as such. Confusing ",36,755,18.875,4.920567375886525,325,6,749,0.0080106809078771,0.0342105263157894,0.9971,214,81,127,47,10,6,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 7, 'DAT': 0, 'MET': 13, 'EXP': 2, 'RES': 1, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 4, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 5, 'PNF': 2, 'REC': 0, 'EMP': 11, 'SUB': 2, 'CLA': 1}",0,2,5,7,0,13,2,1,2,1,0,2,4,0,0,2,0,5,2,0,11,2,1,0.7181833809360939,0.6735682442604992,0.5007378244318665
ICLR2018-HyDAQl-AW-R2,Reject,"The majority of the paper is focused on the observation that (1) making policies that condition on the time step is important in finite horizon problems, and a much smaller component on that (2) if episodes are terminated early during learning (say to restart and promote exploration) that the values should be bootstrapped to reflect that there will be additional rewards received in the true infinite-horizon setting.  1 is true and is well known. This is typically described as finite horizon MDP planning and learning and the optimal policy is well known to be nonstationary and depend on the number of remaining time steps. There are a number of papers focusing on this for both planning and learning though these are not cited in the current draft. I don't immediately know of work that suggests bootstrapping if an episode is terminated early artificially during training but it seems a very reasonable and straightforward thing to do.   ",4,156,26.0,5.134228187919463,91,2,154,0.0129870129870129,0.0625,0.8873,32,15,38,9,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 0, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,0,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,2,0,0,0.2857142857142857,0.2228441796570651,0.12338832428058702
ICLR2018-HyDAQl-AW-R3,Reject,"This paper considers the problem of Reinforcement Learning in time-limited domains. It begins by observing that in time-limited domains, an agent unaware of the remaining time can experience state-aliasing. To combat this problem, the authors suggest modifying the state representation of the policy to include an indicator of the amount of remaining time.  The time-aware agent shows improved performance in a time-limited gridworld and several control domains. Next, the authors consider the problem of learning a time-unlimited policy from time-limited episodes. They show that by bootstrapping from the final state of the time-limited domain, they are able to learn better policies for the time-unlimited case. Pros: The paper is well-written and clear, if a bit verbose. The paper has extensive experiments in a variety of domains. Cons: In my opinion, the substance of the contribution is not enough to warrant a full paper and the problem of time-limited learning is not well motivated: 1) It's not clear how frequently RL agents will encounter time-limited domains of interest. Currently most domains are terminated by failure/success conditions rather than time. The author's choice of tasks seem somewhat artificial in that they impose time limits on otherwise unlimited domains in order to demonstrate experimental improvement. Is there good reason to think RL agents will need to contend with time-limited domains in the future? 2) The inclusion of remaining-time as a part of the agent's observations and resulting improvement in time-limited domains is somewhat obvious. It's well accepted that in any partially observed domain, inclusion of the latent variable(s) as a part of the agent's observation will result in a fully observed domain, less state-aliasing, more accurate value estimates, and better performance. The author's inclusion of the latent time variable as a part of the agent's observations reconfirms this well-known fact, but doesn't tell us anything new. 3) I have the same questions about Partial Episode bootstrapping: Is there a task in which we find our RL agents learning in time-limited settings and then evaluated in unlimited ones? The experiments in this direction again feel somewhat contrived by imposing time limits and then removing them. The proposed solution of bootstrapping from the value of the terminal state v(S_T) clearly works, and I suspect that any RL-practitioner faced with training time-limited policies that are evaluated in time-unlimited settings might come up with the same solution. While the experiments are well done, I don't think the substance of the algorithmic improvement is enough. I think this paper would improve by demonstrating how time-aware policies can help in domains of interest (which are usually not time-limited). I could imagine a line of experiments that investigate the idea of selectively stopping episodes when the agent is no longer experiencing useful transitions, and then showing that the partial episode bootstrapping can save on overall sample complexity compared to an agent that must experience the entirety of every episode.  ",23,479,23.95,5.528260869565218,216,5,474,0.010548523206751,0.0207468879668049,0.9895,134,56,83,31,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 0, 'MET': 7, 'EXP': 5, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 12, 'SUB': 1, 'CLA': 1}",0,1,0,3,0,7,5,5,0,0,0,3,0,0,0,0,0,0,0,1,12,1,1,0.4308496093316217,0.4512859762277171,0.24378780205505918
ICLR2018-HyDMX0l0Z-R1,Reject,"Summary:  The paper studies the problem of learning distributions with disconnected support. The paper is very well written, and the analysis is mostly correct, with some important exceptions. However, there are a number of claims that are unverified, and very important baselines are missing. I suggest improving the paper taking into account the following remarks and I will strongly consider improving the score. Detailed comments:  - The paper is very well written, which is a big plus. - There are a number of claims in the paper that are not supported by experiments, citations, or a theorem. - Sections 3.1 - 3.3 can be summarized to Connected prior + continuous generator  > connected support. Thus, to allow for disconnected support, the authors propose to have a discontinuous generator. However to me it seems that a trivial and important attack to this problem is to allow a simple disconnected prior, such as a mixture between uniforms, or at least an approximately disconnected (given the superexponential decay of the gaussian pdf) of a mixture of gaussians, which is very common. The authors fail to mention this obvious alternative, or explore it further, which I think weakens the paper. - Another standard approach to attacking diverse datasets such as imagenet is adding noise in the intermediate layers of the generator (this was done by EBGAN and the Improved GAN paper by Salimans et al.). It seems to me that this baseline is missing. - Section 3.4, paragraph 3, the outputs corresponding to vectors linearly interpolated from z_1 to z_2 show a smooth. Actually, this is known to not perform very well often, indeed the interpolations are done through great circles in z_1 and z_2. See https://www.youtube.com/watch?v myGAju4L7O8 for example. - Lemma 1 is correct, but the analysis on the paragraph following is flat out wrong. The fact that a certain z has high density doesn't imply that the sample g_theta(z) has high density! You're missing the Jacobian term appearing in the change of variables. Indeed, it's common to see neural nets spreading appart regions of high probability to the extent that each individual output point has low density (this is due in its totallity to the fact that || abla_x g_theta(z)|| can be big. - Borrowing from the previous comment, the evidence to support result 5 is insufficient. I think the authors have the right intuition, but no evidence or citation is presented to motivate result 5. Indeed, DCGANs are known to have extremely sharp interpolations, suggesting that small jumps in z lead to large jumps in images, thus having the potential to assign low probability to tunnels. - A citation, experiment or a theorem is missing showing that the K of a generator is small enough in an experiment with separated manifolds. Until that evidence is presented, section 3.5 is anecdotal. - The second paragraph of section 3.6 is a very astute observation, but again it is necessary to show some evidence to verify this intuition. - The authors then propose to partition the prior space by training separate first layers for the generator in a maximally discriminative way, and then at inference time just sampling which layer to use uniformly. It's important to note that this has a problem when the underlying separated manifolds in the data are not equiprobable. For example, if we use N   2 in CelebRoom but we use 30% faces and 70% bedrooms, I would still expect tunneling due to the fact that one of the linear layers has to cover both faces and bedrooms. - MNIST is known to be a very poor benchmark for image generation, and it should be avoided. - I fail to see an improvement in quality in CelebA. It's nice to see some minor form on clustering when using generator's prediction, but this has been seen in many other algorithms (e.g. ALI) with much better results long before. I have to say also the official baseline for 64x64 images in wgangp (that I've used several times) gives much better results than the ones presented in this paper https://github.com/igul222/improved_wgan_training/blob/master/gan_64x64.py . - The experiments in celebRoom are quite nice, and a good result, but we are still missing a detailed analysis for most of the assumptions and improvements claimed in the paper. It's very hard to make very precise claims about the improvements of this algorithm in such a complex setting without having even studied the standard baselines (e.g. noise at every layer of the generator, which has very public and well established code https://github.com/openai/improved-gan/blob/master/imagenet/generator.py). - I would like to point a lot of tunneling issues can be seen and studied in toy datasets. The authors may want to consider doing targeted experiments to evaluate their assumptions.                        After the rebuttal I've increased my score. The authors did a great job at addressing some of the concerns.  I still think there is more room to be done as to justifying the approach, dealing properly with tunneling when we're not in the somewhat artificial case of equiprobable partitions, and primarily at understanding the extent to which tunneling is a problem in current methods. The revision is a step forward in this direction, but still a lot remains to be done. I would like to see simple targeted experiments aimed at testing how much and in what way tunneling is a problem in current methods before I see high dimensional non quantitative experiments. In the case where the paper gets rejected I would highly recommend the acceptance at the workshop due to the paper raising interesting questions and hinting to a partial solution, even though the paper may not be at a state to be published at a conference venue like ICLR.",43,927,21.55813953488372,5.158620689655172,383,8,919,0.0087051142546245,0.0194871794871794,0.9861,237,113,187,59,9,6,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 4, 'DAT': 5, 'MET': 16, 'EXP': 8, 'RES': 5, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 3, 'EMP': 17, 'SUB': 7, 'CLA': 2}",0,0,4,4,5,16,8,5,0,4,0,6,0,1,1,0,0,2,0,3,17,7,2,0.6483011811111062,0.6774959432464573,0.4605236160135412
ICLR2018-HyDMX0l0Z-R2,Reject,"The authors propose to train multiple generators (with same set of parameters), each of which with a different linear mapping in the first layer. The idea is that the final output of the generator should be a distribution whose support are disconnected. The idea does look interesting. But a lot of details is missing and needs clarification. 1) A lot of technical details are missing. The main formula is given in page 6 (Sec. 4.2), without much explanation. It is also not clear how different generators are combined as a final generator to feed into the discriminator. Also how are the diversity enforced? 2) The experiments are not convincing. It is stated that the new method produces results that are visually better than existing ones. But there is no evidence that this is actually due to the proposed idea. I would have liked to see some demonstration of how the different modes look like, how they are disconnected and collaborate to form a stronger generator. Even some synthetic examples could be helpful.",13,172,13.23076923076923,5.031055900621118,100,0,172,0.0,0.0523255813953488,0.8457,36,20,39,7,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 4, 'DAT': 0, 'MET': 6, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 0}",0,0,0,4,0,6,5,1,0,0,0,1,0,0,0,0,0,0,0,0,10,2,0,0.3589165004641824,0.2279265156441304,0.1573843190922544
ICLR2018-HyDMX0l0Z-R3,Reject,"This paper concerns a potentially serious issue with current GAN based approaches. Complex data distributions, such as natural images, likely lie upon many disconnected manifolds. However standard GANs use continuous noise and generators and must therefore output a connected distribution over inputs. This constraint results in the generator outputting what the paper terms ""tunnels"" regions of output which connect these actually disconnected manifolds but do not correspond to actual samples from valid manifolds. This is an important observation. The paper makes a variety of sensible claims - attributing incoherent samples to these tunnels and stating that complex datasets such as Imagenet are more likely to suffer from this problem. This behavior can indeed be observed during training on toy examples such as a 2d mixture of gaussians. However it is an open question how important this issue is in practice and the paper does not clearly separate this issue from the issue of properly modeling the complicated manifolds themselves. It is admittedly difficult to perform quantitative evaluations on generative models but much more work could be done to demonstrate and characterize the problem in practice.  The tunnel problem motivates the authors proposed approach to introducing discontinuities into the generator. Specifically the paper proposes training N different generators composed of N different linear projections of the noise distribution while sharing all further layers. A projection is chosen uniformly at random during training/sampling. An additional extension adds a loss term for the discriminator/generator to encourage predictability and thus diversity of the projection layers and improves results significantly.  The only experimental results presented are qualitative analysis of samples by the authors. This is a very weak form of evidence suffering from bias as the evaluations are not performed blinded and are of a subjective nature. If the paper intends to present experimental results solely on sample quality then, blinded and aggregated human judgments should be expected. As a reader, I do agree that qualitatively the proposed approach produces higher quality samples than the baseline on CelebRoom but I struggle to see any significant difference on celebA itself. I am uncomfortable with this state of affairs and feel the claims of improvements on this task are unsubstantiated. While discussion is motivated by known difficulties of GANs on highly varied datasets such as Imagenet, experiments are conducted on both MNIST and celebA datasets which are already well handled by current GANs. The proposed CelebRoom dataset (a 50/50 mixture of celebA and LSUN bedrooms) is a good dataset to validate the problem on but it is disappointing that the authors do not actually scale their method to their motivating example. Additionally, utilizing Imagenet would lend itself well to a more quantitative measure of sample quality such as inception score. On the flip side, a toy experiment with known disconnected manifolds, while admittedly toy could increase confidence since it lends itself to more thorough quantitative analysis. For instance, a mixture of disconnected 2d gaussians where samples can be measured to be on or off manifold could be included. At a high level I am not as sure as the authors on the nature of disconnected manifolds and the issue of tunnels. Any natural image has a large variety of transformations that can be applied to it that still correspond to valid natural images. Lighting transformations such as brightening or darkening of the image corresponds to a valid image transformations which allows for a ""lighting tunnel"" to connect all supposedly disjoint image manifolds through very dark/bright images. While this is definitely not the optimal way to approach the problem it is meant as a comment on the non-intuitive and poorly characterized properties of complex high dimensional data manifolds. The motivating observation is an important one and the proposed solution appears to be a reasonable avenue to tackle the problem. However the paper lacks quantitative evidence for both the importance of the problem and demonstrating the proposed solution.",28,648,22.344827586206897,5.641935483870967,291,5,643,0.0077760497667185,0.021505376344086,0.9444,178,79,112,40,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 8, 'DAT': 3, 'MET': 15, 'EXP': 13, 'RES': 5, 'TNF': 0, 'ANA': 6, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 18, 'SUB': 2, 'CLA': 0}",0,0,0,8,3,15,13,5,0,6,0,0,0,0,0,0,0,0,0,0,18,2,0,0.4339421526516144,0.2329021751228742,0.19721766853221034
ICLR2018-HyEi7bWR--R1,Reject,"This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques Comments: -- It's not clear to me how D is determined for each test. Given the definition in Theorem 3.1 it seems like you would have to have some knowledge of how many eigenvalues in W you expect to be close to -1. -- For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant. -- The ylabel in Figure 1 is ""Test Loss"" which I didn't see defined. Is this test loss the cross entropy? If so, I think it would be more effective to label the plot with that. -- The plots in figure 1 and 2 have different colors to represent the same set of techniques. I would suggest keeping a  consistent color scheme -- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified. -- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified. -- How is test set accuracy defined in section 5.3? Classifying digits? Recreating digits? -- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%. However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters. -- Overall it's unclear to me how to completely determine the benefit of this technique over the others because, for each of the tests, different techniques may have superior performance. For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy. scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3. Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset. I don't understand the significance of each test and why the relative performance of the techniques vary from one to the other. -- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability. You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear.",23,505,28.05555555555556,4.993697478991597,219,6,499,0.0120240480961923,0.0310077519379844,0.9497,138,48,81,24,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 16, 'EXP': 4, 'RES': 6, 'TNF': 8, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 8, 'PNF': 6, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,1,2,1,2,16,4,6,8,0,0,0,0,0,0,0,1,8,6,0,10,1,0,0.5759982143412384,0.5622811549884887,0.3604589078348583
ICLR2018-HyEi7bWR--R2,Reject,"This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks. I think the paper is well-written. Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method. I have two questions from authors:  1- What are the hyperparameters that you optimized in experiments? 2- How sensitive is the results to the number of -1 in the diagonal matrix? 3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs. In MNIST experiment, for example, better numbers are reported for larger LSTMs. I think matching the number of hidden units could be helpful. Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000. I appreciate if authors can provide more results in these settings.  ",11,182,18.2,5.226744186046512,104,5,177,0.0282485875706214,0.0432432432432432,0.9631,49,28,32,6,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 6, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 3, 'CLA': 1}",0,1,1,1,3,6,1,3,0,0,0,1,0,0,0,0,0,0,0,0,6,3,1,0.5728631172627007,0.3366564735241911,0.2826077886645909
ICLR2018-HyEi7bWR--R3,Reject,"The paper is clearly written, with a good coverage of previous relevant literature. The contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced. Therefore, the paper must show that this new method performs better in some way compared with previous methods. They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT. Pros: 1. New, relatively simple method for learning orthogonal weight matrices for RNN 2. Clearly written n 3. Quite good results on several relevant tasks. Cons: 1. Technical novelty is somewhat limited 2. Experiments do not evaluate run time, memory use, computational complexity, or stability. Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?",10,137,11.416666666666666,5.7421875,91,1,136,0.0073529411764705,0.0145985401459854,0.9524,40,23,20,13,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 2}",0,0,2,0,1,6,1,2,0,0,0,3,0,0,0,1,0,4,0,0,6,1,2,0.4300494165515464,0.5590979654721945,0.27270743384117946
ICLR2018-HyFaiGbCW-R1,Reject,"The claimed results of  combining transformations in the context of RC was done in the works of Herbert Jaeger on conceptors [1], which also should be cited here. The argument of biological plausibility is not justified. The authors  use an echo-state neural network with standard tanh activations, which is as far away from real neuronal signal processing than  ordinary RNNs used in the field, with the difference that the recurrent weights are not trained.  If the authors want to make the case of biological plausibility, they should use spiking neural networks. The experiment on MNIST seems artificial, in particular transforming the image into a time-series and thereby imposing an artificial temporal structure. The assumption that column_i is obtained  by information  of column_{i-k},..,column_{i-1} is not true for images. To make a point, the authors should use a datasets with related sets of time-series data, e.g EEG or NLP data. In total this paper does not have enough novelty for acceptance and the experiments are not well chosen for this kind of work. Also the authors overstate the claim of biological plausibility (just because we don't train the recurrent weights does not make a method biologically plausible).  [1] H. Jaeger (2014): Controlling Recurrent Neural Networks by Conceptors. Jacobs University technical report Nr 31 (195 pages)   ",7,213,17.75,5.28921568627451,127,1,212,0.0047169811320754,0.0224215246636771,-0.6129,65,28,36,17,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,0,0,1,1,3,2,0,0,0,0,1,2,0,0,1,0,0,0,0,4,0,0,0.4291704015955438,0.2240880945267511,0.19316033908042043
ICLR2018-HyFaiGbCW-R2,Reject,"The technical part of the paper is a nice study for classification with Echo State Networks. The main novelty here is the task itself, classifying different distortions of MNIST data. The actual technique presented is not original, but an application of the standard ESN approach. The task is interesting but by itself I don't find it convincing enough. Moreover, the biological plausibility that is used as an argument at several places seems to be false advertising in my view. The mere presence of recurrent connections doesn't make the approach more biological plausible, in particular given that ridge regression is used for training of the output weights. If biological plausibility was the goal, a different approach should have been used altogether (e.g., what about local training of connections, unsupervised training, ...). Also there is no argument why biological plausibility is supposed to be an advantage. A small number of training examples would have been a more specific and better motivation, given that the number of training examples for humans is only discussed qualitatively and without a reference. The analysis using the PCs is nice; the works by Jaeger on Conceptors (2014) make also use of the principal components of the reservoir states during presentation of patterns (introduction in https://arxiv.org/abs/1406.2671), so seem like relevant references to me.  In my view the paper would benefit a lot from more ambitious task (with good results), though even then I would probably miss some originality in the approach.",11,242,22.0,5.299145299145299,144,2,240,0.0083333333333333,0.0327868852459016,0.9851,72,30,37,18,5,2,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 1, 'MET': 5, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 0}",0,0,2,3,1,5,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,1,0,0.3583427132240913,0.226575924266123,0.16001680916718505
ICLR2018-HyFaiGbCW-R3,Reject,"The paper uses an echo state network to learn to classify image transformations (between pairs of images) into one of fives classes. The image data is artificially represented as a time series, and the goal is generalization of classification ability to unseen image pairs. The network dynamics are studied and are claimed to have explanatory power. The paper is well-written and easy to follow, but I have concerns about the claims it makes relative to how convincing the results are. The focus is on one simple, and frankly now-overused data set (MNIST).  Further, treating MNIST data as a time series is artificial and clunky. Why does the series go from left to right rather than right to left or top to bottom or inside out or something else?  How do the results change if the data is temporalized in some other way? For training in Section 2.4, is M the number of columns for a pair of images? It's not clear how pairs are input in parallel--- one after the other? Concatenated? Interleaved columns?  Something else? What are k, i, j in computing $delta X_k$?  Later, in Section 3.2, it says, As in section 2.2, $xl(mn)$ is the differential reservoir state value of the $m$th reservoir node at time $n$ for input image $l$, but nothing like this is discussed in Section 2.2; I'm confused. The generalization results on this one simple data set seem pretty good. But, how does this kind of approach do on other kinds of or more complex data? I'm not sure that RC has historically had very good success scaling up to real-world problems to date. Table 1 doesn't really say anything. Of course, the diagonals are higher than the off diagonals because these are dot products. True, they are dot products of averages over different inputs (which is why they are less than 1), but still. Also, what Table 1 really seems to say is that the off-diagonals really aren't all that different than the diagonals, and that especially the differences between same and different digits is not very different, suggesting that what is learned is pretty fragile and likely won't generalize to harder problems. I like the idea of using dynamical systems theory to attempt to explain what is going on, but I wonder if it is not being used a bit simplistically or naively. Why were the five transform classes chosen? It seems like the transforms a (same) and e (different) are qualitatively different than transforms b-d (rotated, scaled, blurred). This seems like it should talked about. Thus, we infer, that the reservoir is in fact, simply training these attractors as opposed to training the entire reservoir space.   What does this mean? The reservoir isn't trained at all in ESNs (which is also stated explicitly for the model presented here) ...  For 3.3, why did were those three classes chosen? Was this experiment tried with other subsets of three classes?  Why are results reported on only the one combination of rotated/blurred vs. rotated?  Were others tried?  If so, what were the results?  If not, why?  How does the network know when to take more than the highest output (so it can say that two transforms have been applied)?  In the case of combination, counting either transform as the correct output kind of seems like cheating a bitu2014it over states how well the model is doing. Also, does the order in which the transforms are applied affect their relative representative strength in the reservoir? The comparison with SNNs is kind of interesting, but I'm not sure that I'm (yet) convinced, as there is little detail on how the experiment was performed and what was done (or not) to try to get the SNN to generalize. My suspicion is that with the proper approach, an SNN or similar non-dynamical system could generalize well on these tasks. The need for a dynamical system could be argued to make sense for the camera task, perhaps, as video frames naturally form a time series; however, as already mentioned, for the MNIST data, this is not the case, and the fact that the SNN does not generalize here seems likely due to their under utilization rather than due to an inherent lack of capability. I don't believe that there is sufficient support for this statement in the conclusion, [ML/deep networks] do not work as well for generalization of learning. In generalized learning, RCNs outperform them, due to their ability to function as a dynamical system with 'memory'.  First of all, ML is all about generalization, and there are lots and lots and lots of results showing that many ML systems generalize very well on a wide variety of problems, well beyond just classification, in fact. And, I don't think the the paper has convincingly shown that a dynamical system 'memory' is doing something especially useful, given that the main task studied, that of character recognition (or classification of transformation or even transformation itself), does not require such a temporal ability. ",37,831,29.678571428571427,4.8075,343,9,822,0.010948905109489,0.0354191263282172,0.9966,202,79,177,67,8,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 3, 'DAT': 4, 'MET': 25, 'EXP': 2, 'RES': 3, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 31, 'SUB': 1, 'CLA': 0}",0,0,0,3,4,25,2,3,1,0,0,1,0,2,0,0,0,0,0,0,31,1,0,0.57786698174808,0.2408809452675114,0.26107442651725504
ICLR2018-HyH9lbZAW-R1,Accept,"The authors adapts stochastic natural gradient methods for variational inference with structured inference networks. The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter. The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters. In the experiments the authors generally show improved convergence over SVAE. The idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation. The main motivation seems to be that it is easier to optimize. - In the last two sentences of the updates for theta_PGM you mention that you need to do SVI/VMP to compute the function eta_xtheta. Might this also suffer from non-convergence issues like you argue SVAE does? Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward? - It was not clear to me why we should use a Gaussian approximation for the theta_NN parameters? The prior might be Gaussian but the posterior is not? Is this more of a simplifying assumption? - There has recently been interest in using inference networks as part of more flexible variational approximations for structured models. Some examples of related work missing in this area is Variational Sequential Monte Carlo by Naesseth et al. (2017) / Filtering Variational Objectives by Maddison et al. (2017) / Auto-encoding sequential Monte Carlo Le et al. (2017). -  Section 2.1, paragraph nr 5, algorihtm -> algorithm ",16,273,19.5,5.411538461538462,148,4,269,0.0148698884758364,0.0494699646643109,0.9876,72,40,47,13,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 3, 'DAT': 0, 'MET': 11, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,0,3,3,0,11,3,0,0,0,0,0,1,1,0,0,0,3,1,0,9,0,0,0.431525316821172,0.338543317926457,0.2119604146152936
ICLR2018-HyH9lbZAW-R2,Accept,"This paper presents a variational inference algorithm for models that contain deep neural network components and probabilistic graphical model (PGM) components. The algorithm implements natural-gradient message-passing where the messages automatically reduce to stochastic gradients for the non-conjugate neural network components. The authors demonstrate the algorithm on a Gaussian mixture model and linear dynamical system where they show that the proposed algorithm outperforms previous algorithms. Overall, I think that the paper proposes some interesting ideas, however, in its current form I do not think that the novelty of the contributions are clearly presented and that they are not thoroughly evaluated in the experiments. The authors propose a new variational inference algorithm that handles models with deep neural networks and PGM components. However, it appears that the authors rely heavily on the work of (Khan & Lin, 2017) that actually provides the algorithm. As far as I can tell this paper fits inference networks into the algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an inference network to generate potentials for a conditionally-conjugate distribution and ii) introducing new PGM parameters to decouple the inference network from the model parameters. These ideas are a clever solution to work inference networks into the message-passing algorithm of (Khan & Lin, 2017), but I think the authors may be overselling these ideas as a brand new algorithm. I think if the authors sold the paper as an alternative to (Johnson, et al., 2016) that doesn't suffer from the implicit gradient problem the paper would fit into the existing literature better. Another concern that I have is that there are a lot of conditiona-conjugacy assumptions baked into the algorithm that the authors only mention at the end of the presentation of their algorithm. Additionally, the authors briefly state that they can handle non-conjugate distributions in the model by just using conjugate distributions in the variational approximation. Though one could do this, the authors do not adequately show that one should, or that one can do this without suffering a lot of error in the posterior approximation. I think that without an experiment the small section on non-conjugacy should be removed. Finally, I found the experimental evaluation to not thoroughly demonstrate the advantages and disadvantages of the proposed algorithm. The algorithm was applied to the two models originally considered in (Johnson, et al., 2016) and the proposed algorithm was shown to attain lower mean-square errors for the two models. The experiments do not however demonstrate why the algorithm is performing better. For instance, is the (Johnson, et al., 2016) algorithm suffering from the implicit gradient?  It also would have been great to have considered a model that the (Johnson, et. al., 2016) algorithm would not work well on or could not be applied to show the added applicability of the proposed algorithm. I also have some minor comments on the paper: - There are a lot of typos. - The first two sentences of the abstract do not really contribute anything to the paper. What is a powerful model? What is a powerful algorithm? - DNN was used in Section 2 without being defined. - Using p() as an approximate distribution in Section 3 is confusing notation because p() was used for the distributions in the model.",28,537,24.40909090909091,5.442460317460317,208,7,530,0.0132075471698113,0.018348623853211,0.9522,147,46,97,32,6,6,"{'ABS': 1, 'INT': 0, 'RWK': 10, 'PDI': 2, 'DAT': 0, 'MET': 22, 'EXP': 4, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 4, 'IMP': 0, 'CMP': 6, 'PNF': 2, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 1}",1,0,10,2,0,22,4,0,0,0,0,1,0,0,0,4,0,6,2,0,8,2,1,0.4348677413193442,0.6719097496821965,0.30753935652109476
ICLR2018-HyH9lbZAW-R3,Accept,"The paper seems to be significant since it integrates PGM inference with deep models. Specifically, the idea is to use the structure of the PGM to perform efficient inference. A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part. Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference. The paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates. As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution. The motivation of the paper, and the description of its contribution as compared to existing methods can be improved.  One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs.  Can this be generalized to arbitrary PGM structures?  How about cases when computing Z is intractable? Could the proposed approach be adapted to such cases. I was not very sure as to why the proposed method is more general than existing approaches. Regarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets. the approach shows that the proposed methods converge faster than existing methods. However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting.",15,255,19.615384615384617,5.375,128,4,251,0.0159362549800796,0.0271317829457364,0.8955,67,27,56,9,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 12, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,0,4,1,1,12,1,0,0,0,0,1,0,0,0,0,1,4,0,0,8,0,0,0.4315650055302118,0.3380385230488041,0.21665904342448553
ICLR2018-HyHmGyZCZ-R1,Reject,"The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them. The idea is tested on very small data sets (80 and 50 examples, respectively). The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets. It isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove . Is the point that GloVe is a bad algorithm? That these steps are general? If the latter, then the experimental results are far weaker than what I would find convincing. Why not try on multiple different word embeddings? What happens if you start with random vectors? What happens when you try a bigger data set or a more complex problem?",10,137,22.83333333333333,5.0,87,1,136,0.0073529411764705,0.0507246376811594,-0.7859,32,13,31,10,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 3, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,0,1,1,2,3,3,2,0,0,0,0,0,0,0,0,0,1,0,0,7,1,0,0.4293359970150182,0.3370650779423911,0.21030950151754363
ICLR2018-HyHmGyZCZ-R2,Reject,"This paper proposes a ranking-based similarity metric for distributional semantic models. The main idea is to learn baseline word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called Ranking-based Exponential Similarity Measure (RESM), which is based on the recently proposed APSyn measure. I think the work has several important issues:  1. The work is very light on references. There is a lot of previous work on evaluating similarity in word embeddings (e.g. Hill et al, a lot of the papers in RepEval workshops, etc.); specialization for similarity of word embeddings (e.g. Kiela et al., Mrksic et al., and many others); multi-sense embeddings (e.g. from Navigli's group); and the hubness problem (e.g. Dinu et al.). For the localized centering approach, Hara et al.'s introduced that method.  None of this work is cited, which I find inexcusable.u2028  2.  The evaluation is limited, in that the standard evaluations (e.g. SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work. The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper. It is unclear what exactly helps, in which case, and why.u2028  3. There are technical issues with what is presented, with some seemingly factual errors. For example, In this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance. Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product). Another example, are obtained using the GloVe vector, not using PPMI - there are close relationships between what GloVe learns and PPMI, which the authors seem unaware of (see e.g. the GloVe paper and Omer Levy's work).u2028  4. Then there is the additional question, why should we care? The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings. In other words, what is supposed to be the take-away, and why should we care? As such, I do not recommend it for acceptance - it needs significant work before it can be accepted at a conference. Minor points: - Typo in Eq 10 - Typo on page 6 (/cite instead of cite)",18,455,19.782608695652176,4.8004587155963305,209,3,452,0.0066371681415929,0.0150214592274678,0.8656,130,55,81,24,8,7,"{'ABS': 0, 'INT': 0, 'RWK': 13, 'PDI': 5, 'DAT': 0, 'MET': 5, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 6, 'CMP': 1, 'PNF': 1, 'REC': 2, 'EMP': 9, 'SUB': 3, 'CLA': 1}",0,0,13,5,0,5,4,1,0,1,0,4,3,0,0,0,6,1,1,2,9,3,1,0.5740520239686931,0.783226773987569,0.4552857942805247
ICLR2018-HyHmGyZCZ-R3,Reject,"I hate to say that the current version of this paper is not ready, as it is poorly written. The authors present some observations of the weaknesses of the existing vector space models and list a 6-step approach for refining existing word vectors (GloVe in this work), and test the refined vectors on 80 TOEFL questions and 50 ESL questions. In addition to the incoherent presentation, the proposed method lacks proper justification. Given the small size of the datasets, it is also unclear how generalizable the approach is. Pros:   1. Experimental study on retrofitting existing word vectors for ESL and TOEFL lexical similarity datasets Cons:u000b  1. The paper is poorly written and the proposed methods are not well justified. 2. Results on tiny datasets ",7,124,13.77777777777778,5.110169491525424,73,0,124,0.0,0.015625,-0.9078,35,18,22,6,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 3, 'MET': 4, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 2}",0,0,1,0,3,4,2,1,0,0,0,2,0,0,0,0,0,0,0,0,5,2,2,0.4295258009923288,0.3360089746518808,0.21768864031646287
ICLR2018-HyI5ro0pW-R1,Reject,"This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal. The authors describe two methods to obtain this structure: (1) enforced during training, (2) enforced through regularization and pruning. As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large. However, the authors only conjecture as to the potential of this method without describing clear ways of approaching this subject, which somewhat lessens the strength of their argument. Quality: this paper is of good quality Clarity: this paper is clear, but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text. Originality: this paper introduces block diagonal matrices to structure the weights of a neural network. The idea of structured matrices in this context is not new, but the diagonal block structure appears to be.  Significance: This paper is somewhat significant. PROS  - A new approach to analyzing the behavior of weight matrices during learning - A new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes. CONS - Some of the figures are hard to read (in particular Fig 1 & 2 left) and would benefit from a better layout. - It would be valuable to see experiments on bigger datasets than only MNIST and CIFAR-10. - I understand that the main advantage of this method is the speedup; however, providing the final accuracy as a function of the nonzero entries for slower methods (e.g. the sparse pruning showed in Fig 1. a) would provide a more complete picture. Main questions: - Could you briefly comment on the training time in section 4.1?  - Could you elaborate on the last sentence of section 4.1? - You state: singular values of an IP layer behave according to the MP distribution even after 1000s of training iterations.  Is this a known fact, or something that you observed empirically? In practice, how large must the weight matrix be to observe this behavior? Nitpicks: - I believe the term fully connected is more standard than inner product and would add clarity to the paper, but I may be mistaken. ",21,376,22.11764705882353,5.254237288135593,192,2,374,0.0053475935828877,0.0204603580562659,0.9846,110,43,54,17,8,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 6, 'EXP': 5, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 2}",0,1,0,2,1,6,5,2,2,0,0,3,0,0,0,1,1,0,2,0,10,1,2,0.5732534524957025,0.6724069875749108,0.4047520162068374
ICLR2018-HyI5ro0pW-R2,Reject,"The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning. The implementation of this seems straightforward, and can be done either via initialization or via pruning on the off-diagonals. There are a few ideas the paper discusses:  (1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not worth it until the matrix is extremely sparse. I think this case is well supported via their experiments, and I largely agree. (2) that therefore, block diagonal layers lead to more efficient networks. This point is murkier, because the paper doesn't discuss possible increases in *training time* (due to increased number of iterations) in much detail. At if we only care about running the net, then reducing the time from 0.4s to 0.2s doesn't seem to be that useful (maybe it is for real-time predictions? Please cite some work in that case) (3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train) [as I am not primarly a neural net researcher, I had always thought pruning was done to decrease over-fitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix] (4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (Martingale or Markov chain theory) This is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above). The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don't discuss the implications or reasons of this assumption. The authors seem to understand the literature well, and not being an expert myself, I have the impression they are doing a fair job. The paper could have gone farther experimentally (or theoretically) in my opinion. For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated. I was also wondering about when 2 or more layers are block sparse, do these blocks overlap? i.e., are they randomly permuted between layers so that the blocks mix? And even with a single block, does it matter what permutation you use? (or perhaps does it not matter due to the convolutional structure?) The section on the variance of the weights is rather unclear mathematically, starting with the abstract and even continuing into the paper. We are talking about sample variance? What does DeltaVar mean in eq (2)? The Marchenko-Pastur theorem seemed to even be imprecise, since if y>1, then a < 0, implying that there is a nonzero chance that the positive semi-definite matrix XX' has a negative eigenvalue. I agree this relationship with random matrices could be interesting, but it seems too vague right now. Is there some central limit theorem explanation? Are you sure that you've run enough iterations to fully converge?(Fig 4 was still trending up for b1 64). Was it due to the convolutional net structure (you could test this)? Or, perhaps train a network on two datasets, one which is not learnable (iid random labels), and one which is very easily learnable (e.g., linearly separable). Would this affect the distributions? Furthermore, I think I misunderstood parts, because the scaling in MNIST and CIFAR was different and I didn't see why (for MNIST, it was proportional to block size, and for CIFAR it was independent of block size almost). Minor comment: last paragraph of 4.1, comparing with Sindhwani et al., was confusing to me. Why was this mentioned? And it doesn't seem to be comparable. I have no idea what Toeplitz (3) is.",33,727,36.35,5.051470588235294,320,7,720,0.0097222222222222,0.0384087791495198,-0.6754,157,81,149,65,11,4,"{'ABS': 1, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 16, 'EXP': 9, 'RES': 1, 'TNF': 0, 'ANA': 4, 'FWK': 1, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 22, 'SUB': 2, 'CLA': 0}",1,1,0,2,2,16,9,1,0,4,1,1,1,0,0,0,1,1,0,0,22,2,0,0.7903262824983232,0.4576122270844684,0.44215005877384356
ICLR2018-HyI5ro0pW-R3,Reject,"This paper proposes replacing fully connected layers with block-diagonal fully connected layers and proposes two methods for doing so.  It also make some connections to random matrix theory. The parameter pruning angle in this paper is fairly weak. The networks it is demonstrated on are not particularly large (largeness usually being the motivation for pruning) and the need for making them smaller is not well motivated. Additionally MNIST is a uniquely bad dataset for evaluating pruning methods, since they tend to work uncharacteristically well on MNIST (This can be seen in some of the references the paper cites). The random matrix theory part of this paper is intriguing, but left me wondering and then what?   It is presented as a collection of observations with no synthesis or context for why they are important.  I'm usually quite happy to see connections being made to other fields, but it is not clear at all how this particular connection is more than a curiosity. This paper would be much stronger if it offered some way to exploit this connection. There are two half-papers here, one on parameter pruning and one on applying insights from random matrix theory to neural networks, but I don't see a strong connection between them. ",11,206,20.6,5.024875621890548,118,3,203,0.0147783251231527,0.0236966824644549,-0.1868,40,22,45,21,8,1,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 6, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0,1,1,1,1,6,1,1,0,0,0,2,0,0,0,0,0,0,0,0,8,0,0,0.5727249508055453,0.1154648131550119,0.23094364416745908
ICLR2018-HyI6s40a--R1,Reject,"This paper present a method for detecting adversarial examples in a deep learning classification setting. The idea is to characterize the latent feature space (a function of inputs) as observed vs unobserved, and use a module to fit a 'cluster-aware' loss that aims to cluster similar classes tighter in the latent space. Questions/Comments:  - How is the checkpointing module represented? Which parameters are fit using the fine-tuning loss described on page 3? - What is the rationale for setting the gamma (concentration?) parameters to .01? Is that a general suggestion or a data-set specific recommendation? - Are the checkpointing modules designed to only detect adversarial examples? Or is it designed to still classify adversarial examples in a robust way? Clarity: I had trouble understanding some of this paper. It would be nice to have a succinct summary of how all of the pieces presented fit together, e.g. the original victim network, fine-tuning loss, per-class dictionary learning w/ OMP. Technical: It is hard to tell how some of the components of this approach are technically justified. Novel: I am not familiar enough with adversarial deep learning to assess novelty or impact. ",12,187,23.375,5.497109826589595,109,0,187,0.0,0.0104166666666666,0.3736,52,29,35,6,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 9, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 0, 'CLA': 1}",0,2,0,1,1,9,2,2,0,0,0,2,0,0,0,1,0,0,1,0,8,0,1,0.5022143723320615,0.4487981464883452,0.28640547309638664
ICLR2018-HyI6s40a--R2,Reject,"Summary:  The paper presents an unsupervised method for detecting adversarial examples of neural networks. The method includes two independent components: an 'input defender' which tried to inspect the input, and a 'latent defender' trying to inspect a hidden representation. Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection. The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians. Experiments are presented on MInst, Cifar10, and ImageNet. -tIntroduction: The motivation for detecting adversarial examples is not stated clearly enough. How can such examples be used by a malicious agent to cause damage to a system? Sketching some such scenarios would help the reader understand why the issue is practically important. I was not convinced it is. Page 4:  -tStep 3 of the algorithm is not clear: otHow exactly does HDDA model the data (formally) and how does it estimate the parameters? In the current version, the paper does not explain the HDDA formalism and learning algorithm, which is a main building block in the proposed system (as it provides the density score used for adversarial examples detection). Hence the paper cannot be read as a standalone document. I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper. otWhat is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA? Are they completely different models? Or are the C^I used when learning the HDDA model (and how)?  If these are separate models, how are they used in conjunction to give a final density score? If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the phy(x) representation more class-seperable. Is that right? -tFigure 4, b and c: it is not clear what the (x,y,z) measurements plotted in these 3D drawings are (what are the axis). Page 5: -tSection 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5. However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)  - there is not generative model of p(x|W2)). -tThe authors claim in the last sentence of the section that p(x|W2) is equivalent to 1-p(x|W1), but this is not true: these are two continuous densities, they do not sum to 1, and a model of p(x|W2) is not available (as far as I understand the method) Page 6: -tHow is equation 7) optimized? -tWhich patchs are extracted from images, for training and at inference time? Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps? -tIts not clear enough what exactly is the 'PSNR' value which is used for the adversarial example detection, and what exactly is 'profile the PSNR of legitimate samples within each class'. A formal definition of PSNR and'profiling' is missing (does profiling simply mean finding a threshold for filtering?) Page 7: -tFigure 7 is not very informative. Given the ROC curves in figure 8  and table 1 it is redundant. Page 8: -tThe results in general indicate that the method is much better than chance, but it is not clear if it is practical, because the false alarm rates for high detection are quite high. For example on ImageNet, 14.2% of the innocent images are mistakenly rejected as malicious to get 90% detection rate. I do not think this working point is useful for a real application -tGiven the high flares alarm rate, it is surprising that experiments with multiple checkpoints are not presented (specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the paper). Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method -tThe experiments show that essentially, the latent defenders are stronger than the input defender in most cases. However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it?  And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space? Of which dimension? Overall: Pros: -t A nice idea with some novelty,  based on a non-trivial observation -tThe experimental results how the idea holds some promise Cons -tThe method is not presented clearly enough: the main component modeling the network activity is not explained (the HDDA module used) -tThe results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate) -tExperimental results are partial: results are not presented for multiple defenders, no ablation experiments After revision: Some of my comments were addressed, and some were not. Specifically, results were presented for multiple defenders and some ablation experiments were highlihgted Things not addressed:  - The risk analysis is still not relevant. The authors removed a clearly flawed sentence, but the analysis still assumes that two densities (of 'good' and 'bad' examples) are modeled, while in the work presented only one of them is. Hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear. It would have been better to omit it and use the space to describe HDDA and the specific variant used in this work, as this is the main tool doing the distinction. I believe the paper should be accepted. ",49,980,33.793103448275865,5.0,357,2,978,0.0020449897750511,0.0462776659959758,-0.9009,266,106,198,71,11,7,"{'ABS': 0, 'INT': 5, 'RWK': 1, 'PDI': 4, 'DAT': 1, 'MET': 21, 'EXP': 8, 'RES': 5, 'TNF': 3, 'ANA': 9, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 3, 'REC': 1, 'EMP': 26, 'SUB': 5, 'CLA': 1}",0,5,1,4,1,21,8,5,3,9,0,2,0,1,0,1,0,3,3,1,26,5,1,0.7921140511800006,0.7941108826441245,0.6155638038736945
ICLR2018-HyI6s40a--R3,Reject,"This paper proposes an unsupervised method, called Parallel Checkpointing Learners (PCL), to detect and defend adversarial examples. The main idea is essentially learning the manifold of the data distribution and using Gaussian mixture models (GMMs) and dictionary learning to train a reformer (without seeing adversarial examples) to detect and correct adversarial examples. With PCL, one can use hypothesis testing framework to analyze the detection rate and false alarm of different neural networks against adversarial attacks. Although the motivation is well grounded, there are two major issues of this work: (i) limited  novelty - the idea of unsupervised manifold projection method has been proposed in the previous work; and (ii) insufficient attack evaluations - the defender performance is evaluated against weak attacks or attacks with improper parameters. The details are as follows.  1.  Limited novelty and performance comparison - the idea of unsupervised manifold projection method has been proposed and well-studied in MagNet: a Two-Pronged Defense against Adversarial Examples, appeared in May 2017. Instead of GMMs and dictionary learning in PCL,  MagNet trains autoencoders for defense and provides sufficient experiments to claim its defense capability. On the other hand, the authors of this paper seem to be not aware of this pioneering work and claim To the best of our knowledge, our proposed PCL methodology is the first unsupervised countermeasure that is able to detect DL adversarial samples generated by the existing state-of-the-art attacks, which is obviously not true. More importantly, MagNet is able to defend the adversarial examples very well (almost 100% success) no matter the adversarial examples are close to the information manifold or not. As a result, the resulting ROC and AUC score are expected be better than PCL. In addition, the authors of MagNet also compared their performance in white-box (attacker knowing the reformer), gray-box (having multiple independent reformers), and black-box (attacker not knowing the reformer) scenarios, whereas this paper only considers the last case. 2. Insufficient attack evaluations - the attacks used in this paper to evaluate the performance of PCL are either weak (no longer state-of-the-art) or incorrectly implemented. For FGSM, the iterative version proposed by (Kurakin, ICLR 2017) should be used. JSMA and deep fool are not considered strong attacks now (see Carlini's bypassing 10 detection methods paper). Carlini-Wagner attack is still strong, but the authors only use 40 iterations (should be at least 500) and setting the confidence 0, which is known to be producing non-transferable adversarial examples. In comparison, MagNet has shown to be effective against different confidence parameters. In summary, this paper has limited novelty, incremental contributions, and lacks convincing experimental results due to weak attack implementation.       ",18,432,21.6,5.696897374701671,210,1,431,0.0023201856148491,0.0201342281879194,-0.9247,130,63,79,21,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 4, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,1,9,4,0,9,1,3,0,0,0,0,0,0,0,3,0,3,0,0,10,1,0,0.4315061720717639,0.4503666016414474,0.2440844294148032
ICLR2018-HyKZyYlRZ-R1,Reject,"The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task. The paper is well written and easy to follow. The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. Comments:  * In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet. * The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy. * The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets). * Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as: P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26u201330. D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE. T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015. Revised Review: The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition. I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated. ",36,586,11.72,5.297297297297297,232,2,584,0.0034246575342465,0.0336700336700336,0.9397,193,77,95,33,10,6,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 3, 'DAT': 3, 'MET': 13, 'EXP': 11, 'RES': 3, 'TNF': 1, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 6, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 18, 'SUB': 11, 'CLA': 1}",0,2,0,3,3,13,11,3,1,2,0,2,6,0,0,1,1,0,2,0,18,11,1,0.718609784699303,0.6783682770660152,0.5037180738697802
ICLR2018-HyKZyYlRZ-R2,Reject,"The paper describes a neural end-to-end architecture to solve multiple tasks at once. The architecture consists of an encoder, a mixer, a decoder, and many modality networks to cover different types of input and output pairs for different tasks. The engineering endeavor is impressive, but the paper has little scientific value. Below are a few suggestions to make the paper stronger. It is possible that the encoder, mixer, and decoder are just multiplexing tasks based on the input. One way to analyze whether this happens is to predict the identity of the task from the hidden vectors. If this is the case, how to prevent it from happening? If this does not happen, what is being shared across tasks? This can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding space. Why multitask learning help the model perform better is still unclear. If the model is able to leverage knowledge learned from one task to perform another task, then we expect to see either faster convergence or good performance with fewer samples. The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning? If the modality network is shared across multiple tasks, we expect the learned hidden representation produced by the modality network is more universal. If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task? Reporting per-token accuracies, such as those in Table 2, is problematic. It's unclear how to compute per-token accuracies for structured prediction tasks, such as speech recognition, parsing, and translation. Furthermore, based on the results in Table 2, the model clearly fails on the speech recognition task. The author should also report the standard speech recognition metric, word error rates (WER), for the speech recognition task in Table 1. ",19,329,21.933333333333334,5.09375,158,0,329,0.0,0.0333333333333333,0.9134,90,38,61,12,9,2,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 6, 'EXP': 6, 'RES': 5, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 1, 'CLA': 0}",0,2,0,2,1,6,6,5,3,1,0,1,0,0,0,0,0,0,0,0,13,1,0,0.6448933046680319,0.2296857114403379,0.2887141972960942
ICLR2018-HyKZyYlRZ-R3,Reject,"The paper presents a multi-task architecture that can perform multiple tasks across multiple different domains. The authors design an architecture that works on image captioning, image classification, machine translation and parsing. n The proposed model can maintain performance of single-task models and in some cases show slight improvements. This is the main take-away from this paper. There is a factually incorrect statement - depthwise separable convolutions were not introduced in Chollet 2016. Section 2 of the same paper also notes it (depthwise convolutions can be traced back to at least 2012).",6,90,15.0,5.833333333333333,65,0,90,0.0,0.0109890109890109,0.2516,27,12,14,4,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 1, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,0,0,1,0,1,0,0,0,2,0,0,0,0,0,2,0,0,2,0,0,0.3572687638532917,0.2229613422142551,0.16246468737417033
ICLR2018-HyN-ZvlC--R1,Reject,"This paper presents two methods for imposing a margin on discriminative loss functions, one which uses the margin between the reference transcription and alternatively hypothesized transcriptions (LMLM), and another which compares all alternative candidates and uses a margin between those with a better system objective (WER or bleu) and those with a worse system objective (rank-LMLM). Some interesting results on the development set show the importance of things like warm starting on large language model training data. The methods presented here could be of interest to those training language models for use in specific systems, and the paper reads reasonably clearly .  The principal shortcoming of the paper is that there was essentially no effort to establish that the baseline systems that are being improved through reranking via these methods are decent baselines for such a use, or to really specify these systems in a way that would allow for replication of the results being presented in the paper. Sufficient specification of the exact training data and procedure is standard in papers that purport to establish methods to improve upon such baselines, yet such information is sorely lacking in this paper.  Further, the speech data sets, Fisher and Wall St. Journal, have what would seem to be very high word error rates versus what should be possible with standard open-source speech recognizers such as Kaldi.  For example, by referencing a page that attempts to establish the state of the art on standard data sets (https://github.com/syhw/wer_are_we), we can find links to papers by Povey et al (http://www.danielpovey.com/files/2016_interspeech_mmi.pdf) and the Deep 2 paper in your citations, which themselves include baselines from other papers that cut the error rate in half versus even your best scoring systems, let alone your baselines. Similarly, your Bleu score on Vietnamese to English translation is way below what were reported (even by the organizer baseline) for the IWSLP conference where the data became available: https://github.com/magizbox/underthesea/wiki/SOTA-Machine-Translation:-IWSLT-2015  Granted, the competing systems also were outperformed by the organizer baseline for that task at IWSLT 2015, but not by the degree to which your system is.  Again, your best performing system (using your new methods) has performance far below the worst reported competing system.  The cavalier presentation of specific details regarding your baseline systems (which is critical for any sort of replicability) and the uniformly weak performance of these systems relative to widely reported results, leads me to discount the probability that your methods would actually result in improvements on truly solid baselines. I would have preferred one domain experiment carried out with appropriately rock solid documentation of the ball-park competitive baseline system to these results. Overall, the method is interesting and the dev set experiments were informative, but ultimately the experiments were not. Revision:  Having read the author response for this paper, I am encouraged by the updated baseline for WSJ and the additional explication about the competing Fisher systems. The additional information about the systems included in section 4.4 is pretty nominal, though, and I worry about the ability of others to replicate these results. I would have felt better about the results if there were reported results from other papers included here, instead of the authors' attempt to create a baseline from the given data, which may or may not (as we have seen) represent a strong enough baseline from which to draw conclusions. That is to say that I still have some reservations (though less than I had before). I am including this additional information in my review, for use by the area chair, but otherwise leaving my assessment as-is. ",19,592,31.157894736842103,5.432525951557094,288,2,590,0.0033898305084745,0.0183028286189683,0.986,170,61,98,33,9,5,"{'ABS': 0, 'INT': 0, 'RWK': 8, 'PDI': 1, 'DAT': 7, 'MET': 7, 'EXP': 7, 'RES': 8, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 6, 'PNF': 0, 'REC': 1, 'EMP': 11, 'SUB': 4, 'CLA': 0}",0,0,8,1,7,7,7,8,0,0,0,1,1,1,0,0,1,6,0,1,11,4,0,0.6460453432951179,0.5626809722148995,0.4096273466482372
ICLR2018-HyN-ZvlC--R2,Reject,"The main contribution of this paper are: (a) replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion, (b) propose two large margin criterion -- difference in likelihood and difference in rank (WER or BLUE ordered) hypotheses, (c) demonstrate performance gains two standard tasks -- an ASR task on Wall Street Journal (small task) and an MT task. In addition, they provide examples in Figure (1) and (2) that illustrate the effect of the cost function on training. Their illustration in Figure 4 is also helpful in seeing the impact of using a warm start with a generative model. ",5,102,25.5,5.397849462365591,66,0,102,0.0,0.0095238095238095,0.7269,40,12,11,1,3,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,3,0,1,2,0,0,0,0,0,0,0,0,0,1,0,5,0,0,0.2148093964739902,0.2247100519615941,0.09799704736999813
ICLR2018-HyN-ZvlC--R3,Reject,"A large margin , end to end language model that uses a discriminative objective function is proposed. The proposed objective imposes a hinge loss on the margin to ensure that the ground truth is at least  some fixed amount larger than the imposter. A variant on this, which also incorporates the ranks of the imposters sorted by a metric such as edit distance or BLEU metric with respect to the ground truth is also introduced. The paper is missing some of the original references to a discriminative LM (DLM) as well as  references to the use of a NN LM directly in decoding (presented in ICASSP and Interspeech conferences over the last 5 years). For example, H.-K. J. Kuo, E. Fosler-Lussier, H. Jiang, and C.-H. Lee, ""Discriminative training of language models for speech recognition,"" in Proc. ICASSP, vol. 1, 2002, pp. 325u2013328. Have you considered the widely-used NCE method to handle the large vocabulary? The dev perplexity quoted in Section 4 for a 5 gram LM is very high. Also Table 4 and Table 5 on WSJ and  FIsher  show baseline experiments that are quite far away from the state-of-the-art in these tasks. Even if you assume that you use the simplest possible acoustic model and/or an open source tool kit for the decoder,  these error rates are high (WSJ error rates are lower than 10%, not 16.7%). Even if the LM is trained on the common-crawl corpus, it has  a very low OOV rate, and fine tuning on the tasks only lowers it b t 1%. For reference,  please see papers from Saon et al., Seide et al, Povey et al, Yajie Miao et al in various ICASSP, Interspeech and arXiv papers. Comparisons with weak baselines can significantly color the conclusions. On the Fisher test set, the interpolated LM offers very little over the baseline LM in Table 5. This is contrary to what is observed in the literature. There is not much difference between rankLM and LMLM as well to draw a clear conclusion between the two. Given that this is n-best rescoring, how are the N-best lists generated? You state that they are extracted from  64 beam candidates, are they unique N-best lists? Can this method be applied to lattices? What is the perplexity of all the language models corresponding to  Tables 4 and 5? This would have been useful to study in itself. In the SMT tasks, the baselines reported seem to be far away from results presented in the literature on the IWSLT task (see http://workshop2015.iwslt.org/downloads/IWSLT_2015_EP_3.pdf) While the proposed objective is interesting and meaningful for several conversational applications, as well as sentence modeling, the presented experimental results are not convincing.",21,444,18.5,4.835322195704057,225,0,444,0.0,0.0066079295154185,0.9044,132,56,64,27,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 6, 'PDI': 2, 'DAT': 0, 'MET': 11, 'EXP': 4, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 3, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 3, 'CLA': 0}",0,1,6,2,0,11,4,3,2,0,0,0,3,0,0,0,0,4,0,0,9,3,0,0.5748103070711199,0.33887383350029,0.29480260288675464
ICLR2018-HyPpD0g0Z-R1,Reject,"The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization. The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or ""style"" features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy. The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings. Although I really wanted to like the paper, I have several concerns, First and most importantly, the paper is not citing several important related work. Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change. Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).33.07 Besides, there are also other less related papers, e.g. http://proceedings.mlr.press/v28/zhang13d.pdf, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052/0, https://arxiv.org/abs/1707.09724, (or potentially https://arxiv.org/abs/1507.05333 and https://arxiv.org/abs/1707.06422), that I think may be mentioned for a more complete picture. Since there is some related work, it may be also worth to compare with it, or use the same datasets. I'm also not very happy with the term ""counterfactual"". As the authors mention in footnote, this is not the correct use of the term, since counterfactual means ""against the fact"". For example, a counterfactual query is ""we gave the patient a drug and the patient died, what would have happened if we didn't give the drug? "" In this case, these are just different interventions on possibly the same object. I'm not sure that in the practical applications one can assure that the noise variables stay the same, which, as the authors correctly mention, would make it a bit closer to counterfactuals. It may sound pedantic, but I don't understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn't vary too much in the different images of the single object. **EDIT**: I was satisfied with the clarifications from the authors and I appreciated the changes that they did with respect to the related work and terminology, so I changed my evaluation from a 5 (marginally below threshold) to a 7 (good paper, accept).",14,437,24.27777777777778,5.124121779859485,226,5,432,0.011574074074074,0.0227790432801822,0.9185,118,53,69,36,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 2, 'REC': 1, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,1,5,2,1,7,1,2,0,0,0,1,2,0,0,0,0,2,2,1,5,2,0,0.6448076419087893,0.5583287932842422,0.4052490404120846
ICLR2018-HyPpD0g0Z-R2,Reject,"Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same. As authors note, this constraint can be converted into a KKT style penalty with KKT multiplier lambda. Thus this is very  similar to other regularizers that increase smoothness of the function, such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group, as well as manifold regularization (see e.g. Belkin, Niyogi et al. JMLR). Heck, in practie ridge regularization will also do something similar for many function classes.  Experiments didn't compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples). It's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results. That made it hard to get excited about the results in a vacuum. Would this proposed strategy have thwarted the Russian tank legend problem? Would it have fixed the Google gorilla problem?Why or why not? Overall, I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples. Page 2: calling additional instances of the same person ""counterfactual observations"" didn't seem consistent with the usual definition of that term... maybe I am just missing the semantic link here, but this isn't how we usually use the term counterfactual in my corner of the field. Re: ""one creates additional samples by modifying..."" be nice to quote more of the early work doing this, I believe the first work of this sort was Scholkopf's, he called it ""virtual examples"" and I'm pretty sure he specifically did it for rotation MNIST images (and if not exactly that, it was implied). I think the right citation is ""Incorporating invariances in support vector learning machines "" Scholkopf, Burges, Vapnik 1996, but also see Decoste * Scholkopf 2002 ""Training invariant support vector machines."" .",13,349,24.928571428571427,5.235820895522388,204,4,345,0.0115942028985507,0.0536723163841807,0.9609,93,47,68,22,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,4,1,1,5,2,2,0,0,0,1,1,0,0,0,0,3,0,0,6,1,1,0.6442412727038516,0.4477885567330393,0.3514929110530455
ICLR2018-HyPpD0g0Z-R3,Reject,"This paper aims at robust image classification against adversarial domain shifts. In the used model, there are two types of latent features, core features and style features, and the goal is to achieved by avoiding using the changing style features. The proposed method, which makes use of grouping information, seems reasonable and useful. It is nice that the authors use counterfactual regularization. But I failed to see a clear, new contribution of using this causal regularization, compared to some of the previous methods to achieve invariance (e.g., relative to translation or rotation). For examples of such methods, one may see the paper Transform Invariant Auto-encoder (by Matsuo et al.) and references therein. The data-generating process for the considered model, given in Figure 2, seems to be consistent with Figure 1 of the paper Domain Adaptation with Conditional Transferable Components (by Gong et al.). Perhaps the authors can draw the connection between their work and Gong et al.'s work and the related work discussed in that paper. Below are some more detailed comments, In Introduction, it would be nice if the authors made it clear that Their high predictive accuracy might suggest that the extracted latent features and learned representations resemble the characteristics our human cognition uses for the task at hand.  Why do the features human cognition uses give an optimal predictive accuracy? On page 2, the authors claimed that These are arguably one reason why deep learning requires large sample sizes as large sample size is clearly not per se a guarantee that the confounding effect will become weaker.  Could the authors give more detail on this? A reference would be appreciated. ",11,273,22.75,5.240601503759399,159,5,268,0.0186567164179104,0.0326086956521739,0.9648,83,33,44,9,7,5,"{'ABS': 0, 'INT': 3, 'RWK': 3, 'PDI': 0, 'DAT': 1, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 0}",0,3,3,0,1,5,1,0,1,0,0,0,1,0,0,1,0,3,1,0,3,2,0,0.5012127821185375,0.5571404720479429,0.3115508296998975
ICLR2018-HyRVBzap--R1,Accept,"The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar. The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks. The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks. The idea proposed is fairly straight-forward. Despite being a simple approach, the experimental results are quite promising. The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights. As pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings. Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.? Table 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive.  Do the authors have any insight on this? Comments: 1. The writing of the paper could be improved. For example, Transferability analysis in section 1 is barely understandable; 2. Arrow in Figure 3 are not quite readable; n3. The paper is over 11 pages. The authors might want to consider shrink it down the recommended length. ",15,237,15.8,5.721238938053097,130,2,235,0.0085106382978723,0.0334728033472803,-0.8156,64,32,41,14,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 2, 'ANA': 1, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 3}",0,1,0,3,1,4,2,2,2,1,0,4,0,0,0,0,0,0,2,0,7,1,3,0.6440277709914849,0.4484000281190137,0.35876553619531765
ICLR2018-HyRVBzap--R2,Accept,"This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space. This is an interesting idea which, from a robustness point of view (Xu et al, 2013) makes sense. Note that a similar strategy has been used in the recent past under the name of stability training. The proposed method works well on CIFAR and MNIST datasets. My main concerns are:  t- The adversarial objective and the stability objective are potentially conflicting. Indeed when the network misclassifies an example, its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version (that of the ground truth label). Have the authors considered this issue? Can they elaborate more on how they with this? t- It may be significantly more difficult to make this work in such setting due to the dimensionality of the data. Did the authors try such experiment? It would be interesting to see these results. Lastly, The insights regarding label leaking are not compelling. Label leaking is not a mysterious phenomenon. An adversarially trained model learns on two different distributions. Given the fixed size of the hypothesis space explored (i.e., same architecture used for vanilla and adversarial training), It is natural that the statistics of the simpler distribution are captured better by the model. Overall, the paper contains valuable information and a method that can contribute to the quest of more robust models. I lean on accept side.     ",17,261,17.4,5.228,146,1,260,0.0038461538461538,0.0112359550561797,0.8616,70,38,42,13,8,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 8, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 10, 'SUB': 3, 'CLA': 0}",0,0,2,2,1,8,4,1,0,0,1,2,0,0,0,0,1,2,0,1,10,3,0,0.5736237101724804,0.5614836880429752,0.3658492477451074
ICLR2018-HyRVBzap--R3,Accept,"The paper presents a novel adversarial training setup, based on distance based loss of the feature embedding. + novel loss + good experimental evaluation + better performance - way too long - structure could be improved - pivot loss seems hacky The distance based loss is novel, and significantly different from prior work. It seems to perform well in practice as shown in the experimental section. The experimental section is extensive, and offers new insights into both the presented algorithm and baselines. Judging the content of the paper alone, it should be accepted. However, the exposition needs significant improvements to warrant acceptance. First, the paper is way too long and unfocused.  The recommended length is 8 pages + 1 page for citations. This paper is 12+1 pages long, plus a 5 page supplement. I'd highly recommend the authors to cut a third of their text, it would help focus the paper on the actual message: pushing their new algorithm. Try to remove any sentence or word that doesn't serve a purpose (help sell the algorithm). The structure of the paper could also be improved. For example the cascade adversarial training is buried deep inside the experimental section. Considering that it is part of the title, I would have expected a proper exposition of the idea in the technical section (before any results are presented). While condensing the paper, consider presenting all technical material before evaluation. Finally, the pivot loss seems a bit hacky. First, the pivot objective and bidirectional loss are exactly the same thing. While the bidirectional loss is a proper loss and optimized as such (by optimizing both E^adv and E), the pivot objective is no loss function, as it does not correspond to any function any optimization algorithm could minimize. I'd recommend the just remove the pivot objective, or at least not call it a loss. In summary, the results and presented method are good, and eventually deserve publication. However the exposition needs to significantly improve for the paper to be ready for ICLR.",27,330,15.714285714285714,5.205696202531645,156,3,327,0.0091743119266055,0.0177514792899408,0.9806,92,38,65,21,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 7, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 10, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 3, 'IMP': 0, 'CMP': 1, 'PNF': 8, 'REC': 2, 'EMP': 10, 'SUB': 4, 'CLA': 0}",0,1,2,1,0,9,7,5,0,0,0,10,0,1,1,3,0,1,8,2,10,4,0,0.5746593478347777,0.7842364223241537,0.45498405288042876
ICLR2018-HyRnez-RW-R1,Accept,"The authors present a scalable model for questioning answering that is able to train on long documents. On the TriviaQA dataset, the proposed model achieves state of the art results on both domains (wikipedia and web). The formulation of the model is straight-forward, however I am skeptical about whether the results prove the premise of the paper (e.g. multi-mention reasoning is necessary). Furthermore, I am slightly unconvinced about the authors' claim of efficiency. Nevertheless, I think this work is important given its performance on the task. 1. Why is this model successful? Multi-mention reasoning or more document context? I am not convinced of the necessity of multi-mention reasoning, which the authors use as motivation, as shown in the examples in the paper. For example, in Figure 1, the answer is solely obtained using the second last passage. The other mentions provide signal, but does not provide conclusive evidence. Perhaps I am mistaken, but it seems to me that the proposed model cannot seem to handle negation, can the authors confirm/deny this? I am also skeptical about the computation efficiency of a model that scores all spans in a document (which is O(N^2), where N is the document length). Can you show some analysis of your model results that confirm/deny this hypothesis? 2. Why is the computational complexity not a function of the number of spans? It seems like the derivations presents several equations that score a given span. Perhaps I am mistaken, but there seems to be n^2 spans in the document that one has to score. Shouldn't the computational complexity then be at least O(n^2), which makes it actually much slower than, say, SQuAD models that do greedy decoding O(2n + nm)? Some minor notes - 3.3.1 seems like an attention computation in which the attention context over the question and span is computed using the question. Explicitly mentioning this may help the reading grasp the formulation. - Same for 3.4, which seems like the biattention (Seo 2017) or coattention (Xiong 2017) from previous squad work. - The sentence We define ... to be the embeddings of the l words of the sentence that contains s. is not very clear. Do you mean that the sentence contains l words? It could be interpreted that the span has l words. - There is a typo in your 3.7 level 1 complexity: there is an extra O inside the big O notation.",25,395,17.954545454545453,5.038356164383561,182,9,386,0.0233160621761658,0.0523690773067331,0.9634,107,40,78,19,9,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 13, 'EXP': 0, 'RES': 5, 'TNF': 3, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 17, 'SUB': 1, 'CLA': 4}",0,1,1,1,1,13,0,5,3,2,0,2,0,0,0,0,0,1,0,0,17,1,4,0.6461989899034767,0.4546391686144941,0.35544750801577996
ICLR2018-HyRnez-RW-R2,Accept,"This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches. The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence. The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline. It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs. Overall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction. On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning. As such, I am borderline on its acceptance. * The TriviaQA leaderboard shows a submission from 9/24/17 (by chrisc) that has significantly higher EM/F1 scores than the proposed model. Why is this result not compared to in Table 1? Detailed comments: - Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017? This may allow you to avoid truncation altogether. Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient. - How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions? - What is the average number of sentences per document? It's hard to get an idea of how reasonable the chosen truncation thresholds are without this. - In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased. Did the authors try truncating after more words (e.g., 10k)? - I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works. - Krasner misspelled multiple times as Kramer",19,355,25.357142857142858,5.165680473372781,212,3,352,0.0085227272727272,0.0303030303030303,0.9283,94,40,71,21,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 10, 'EXP': 5, 'RES': 5, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,1,2,0,1,10,5,5,1,1,0,2,1,0,0,0,1,5,0,1,9,1,1,0.7172309407578833,0.6721109763741704,0.5013005532588887
ICLR2018-HyRnez-RW-R3,Accept,"This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long). Overall, I think it is a nice demonstration that non-recurrent models can work so well, but I also don't find the results strikingly surprising.  It is also a bit hard to get the main takeaway messages. It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547). But all the other components seem to have been demonstrated previously in other papers. An important feature of this model is it is easier to parallelize and speed up the training/testing processes. However, I don't see any demonstration of this in the experiments section. Also, I am a bit disappointed by how ""cascades"" are actually implemented. I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on). The proposed model just simply sums up all the occurrences of candidate answers throughout the full document. 3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity. I am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not.  ",13,268,20.615384615384617,5.37109375,161,3,265,0.0113207547169811,0.040590405904059,0.9721,67,33,54,31,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 4, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 3, 'CLA': 0}",0,1,1,1,1,5,4,6,0,0,0,0,1,0,0,0,0,1,0,0,9,3,0,0.5729664367223835,0.33852234582872,0.2839389017315412
ICLR2018-HyTrSegCb-R1,Reject,"The paper is a pain to read. Most of the citation styles are off (i.e., without parentheses). Most of the sentences are not grammatically correct. Most, if not all, of the determiners are missing. It is ironic that the paper is proposing a model to generate grammatically correct sentences, while most of the sentences in the paper are not grammatically correct. The experimental numbers look skeptical. For example, 1/3 of the training results are worse than the test results in Table 1. It also happens a few times in Table 5.  Either the models are not properly trained, or the models are heavily tuned on the test set. The running times in Table 9 are also skeptical. Why are the Concorde models faster than unigrams and bigrams? Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?",12,152,13.818181818181818,4.724137931034483,75,1,151,0.0066225165562913,0.0261437908496732,-0.7308,32,18,25,14,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 2, 'EXP': 4, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 4, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 2}",0,0,0,0,1,2,4,1,2,0,0,4,1,0,0,0,0,2,4,0,3,1,2,0.500668333873769,0.5571824748246955,0.3132440369744712
ICLR2018-HyTrSegCb-R2,Reject,"In this work, the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sentence. The proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task. The authors evaluate their technique using three morphologically rich languages French, Polish and Russian and obtain promising results. The morphological agreement task would be an interesting contribution of the paper, with wider potential. But one concern that I have is regarding the evaluation metrics used for it. Firstly, word accuracy rate doesn't seem appropriate, as it does not measure morphological agreement. Secondly, sentence accuracy (w.r.t. the sentences from which the normalized sentences are derived) is not indicative of morphological agreement: even wrong sentences in the output could be perfectly valid in terms of agreement. A grammatical error rate (fraction of grammatically wrong sentences produced) would probably be a better measure. Another concern I have is regarding the quality of the baseline: Additional variants of the baseline models should be considered and the best one reported. Specifically, in the conversation task, have the authors considered switching the order of normalized answer and context in the input? Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence-to-sequence translation models). Also, many experimental details are missing from the draft: -- What are the sizes of the train/test sets derived from the OpenSubtitles database? -- Details of the validation sets used to tune the models. -- In Section 5.4, no details of the question-answer corpus are provided. How many pairs were extracted? How many were used for training and testing? -- In Section 5.4.1, how many assessors participated in the evaluation and how many questions were evaluated? -- In some of the tables (e.g. 6, 7, 8) which show example sentences from Polish, Russian and French, please provide some more information in the accompanying text on how to interpret these examples (since most readers may not be familiar with these languages). Pros: -- Efficient model -- Proposed architecture is general enough to be useful for other sequence-to-sequence problems Cons: -- Evaluation metrics for the morphological agreement task are unsatisfactory -- It would appear that the baselines could be improved further using standard techniques",23,373,23.3125,5.66016713091922,180,2,371,0.0053908355795148,0.0235602094240837,0.9895,108,50,65,18,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 4, 'MET': 14, 'EXP': 6, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 11, 'SUB': 10, 'CLA': 1}",0,1,1,2,4,14,6,3,0,1,0,0,0,0,0,0,0,1,1,0,11,10,1,0.5753910237332706,0.5627352184788785,0.3629354010919919
ICLR2018-HyTrSegCb-R3,Reject,"The key contributions of this paper are: (a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a standard form and then into their correct morphological form, (b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context, (c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and (d) they demonstrate clear and substantial performance gains on a dialog question answer task. Their analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences. As an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. The only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.",7,160,40.0,5.308724832214765,100,0,160,0.0,0.0124223602484472,0.9723,50,20,20,3,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 3, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,2,0,3,1,1,0,0,0,0,0,0,1,0,1,0,4,0,0,0.4289330244706321,0.3351992056378622,0.21790901908154384
ICLR2018-HyUNwulC--R1,Accept,This paper focuses on accelerating RNN by applying the method from Blelloch (1990). The application is straightforward and thus technical novelty of this paper is limited. But the results are impressive. One concern is the proposed technique is only applied for few types of RNNs which may limit its applications in practice. Could the authors comment on this potential limitation?,5,60,12.0,5.271186440677966,47,1,59,0.0169491525423728,0.0333333333333333,0.296,16,6,12,2,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,2,2,0,2,0,1,0,0,0,1,0,0,0,1,2,0,0,0,1,0,0,0.4289614084494529,0.3333813699817812,0.21302616414684272
ICLR2018-HyUNwulC--R2,Accept,"# Summary and Assessment  The paper addresses an important issueu2013that of making learning of recurrent networks tractable for sequence lengths well beyond 1'000s of time steps. A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor. The authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator. Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively. In the following, the authors explore the landscape of RNNs satisfying the necessary conditions. The performance is investigated in terms of wall clock time.  Further, experimental results of problems with previously untacked sequence lengths are reported. The paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies. To me, the execution seems sound. The experiments back up the claim. ## Minor - I challenge the claim that thousands and millions of time steps are a common issue in ""robotics, remote sensing, control systems, speech recognition, medicine and finance"", as claimed in the first paragraph of the introduction. IMHO, most problems in these domains get away with a few hundred time steps; nevertheless, I'd appreciate a few examples where this is a case to better justify the method.",12,238,19.83333333333333,5.466666666666667,149,1,237,0.0042194092827004,0.0081632653061224,0.8445,80,29,31,14,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 7, 'PDI': 4, 'DAT': 0, 'MET': 5, 'EXP': 4, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,0,7,4,0,5,4,1,0,0,0,1,1,0,0,0,1,0,0,0,7,1,0,0.5019201828301358,0.3370650779423911,0.25655496273891165
ICLR2018-HyUNwulC--R3,Accept,"This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation. The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM. Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997). A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure. The paper is written very well, with explanation (as opposed to obfuscation) as the goal. Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures. The paper provides argument and experimental evidence against the rotation used typically in RNNs.  While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets. While the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.",9,201,22.33333333333333,5.712820512820513,127,0,201,0.0,0.0295566502463054,0.9846,69,28,25,11,8,5,"{'ABS': 1, 'INT': 0, 'RWK': 6, 'PDI': 2, 'DAT': 1, 'MET': 4, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 4, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 2}",1,0,6,2,1,4,2,0,0,0,0,2,1,0,0,1,4,1,0,0,5,0,2,0.5727725017434604,0.5582686303111252,0.35752331894539585
ICLR2018-HyWrIgW0W-R1,Accept,"The paper takes a closer look at the analysis of SGD as variational inference, first proposed by Duvenaud et al. 2016 and Mandt et al. 2016. In particular, the authors point out that in general, SGD behaves quite differently from Langevin diffusion due to the multivariate nature of the Gaussian noise. As the authors show based on the Fokker-Planck equation of the underlying stochastic process, there exists a conservative current (a gradient of an underlying potential) and a non-conservative current (which might induce stationary persistent currents at long times). The non-conservative part leads to the fact that the dynamics of SGDtmay show oscillations, and these oscillations may even prevent the algorithm from converging to the 'right' local optima. The theoretical analysis is carried-out very nicely, and the theory is supported by experiments on two-dimensional toy examples, and Fourier-spectra of the iterates of SGD. This is a nice paper which I would like to see accepted. In particular I appreciate that the authors stress the importance of 'non-equilibrium physics' for understanding the SGD process. Also, the presentation is quite clear and the paper well written. There are a few minor points which I would like to ask the authors to address:  1. Why cite Kingma and Welling as a source for variational inference intsection 3.1? VI is a much oldertfield, and Kingma and Welling proposed a very special form of VI, namely amortized VI with inference networks. A better citation would be Jordan ettal 1999. 2. I'm not sure how much to trust the Fourier-spectra. In particular, perhaps the deviations from Brownian motion could also be due to the discretetnature of SGD (i.e. that the continuous-time formalism is only an approximation of a discrete process). Could you elaborate on this? 3. Could you give the reader more details on how the uncertainty estimates on the Fourier transformations were obtained?  Thanks.",15,309,16.263157894736842,5.350694444444445,165,3,306,0.0098039215686274,0.0192926045016077,0.9884,78,49,40,19,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 7, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,1,7,0,0,9,2,0,0,1,0,2,1,0,0,0,0,1,1,1,4,2,0,0.5026160789243405,0.5575281043684059,0.3167339401546864
ICLR2018-HyWrIgW0W-R2,Accept,"The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation. They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank, as evidenced in their experiments. Overall the paper is written quite well, and the authors do a good job of explaining their thesis. However I was unable to identify any real novelty in the theory: the Fokker-Planck equation has been widely used in analysis of stochastic noise in MCMC samplers in recent years, and this paper mostly rephrases those results. Also the fact that SGD theory only works for isotropic noise is well known, and that there is divergence from the true loss function in case of low rank noise is obvious. Thus I found most of section 3 to be a reformulation of known results, including Theorem 5 and its proof.  Same goes for section 5; the symmetric- anti symmetric split is a common technique used in the stochastic MCMC literature over the last few years, and I did not find any new insight into those manipulations of the Fokker-Planck equation from this paper. Thus I think that although this paper is written well, the theory is mostly recycled and the empirical results in Section 4 are known; thus it is below acceptance threshold due to lack of novelty.",8,239,29.875,4.964757709251101,127,1,238,0.0042016806722689,0.0083333333333333,0.6115,65,33,40,16,7,6,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 1, 'SUB': 0, 'CLA': 2}",0,1,3,1,0,5,1,1,0,0,0,4,0,0,0,3,0,2,1,1,1,0,2,0.5012980006840583,0.666955179463747,0.3563176406778531
ICLR2018-HyWrIgW0W-R3,Accept,"This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models. It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function. However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function. The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.  I think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.  First, Assumption 4 seems a bit too abstract. It is not easy to see what the assumption means. It would be better if an example is given, which is verified to satisfy the assumption. Another comment is related to the overall content of this paper. Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function.",8,224,20.363636363636363,5.187214611872146,114,4,220,0.0181818181818181,0.0221238938053097,0.96,56,35,41,15,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,6,0,0,0,0,0,2,0,0,0,0,0,0,0,0,4,1,0,0.2870106650912595,0.2240880945267511,0.12921576142832863
ICLR2018-HyXBcYg0b-R1,Reject,"The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version. Summary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs. It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees. The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms. They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering. Main comments: It would strengthen the paper to also compare all these network learning based approaches to variational ones. For instance, to a spectral clustering method for the semi supervised clustering, or solving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006. The abstract and the conclusion should be revised, they are very vague. - The abstract should be self contained and should not contain citations. - The authors should clarify which problem they are dealing with. - instead of the  umerical result show the performance of the new model, give some numerical results here, otherwise, this sentence is useless. - we propose ... as propose -> unclear: what do you propose? Minor comments: - You should make sentences when using references with the author names format. Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997) - As Eq 2 -> As the minimization of Eq 2 (same with eq 4) - Don't start sentences with And, or But  ",15,266,17.733333333333334,5.486274509803922,153,0,266,0.0,0.0249110320284697,-0.6349,84,29,47,11,9,5,"{'ABS': 2, 'INT': 1, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 6, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 5, 'REC': 0, 'EMP': 1, 'SUB': 3, 'CLA': 2}",2,1,2,3,0,6,1,1,0,0,0,1,2,0,0,0,0,3,5,0,1,3,2,0.6443210943156529,0.5563306444526458,0.40327423239338706
ICLR2018-HyXBcYg0b-R2,Reject,"The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community. In particular, a tree structured LSTM is taken and modified. The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality. My biggest concern is novelty, as the modifications are minor. In particular, the formulation can be seen in a different way. As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate). This is a minor modification. Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper. A second concern is the presentation of the paper, which can be confusing at some points. A major example is the mathematical description of the methods. When reading the description as given, one should actually infer that Graph ConvNets and Graph RNNs are the same thing, which can be seen by the fact that equations (1) and (6) are equivalent. Another example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies. Generally, a clear introduction of the problem is also missing. What are the inputs, what are the outputs, what kind of problems should be solved? The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph? The model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality. A couple of details :  - the length of a graph is not defined. The size of the set of nodes might be meant. - at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing. RNNs are not restricted to NLP and I think there is no need to introduce an application at this point. - It is unclear what does the following sentence means: ConvNets are more pruned to deep networks than RNNs? - What are heterogeneous graph domains? ",22,411,20.55,4.987212276214834,203,4,407,0.0098280098280098,0.0191387559808612,0.2936,108,41,85,17,8,4,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 4, 'DAT': 1, 'MET': 13, 'EXP': 0, 'RES': 1, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 10, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,2,2,4,1,13,0,1,4,0,0,2,0,0,0,1,0,1,10,0,5,0,0,0.5748300959014244,0.4474863944980463,0.32454433430929397
ICLR2018-HyXBcYg0b-R3,Reject,"The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets. There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture. The Related work part of this paper makes a good description of both topics. I would expect the paper elaborate more (at least in a more explicit way) about the relationship between the two models (the proposed graph LSTM and the proposed Gated Graph ConvNets). The authors claim that the innovative of the graph Residual ConvNets architecture, but experiments and the model section do not clearly explain the merits of Gated Graph ConvNets over Graph LSTM. The presentation may raise some misunderstanding.  A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting. In the section of experiments, they compare 5 different methods on two graph mining tasks. These two proposed neural network models seem performing well empirically. In my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length,  and both models worth future stuies for speicific problems. ",10,197,17.90909090909091,5.547368421052632,113,1,196,0.0051020408163265,0.025,0.8277,62,32,24,7,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,1,5,1,0,4,3,0,0,1,1,1,0,0,0,0,1,4,1,0,4,2,0,0.5726734909628787,0.5578795920399758,0.35895438622401543
ICLR2018-Hy_o3x-0b-R1,Reject,"The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs. The numbers in the tables are good but I have several comments on the motivation, originality and experiments. Most parts of the paper provide a detailed review of the literature. However, the resulting model is quite like a combination of the existing advances and the main contribution of the paper, i.e. the convolution stochastic layer, is not well discussed. Why should we introduce the convolution stochastic layers? Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture? What's the exact challenge of training VAEs addressed by the convolution stochastic layer? Please strengthen the motivation and originality of the paper. Though the results are good, I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results? Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images? According to the experimental setting in the Section 3 (Page 5 Paragraph 2), In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to Su00f8nderby et al. (2016)) and for the natural images they are spatial (cf. Table 1). There was no significant difference when using feature maps (as compared to dense layers) for modelling gray-scaled images there is no stochastic convolutional layer. Then is there anything new in FAME on the gray images? Furthermore, how could FAME advance the previous state-of-the-art? It seems because of other factors instead of the stochastic convolutional layer. The results on the natural images are not complete. Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets. The quality of the samples on the CIFAR10 dataset seems not competitive to the baseline papers listed in the table. Though the visual quality does not necessarily agree with the NLL results but such large gap is still strange. Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer? Meanwhile, why FAME cannot obtain good generation results on the CIFAR10 dataset? Is it because there is a lot randomness in the stochastic convolutional layer? It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results. Minor things:  Please rewrite the sentence When performing reconstructions during training ... while also using the stochastic latent variables z   z 1 , ..., z L.",25,458,25.444444444444443,5.43348623853211,192,2,456,0.0043859649122807,0.0258620689655172,0.9955,132,70,59,22,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 1, 'DAT': 4, 'MET': 11, 'EXP': 5, 'RES': 10, 'TNF': 3, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 5, 'PNF': 1, 'REC': 0, 'EMP': 12, 'SUB': 4, 'CLA': 1}",0,1,5,1,4,11,5,10,3,1,0,2,0,0,0,3,0,5,1,0,12,4,1,0.718155974183461,0.6743870933727,0.5053300621029824
ICLR2018-Hy_o3x-0b-R2,Reject,"The description of the proposed method is very unclear. From the paper it is very difficult to make out exactly what architecture is proposed. I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior? Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training. I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior? That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns? * The word layers is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder. E.g. The top stochastic layer z_L in FAME is a fully-connected dense layer. No, z_L is a vector of latent variables. Are you saying the encoder produces it using a fully-connected layer? * Section 2.2 starts talking about deterministic layers h. Are these part of the encoder or decoder? What is meant by  umber of layers connecting the stochastic latent variables? * Section 2.3: What is meant by reconstruction data? If my understanding of the method is correct, the novelty is limited. Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al. The reported likelihood results are very impressive though, and would be reason for acceptance if correct. However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood. There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound. Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities eps: this is not guaranteed to give the right result. Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y). Don't use log(var+eps) with var softplus(x), but instead use var softplus(x)+eps or parameterize the variance directly in the log domain).",22,350,17.5,4.970149253731344,184,3,347,0.0086455331412103,0.0282485875706214,0.8861,97,44,77,20,7,5,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 14, 'EXP': 9, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,0,1,0,2,14,9,2,1,0,0,1,0,0,0,1,0,1,2,1,13,0,0,0.504033856445473,0.5630806136974745,0.31654818314716804
ICLR2018-Hy_o3x-0b-R3,Reject,"Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples). Summary  This paper proposes VAE modifications that allow for the use multiple layers of latent variables. The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1],  (2) the latent variable parameters are functions of a CNN,  and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3]. Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported. Samples are shown for CIFAR 10, MNIST, and OMNIGLOT.  Evaluation  Pros:  The paper's primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off). This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result. I commend the authors for making their code available already via DropBox. Lastly, I like how the authors isolated the effect of the concatenation via the 'FAME No Concatenation' results. Cons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper.  In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments). I find the authors' claim that FAME is performing superior global modeling interesting.  Is there a way to support this experimentally? Also, I would have liked to see results w/o the CNN parametrization; how important was this choice? Conclusion  While the paper's conceptual novelty is low, the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable.  I recommend the paper's acceptance for this reason. [1]  C. Sonderby et al., ""Ladder Variational Autoencoders.""  NIPS 2016. [2]  A. van den Oord et al., ""Conditional Image Generation with PixelCNN Decoders."" ArXiv 2016. [3]  I. Gulrajani et al., ""PixelVAE: A Latent Variable Model for Natural Images.""  ICLR 2017. ",20,375,17.045454545454547,5.351744186046512,192,0,375,0.0,0.0076335877862595,0.9853,145,48,58,10,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 7, 'MET': 9, 'EXP': 5, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 2, 'EMP': 5, 'SUB': 1, 'CLA': 0}",0,1,2,0,7,9,5,5,0,0,0,2,1,1,0,2,0,2,0,2,5,1,0,0.6457687216548882,0.5582254559088007,0.4120040963050166
ICLR2018-HydnA1WCb-R1,Reject,"This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means). Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples. However, several important concepts in the paper are not well explained or motivated. For example, it is a bit misleading to use the word covariance throughout the paper, when the best model only employs a scalar estimate of the variance. A related, and potentially technical problem is in computing the prototype's mean and variance (section 3.3). Eq. 5 and 6 are not well motivated, and the claim of optimal under eq.6 is not explained. More importantly, eq. 5 and 6 do not use any covariance information (off-diagonal elements of S) --- as a result, the model is likely to ignore the covariance structure even when using full covariance estimate. The distance function (eq. 4) is d Mahalanobis distance, instead of linear Euclidean distance. While the paper emphasises the importance of the form of loss function, the loss function used in the model is given without explanation (and using cross-entropy over distances looks hacky). In addition, the experiments are too limited to support the claimed benefits from encoding uncertainty. Since the accuracies on omniglot data from recent models are already close to perfect, it is unclear whether the marginally improved number reported here is significant. In addition, more analysis may better support existing claims. For example, showing subsampled images indeed had higher uncertainty, rather than only the histogram for all data points. Pros: -Interesting problem and interesting direction.",13,280,16.470588235294116,5.531835205992509,157,3,277,0.0108303249097472,0.0320284697508896,0.8918,87,32,45,23,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 3, 'MET': 4, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 0}",0,1,2,2,3,4,1,2,0,2,1,1,0,0,0,0,1,1,1,0,8,3,0,0.7153286811444207,0.5601226106160992,0.4462307843294763
ICLR2018-HydnA1WCb-R2,Reject,"The paper extends the prototypical networks of Snell et al, NIPS 2017 for one shot learning. Snell et al use a soft kNN classification rule, typically used in standard metric learning work (e.g. NCA, MCML), over learned instance projections, i.e. distances are computed over the learned projections. Each class is represented by a class prototype which is given by the average of the projections of the class instances. Classification is done with soft k-NN on the class prototypes. The distance that is used is the Euclidean distance over the learned representations, i.e. (z-c)^T(z-c), where z is the projection of the x instance to be classified and c is a class prototype, computed as the average of the projections of the support instances of a given class. The present paper extends the above work to include the learning of a Mahalanobis matrix, S, for each instance, in addition to learning its projection. Thus now the classification is based on the Mahalanobis distance: (z-c)^T S_c (z-c). On a conceptual level since S_c should be a PSD matrix it can be written as the square of some matrix, i.e. S_c   A_c^TA_c, then the Mahanalobis distance becomes (A_c z - A_c c)^T ( A_c z-A_c c), i.e. in addition to learning a projection as it is done in Snell et al, the authors now learn also a linear transformation matrix which is a function of the support points (i.e. the ones which give rise to the class prototypes). The interesting part here is that the linear projection is a function of the support points. I wonder though if such a transformation could not be learned by the vanilla prototypical networks simply by learning now a projection matrix A_z as a function of the query point z. I am not sure I see any reason why the vanilla prototypical networks cannot learn to project x directly to A_z z and why one would need to do this indirectly through the use of the Mahalanobis distance as proposed in this paper. On a more technical level the properties of the learned Mahalanobis matrix, i.e. the fact that it should be PSD, are not really discussed neither how this can be enforced especially in the case where S is a full matrix (even though the authors state that this method was not further explored). If S is diagonal then the S generation methods a) b) c) in the end of section 3.1 will make sure that S is PSD, I do not think that this is the case with d) though. In the definition of the prototypes the component wise weigthing (eq. 5) works when the Mahalanobis matrix is diagonal (even though the weighting should be done by the sqrt of it), how would it work if it was a full matrix is not clear. On the experiments side the authors could have also experimented with miniImageNet and not only omniglot as is the standard practice in one shot learning papers. I am not sure I understand figure 3 in which the authors try to see what happens if instead of learning the Mahalanobis matrix one would learn a projection that would have as many additional dimensions as free elements in the Mahalanobis matrix. I would expect to see a comparison of the vanilla prototypical nets against their method for each one of the different scenarios of the free parameters of the S matrix, something like a ratio of accuracies of the two methods in order to establish whether learning the Mahalanobis matrix brings an improvement over the prototypical nets with an equal number of output parameters.    ",17,598,23.0,4.816363636363636,213,1,597,0.0016750418760469,0.0379537953795379,0.9735,189,51,89,33,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 1, 'MET': 8, 'EXP': 1, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 6}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,1,3,0,1,8,1,0,1,0,0,0,0,6,0,0,0,1,1,0,6,1,0,0.5019589609420461,0.4475542316186593,0.27813839163982934
ICLR2018-HydnA1WCb-R3,Reject,"SUMMARY: This work is about prototype networks for image classification. The idea is to jointly embed an image and a confidence measure into a latent space, and to use these embeddings to define prototypes together with confidence estimates. A Gaussian model is used for representing these confidences as covariance matrices. Within a class, the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix S-C, and this S_C defines the tensor in the Mahalanobis metric for measuring the distances to the prototype. EVALUATION: CLARITY: I found the paper difficult to read. In principle, the idea seems to be clear, but then the description and motivation of the model remains very vague. For instance, what is the the precise meaning of an image-specific covariance matrix (supported by just one point)?  What is the motivation to just average the inverse covariance matrices to compute S_C?  Why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding space?  NOVELTY: Honestly, I had difficulties to see which parts of this work could be sufficiently novel.  The idea of using a Gaussian model and its associated Mahalanobis metric is certainly interesting,  but also a time-honored concept.  The experiments focus very specifically on the omniglot dataset, and it is not entirely clear to me what  should be concluded from the results presented.  Are you sure that there is any significant improvement over the models in (Snell et al, Mishra et al, Munkhandalai & Yu, Finn et al.)?       ",15,251,25.1,5.222222222222222,142,1,250,0.004,0.0337078651685393,0.9829,76,26,37,16,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 5, 'DAT': 1, 'MET': 8, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 3}",0,1,1,5,1,8,1,1,0,0,0,2,0,0,0,1,0,1,0,0,7,0,3,0.5734625344805376,0.4483384591952104,0.31936134370792607
ICLR2018-HyfHgI6aW-R1,Accept,"The paper presents a method for navigating in an unknown and partially observed environment is presented. The proposed approach splits planning into two levels: 1) local planning based on the observed space and 2) a global planner which receives the local plan, observation features, and access to an addressable memory to decide on which action to select and what to write into memory. The contribution of this work is the use of value iteration networks (VINs) for local planning on a locally observed map that is fed into a learned global controller that references history and a differential neural computer (DNC), local policy, and observation features select an action and update the memory. The core concept of learned local planner providing additional cues for a global, memory-based planner is a clever idea and the thorough analysis clearly demonstrates the benefit of the approach. The proposed method is tested against three problems: a gridworld, a graph search, and a robot environment. In each case the proposed method is more performant than the baseline methods. The ablation study of using LSTM instead of the DNC and the direct comparison of CNN + LSTM support the authors' hypothesis about the benefits of the two components of their method. While the author's compare to DRL methods with limited horizon (length 4), there is no comparison to memory-based RL techniques. Furthermore, a comparison of related memory-based visual navigation techniques on domains for which they are applicable should be considered as such an analysis would illuminate the relative performance over the overlapping portions problem domains  For example, analysis of the metric map approaches on the grid world or of MACN on their tested environments. Prior work in visual navigation in partially observed and unknown environments have used addressable memory (e.g., Oh et al.) and used VINs (e.g., Gupta et al.) to plan as noted. In discussing these methods, the authors state that these works are not comparable as they operate strictly on discretized 2d spaces. However, it appears to the reviewer that several of these methods can be adapted to higher dimensions and be applicable at least a subclass (for the euclidean/metric map approaches) or the full class of the problems (for Oh et al.), which appears to be capable to solve non-euclidean tasks like the graph search problem. If this assessment is correct, the authors should differentiate between these approaches more thoroughly and consider empirical comparisons. The authors should further consider contrasting their approach with ""Neural SLAM"" by Zhang et al. A limitation of the presented method is requirement that the observation ""reveals the labeling of nearby states. "" This assumption holds in each of the examples presented: the neighborhood map in the gridworld and graph examples and the lidar sensor in the robot navigation example. It would be informative for the authors to highlight this limitation and/or identify how to adapt the proposed method under weaker assumptions such as a sensor that doesn't provide direct metric or connectivity information such as a RGB camera.  Many details of the paper are missing and should be included to clarify the approach and ensure reproducible results.  The reviewer suggests providing both more details in the main section of the paper and providing the precise architecture including hyperparameters in the supplementary materials section.  ",19,543,27.15,5.374045801526718,246,3,540,0.0055555555555555,0.0163636363636363,0.0688,159,75,80,12,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 3, 'DAT': 0, 'MET': 13, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 4, 'CLA': 0}",0,0,2,3,0,13,0,1,0,2,0,1,0,0,0,0,0,3,0,0,9,4,0,0.4317975409435562,0.3388633474514215,0.2193034822554215
ICLR2018-HyfHgI6aW-R2,Accept,"The paper addresses the important problem of planning in partially observable environments with sparse rewards, and the empirical verification over several domains is convincing. My main concern is that the structure of these domains is very similar - essentially, a graph where only neighboring vertices are directly observable, and because of this, the proposed architecture might not be applicable to planning in general POMDPs (or, in their continuous counterparts, state-space models). The authors claim that what is remembered by the planner does not take the form of a map, but isn't the map estimate hat{m} introduced at the end of Section 2.1 precisely such a map? From Section 2.4, it appears that these map estimates are essential in computing the low-level policies from which the final, high-level policy is computed. If the ability to maintain and use such local maps is essential for this method, its applicability is likely restricted to this specific geometric structure of domains and their observability. Some additional comments:  P. 2, Section 2.1: does H(s) contain 0s for non-observable and 1s for observable states? If yes, please state it.  P. 3: the concatenation of state and observation histories is missing from the definition of the transition function. P. 3, Eq. 1: overloaded notation - if T is the transition function for the large MDP on histories, it should not be used for the transition function between states. Maybe the authors meant to use f() for that transition? P. 3, Eq. 3: the sum is over i, but it is not clear what i indexes. P.3, end of Section 2.1: when computing the map estimate hat{m}, shouldn't the operator be min, that is, a state is assumed to be open (0), unless one or more observations show that it is blocked (-1)? P.5: the description of the reward function is inconsistent - is it 0 at the goal state, or >0? P. 11, above Fig. 9: typo, we observe that the in the robot world    ",13,325,19.11764705882353,5.071917808219178,157,4,321,0.0124610591900311,0.0269461077844311,0.8065,90,35,53,13,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 0, 'TNF': 1, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 0, 'CLA': 2}",0,0,0,2,0,10,0,0,1,0,1,0,0,0,0,0,1,0,0,0,6,0,2,0.2880253146002034,0.3365242555784022,0.14440567438540927
ICLR2018-HyfHgI6aW-R3,Accept,"Summary:  A method is proposed for robot navigation in partially observable scenarios. E.g. 2D navigation in a grid world from start to goal but the robot can only sense obstacles in a certain radius around it. A learning-based method is proposed here which takes the currently discovered partial map as input to convolutional layers and then passes through K-iterations of a VIN module to a final controller.  The controller takes as input both the convolutional features, the VIN module and has access to a differential memory module.[PDI-NEU,[MET-NEU], [null], [SMY], [GEN]] A linear layer takes inputs from both the controller and memory and predicts the next step of the robot. This architecture is termed as MACN. In experiments on 2D randomly generated grid worlds, general graphs and a simulated ground robot with a lidar, it is shown that memory is important for navigating partially observable environments and that the VIN module is important to the architecture since a CNN replacement doesn't perform as well. Also larger start-goal distances can be better handled by increasing the memory available. Comments:  - My main concern is that there are no non-learning based obvious baselines like A*, D*, D*-Lite and related motion planners which have been used for this exact task very successfully and run on real-world robots like the Mars rover. In comparison to the size of problems that can be handled by such planners the experiments shown here are much smaller and crucially the network can output actions which collide with obstacles while the search-based planners by definition will always produce feasible paths and require no training data. I would like to see in the experimental tables, comparison to path lengths produced by MACN vs. those produced by D*-Lite or Multi-Heuristic A*. While it is true that motion-planning will keep the entire discovered map in memory for the problem sizes shown here (2D maps: 16x16, 32x32, 64x64 bitmaps, general graphs: 9, 16, 25, 36 nodes) that is on the order of a few kB memory only. For the 3D simulated robot which is actually still treated as a 2D task due to the line lidar scanner MxN bitmap is not specified but even a few Mb is easily handled by modern day embedded systems. I can see that perhaps when map sizes exceed say tens of Gbs then perhaps MACN's memory will be smaller to obtain similar performance since it may learn better map compression to better utilize the smaller budget available to it. But experiments at that scale have not been shown currently. - Figure 1: There is no sensor (lidar or camera or kinect or radar) which can produce the kind of sensor observations shown in 1(b) since they can't look beyond occlusions. So such observations are pretty unrealistic. - The parts of the map that lie within the range of the laser scanner are converted to obstacle-free ...: How are occluded regions marked?",18,479,25.21052631578948,5.042222222222223,237,4,475,0.0084210526315789,0.0123456790123456,0.9648,145,54,84,33,7,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 4, 'DAT': 0, 'MET': 9, 'EXP': 7, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,1,4,0,9,7,1,2,0,0,0,0,1,0,0,0,2,0,0,10,0,0,0.5027199092068033,0.2279370016929989,0.22943608572566687
ICLR2018-Hyg0vbWC--R1,Accept,"This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page. The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention.  In general, the paper is well-written and the main ideas are clear. However, my main concern is the evaluation. It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches. Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact. Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper. More importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content. For example, Figure 5-7 show variable sizes of the generated outputs. With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference. It would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures. Did you run any statistical significance test on the evaluation results? Authors claim that the proposed model can generate fluent, coherent output, however, no evaluation has been conducted to justify this claim. The human evaluation only compares two alternative models for preference, which is not enough to support this claim. I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization. Does Figure 8 show an example input after the extractive stage or before? Please clarify.  --------------- I have updated my scores as authors clarified most of my concerns.",17,399,24.9375,5.689119170984456,210,0,399,0.0,0.0222772277227722,0.9814,121,52,67,17,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 12, 'EXP': 2, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 3, 'CLA': 1}",0,1,3,2,1,12,2,2,2,0,0,2,0,0,0,0,0,4,0,1,6,3,1,0.6460401462285732,0.5592301834179833,0.406593779969113
ICLR2018-Hyg0vbWC--R2,Accept,"This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles). First relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking. Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017). A mixture of experts layer further improves performance. The proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network. On its own this modification improves perplexity (on longer sequences) but not the Rouge score; however the architecture enables memory-compressed attention which is more scalable to long input sequences. It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given. Computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of training. In the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the ICLR community. It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available. The training data is significantly larger than the CNN/DailyMail single-document summarization dataset. The paper presents strong quantitative results and qualitative examples. Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage. In some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length. While the proposed approach is more scalable, it is hard to judge the extend of this. So while the performance of the overall system is impressive, it is hard to judge the significance of the technical contribution made by the paper. --- The additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted.  ",18,363,20.166666666666668,5.643258426966292,189,2,361,0.0055401662049861,0.0136612021857923,0.9752,99,51,57,19,8,3,"{'ABS': 0, 'INT': 3, 'RWK': 2, 'PDI': 0, 'DAT': 2, 'MET': 9, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 0, 'CLA': 0}",0,3,2,0,2,9,2,4,0,0,1,2,0,0,1,0,1,0,0,0,13,0,0,0.5738964451176447,0.340796822551449,0.2928277614262337
ICLR2018-Hyg0vbWC--R3,Accept,"The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated. Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer. A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences. The outputs are evaluated by ROUGE-L and test perplexity. There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences. This paper is quite original and clearly written. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. The main weakness is that I would have liked to see more analysis and comparisons in the evaluation. Evaluation: Currently, only neural abstractive methods are compared. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. Do redundancy cues which work for multi-document news summarization still work for this task? Extractiveness analysis: I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. Does higher extractiveness correspond to higher or lower system ROUGE scores?  This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help. A further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not. I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection. In this case, the problem could become less interesting, as no real analysis is required to do well here. Overall, I quite like this line of work, but I think the paper would be a lot stronger and more convincing with some additional work. ---- After reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper. This is a very nice contribution. ",22,451,22.55,5.244239631336406,208,1,450,0.0022222222222222,0.0110132158590308,0.9824,121,61,81,28,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 3, 'DAT': 1, 'MET': 12, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 4, 'FWK': 0, 'OAL': 5, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 7, 'CLA': 1}",0,1,0,3,1,12,1,1,0,4,0,5,1,0,0,1,1,2,0,0,7,7,1,0.6459836892065367,0.6711556328828433,0.45985750583994406
ICLR2018-HyiAuyb0b-R1,Accept,"This paper includes several controlled empirical studies comparing MC and TD methods in predicting of value function with complex DNN function approximators. Such comparison has been carried out both in theory and practice for simple low dimensional environments with linear (and RKHS) value function approximation showing how TD methods can have much better sample complexity and overall performance compared to pure MC methods. This paper shows some results to the contrary when applying RL to complex perceptual observation space. The main results include: (1) In a rollout update a mix of MC and TD update (i.e. a rollout of > 1 and < horizon) outperforms either extreme.  This is inline with TD-lambda analysis in previous work. (2) Pure MC methods can outperform TD methods when the rewards becomes noisy. (3) TD methods can outperform pure MC methods when the return is mostly dominated by the reward in the terminal state. (4) MC methods tend to degrade less when the reward signal is delayed. (5) Somewhat surprising: MC methods seems to be on-par with TD methods when the reward is sparse and even longer than the rollout horizon. (6) MC methods can outperform TD methods with more complex and high dimensional perceptual inputs. The authors conjecture that several of the above observations can be explained by the fact that the training target in MC methods is ground truth and do not rely on bootstrapping from the current estimates as is done in a TD rollout. They suggest that training on such signal can be beneficial when training deep models on complex perceptual input spaces. The contributions of the paper are in parts surprising and overall interesting. I believe there are far more caveats in this analysis than what is suggested in the paper and the authors should avoid over-generalizing the results based on a few domains and the analysis of a small set of algorithms. Nonetheless I find the results interesting to the RL community and a starting point to further analysis of the MC methods (or adaptations of TD methods) that work better with image observation spaces. Publishing the code, as the authors mentioned, would certainly help with that. Notes: - I find the description of the Q_MC method presented in the paper very confusing and had to consult the reference to understand the details. Adding a couple of equations on this would improve the readability of the paper. - The first mention of partial observability can be moved to the introduction. - Adding results for m 3 to table 2 would bring further insight to the comparison. - The results for the perceptual complexity experiment seem contradictory and inconclusive. One would expect Q_MC to work well in Grid Map domain if the conjecture put forth by the authors was to hold universally. - In the study on reward sparsity, although a prediction horizon of 32 is less than the average steps needed to get to a rewarding state, a blind random walk might be enough to take the RL agent to a close-enough neighbourhood from which a greedy MC-based policy has a direct path to the goal. What is missing from this picture is when a blind walk cannot reach such a state, e.g. when a narrow corridor is present in the environment. Such a case cannot be resolved by a short horizon MC method. In other words, a sparse reward setting is only difficult if getting into a good neighbourhood requires long term planning and cannot be resolved by a (pseudo) blind random walk. - The extrapolation of the value function approximator can also contribute to why the limited horizon MC method can see beyond its horizon in a sparse reward setting. That is, even if there is no way to reach a reward state in 32 steps, an MC value function approximation with horizon 32 can extrapolate from similar looking observed states that have a short path to a rewarding state, enough to be better than a blind random walk. It would have been nice to experiment with increasing model complexity to study such effect. ",29,670,20.9375,5.062200956937799,282,2,668,0.0029940119760479,0.013235294117647,0.9931,198,89,107,22,8,5,"{'ABS': 0, 'INT': 3, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 20, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 7, 'PNF': 2, 'REC': 0, 'EMP': 15, 'SUB': 3, 'CLA': 1}",0,3,2,2,0,20,2,5,0,2,0,1,0,0,0,0,0,7,2,0,15,3,1,0.5767067699565134,0.5652408569269434,0.3684627063202003
ICLR2018-HyiAuyb0b-R2,Accept,"This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length. Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity. The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings. There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations. They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom. Overall, I find the paper important for furthering the understanding of fundamental RL algorithms. However, my main concern is regarding a confounding factor that may have influenced the results: Q_MC uses a multi-headed model, trained on different horizon lengths, whereas the other models seem to have a single prediction head. May this helped Q_MC have better perceptual capabilities? A couple of other questions: - I couldn't find any mention of eligibility traces - why?",9,238,29.75,5.713004484304933,150,3,235,0.0127659574468085,0.0165975103734439,0.9829,71,39,34,13,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 0}",0,1,1,1,0,4,2,3,0,0,1,0,0,0,0,0,0,1,0,0,6,1,0,0.5009481759184815,0.3364431205075482,0.253749678811593
ICLR2018-HyiAuyb0b-R3,Accept,"The authors present a testing framework for deep RL methods in which difficulty can be controlled along a number of dimensions, including: reward delay, reward sparsity, episode length with terminating rewards, binary vs real rewards and perceptual complexity. The authors then experiment with a variety of TD and MC based deep learners to explore which methods are most robust to increases in difficulty along these dimensions. The key finding is that MC appears to be more robust than TD in a number of ways, and in particular the authors link this to domains with greater perceptual challenges. This is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition. The authors discover some interesting characteristics of MC based Deep-RL which may influence future work in this area, and dig down a little to uncover the principles a little. The testing framework will be made public too, which adds to the value of this paper. I recommend the paper for acceptance and expect it will garner interest from the community. Detailed comments   u2022 [p4, basic health gathering task] The goal is to survive and maintain as much health as possible by collecting health kits... The reward is +1 when the agent collects a health kit and 0 otherwise.  The reward suggests that the goal is to collect as many health kits as possible, for which surviving and maintaining health are secondary. u2022 [p4, Delayed rewards] It might be interesting to have a delay sampled from a distribution with some known mean. Otherwise, the structure of the environment might support learning even when the reward delay would otherwise not. u2022 [p4, Sparse rewards] I am not sure it is fair to say that the general difficulty is kept fixed. Rather, the average achievable reward for an oracle (that knows whether health packs are) is fixed. u2022 [p6] Dosovitskiy & Koltun (2017) have not tested DFP on Atari games.  Probably fairer/safer to say: did not report results on Atari games. ",16,342,20.11764705882353,5.080745341614906,173,6,336,0.0178571428571428,0.0459770114942528,0.996,96,44,69,20,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,2,1,0,8,2,4,0,0,1,2,0,0,0,0,1,2,0,1,9,0,0,0.5735591244053356,0.4495372664803782,0.31589860133288084
ICLR2018-HyiS6k-CW-R1,Reject,"OVERVIEW: The authors present results from several state-of-the-art generative models trained on a facial dataset for learning a general facial identity space. STRENGTHS: The paper in general is well written and easy to ready. I appreciate the idea of the Turing test and qualitative results presented are quite impressive. Also, the use of  diverse state-of-the-art generative models is also a strong point. WEAKNESSES: While the strengths mentioned above are obvious I had the impression through the whole paper that a whole part is missing. My list of concerns are the following:       The authors state as their first contribution the presentation of a novel dataset. This is nice but I see the data is actually already available as the photographic work of an artist. So what's the authors' contribution? As I understand from the paper it is just compiling this already available data. The major problem of this paper in my opinion is the total lack of technical details. In this sense the results cannot by any means by reproduced. While the authors use a set of very novel generative models there is absolutely no detail on how do they train them. We are just shown some very impressive qualitative results which are indeed admirable but without further details I cannot judge them as true or not. I strongly recommend to the authors, to provide technical details of topologies used, hyper parameters and any other important detail that would help a third party research to reproduce these results. Also it is really hard to understand how could they obtain such impressive result by doing an unsupervised training on a dataset containing 3353 samples taking into account the high capacity of the models they are using. In section 2.1 the authors mention that facial landmarks have been detected using a 'pre-trained ensemble-of-regresion-trees detector (Gerbrands, 1981)'. I know very well the facial alignment literature and I do not understand this reference. This I do not think is a reference to a facial alignment method bu t rather a set of general purpose linear algebra methods. Taking into account this major weaknesses I cannot accept this paper and I do not think it is worth discussing results and applications in Sections 3,4,5 before authors detail, explain and clarify how exactly they have obtained these results. ",19,381,20.05263157894737,5.286118980169972,186,2,379,0.0052770448548812,0.0154241645244215,0.9752,93,58,70,27,7,7,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 7, 'MET': 6, 'EXP': 6, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 6, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 1, 'EMP': 3, 'SUB': 5, 'CLA': 1}",0,0,2,1,7,6,6,5,0,0,0,3,0,0,0,6,0,3,1,1,3,5,1,0.5022926170561506,0.7799082617177202,0.396893089942496
ICLR2018-HyiS6k-CW-R2,Reject,"This paper proposes a new space for reasoning about human identity. It proposes a new dataset based on an artist's work, and compares existing methods in terms of the realism of the synthetic faces they can create. Pros: + The results are very pleasing visually. + The authors show that one of the existing methods can fairly successfully fool humans to believe its synthetic results are actual human faces. Cons: - There is no new methodology proposed. - If the main contribution is the dataset, perhaps the claim that it is uniquely diverse could be justified with some quantitative arguments / statistics, comparing to other datasets. - Since there are existing methods to generate images from a textual description (e.g. Zhang ICCV 2017, StackGAN), Fig. 10 merits a comparison to those. - It would have been convincing to see an experiment showing actual use of the proposed method for navigating the face space, e.g. for finding criminals based on a description. Questions: - Inventing plausible fine details while preserving identity -- since identity is created and there is no ground truth, where does the line between fine detail and  ew identity lie? - Some notation is not defined in the equation on the last page.",10,195,16.25,5.148936170212766,123,2,193,0.0103626943005181,0.0097087378640776,0.7211,53,22,44,7,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 4, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 2, 'CLA': 0}",0,1,3,0,2,4,2,1,1,0,0,0,0,0,0,1,0,2,1,0,2,2,0,0.5010338386777241,0.55640135205591,0.31841840446470115
ICLR2018-HyiS6k-CW-R3,Reject,"This paper investigates identity space learning with well-controlled variations using an artistic portraits dataset. Especially, the authors propose a visual Turing test to evaluate the synthesize quality of three generative models: WGAN-GP, DFC-VAE, and Pixel VAE. The submission has following PROS:  + The proposed visual Turing test provides a novel solution to evaluate the generation quality. The test not only distinguishes real from synthesized faces but also evaluates the observer ability by determining whether the observer is a human. This is a merit compared with existing protocols used in generation evaluation. + The generated face images are very impressive, especially the improved 512x512-pixel outputs. + The paper presents a promising application in police composite sketching, which can significantly improve human-in-the-loop search in face modeling. However, the submission also suffers from multiple CONS:  - The novelty of this paper is limited. The only novelty I can pinpoint is the proposed visual Turing test. The dataset, as well as all investigated models/approaches, are existing work. The visual Turing test is interesting but not concrete enough to support an ICLR publication. - A very small dataset (3,300 subjects and 3,353 images) is used in the whole investigation. It is doubtful that the conclusion or results obtained in this small dataset could be scaled up to real-world applications or datasets (millions of subjects and images). It would be favorable to empirically prove this by designing additional experiments. - Missing details. (a)In section 4, how to use formal method (Ledig et al., 2016) to enlarge the portrait from 64x64 to 512x512 is unclear. (b) Lacking details of the model setups and training strategies. The generation models are usually highly sensitive to details settings. The readers can hardly reproduce the results or evaluate possible performance by reading the paper. (c) If the paper length is limited, a supplementary material about those details would be preferred. - Typos. (a) Page 7, Figure 8 shows the seeds and example images for 10 rounds... ----> Figure 8 should be Figure 10 (b) Page 4, yet is is unclear how many pixels are required... ----> yet is is",22,339,14.125,5.536507936507936,183,1,338,0.0029585798816568,0.0228571428571428,0.9718,98,45,68,20,11,9,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 3, 'MET': 7, 'EXP': 2, 'RES': 5, 'TNF': 2, 'ANA': 3, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 4, 'IMP': 1, 'CMP': 1, 'PNF': 2, 'REC': 1, 'EMP': 4, 'SUB': 7, 'CLA': 2}",0,2,2,2,3,7,2,5,2,3,1,1,0,0,1,4,1,1,2,1,4,7,2,0.7878346666548215,1.0,0.7721779813162072
ICLR2018-Hyig0zb0Z-R1,Reject,"This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. It is fair to say that this paper contains almost no novelty. This paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. Prior work along this line includes [3, 4, 5, 6, 7]. Using MFSC, or more commonly known as log mel filter bank outputs, has been pretty common since [8]. Having a separate subsection (2.1) discussing this seems unnecessary. Arguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. It is unclear why discriminative training, such as MMI, sMBR, and lattice-free MMI, is mentioned in section 2.3. Discriminative training is not invented to overcome the lack of manual segmentations, and is equally applicable to the case where we have manual segmentations. The authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters. In addition, ASG requires additional forward-backward computation for computing the partition function (second term in eq 3). There is no reason to believe that ASG can be faster than CTC in both training and decoding. The connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function.  Otherwise, the objective won't be a proper probability distribution. The citation style in section 2.4 seems off. Also see [4] for a great description of how beam search is done in CTC. Details about training, such as the optimizer, step size, and batch size, are missing. Does no batching (in section 3.2) means a batch size of one utterance? In the last paragraph of section 3.2, why is there a huge difference in real-time factors between the clean and other set? Something is wrong unless the authors are using different beam widths in the two settings. The paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet. It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet. Similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj. None of the comparison in table 4 is really meaningful, because none of the other systems have parameters as many as 19 layers of convolution. Why does CTC fail when trained without the blanks? Is there a way to fix it besides using ASG? It is also unclear why speaker-adaptive training is not needed. At which layer do the features become speaker invariant? Can the system improve further if speaker-adaptive features are used instead of log mels?  This paper would be much stronger if the authors can include these experiments and analyses. [1] R Collobert, C Puhrsch, G Synnaeve, Wav2letter: an end-to-end convnet-based speech recognition system, 2016  [2] Y Dauphin, A Fan, M Auli, D Grangier, Language modeling with gated convolutional nets, 2017  [3] A Graves and N Jaitly, Towards End-to-End Speech Recognition with Recurrent Neural Networks, 2014 [4] A Maas, Z Xie, D Jurafsky, A Ng, Lexicon-Free Conversational Speech Recognition with Neural Networks, 2015 [5] Y Miao, M Gowayyed, F Metze, EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding, 2015  [6] D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, End-to-end attention-based large vocabulary speech recognition, 2016 [7] W Chan, N Jaitly, Q Le, O Vinyals, Listen, attend and spell, 2015 [8] A Graves, A Mohamed, G Hinton, Speech recognition with deep recurrent neural networks, 2013 [9] H Tang, L Lu, L Kong, K Gimpel, K Livescu, C Dyer, N Smith, S Renals, End-to-End Neural Segmental Models for Speech Recognition, 2017",38,680,25.185185185185187,5.293333333333333,309,2,678,0.0029498525073746,0.0145985401459854,0.9761,253,97,97,27,10,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 17, 'EXP': 8, 'RES': 2, 'TNF': 1, 'ANA': 3, 'FWK': 0, 'OAL': 2, 'BIB': 6, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 1, 'REC': 0, 'EMP': 13, 'SUB': 4, 'CLA': 0}",0,1,1,1,0,17,8,2,1,3,0,2,6,0,0,1,0,3,1,0,13,4,0,0.7191046831344086,0.5635733994130157,0.4580515818266547
ICLR2018-Hyig0zb0Z-R2,Reject,"The paper describes some interesting work but for a combination of reasons I think it's more like a workshop-track paper. There is not much that's technically new in the paper-- at least not much that's really understandable. There is some text about a variant of CTC, but it does not explain very clearly what was done or what the motivation was. There are also quite a few misspellings.  Since the system is presented without any comparisons to alternatives for any of the individual components, it doesn't really shed any light on the significance of the various modeling decisions that were made. That limits the value. If rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.",8,120,17.142857142857142,4.947826086956522,82,2,118,0.0169491525423728,0.0413223140495867,0.6934,22,12,24,13,4,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 1, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,0,1,0,0,2,1,0,0,0,0,6,0,0,2,1,2,1,0,0,1,0,1,0.2861915718683231,0.6667243692260827,0.19677549773081726
ICLR2018-Hyig0zb0Z-R3,Reject,"The paper is interesting, but needs more work, and should provide clear and fair comparisons. Per se, the model is incrementally new, but it is not clear what the strengths are, and the presentations needs to be done more carefully. In detail: - please fix several typos throughout the manuscript, and have a native speaker (and preferably an ASR expert) proofread the paper Introduction - please define HMM/GMM model (and other abbreviations that will be introduced later), it cannot be assumed that the reader is familiar with all of them (ASG is used before it is defined, ...) - The standard units that most ASR systems use can be called senones, and they are context dependent sub-phonetic units (see http://ssli.ee.washington.edu/~mhwang/), not phonetic states. Also the units that generate the alignment and the units that are trained on an alignment can be different (I can use a system with 10000 states to write alignments for a system with 3000 states) - this needs to be corrected. - When introducing CNNs, please also cite Waibel and TDNNs - they are *the same* as 1-d CNNs, and predate them. They have been extended to 2-d later on (Spatio-temporal TDNNs) - The most influential deep learning paper here might be Seide, Li, Yu Interspeech 2011 on CD-DNN-HMMs, rather than overview articles - Many papers get rid of the HMM pipeline, I would add https://arxiv.org/abs/1408.2873, which predates Deep Speech - What is a sequence-level variant of CTC? CTC is a sequence training criterion - The reason that Deep Speech 2 is better on noisy test sets is not only the fact they trained on more data, but they also trained on  oisy (matched) data - how is this an end-to-end approach if you are using an n-gram language model for decoding? Architecture - MFSC are log Filterbanks ... - 1D CNNs would be TDNNs - Figure 2: can you plot the various transition types (normalized, un-normalized, ...) in the plots? not sure if it would help, but it might - Maybe provide a reference for HMM/GMM and EM (forward backward training) - MMI was also widely used in HMM/GMM systems, not just NN systems - the blank states do *not* model garbage frames, if one wants to interpret them, they might be said to model  on-stationary frames between CTC peaks, but these are different from silence, garbage, noise, ... - what is the relationship of the presented ASG criterion to MMI? the form of equation (3) looks like an MMI criterion to me? Experiments - Many of the previous comments still hold, please proofread - you say there is no complexity incrase when using logadd - how do you measure this? number of operations? is there an implementation of logadd that is (absolutely) as fast as add? - There is discussion as to what i-vectors model (speaker or environment information) - I would leave out this discussion entirely here, it is enough to mention that other systems use adaptation, and maybe re-run an unadapted baselien for comparsion - There are techniques for incremental adaptation and a constrained MLLR (feature adaptation) approaches that are very eficient, if one wnats to get into this - it may also be interesting to discuss the role of the language model to see which factors influence system performance - some of the other papers might use data augmentation, which would increase noise robustness (did not check, but this might explain some of the results in table 4) - I am confused by the references in the caption of Table 3 - surely the Waibel reference is meant to be for TDNNs (and should appear earlier in the paper), while p-norm came later (Povey used it first for ASR, I think) and is related to Maxout - can you also compare the training times? Conculsion - can you show how your approach is not so computationally expensive as RNN based approaches? either in terms of FLOPS or measured times ",34,624,78.0,4.877685950413223,295,9,615,0.0146341463414634,0.0332829046898638,0.9877,161,62,123,49,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 21, 'EXP': 3, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 1}",0,1,2,0,0,21,3,1,2,0,0,4,1,1,0,1,0,4,2,0,14,1,1,0.6482734190034958,0.6751651699149986,0.4531371393279875
ICLR2018-HylgYB3pZ-R1,Reject,"This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector). The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero. This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient. The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero. Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value. Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task. Test accuracy is not improved, however. Overall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments. On the theoretical side, the linearly constrained weights are only shown to work for a very special case. There can be many other approaches to mitigate the impact of angle bias. For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments? When the mean of input is zero, there is no angle bias in the first layer. Also, what about if we include the bias term so that b + w a is the preactivation value? On the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments. It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines. It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is. Minor comments:  In Section 2.2, is Layer 1 the input layer or the next?",17,404,26.933333333333334,4.82051282051282,172,2,402,0.0049751243781094,0.0147783251231527,0.7574,112,43,69,20,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 11, 'EXP': 8, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,1,1,1,3,11,8,4,0,0,0,0,0,0,0,0,0,1,0,0,9,0,0,0.5033445788377227,0.227197881700966,0.2289937108249944
ICLR2018-HylgYB3pZ-R2,Reject,"The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article). The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction. Authors chose to add an additional optimization constraint LCW (|w| 0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x| 0 and unit std to do the same. Clearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU. When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input. While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features. Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN.",7,177,25.285714285714285,4.957831325301205,100,3,174,0.0172413793103448,0.0224719101123595,0.0,54,16,32,8,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,7,1,2,0,0,0,0,0,0,0,0,0,1,0,0,5,0,0,0.3586997117182746,0.2247100519615941,0.16153530453469975
ICLR2018-HylgYB3pZ-R3,Reject,"Pros: The paper is easy to read. Logic flows naturally within the paper.   Cons:  1. Experimental results are neither enough nor convincing. Only one set of data is used throughout the paper: the Cifar10 dataset, and the architecture used is only a 100 layered MLP. Even though LCW performs better than others in this circumstance, it does not prove its effectiveness in general or its elimination of the gradient vanishing problem. For the 100 layer MLP, it's very hard to train a simple MLP and the training/testing accuracy is very low for all the methods. More experiments with different number of layers and different architecture like ResNet should be tried to show better results. In Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem. The proposed concept is only analyzed in MLP with Sigmoid activation function. In the experimental parts, the authors claim they use both ReLU and Sigmoid function, but no comparisons are reflected in the figures. 2. The whole standpoint of the paper is quite vague and not very convincing. In section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers. However, the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connection. The whole analysis of the connection is built solely on this one sentence At the same time, the output does not change if we adjust the weight vectors in Layer 1, which is nowhere verified. Further, the phenomenon is only tested on random initialization. When the network is trained for several iterations and becomes more settled, it is not clear how angle affect affects gradient vanishing problem. Minors: 1. Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems. 2. 'patters' -> 'patterns'   3. In section 2.3, reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs. Only reason 1 mentions the gradient vanishing problem, even though the title of this section is Relation to Vanishing Gradient Problem.  ",20,360,14.4,5.149122807017544,183,2,358,0.0055865921787709,0.0190217391304347,-0.965,104,42,58,24,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 7, 'EXP': 4, 'RES': 3, 'TNF': 3, 'ANA': 3, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 2}",0,1,0,0,1,7,4,3,3,3,0,3,0,0,0,0,0,0,4,0,10,2,2,0.5734865875705714,0.4504145797086167,0.32477329656563664
ICLR2018-HymYLebCb-R1,Reject,"The paper proposes to use 2-d image representation techniques as a means of learning representations of graphs via their adjacency matrices. The adjacency matrix (or a subgraph of it) is first re-ordered to produce some canonical ordering which can then be fed into an image representation method. This can then be fed into a classifier. This is a little too unprincipled for my taste. In particular the paper uses a Caffe reference model on top of the adjacency matrix, rather than learning a method specifically for graphs. Perhaps this is due to a lack of available graph training data, but it doesn't seem to make a lot of sense. Maybe I missed or overlooked some detail, but I didn't spot exactly what the classification task was. I think the goal is to identify which of the graphs a subgraph belongs to? I'm not sure how relevant this graph classification task is. The method does prove that the Caffe reference model maintains some information that can be used for classification, but this doesn't really suggest a generalizable method that we could confidently use for a variety of tasks. It's surprising that it works at all, but ultimately doesn't reveal a big scientific finding that could be re-used.",11,206,20.6,5.073684210526316,114,3,203,0.0147783251231527,0.0436893203883495,0.2862,49,22,40,17,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,1,0,1,0,7,0,0,0,0,1,3,0,0,2,0,1,0,0,0,5,0,0,0.3587348474416663,0.3358308289836734,0.1767480085520117
ICLR2018-HymYLebCb-R2,Reject,"The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems. The image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix. The experimental results supports the claim. It is very positive that the figures are very helpful for delivering the information. The work seems to be a little bit incremental. The proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix. A novelty of this work seems to be transforming a graph into an image. By the proposed representation, the authors are able to apply image classification methods (supervised or unsupervised) to subgraph classification. It will be better if the authors could provide more details in the methodology or framework section. The experiments on 9 networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based methods. It seem to be promising when using transfer learning. The last two process figures in 1.1 can be improved. No caption or figure number is provided. It will be better to make the notations easy to understand and avoid any notation in a sentence without explanation nearby. For example: the test example is correctly classified if and only if its ground truth matches C.(P5) We carry out this exercise 4 times and set n to 8, 16, 32 and 64 respectively. (P6)  Some minor issues: Zhu et al.(2011) discuss heterogeneous transfer learning where in they use...(P3) Each label vector (a tuple of label, label-probability pairs). (incomplete sentence?P5)",17,263,15.470588235294118,5.518367346938776,144,2,261,0.0076628352490421,0.0151515151515151,0.9785,87,19,49,9,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 6, 'EXP': 2, 'RES': 0, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 6, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,1,1,1,0,6,2,0,4,0,0,5,0,0,0,1,0,0,6,0,4,2,0,0.5015887254749541,0.4467248378763115,0.2841078258716821
ICLR2018-HymYLebCb-R3,Reject,"This paper views graph classification as image classification, and shows that the CNN model adapted from image net can be effectively adapted to the graph classification. The idea is interesting and the result looks promising, but I do not understand the intuition behind the success of analogizing graph with images. Fundamentally, a convolutional filter stands for a operation within a small neighborhood on the image. However, it is unclear how it means for the graph representation. Is the neighborhood predefined? Are the graph nodes pre-ordered?  I am also curious with the effect of pre-trained model from ImageNet. Since the graph presentation does not use color channels,  pre-trained model is used different from what it was designed to. I would imagine the benefit of using ImageNet is just to bring a random, high-dimensional embedding. In addition, I wonder whether it will help to fine-tune the model on the graph classification data. Could this submission show some fine-tune experiments?",12,157,17.444444444444443,5.456375838926174,96,0,157,0.0,0.0377358490566037,0.9598,45,15,31,8,6,2,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 8, 'EXP': 1, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 9, 'SUB': 0, 'CLA': 0}",0,0,0,2,2,8,1,1,1,0,0,0,0,0,0,0,0,0,1,0,9,0,0,0.4304181639696704,0.227197881700966,0.1913720124979238
ICLR2018-HymuJz-A--R1,Reject,"Quality  This paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data. This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization. Clarity  The rationale in the paper is straightforward. I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before. Originality  While others have pointed out limitations before, this paper considers relational networks for the first time. Significance   This work demonstrates failures of relational networks on relational tasks, which is an important message. At the same time, no new architectures are presented to address these limitations. Pros  Important message about network limitations. Cons  Straightforward testing of network performance on specific visual relation tasks. No new theory development. Conclusions drawn by testing on out of sample data may not be completely valid.",11,167,15.181818181818182,5.9397590361445785,101,2,165,0.0121212121212121,0.0402298850574712,-0.5719,57,21,31,7,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 1}",0,1,0,2,0,4,1,1,0,1,0,2,0,0,0,2,0,0,0,0,4,2,1,0.5008552950112602,0.446462100841813,0.2765936448023215
ICLR2018-HymuJz-A--R2,Reject,"The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations. They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be. They show that while recent approaches (e.g. relational networks) can generalize reasonably well on some tasks, these results do not generalize as well to held-out-object scenarios as might have been assumed. Clarity:  The paper is fairly clearly written.  I think I mostly followed it. Quality:  I'm intrigued by but a little uncomfortable with the generalization metrics that the authors use. The authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image conditions. The authors state that . . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset.   Taken literally, this would rule out a lot of modern machine learning, even obviously very good work. On the other hand, it's clear that at some point, generalization needs to occur in testing ability to understand relationships. I'm a little worried that it's in the eye of the beholder whether a given generalization should be expected to work or not. There are essentially three scenarios of generalization discussed in the paper:         (a) various generalizations of image parameters in the PSVRT dataset    (b) various hold-outs of the image parameters in the sort-of-CLEVR dataset   (c) from sort-of-CLEVR objects to PSVRT bit patterns The result that existing architectures didn't do very well at these generalizations (especially b and c) *may* be important -- or it may not. Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR, it would generalize to most other situations that might be encountered. After all, when we humans generalize to understanding relationships, exactly what variability is present in our training sets as compared to our testing situations? How do the authors know that humans are effectively generalizing rather than just interpolating within their (very rich) training set? It's not totally clear to me that if totally naive humans (who had never seen spatial relationships before) were evaluated on exactly the training/testing scenarios described above, that they would generalize particularly well either. I don't think it can just be assumed a priori that humans would be super good this form of generalization. So how should authors handle this criticism? What would be useful would either be some form of positive control. Either human training data showing very effective generalization (if one could somehow make  ovel relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN. If such were present, I'd rate this paper significantly higher. Also, I can't tell if I really fully believe the results of this paper. I don't doubt that the authors saw the results they report. However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different. I can't point to exactly what would have to be different to make things work, because it's really hard to do that ahead of actually trying to do the work.  However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at ICLR. This will give the people working on (e.g.) CNN+RN somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly. I myself am very curious about what would happen and would love to see this exchange catalyzed. Originality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be. However, as the authors here note, there's been some recent work (e.g. Santoro 2017) in the area. I think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful, and is a somewhat novel contribution. ",34,716,21.058823529411764,5.181950509461426,305,16,700,0.0228571428571428,0.0592193808882907,0.9977,158,69,153,77,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 8, 'MET': 15, 'EXP': 2, 'RES': 9, 'TNF': 0, 'ANA': 0, 'FWK': 2, 'OAL': 7, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 3, 'CMP': 3, 'PNF': 0, 'REC': 2, 'EMP': 18, 'SUB': 0, 'CLA': 2}",0,1,4,1,8,15,2,9,0,0,2,7,0,1,0,1,3,3,0,2,18,0,2,0.719090957548406,0.677671277013292,0.4909978512320588
ICLR2018-HymuJz-A--R3,Reject,"Strengths:  -tThere is an interesting analysis on how CNN's perform better Spatial-Relation problems in contrast to Same-Different problems, and how Spatial-Relation problems are less sensitive to hyper parameters. -tThe authors bring a good point on the limitations of the SVRT dataset u2013 mainly being the difficulty to compare visual relations due to the difference of image structures on the different relational tasks and the use of simple closed curves to characterize the relations, which make it difficult to quantify the effect of image variability on the task. And propose a challenge that addresses these issues and allows controlling different aspects of image variability. -tThe paper shows how state of the art relational networks, performing well on multiple relational tasks, fail to generalize to same-ness relationships, Weaknesses:  -tWhile the proposed PSVRT dataset addresses the 2 noted problems in SVRT, using only 2 relations in the study is very limited. -tThe paper describes two sets of relationships, but it soon suggests that current approaches actually struggle in Same-Different relationships. However, they only explore this relationship under identical objects. It would have been interesting to study more kinds of such relationships, such as equality up to translation or rotation, to understand the limitation of such networks. Would that allow improving generalization to varying item or image sizes? Comments:  -tIn page 2, authors suggest that from that Gu00fclu00e7ehre, Bengio (2013) that for visual relations ""failure of feed-forward networks [...] reflects a poor choice of hyper parameters. This seems to contradict the later discussion, where they suggest that probably current architectures cannot handle such visual relationships. -tThe point brought about CNN's failing to generalize on same-ness relationships on sort-of-CLEVR is interesting, but it would be good to know why PSVRT provides better generalization. What would happen if shapes different than random squared patterns were used at test time? -tAuthors reason about biological inspired approaches, using Attention and Memory, based on existing literature. While they provide some good references to support this statement it would have been interesting to show whether they actually improve TTA under image parameter variations ",15,342,28.5,5.736526946107785,192,3,339,0.0088495575221238,0.0230547550432276,0.9373,106,42,62,13,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 3, 'MET': 10, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 5, 'CLA': 0}",0,1,4,1,3,10,1,0,0,0,0,1,0,0,0,0,0,1,0,0,8,5,0,0.5025653742110009,0.33811374141052,0.25335323880883176
ICLR2018-Hyp-JJJRW-R1,Reject,"This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or style as used in this paper). This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer. Experiments on MNIST are provided to analyse what this approach learns. Unfortunately, I fail to see a significantly valuable contribution from this work. First, the paper could do a better job at motivating the problem being addressed . Why is it important to separate class from style? Should it allow better classification performance? If so, it's never measured in this work. If that's not the motivation, then what is it? Second, all experiments were conducted on the MNIST dataset. In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method. Finally, the results are not particularly impressive. I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input). Also, that the style representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result. And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully. The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great. But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective. For all these reasons, I'm afraid I must recommend this paper be rejected.",18,306,20.4,5.421602787456446,168,1,305,0.0032786885245901,0.022801302931596,0.826,71,38,54,33,7,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 3, 'MET': 7, 'EXP': 8, 'RES': 5, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 3, 'CLA': 0}",0,0,0,2,3,7,8,5,4,0,0,2,0,0,0,0,0,0,0,1,8,3,0,0.5025741248273197,0.3379003883938771,0.2533678097853979
ICLR2018-Hyp-JJJRW-R2,Reject,"The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called style memory, which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally style and content, is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible. The use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)",9,218,21.8,5.811320754716981,116,5,213,0.0234741784037558,0.0596330275229357,0.8082,57,28,38,16,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 1, 'EXP': 1, 'RES': 3, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,1,2,1,1,1,3,2,0,0,0,1,0,0,0,0,1,2,0,3,0,0,0.6430511444651241,0.3346388171268226,0.3114472715104378
ICLR2018-Hyp-JJJRW-R3,Reject,"The paper proposes combining classification-specific neural networks with auto-encoders. This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction. The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders). The authors minimize the loss function by greedy layer-wise training as is done in several prior works. The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction). For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different. The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images. While the experimental results are interesting they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders. Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. ",10,186,18.6,6.108695652173913,99,1,185,0.0054054054054054,0.0159574468085106,-0.7579,53,17,34,9,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 8, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,1,1,0,1,8,3,0,0,0,0,0,0,0,1,0,1,0,0,2,0,0,0.4292862028988082,0.3339552907681763,0.2170486843179995
ICLR2018-Hyp3i2xRb-R1,Reject,"The paper investigates the iterative estimation view on gated recurrent networks (GNN). Authors observe that the average estimation error between a given hidden state and the last hidden state  gradually decreases toward zeros. This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time. Given this observation, authors then propose RIN, a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix. Authors evaluate their RIN on the adding, sequential MNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models. Questions: - Section 2 suggests that use of the gate  in GNNs encourages to learn an identity mapping. Does the average iteration error behaves differently in case of a tanh-RNN ? - It seems from Figure 4 (a) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end. What could explain this phenomenon? - While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN). What could explain this difference in the performances? - Unless I am mistaken, Gated Orthogonal Recurrent Units: On Learning to Forget from Jing et al. also reports better performances for the LSTM (and GRU) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively? - Quality/Clarity: The paper is well written and pleasant to read n - Originality: Looking at RNN from an iterative refinement point of view seems novel. - Significance: While looking at RNN from an iterative estimation is interesting, the experimental part does not really show what are the advantages of the propose RIN. In particular, the LSTM baseline seems to weak compared to other works.",17,319,26.58333333333333,5.198697068403908,162,4,315,0.0126984126984126,0.0182370820668693,0.8823,103,35,50,12,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 2, 'DAT': 1, 'MET': 11, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,3,2,1,11,1,1,0,0,0,1,0,0,0,1,0,3,0,0,7,0,1,0.5741507682330418,0.4484105141678822,0.3247531828276823
ICLR2018-Hyp3i2xRb-R2,Reject,"Here are my main critics of the papers:  1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)? If so your phrase is zero given a sequence of inputs X1, ...,T is misleading. 2. Lack of motivation for IE or UIE. Where is your background material? I do not understand why we would like to assume (1), (2), (3). Why the same intuition of UIE can be applied to RNNs? 3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization. Not much novelty. 4. The experimental results are not convincing. It's not compared against any previous published results. E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1]. Also it only has been tested on very simple datasets. [1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations. Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro.",14,163,9.588235294117649,5.055172413793104,110,1,162,0.0061728395061728,0.0304878048780487,-0.584,47,19,28,14,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 2, 'MET': 7, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 2, 'CLA': 0}",0,0,3,0,2,7,1,2,0,0,0,1,2,0,0,1,0,2,0,0,9,2,0,0.5017663141249161,0.4496439429886997,0.28070116668704526
ICLR2018-Hyp3i2xRb-R3,Reject,"Summary:  The authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix. This identity connection acts as a ""surrogate memory"" component, preserving hidden activations over time steps. The experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames. It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI. The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs. Review:  I very much like the paper. The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures! I have a few comments and questions: 1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise? If bit-wise, can you elaborate why? I might have missed something. 2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c? Also some curves in the appendix stop abruptly without visible explosions. Were these experiments run until completion? If so, would it be possible to plot the complete curves? 3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task. Optimal hyperparameters are usually model-specific. Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript. Le et al. (2015) for instance perform a coarse grid search for each model. 4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up. There is no (multiplicative) gating as in Highway Networks. 5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating. The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions. 6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful. Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations. Would RINs readily learn to reset parts of the hidden state? 7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.  Overall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6).  After a revision I would consider to increase the score. References: Henaff, Mikael, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory tasks.  In International Conference on Machine Learning, pp. 2034-2042. 2016. Le, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941 (2015).",33,570,16.285714285714285,5.453510436432637,290,7,563,0.0124333925399644,0.0191304347826086,0.9782,175,69,85,32,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 2, 'DAT': 0, 'MET': 14, 'EXP': 11, 'RES': 1, 'TNF': 3, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 5, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 6, 'PNF': 5, 'REC': 1, 'EMP': 10, 'SUB': 1, 'CLA': 0}",0,1,4,2,0,14,11,1,3,0,0,2,5,0,0,0,0,6,5,1,10,1,0,0.6474139485712509,0.5619852609503054,0.40987027735949977
ICLR2018-HyrCWeWCb-R1,Accept,"Clarity  The paper is well-written and clear. Originality The paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy. The paper leverages a novel method in determining the coefficient of relative entropy. Significance - Trust-PCL achieves overall competitive with state-of-the-art external implementations. - Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance. - Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant. - Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. alpha and update frequency on phi) would be helpful. Pros: - The paper is well-written and clear. - Competitive with state-of-the-art external implementations - Significant empirical advantage over TRPO. -  Open source codes. Cons: - No ablation studies.  ",12,134,10.307692307692308,6.206106870229007,80,0,134,0.0,0.0068027210884353,0.9661,54,30,10,3,6,6,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 2, 'CLA': 2}",0,2,3,0,0,5,0,2,0,2,0,2,0,0,0,1,1,3,0,0,3,2,2,0.4298845318420269,0.6683327182299081,0.3064475573692926
ICLR2018-HyrCWeWCb-R2,Accept,"This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. The major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing. In experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. What is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. Another issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.",15,209,14.928571428571429,5.562814070351759,116,1,208,0.0048076923076923,0.0095693779904306,0.2479,65,23,38,12,5,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 4, 'CLA': 0}",0,1,0,0,0,7,4,3,0,0,0,1,0,0,0,0,0,1,2,0,6,4,0,0.3590140009861101,0.447935830067427,0.20473732985452897
ICLR2018-HyrCWeWCb-R3,Accept,"The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent. I am not an expert in this area. It is hard to judge the significance of this extension. The paper largely follows the work of Nachum et al 2017. The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training. However, the relative entropy term added seems like a marginal modification.",6,85,14.166666666666666,5.185185185185185,57,1,84,0.0119047619047619,0.0352941176470588,0.9169,28,12,11,4,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,1,1,0,3,0,0,0,0,0,1,0,1,0,1,1,0,0,0,1,0,0,0.4290718127989979,0.3333333333333333,0.21265338003638984
ICLR2018-HytSvlWRZ-R1,Reject,"This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. Differently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. The framework is represented by a feed-forward deep architecture analogous to a residual network. At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables. The idea of explicitly accounting for the boundedness of clinical scores is interesting, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales. For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application. Furthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. The proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, ...). Overall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders. Moreover, the proposed experimental paradigm appears flawed. What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)? Other remarks.   - In section 2.2 and 4 there is some confusion between iteration indices and samples indices ""i"". - Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables. -  Figure 2, synthetic data. The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations. The loss function of Figure 2. b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure. These aspects may be a sign of convergence issues. - The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is a crucial parameters that is however not discussed nor analysed in the paper. - The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance. In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting. - The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Is there anything that I am missing? ",26,504,19.384615384615383,5.564210526315789,244,3,501,0.0059880239520958,0.0116504854368932,0.9148,146,60,78,33,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 15, 'EXP': 1, 'RES': 7, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 2, 'REC': 0, 'EMP': 18, 'SUB': 1, 'CLA': 0}",0,2,1,1,2,15,1,7,1,1,0,0,0,0,0,0,0,2,2,0,18,1,0,0.6467556534755937,0.4551964523177683,0.3707450127980962
ICLR2018-HytSvlWRZ-R2,Reject,"The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. The topic is important. Experiments on real data show improvements compared to several traditional approaches. My major concerns are as follows. 1. The paper is not self-contained. The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1. However, for some key steps in the proof, they refer to other references. If this is due to space limitation in the main text, they may want to provide a complete proof in the appendix. 2. The experiments are unconvincing. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. A major merit of DNN is that it can automatically extract useful features. However, in this experiment, the features are handcrafted before they are fed into the models. Thus, I would like to see a comparison between SN with vanilla DNN. ",12,158,9.875,5.340136054421769,98,1,157,0.0063694267515923,0.025,0.7501,43,19,27,7,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 2, 'MET': 6, 'EXP': 3, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 0}",0,1,2,1,2,6,3,0,0,0,0,2,0,0,0,0,0,4,1,0,4,2,0,0.5015853496026688,0.4467684809288647,0.2813339119133662
ICLR2018-HytSvlWRZ-R3,Reject,"This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded. Some novel contributions: 1. Layer by layer feedforward training process, no back-prop. 2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data) Weakness : 1. The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literature. 2. The proof part(Section 2.2) can be extended with more details in Appendix. 3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others. 4. Typo: In Table2,3,5, Multi-l_{2,1} (denotes the L2,1 norm) were written wrong. 5. In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation. 6. Extra models like Deep Networks with/without matrix factorization could be added. ( As proposed model is a deep model, the lack of comparison with deep methods is dubious) 7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin. Extensive experiments could be added. 8. The performance on One-Layer Subspace Network (with only the input features) could be added. Conclusion: Though with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network.  ",16,294,11.76,5.563636363636364,156,1,293,0.0034129692832764,0.0066889632107023,0.3563,91,44,44,19,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 2, 'MET': 6, 'EXP': 8, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 7, 'SUB': 6, 'CLA': 1}",0,1,2,2,2,6,8,1,1,0,0,0,0,0,0,3,0,0,1,0,7,6,1,0.5734587810435887,0.559910897875257,0.36683347409361816
ICLR2018-SkYXvCR6W-R1,Reject,"The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row. The idea is interesting, and overall introduction is clear. However, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015. The results using this particular encoding are not better than any previous work. The network architecture seems to be arbitrary and unusual. It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels. The depth of the network is only 5, even with many layers listed in table 5. It uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features. This does not seem to be reasonable. Overall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018.",10,194,19.4,5.1256830601092895,111,1,193,0.005181347150259,0.0307692307692307,-0.4124,49,27,31,9,7,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,2,1,2,0,6,0,2,1,0,0,1,0,0,1,0,0,1,0,0,5,2,0,0.501391481252804,0.4470389506921378,0.279683027024622
ICLR2018-SkYXvCR6W-R2,Reject,"The manuscript proposed to use prefix codes to compress the input to a neural network for text classification. It builds upon the work by Zhang & LeCun (2015) where the same tasks are used. There are several issues with the paper and I cannot recommend acceptance of the paper in the current state. - It looks like it is not finished. - the datasets are not described properly. - It is not clear to me where the baseline results come from. They do not match up to the Zhang paper (I have tried to find the matching accuracies there). - It is not clear to me what the baselines actually are or how I can found more info on those. - the results are not remarkable. Because of this, the paper needs to be updated and cleaned up before it can be properly reviewed. On top of this, I do not enjoy the style the paper is written in, the language is convoluted. For example: ""The effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification "" I do not know which message the paper tries to get across here. As a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive. The acknowledgements should not be included here either.   ",15,236,15.733333333333333,4.7631578947368425,125,0,236,0.0,0.073170731707317,-0.5449,52,19,53,21,7,6,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 1, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 8, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 4, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 4}",0,2,2,2,1,1,0,2,0,0,0,8,0,0,0,0,0,3,4,1,3,1,4,0.5005389402426186,0.6685730186347048,0.33398219572347937
ICLR2018-SkYXvCR6W-R3,Reject,"This paper proposes a new character encoding scheme for use with character-convolutional language models. This is a poor quality paper, is unclear in the results (what metric is even reported in Table 6), and has little significance (though this may highlight the opportunity to revisit the encoding scheme for characters).",2,50,25.0,5.574468085106383,36,1,49,0.0204081632653061,0.06,0.228,14,7,9,1,4,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,1,0,0,0,2,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0.2859644778280704,0.2222222222222222,0.12372852139870513
ICLR2018-Skz_WfbCZ-R1,Accept,"The authors prove a generalization guarantee for deep neural networks with ReLU activations, in terms of margins of the classifications and norms of the weight matrices. They compare this bound with a similar recent bound proved by Bartlett, et al. While, strictly speaking, the bounds are incomparable in strength, the authors of the submission make a convincing case that their new bound makes stronger guarantees under some interesting conditions. The analysis is elegant. It uses some existing tools, but brings them to bear in an important new context, with substantive new ideas needed. The mathematical writing is excellent. Very nice paper. I guess that networks including convolutional layers are covered by their analysis. It feels to me that these tend to be sparse, but that their analysis still my provides some additional leverage for such layers. Some explicit discussion of convolutional layers may be helpful.  ",10,145,13.181818181818182,5.390070921985815,98,2,143,0.0139860139860139,0.0204081632653061,0.9699,37,25,23,4,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 5, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 2, 'CLA': 1}",0,1,1,1,0,4,0,0,0,5,0,1,0,0,0,0,0,2,0,0,4,2,1,0.4294208824352476,0.4465341558144848,0.24190945626299595
ICLR2018-Skz_WfbCZ-R2,Accept,"This paper provides a new generalization bound for feed forward networks based on a PAC-Bayesian analysis. The generalization bound depends on the spectral norm of the layers and the Frobenius norm of the weights. The resulting generalization bound is similar (though not comparable) to a recent result of Bartlett et al (2017), however the technique is different since this submission uses PAC-Bayesian analysis. The resulting proof is more simple and streamlined compared to that of Bartlett et al (2017). The paper is well presented, the result is explained and compared to other results, and the proofs seem correct. The result is not particularly different from previous ones, but the different proof technique might be a good enough reason to accept this paper. Typos: Several citations are unparenthesized when they should be. Also, after equation (6) there is a reference command that is not compiled properly.  ",10,145,16.11111111111111,5.372262773722627,82,1,144,0.0069444444444444,0.0204081632653061,0.8316,39,17,26,10,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 3, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 0}",0,1,2,1,0,5,0,3,0,0,0,2,1,0,0,0,0,3,3,1,2,0,0,0.5012380789509921,0.445423864841274,0.2820535361074243
ICLR2018-Skz_WfbCZ-R3,Accept,"This paper combines a simple PAC-Bayes argument with a simple perturbation analysis (Lemma 2) to get a margin based generalization error bound for ReLU neural networks (Theorem 1) which depends on the product of the spectral norms of the layer parameters as well as their Frobenius norm. The main contribution of the paper is the simple proof technique to derive Theorem 1, much simpler than the one use in the very interesting work [Bartlett et al. 2017] (appearing at NIPS 2017) which got an analogous bound but with a dependence on the l1-norm of the layers instead of the Frobenius norm. The authors make a useful comparison between these bounds in Section 3 showing that none is dominating the others, but still analyzing their properties in terms of structural properties of the weight matrices. I enjoyed reading this paper. One could think that it makes a somewhat incremental contribution with respect to the more complete work (both theory and practice) from [Bartlett et al. 2017]. Nevertheless, the simplicity and elegance of the proof as well as the result might be useful for the community to get progress on the theoretical analysis of NNs. The paper is well written, though I make some suggestions for the camera ready version below to improve clarity. I verified most of the math.    Detailed suggestions     1) The authors should specify in the abstract and in the introduction that they are analyzing feedforward neural networks *with ReLU activation functions* so that the current context of the result is more transparent. It is quite unclear how one could generalize the Theorem 1 to arbitrary activation functions phi given the crucial use of the homogeneity of the ReLU at the beginning of p.4. Though the proof of Lemma 2 only appears to be using the 1-Lipschitzness property of phi as well as phi(0)  0. (Unless they can generalize further; I also suggest that they explicitly state in the (interesting) Lemma 2 that it is for the ReLU activations (like they did in Theorem 1)). 2) A footnote (or citation) could be useful to give a hint on how the inequality 1/e beta^(d-1) <  tilde{beta}^(d-1) <  e beta^(d-1) is proven from the property |beta-tilde{beta}|<  1/d beta (middle of p.4). 3) Equation (3) -- put the missing 2 subscript for the l2 norm of |f_(w+u)(x) - f_w(x)|_2 on the LHS (for clarity). 4) One extra line of derivation would be helpful for the reader to rederive the bound|w|^2/2sigma^2  <  O(...) just above equation (4). I.e. first doing the expansion keeping the beta terms and Frobenius norm sum, and then going directly to the current O(...) term. 5) bottom of p.4: use hat{L}_gamma   1 instead of L_gamma  1 for more clarity. 6) Top of p.5: the sentence Since we need tilde{beta} to satisfy (...) is currently awkwardly stated. I suggest instead to say that |tilde{beta}- beta| <  1/d (gamma/2B)^(1/d) is a sufficient condition to have the needed condition |tilde{beta}-beta| <  1/d beta over this range, thus we can use a cover of size dm^(1/2d).   7) Typo below (6): citetbarlett2017...  8) Last paragraph p.5: Recalling that W_i is *at most* a hxh matrix (as your result do not require constant size layers and covers the rectangular case).  ",20,527,21.08,4.997881355932203,245,3,524,0.0057251908396946,0.017921146953405,0.997,175,54,75,29,10,5,"{'ABS': 1, 'INT': 2, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 16, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 2, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 3, 'PNF': 7, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 3}",1,2,2,1,0,16,2,2,0,2,1,3,0,0,0,0,1,3,7,0,5,0,3,0.7184020575053192,0.5588093940938355,0.45391848258180373
ICLR2018-Sy-tszZRZ-R1,Reject,"This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. The improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. The improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. The improved upper bound for maxout networks follows a similar intuition but appears to be novel. The paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. Here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses.     ",11,192,13.714285714285714,5.243243243243243,92,2,190,0.0105263157894736,0.0151515151515151,0.9813,52,36,24,5,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,2,2,0,6,3,1,0,0,0,0,0,0,0,1,0,1,0,0,4,1,0,0.4301345684880089,0.4463103167489733,0.24353408478228045
ICLR2018-Sy-tszZRZ-R2,Reject,"Paper Summary:  This paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network. It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions. These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. This paper improves on the upper bound given by [2] and the lower bound given by [1]. They also provide a tight bound for the one dimensional input case. Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST Main Comments: The paper is very well written and clearly states and explains the contributions. However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds, with no other novel interpretations or insights into deep architectures. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better. I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful. Overall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance. [1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio [2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein",20,369,26.357142857142858,5.378612716763006,180,1,368,0.0027173913043478,0.0135135135135135,0.9863,103,57,57,28,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 2, 'MET': 7, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 2, 'CLA': 2}",0,1,4,1,2,7,2,5,0,0,1,3,0,0,0,1,0,2,0,1,7,2,2,0.6450356909655438,0.67070338541209,0.45782889471860555
ICLR2018-Sy-tszZRZ-R3,Reject,"This is quite an interesting paper. Thank you. Here are a few comments:  I think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix. However I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved). I can appreciate though that this a fine line to walk. Also in the appendix, please restate the lemma that is being proven. Otherwise one will have to scroll up and down all the time to understand the proof. I think the paper could also discuss a bit more in detail the results provided. For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice. Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets. **************** I had reduced my score based on the observation made by Reviewer 1 regarding the talk Montufar at SampTA. Could the authors prioritize clarification to that point ! - Thanks for the clarification and adding this citation. ",12,200,15.384615384615383,5.010695187165775,122,5,195,0.0256410256410256,0.0292682926829268,0.9787,63,19,40,14,6,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 5, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 5, 'BIB': 1, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 2, 'SUB': 5, 'CLA': 1}",0,0,1,0,0,5,0,2,0,0,0,5,1,1,0,0,1,0,2,1,2,5,1,0.4298095741514789,0.6677768990585988,0.29942107675928137
ICLR2018-Sy1f0e-R--R1,,"In the paper, the authors discuss several GAN evaluation metrics. Specifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy. For those properties raised, the authors experimentally evaluated whether existing metrics satisfy those properties or not. Section 4 summarizes the results, which concluded that the Kernel MMD and 1-NN classifier in the feature space are so far recommended metrics to be used. I think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. In particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. The result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1-NN classifier), seems to be reasonable. This would not be a surprising result as the ultimate goal of GAN is mimicking the data distribution. However, the result is supported by exhaustive experiments making the result highly convincing. Overall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field. ",10,195,17.727272727272727,5.486910994764398,105,3,192,0.015625,0.0255102040816326,0.9784,53,25,41,12,7,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 2, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,2,2,5,0,0,1,1,0,0,0,0,1,0,0,1,7,1,0,0.5005591954763312,0.4481761890535022,0.28101044779662493
ICLR2018-Sy1f0e-R--R2,,"Thanks for an interesting paper. The paper evaluates popular GAN evaluation metrics to better understand their properties. The  ovelty of this paper is a bit hard to assess. However, I found their empirical evaluation and experimental observations to be very interesting. If the authors release their code as promised, the off-the-shelf tool would be a very valuable contribution to the GAN community. In addition to existing metrics, it would be useful to add Frechet Inception Distance (FID) and Multi-scale structural similarity (MS-SSIM). Have you considered approximations to Wasserstein distance? E.g. Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs: Comparison of Maximum Likelihood and GAN-based training of Real NVPs https://arxiv.org/pdf/1705.05263.pdf How sensitive are the results to hyperparameters? It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf. Appendix G in https://arxiv.org/pdf/1706.04987.pdf) Do you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics? Some of the metrics don't capture perceptual similarity, but I'm curious to hear what you think.  ",12,180,16.363636363636363,5.6,113,2,178,0.0112359550561797,0.0163934426229508,0.9488,50,32,35,8,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,0,2,0,0,8,2,2,0,1,0,2,0,0,0,1,0,4,0,0,5,2,0,0.4305905777627739,0.4473904383637077,0.24375282481234992
ICLR2018-Sy1f0e-R--R3,,"The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). The paper is well written, clear, organized and easy to follow. Given that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons. They analyzed discriminability, mode collapsing and dropping, robustness to transformations, efficiency and overfitting. Although this work and its results are very useful for practitioners, it lacks in two aspects. First, it only considers a single task for which GANs are very popular.  Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions. Some of the conclusions could be further clarified with additional experiments (e.g., Sec 3.6 'while the reason that RMS also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the ImageNet dataset'). ",9,172,19.11111111111111,5.431137724550898,115,2,170,0.0117647058823529,0.0172413793103448,0.9019,49,18,29,10,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 3, 'EXP': 1, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 2, 'SUB': 4, 'CLA': 1}",0,1,0,1,1,3,1,5,0,2,0,1,0,0,0,0,0,0,1,0,2,4,1,0.5721764826075948,0.4453864314042518,0.3229806879844989
ICLR2018-Sy1f0e-R--R4,,"This paper introduces a comparison between several approaches for evaluating GANs. The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared. They compare the evaluation methods based on five criteria termed disciminability, mode collapsing and mode dropping, sample efficiency,computation efficiency, and robustness to transformation. This paper has some interesting insights and a few ideas of how to validate an evaluation method. The topic is an important one and a very difficult one. However, the work has some problems in rigor and justification and the conclusions are overstated in my view. Pros -Several interesting ideas for evaluating evaluation metrics are proposed -The authors tackle a very challenging subject Cons -It is not clear why GANs are the only generative model considered -Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper. -The evaluations rely on using a pre-trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification -The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for ""discriminativeness"" and seems like something that can be gamed. - The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported. Several references I suggest: https://arxiv.org/abs/1706.08500 (FID score) https://arxiv.org/abs/1511.04581 (MMD as evaluation)",16,276,23.0,5.687732342007435,154,1,275,0.0036363636363636,0.0324909747292418,-0.8064,86,41,45,15,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 9, 'PDI': 6, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 5, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 8, 'SUB': 3, 'CLA': 2}",0,1,9,6,0,3,2,1,0,2,0,5,0,1,0,0,2,1,0,0,8,3,2,0.6445905200175901,0.5602517823354013,0.40196944172158033
ICLR2018-Sy21R9JAW-R1,Accept,"This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances: - epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training. - This gives a more unified way of understanding, and implementing the methods. - The paper points out situations when the methods are equivalent - The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity - The paper proposes a new objective function to measure joint sensitivity Overall, I believe this paper to be a useful contribution to the literature. It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods. Especially the latter will be appreciated.",9,130,21.666666666666668,5.848,81,0,130,0.0,0.0148148148148148,0.8977,39,19,26,3,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 7, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 3, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,7,1,0,0,3,0,1,0,0,1,1,0,0,0,0,5,0,0,0.2872648771582263,0.3358211630727052,0.14567979894714264
ICLR2018-Sy21R9JAW-R2,Accept,"The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space. The main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps-LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ('sensitivity n') that generalizes the earlier defined properties of 'completeness' and 'summation to delta'. The unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-LRP and DeepLIFT substantially more easy on modern frameworks. However, as correctly stated by the authors some of the unification (e.g. relation between LRP and Gradient*Input) has been already mentioned in prior work. Sensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination. While the measure shows interesting trends towards a linear behaviour for simpler methods, it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different methods. Furthermore, The authors could comment on the relation between sensitivity-n and region perturbation techniques (Samek et al., IEEE TNNLS, 2017). Sensitivtiy-n seems to be an extension of the region perturbation idea to me. It would be interesting to see the relation between the unified gradient-based explanation methods and approaches (e.g. Saliency maps, alpha-beta LRP, Deep Taylor, Deconvolution Networks, Grad-CAM, Guided Backprop ...) which do not fit into the unification framework. It's good that the author mention these works, still it would be great to see more discussion on the advantages/disadvantages, because these methods may have some nice theoretically properties (see e.g. the discussion on gradient vs. decompositiion techniques in Montavon et al., Digital Signal Processing, 2017) which can not be incorporated into the unified framework.",11,329,23.5,5.7115987460815045,181,2,327,0.0061162079510703,0.0090361445783132,0.9872,106,41,42,19,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 9, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 6, 'SUB': 4, 'CLA': 0}",0,1,4,1,0,9,0,1,0,0,0,0,0,0,0,0,0,2,0,0,6,4,0,0.3593858577602864,0.3368803125897026,0.18350479391023472
ICLR2018-Sy21R9JAW-R3,Accept,"The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results. The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function. A second part of the paper looks at whether explanations are global or local. The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case. The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets. Results further outline the resemblance between the compared methods. In the appendix, the last step of the proof below Eq. 7 is unclear. As far as I can see, the variable g_i^LRP wasn't defined, and the use of Eq. 5 to achieve this last could be better explained. There also seems to be some issues with the ordering i,j, where these indices alternatively describe the lower/higher layers, or the higher/lower layers.",9,191,15.916666666666666,5.239130434782608,117,1,190,0.0052631578947368,0.0261780104712041,0.7845,55,22,29,8,5,1,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 8, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,1,0,0,1,8,1,2,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0.3589499038320592,0.112355025980797,0.14393471815208214
ICLR2018-Sy3fJXbA--R1,Reject,"The authors extend the ResNeXt architecture. They substitute the simple add operation with a selection operation for each input in the residual module. The selection of the inputs happens through gate weights, which are sampled at train time. At test time, the gates with the highest values are kept on, while the other ones are shut. The authors fix the number of the allowed gates to K out of C possible inputs (C is the multi-branch factor in the ResNeXt modules). They show results on CIFAR-100 and ImageNet (as well as mini ImageNet). They ablate the choice of K, the binary nature of the gate weights. Pros: (+) The paper is well written and the method is well explained (+) The authors ablate and experiment on large scale datasets Cons: (-) The proposed method is a simple extension of ResNeXt (-) The gains are reasonable, yet not SOTA, and come at a price of more complex training protocols (see below) (-) Generalization to other tasks not shown The authors do a great job walking us through the formulation and intutition of their proposed approach. They describe their training procedure and their sampling approach for the gate weights. However, the training protocol gets complicated with the introduction of gate weights. In order to train the gate weights along with the network parameters, the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen. This makes training of such networks cumbersome. In addition, the authors report a loss in performance when the gates are not discretized to {0,1}. This means that a liner combination with the real-valued learned gate parameters is suboptimal. Could this be a result of suboptimal, possibly compromised training? While the CIFAR-100 results look promising, the ImageNet-1k results are less impressive. The gains from introducing gate weights in the input of the residual modules vanish when increasing the network size. Last, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks. Have the authors experimented with other tasks, e.g. object detection, to verify that their approach leads to better performance in a more diverse set of problems? ",25,357,18.789473684210527,5.130813953488372,168,1,356,0.0028089887640449,0.0137741046831955,0.9643,112,37,59,14,9,4,"{'ABS': 0, 'INT': 2, 'RWK': 1, 'PDI': 1, 'DAT': 3, 'MET': 9, 'EXP': 13, 'RES': 5, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 3, 'CLA': 1}",0,2,1,1,3,9,13,5,0,1,0,1,0,0,0,0,0,2,0,0,12,3,1,0.6462135372478646,0.45161649180155,0.368327463624476
ICLR2018-Sy3fJXbA--R2,Reject,"The paper is clear and well written. It is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt. This paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the gates of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2)). The main contribution appears to be the optimisation procedure for the binary masking tensor g. But this procedure is not justified: does each step minimise the loss? This seems unlikely due to the sampling. Can the authors show that the procedure will always converge? It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017). ",8,154,22.0,5.006666666666667,104,3,151,0.0198675496688741,0.032258064516129,0.7494,42,19,27,10,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,0,2,1,0,5,1,0,0,0,0,1,0,0,0,0,0,2,0,0,4,0,1,0.3582241135003797,0.3353163681950522,0.17843370105745981
ICLR2018-Sy3fJXbA--R3,Reject,"The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel. The input to each model is a sparse sum of the outputs of modules in the previous set. The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on. Pros: - The connectivity is constrained to be sparse between modules, and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weights. Furthermore, this learning extends to large-scale image datasets. - There is indeed a boost in classification performance, and the approach shows promise for automatically reducing the number of parameters in the network. Cons: - Overall, the approach seems to be an incremental improvement over the previous work ResNeXt. - The datasets used are not very interesting: Cifar is too small, and ImageNet is essentially solved. From the standpoint of the computer vision community, increasing performance on these datasets is no longer a meaningful objective. - The modifications add complexity. The paper is well written and conceptually simple. However, I feel the paper demonstrates neither enough novelty nor enough of a performance gain for me to advocate acceptance.   ",12,208,16.0,5.547738693467337,121,1,207,0.0048309178743961,0.0138888888888888,0.964,63,20,35,14,6,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 5, 'MET': 3, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 1}",0,2,2,2,5,3,0,0,0,0,0,2,0,0,0,1,0,2,0,1,6,0,1,0.4294269012601776,0.5587825052869604,0.272420042319877
ICLR2018-Sy3nGCYXz-R1,,"This paper gives an empirical estimation of the intrinsic dimensionality of the convolutional neural network VGG19 due to Simonyan and Zisserman. To estimate the ID, the authors apply singular value decomposition to the matrix of activation vectors at each of the layers of the network, in which the intrinsic dimension is determined (in a more or less standard way) by the rank at which two consecutive singular values has a ratio exceeding some threshold. For the convolutional layers of VGG19, they observe that the sum of IDs for each feature map is roughly equal to the ID of the matrix formed by concatenating the vectors over all feature maps. They also observe that the ID drops with each successive layer. The authors' findings are intuitively obvious, and certainly not surprising. Although a thorough and careful empirical investigation of this phenomenon would be a welcome addition to the research literature, this paper does not yet reach this standard. First and foremost, a result for a single neural network does not constitute enough evidence to justify the authors' conclusions. Second, the latter half of the paper is concerned with details of the experimental results, without offering any insights as to the implications for deep learning. Third, the paper is not well presented and organized: the introduction is scant; the notational formulism is not at all clear, rigorous, or consistent; the paper overall lacks polish, with many grammatical errors. Overall, I feel that while this line of research is worthwhile, at this stage the work is not yet ready for publication.",10,258,25.8,5.1673306772908365,142,1,257,0.0038910505836575,0.0232558139534883,0.7582,69,40,31,15,5,7,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 1, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,1,0,0,0,2,2,4,0,0,0,3,0,0,1,0,1,0,1,1,3,1,1,0.3577371883425799,0.7790216926474637,0.2802170509201063
ICLR2018-Sy3nGCYXz-R2,,"This paper analyzed the dimensionality of feature maps and fully connected layers of pre-trained CNN on images within a same category. The local dimensionality of this paper is the SVD dimensionality on the augmented images of the same class images which classified by the neural network as high probability. However, the motivation of the analysis of this paper is unclear. I could not understand how such analysis contributes advance of representation learning. Further, the analysis of this paper is not convincing. -      I cannot believe the sum of SVD dimensionality of each feature maps becomes equals to the dimensionality to concatenated feature maps. As shown in in Fig.8, the estimated dimensions and original dimensions are very different. Although by looking some features maps, this rule might be hold as shown in Fig.5. However, the analysis is done on small examples without any theoretical analysis. -tThe authors experimented one trained CNN and tested on images on only three categories (Persian Cat, Container Ship, and Volcao). It is not clear if the same rules holds to other CNNs, and images of other categories.  ",11,180,15.0,5.192090395480226,99,1,179,0.0055865921787709,0.0585106382978723,-0.1882,57,22,25,11,8,3,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 2, 'ANA': 4, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,2,0,1,1,3,1,1,2,4,0,0,0,0,0,0,0,0,1,0,6,2,0,0.5720519306880117,0.3365497970158697,0.27790211125446745
ICLR2018-Sy3nGCYXz-R3,,"This paper try to analyze the intrinsic structure of VGG19 and give a new insight of deep neural networks. The authors propose to use SVD tools to estimate the dimension of the deep manifolds, and conduct experiments on three categories of ImageNet. The papers are written well and easy to follow. The analysis of manifold structure of DNN is important direction, but I am afraid novelty and insight of this work is not enough for acceptance. pros:  1. The paper is well written and easy to follow. 2. Manifold analysis of the intrinsic structure of DNN is a important direction for further study. cons:  1. SVD is a standard tool for subspace and manifold analysis for decades of years. I do not think using it in DNN is a big contribution. 2. The authors should explain why choosing VGG19 for analysis. Do other deep neural networks, such as Resnet, Googlenet, can  have the same phenomenon? 3. Why the authors choose Persian Cat, Container Ship, and Volcano in the experiments? Do other categories have the similar results? 4. The authors can indicate the application scenario of this work. For example, this work may guide to design better CNN structure for higher accuracy and lower computation cost. It may help the readers better understand the values of this work.     ",15,217,11.421052631578949,4.907317073170732,107,3,214,0.014018691588785,0.0266666666666666,0.9679,68,28,35,6,9,5,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 2, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 0, 'IMP': 3, 'CMP': 0, 'PNF': 0, 'REC': 1, 'EMP': 6, 'SUB': 0, 'CLA': 2}",0,2,0,0,1,5,2,1,0,2,2,4,0,1,1,0,3,0,0,1,6,0,2,0.644134732839686,0.5588425510975203,0.4034626027010595
ICLR2018-Sy4c-3xRW-R1,Reject,"This paper propose an adaptive dropout strategy for class logits. They learn a distribution q(z | x, y) that randomly throw class logits.  By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks. They learn the dropout distribution by variational inference with concrete relaxation. Overall I think this is a good paper. The technique sounds, the presentation is clear and I have not seen similar paper elsewhere (not 100% sure about the originality of the work though). Pro: * General algorithm  Con: * The experiment is a little weak. Only on CIFAR100 the proposed approach is much better than other approaches. I would like to see the results on more datasets. Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut.",12,132,13.2,5.314516129032258,91,2,130,0.0153846153846153,0.0583941605839416,0.7869,42,23,21,9,8,5,"{'ABS': 0, 'INT': 2, 'RWK': 2, 'PDI': 2, 'DAT': 2, 'MET': 6, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,2,2,2,2,6,1,1,0,0,0,3,0,0,0,1,0,2,1,0,3,1,0,0.5729717670470446,0.5569166329824315,0.3470118453189614
ICLR2018-Sy4c-3xRW-R2--annotated.txt,,"Pros - The proposed model is a nice way of multiplicatively combining two features :   one which determines which classes to pay attention to, and other that provides useful features for discrimination. - The adaptive component seems to provide improvements for small dataset sizes and large number of classes. Cons - One can easily see that if o_t(x; w)   0, then class t becomes neutral in the   classification and the gradients are not back-propagated from it.  : This does not seem to be true. Even if the logits are zero, the class would have a non-zero probability and would receive gradients. Do the authors mean exp(o_t(x;w))   0 ? - Related to the above, it should be clarified what is meant by dropping a   class. Is its logit set to zero or -infty ? Excluding a class from the softmax is equivalent to having a logit of -infty, not zero. However, from the equations in the paper it seems that the logit is set to zero. This would not result in excluding the unit. The overall effect would just be to raise the magnitude of logits across the entire softmax. - It seems that the model benefits from at least two separate effects - one is   the attention mechanism provided by the sigmoids, and the other is the stochasticity during training. Presently, it is not clear if only one of the components is providing most of the benefits, or if both things are useful. It would be great to compare this model to a non-stochastic one which just has the multiplicative effects applied in a deterministic way (during both training and testing). - The objective of the attention mechanism that sets the dropout mask seems to   be the same as the primary objective of classifying the input, and the attention mechanism is prevented from solving the task by adding an extra entropy regularization. It would be useful to explain more why this is needed. Would it not be fine if the attention mechanism did a perfect job of selecting the class ? Quality The paper makes relevant comparisons and is overall well-motivated. However, some aspects of the paper can be improved by adding more explanations. Clarity Some crucial aspects of the paper are unclear as mentioned above. Originality The main contribution of the paper is similar to multiplicative gating. The added stochasticity and the model ensembling interpretation is probably novel. However, experiments are insufficient to determine whether it is this novelty that contributes to improved performance or just the gating. Significance This paper makes incremental improvements and would be of moderate interest to the machine learning community. Typos : - In Eq 3, the numerator has z_t. Should that be z_y ? - In Eq 5, the denominator has z_y. Should that be z_t ?",29,447,17.88,4.969976905311778,196,5,442,0.0113122171945701,0.0292275574112734,0.9931,103,46,94,19,7,6,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 2, 'MET': 19, 'EXP': 4, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 1, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 18, 'SUB': 3, 'CLA': 3}",0,0,1,0,2,19,4,4,0,0,1,3,0,0,0,2,1,2,0,0,18,3,3,0.505056079457373,0.6777778363590563,0.35539141178017813
ICLR2018-Sy4c-3xRW-R3,Reject,"The paper discusses dropping out the pre-softmax logits in an adaptive manner. This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout. In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference. The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour. A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1. I would have liked to have seen results on ImageNet. I don't find (the too small) Figure 2 to be compelling evidence that our dropmax effectively prevents overfiting by converging to much lower test loss. The test loss in question looks like a noisy version of the base test loss with a slightly lower mean. There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage. Figure 3 illustrates the idea nicely. Which of the MNIST models from Table 1 was used? ",11,211,19.181818181818183,5.082474226804123,127,1,210,0.0047619047619047,0.0094339622641509,0.8823,57,29,32,11,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 2, 'MET': 2, 'EXP': 0, 'RES': 5, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 2, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 1}",0,1,1,1,2,2,0,5,4,0,0,1,0,0,0,0,0,1,2,0,6,1,1,0.5720075557352072,0.5587269116535737,0.3644703303829119
ICLR2018-SyGT_6yCZ-R1,Reject,"This paper deals with early stopping but the contributions are limited. This work would fit better a workshop as a preliminary result, furthermore it is too short. Following a short review section per section. Intro: The name SFC is misleading as the method consists in stopping early the training with an optimized learning schedule scheme. Furthermore, the work is not compared to the appropriate baselines. Proposal: The first motivation is not clear. The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks. Besides, the training time of a CNN on CIFAR10 or even ImageNet is now quite small(for reasonable architectures), which allows fast benchmarking. The second motivation, w.r.t. IB seems interesting but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done. The section 3 is quite long and could be compressed to improve the relevance of this experimental section. All the accuracies(unsup dict, unsup, etc) on CIFAR10/CIFAR100 are reported from the paper (Oyallon & Mallat, 2015), ignoring 2-3 years of research that leads to new numerical results. Furthermore, this supervised technique is only compared to unsupervised or predefined methods, which is is not fair and the training time of the Scattering Transform is not reported, for example. Finally, extracting features is mainly useful on ImageNet (for realistic images) and this is not reported here. I believe re-thinking new learning rate schedules is interesting, however I recommend the rejection of this paper.",16,259,16.1875,5.315789473684211,147,1,258,0.003875968992248,0.0307692307692307,0.956,71,32,49,22,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 1, 'MET': 7, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 1, 'EMP': 7, 'SUB': 6, 'CLA': 0}",0,1,2,2,1,7,2,1,0,0,0,4,0,1,0,1,0,2,1,1,7,6,0,0.6447205355199509,0.6710489563745219,0.449932094490433
ICLR2018-SyGT_6yCZ-R2,Reject,"This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. In the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets. Pros: The paper compares different classifiers on three datasets. Cons: - Considering an adaptive schedule of the learning decay is common practice in modern machine learning. Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch. - It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers. - There are many spelling errors - Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore. It is well known that CNN features are much better if enough data is available.  ",10,191,17.363636363636363,5.016304347826087,116,0,191,0.0,0.0203045685279187,0.4133,56,23,36,10,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 2, 'MET': 7, 'EXP': 0, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 1}",0,1,1,2,2,7,0,1,1,0,0,1,0,0,0,2,0,1,0,0,3,1,1,0.5730251147130286,0.5568445780097596,0.3604919244310922
ICLR2018-SyGT_6yCZ-R3,Reject,"I am not sure how to interpret this paper. The paper seems to be very thin technically, unless I missed some important details. Two proposals in the paper are:  (1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training, and  (2) Extract the penultimate layer output as features to train a conventional classifier such as SVM. I don't understand why (1) differs from other approaches, in the sense that one cannot simply reduce the number of epochs without hurting performance. And for (2), it is a relatively standard approach in utilizing CNN features. Essentially, if I understand correctly, this paper is proposing to prematurely stop training an use the intermediate feature to train a conventional classifier (which is not that away from the softmax classifier that CNNs usually use). I fail to see how this would lead to superior performance compared to conventional CNNs.",8,153,21.857142857142858,5.191489361702128,93,2,151,0.0132450331125827,0.0516129032258064,-0.5274,39,14,30,13,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 6, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,0,1,2,0,6,0,0,0,0,0,1,0,1,0,0,0,2,0,0,2,1,0,0.3584531175736362,0.3340724533253663,0.17490752885669514
ICLR2018-SyJS-OgR--R1,Accept,"This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way. On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy. The paper is interesting and easy to follow. I have several comments: 1.tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance. Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency? 2.tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models. It would be interesting to see a comparison or discussion.  [1] C Gulcehre, Mollifying Networks, 2016 3. tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?",8,169,21.125,5.425806451612903,93,0,169,0.0,0.0176470588235294,0.9711,47,24,32,4,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 7, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 5, 'SUB': 2, 'CLA': 0}",0,0,2,0,1,7,4,2,1,0,0,1,0,0,0,1,0,1,0,0,5,2,0,0.5018959298529274,0.4470389506921378,0.2836623660610511
ICLR2018-SyJS-OgR--R2,Accept,"I enjoyed reading the paper. This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets. In general I have a positive opinion about the paper, however, I'd like to ask for some clarifications. I'm not fully convinced by the interpretation of Eq. 5: ""... d is inversely proportional to the norm of the residual modules G(Yj)"". Since F(Yj) is not a constant, I think that d is inversely proportional to ||G(Yj)||/||F(Yj)||, however, in the interpretation the dependence on ||F(Yj)|| is ignored. Could the authors comment on that? Section 4. 1 "" Each cycle itself can be regarded as a training process, thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycle. "" Is there any empirical evidence for this? What would happen if the learning rate is not reset at the beginning of each cycle? Questions with respect to dynamical systems point of view: Eq. 4 assumes small value of h. However, for ResNet there is no guarantee that the h would be small (e. g. in Appendix C the values between 0.25 and 1 are used). Would the authors be willing to comment on the importance of the value of h? In figure 1, pooling (strided convolutions) are not depicted between network stages. I have one question w.r.t. feature maps dimensionality changes inside a CNN: how does pooling (or strided convolution) fit into dynamical systems view? Table 3 and 4. I assume that the training time unit is a minute, I couldn't find this information in the paper. Is the batch size the same for all models (100 for CIFAR and 32 for STL-10)? I understand that the models with different #Blocks have different capacity, for clarity, would it be possible to add # of parameters to each model? For multilevel method, would it be possible to show intermediate results in Table 3 and 4, e. g. at the end of cycle 1 and 2? I see these results in Figure 6, however, the plots are condensed and it is difficult to see the exact number at the end of each cycle. The citation (E, 2017) seems to be wrong, could the authors check it? ",20,386,17.545454545454547,4.781976744186046,170,2,384,0.0052083333333333,0.0358056265984654,0.9759,118,32,63,17,6,4,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 11, 'EXP': 5, 'RES': 2, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 1, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 14, 'SUB': 3, 'CLA': 1}",0,0,0,0,0,11,5,2,2,0,0,3,1,0,0,0,0,0,2,0,14,3,1,0.4315879703456272,0.4528048130378493,0.24092135374042084
ICLR2018-SyJS-OgR--R3,Accept,"This paper proposes a new method to train residual networks in which one starts by training shallow ResNets, doubling the depth and warm starting from the previous smaller model in a certain way, and iterating. The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation. This interpretation plays a role in the proposed training method by informing how the ""step sizes"" in the Euler discretization should change when doubling the depth of the network. The punchline of the paper is that the authors are able to achieve similar performance as ""full ResNet training"" but with significantly reduced training time. Overall, the proposed method is novel u2014 even though this idea of going from shallow to deep is natural for residual networks, tying the idea to the dynamical systems perspective is elegant. Moreover the paper is clearly written. Experimental results are decent u2014 there are clear speedups to be had based on the authors' experiments. However it is unclear if these gains in training speed are significant enough for people to flock to using this (more complicated) method of training. I only have a few small questions/comments: * A more naive way to do multi-level training would be to again iteratively double the depth, but perhaps not halve the step size.  This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint. * One thing I'm unclear on is how convergence was assessed... my understanding is that the training proceeds for a fixed number of epochs (?) - but shouldn't this also depend on the depth in some way? * Would the speedups be more dramatic for a larger dataset like Imagenet? * Finally, not being very familiar with multigrid methods from the numerical methods literature u2014 I would have liked to hear about whether there are deeper connections to these methods.   ",13,323,24.846153846153847,5.132686084142395,173,2,321,0.0062305295950155,0.027027027027027,0.99,84,43,62,17,7,4,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 1, 'DAT': 1, 'MET': 7, 'EXP': 8, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,0,3,1,1,7,8,2,0,0,0,1,0,0,0,0,1,2,0,0,7,0,1,0.5023212009221462,0.4482933516106923,0.2816051977796508
ICLR2018-SyL9u-WA--R1,Reject,"This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training. The proposed method improved two previous papers: 1) stronger expressive power than Mahammedi et al. (2017), 2) faster gradient update than Vorontsov et al. (2017) . The proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power. The experimental results also look promising. It would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe). If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum.",7,135,16.875,5.576,90,0,135,0.0,0.0145985401459854,0.5859,39,23,19,5,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 2, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 1, 'CLA': 0}",0,1,2,1,0,2,1,2,0,1,0,0,0,0,0,0,0,2,0,0,3,1,0,0.5003863819088781,0.3346944107602093,0.2535285485959769
ICLR2018-SyL9u-WA--R2,Reject,"This paper suggests a reparametrization of the transition matrix. The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks. The paper is well-written and authors explain related work adequately. The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary. The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact. I have two comments on the experiment section:  - Choice of experiments. Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common. For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs. For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported. - Stopping condition.  The plots suggest that the optimization has stopped earlier for some models. Is this because of some stopping condition or because of gradient explosion? Is there a way to avoid this? - Quality of figures. Figures are very hard to read because of small font. Also, the captions need to describe more details about the figures.",16,206,14.714285714285714,5.39,108,2,204,0.0098039215686274,0.0189573459715639,-0.9115,65,22,36,6,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 2, 'MET': 4, 'EXP': 5, 'RES': 2, 'TNF': 3, 'ANA': 0, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 2, 'PNF': 3, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 1}",0,1,0,1,2,4,5,2,3,0,1,1,0,0,0,0,1,2,3,0,9,1,1,0.6440908909193477,0.671882626550207,0.4552128590718953
ICLR2018-SyL9u-WA--R3,Reject,"The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN. My concerns with the paper include:   a) the paper says that the same method works for convolutional neural networks but I couldn't find anything about convolution. b) the theoretical analysis might be misleading --- clearly section 6.2 shouldn't have title ALL CRITICAL POINTS ARE GLOBAL MINIMUM because 0 is a critical point but it's not a global minimum. Theorem 5 should be phrased as   all critical points of the population risk that is non-singular are global minima. c) the paper should run some experiments on language applications where RNN is widely used d) I might be wrong on this point, but it seems that the GPU utilization of the method would be very poor so that it's kind of impossible to scale to large datasets?  ",6,141,28.2,5.0458015267175576,87,3,138,0.0217391304347826,0.081081081081081,-0.9696,38,18,24,7,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 4, 'EXP': 1, 'RES': 0, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,0,1,1,4,1,0,0,1,0,0,0,0,0,0,0,0,0,0,4,1,0,0.4293220049127825,0.2240880945267511,0.18221990829513252
ICLR2018-SyMvJrdaW-R1,Accept,"Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized. As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100. One thing that is currently not very clear to me is about the rotational symmetry. The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer. The rotation of the filters (as 2D images or images with depth) seem to be quite different from rotating a general N-dim vectors in an abstract Euclidean space. It would be helpful to make the description here more explicit and clear.",7,159,22.714285714285715,5.197368421052632,97,0,159,0.0,0.0377358490566037,0.8726,40,24,21,6,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 0, 'DAT': 1, 'MET': 3, 'EXP': 1, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 0}",0,1,1,0,1,3,1,2,0,1,0,0,0,0,0,0,0,2,0,0,4,0,0,0.5005560861202788,0.2242052570839411,0.22189161916084485
ICLR2018-SyMvJrdaW-R2,Accept,"Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100. While the experiments are not particularly impressive, I liked the originality of this paper. ",3,52,17.333333333333332,5.7,38,0,52,0.0,0.0188679245283018,0.0374,15,8,7,2,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 1, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,1,0,0,1,1,2,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0.3572290529345658,0.2222222222222222,0.16105267390550132
ICLR2018-SyMvJrdaW-R3,Accept,"The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space. This expression is used to develop a new layer called a ""warp layer"" which essentially tries to compute several layers of the residual network using the Taylor expansion expression u2014 however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with. Finally the authors stack these warp layers to create a ""warped resnet"" which they show does about as well as an ordinary ResNet but has better parallelization properties. To me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple. However there are several weaknesses to the paper (or maybe just things I didn't understand). First,  a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model, which I am afraid I simply was not able to follow. Some of the notation is confusing here u2014 for example, presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix, which the notation suggests to be the case. It is also never precisely spelled out what the final theoretical guarantee is (preferably the authors would do this in the form of a proposition or theorem). Throughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear. The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same. Finally the experimental results are okay but perhaps a bit preliminary. I have a few recommendations here: * It would be stronger to evaluate results on a larger dataset like ILSVRC. * The relative speed-up of WarpNet compared to ResNet needs to be better explained u2014 the authors break the computation of the WarpNet onto two GPUs, but it's not clear if they do this for the (vanilla) ResNet as well. In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch. Even in a streaming mode where images need to be evaluated one by one, there are ways to pipeline execution of the residual blocks, and I do not see any discussion of these alternatives in the paper. * In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion. It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values. A few remaining questions for the authors: * There is a parallel submission (presumably by different authors called ""Residual Connections Encourage Iterative Inference"") which contains some related insights. I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa? * On implementation - the authors mention using Tensorflow's auto-differentiation. My question here is u2014 are gradients being re-used intelligently as suggested in Section 3.1? * I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now. How much of the ideas offered in this paper would then generalize to non-resnet settings?  ",24,624,29.714285714285715,5.0033840947546535,288,6,618,0.0097087378640776,0.0347003154574132,0.9915,151,62,119,52,9,3,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 1, 'DAT': 3, 'MET': 12, 'EXP': 9, 'RES': 5, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 13, 'SUB': 4, 'CLA': 0}",0,1,2,1,3,12,9,5,0,2,0,1,0,0,0,0,0,3,0,0,13,4,0,0.6466990187760686,0.3413511771907933,0.32328421305643457
ICLR2018-SyProzZAW-R1,Accept,"Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks. This paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c. In this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables. The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2). This is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation). Overall, I like the paper. Pros: new and interesting result, theoretically sound. Cons: nothing major. Comments and clarifications: * What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary? Could you comment on this problem? * Is the assumption that sigma has Taylor expansion to order d tight? (That is, are there counter examples for relaxations of this assumption?) * As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs, but ReLUs network perform well in practice. Could you provide some further comments on this?  ",15,262,26.2,5.166666666666667,140,0,262,0.0,0.011070110701107,0.8431,84,41,41,7,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 10, 'EXP': 5, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 9, 'SUB': 1, 'CLA': 0}",0,0,1,0,0,10,5,3,0,0,0,2,0,1,0,0,0,1,0,0,9,1,0,0.4313247633557947,0.3383089928120771,0.22010400832196175
ICLR2018-SyProzZAW-R2,Accept,"The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations. It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial. By focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question. Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question. Here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation. Theorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by Lin et al that I did not verify). The idea being to construct a superposition of Taylor approximations of the individual monomials. Here it would be good to be more specific about the domain. Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g. the origin. The paper mentions that ``the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series''. However, a ReLU can also be approximated by a smooth function and a Taylor series. Theorem 4.1 seems to be implied by Theorem 4.2. Similarly, parts of Section 4.2 seem to follow directly from the previous discussion. In page 1 ```existence proofs' without explicit constructions'' This is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions.   ",14,289,16.055555555555557,5.364312267657993,142,2,287,0.0069686411149825,0.0205479452054794,0.9375,80,43,40,20,5,3,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,2,3,2,0,10,0,2,0,0,0,0,0,0,0,0,0,2,0,0,4,1,0,0.3596960826555677,0.3353163681950522,0.18127729636752282
ICLR2018-SyProzZAW-R3,Accept,"Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons. The paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth. +ves: Explaining the power of depth in NNs is fundamental to an understanding of deep learning. The paper is very easy to follow. and the proofs are clearly written. The theorems provide exponential gaps for very simple polynomial functions. -ves: 1. My main concern with the paper is the novelty of the contribution to the techniques. The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas. 2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.). 3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(u03b4x)| <  u03b4^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation. Despite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR.",11,242,17.285714285714285,5.251082251082251,124,0,242,0.0,0.0040816326530612,0.7612,79,34,29,12,5,8,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 0, 'DAT': 0, 'MET': 8, 'EXP': 1, 'RES': 6, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 2, 'IMP': 1, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 5, 'SUB': 1, 'CLA': 1}",0,0,2,0,0,8,1,6,0,0,0,1,0,0,1,2,1,2,1,0,5,1,1,0.359253199305281,0.8915389887699688,0.32101401626337916
ICLR2018-SySaJ0xCZ-R1,Reject,"This paper proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources. The high-level idea is similar to the evolution method of [Real et al. 2017], but the mutation preserves net2net properties, which means the mutated network does not need to retrain from scratch. Compared to other papers on neural architecture search, the required computational resource is impressively small: close to state-of-the-art result in one day on a single GPU. However, it is not clear to me what contribute to the massive improvement of speed. Is it due to the network morphing that preserve equality? Is it due to a good initial network structure? Is it due to the well designed mutation operations? Is it due to the simple hill climbing procedure (basically evolution that only preserve the elite)? Is it due to a well crafted search space that is potentially easier? The experiments in this paper does not provide enough evidence to tease apart the possible causes of this dramatic reduction on computational resources. And the comparisons to other papers seems not fair since they all operate on different search space. In summary, getting net2net to work for architecture search is interesting. And I love the results.  These are very impressive numbers for neural architecture search. However, I am not convinced that the improve is resulted from a better algorithm. I would suggest that the paper carefully evaluates each component of the algorithm and understand why the proposed method takes far less computational resources.",16,255,21.25,5.288617886178862,135,1,254,0.0039370078740157,0.04296875,0.9894,65,42,45,19,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 4, 'PDI': 1, 'DAT': 1, 'MET': 10, 'EXP': 1, 'RES': 4, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 12, 'SUB': 2, 'CLA': 0}",0,0,4,1,1,10,1,4,0,0,0,0,0,0,0,0,0,3,0,0,12,2,0,0.4312317269807709,0.3405158667393074,0.21361871322259776
ICLR2018-SySaJ0xCZ-R2,Reject,"This paper presents a method to search neural network architectures at the same time of training. It does not require training from scratch for each architecture, thus dramatically saves the training time. The paper can be understood with no problem. Moderate novelty, network morphism is not novel, applying it to architecture search is novel. Pros: 1. The required time for architecture searching is significantly reduced. 2. With the same number or less of parameters, this method is able to outperform previous methods, with much less time. However, the method described is restricted in the following aspects. 1. The accuracy of the training set is guaranteed to ascend because network morphism is smooth and number of params is always increasing, this also makes the search greedy , which could be suboptimal. In addition, the algorithm in this paper selects the best performing network at each step, which also hampers the discover of the optimal model. n 2. Strong human prior, network morphism IV is more general than skip connection, for example, a two column structure belongs to type IV. However, in the implementation, it is restricted to skip connection by addition. This choice could be motivated from the success of residual networks. This limits the method from discovering meaningful structures. For example, it is difficult to discover residual network denovo. This is a common problem of architecture searching methods compared to handcrafted structures. 3. The comparison with Zoph & Le is not fair because their controller is a meta-network and the training happens only once. For example, the RNNCell discovered can be fixed and used in other tasks, and the RNN controller for CNN architecture search could potentially be applied to other tasks too (though not reported). ",19,284,12.347826086956522,5.306569343065694,139,0,284,0.0,0.0174216027874564,0.8602,84,28,54,18,5,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 14, 'EXP': 5, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 3, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,1,0,0,0,14,5,2,0,0,0,3,0,0,0,3,0,4,0,0,10,0,0,0.3608866772977031,0.3393726530875263,0.1831883843540765
ICLR2018-SySaJ0xCZ-R3,Reject,"This paper proposes a variant of neural architecture search. It uses established work on network morphisms as a basis for defining a search space. Experiments search for effective CNN architectures for the CIFAR image classification task. Positives:  (1) The approach is straightforward to implement and trains networks in a reasonable amount of time. (2) An advantage over prior work, this approach integrates architectural evolution with the training procedure.  Networks are incrementally grown; child networks are initialized with learned parameters from their parents. This eliminates the need to restart training when making an architectural change, and drastically speeds the search. Negatives:  (1) The state-of-the-art CNN architectures are not mysterious or difficult to find, despite the paper's characterization of them being so. Indeed, ResNet and DenseNet designs are both guided by extremely simple principles: stack a series of convolutional layers, pool occasionally, and use some form of skip-connection throughout. The need for architectural search is unclear. (2) The proposed search space is boring. As described in Section 4, the possibly evolutionary changes are limited to deepening the network, widening the network, and adding a skip connection. But these are precisely the design aspects that have been well-explored by human trial and error and for which good rules of thumb are already available. (3) As a consequence of (1) and (2), the result is essentially rigged. Since only depth, width, and skip connections are considered, the end network must end up looking like a ResNet or DenseNet, but with some connections pruned. There is no way to discover a network outside of the principled design space articulated in point (1) above. Indeed, the discovered network diagrams (Figures 4 and 5) fall in this space. (4) Performance is worse than the best hand-designed baselines. One would hope that, even if the search space is limited, the discovered networks might be more efficient or higher performing in comparison to the human designs which fall within that same space. However, the results in Tables 3 and 4 show this not to be the case. The best human designs outperform the evolved networks. Moreover, the evolved networks are woefully inefficient in terms of parameter count. Together, these negatives imply the proposed approach is not yet at the point of being useful in practice. I think further work is required (perhaps expanding the search space) to resolve the current limitations of automated architecture search. Misc:  Tables 3 and 4 would be easier to parse if resources were simply reported in terms of total GPU hours.",25,417,16.68,5.483375959079284,223,4,413,0.009685230024213,0.0213776722090261,0.9741,121,42,72,24,8,5,"{'ABS': 0, 'INT': 2, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 14, 'EXP': 4, 'RES': 3, 'TNF': 3, 'ANA': 0, 'FWK': 1, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 2, 'REC': 0, 'EMP': 14, 'SUB': 1, 'CLA': 0}",0,2,3,0,0,14,4,3,3,0,1,0,0,2,0,0,1,4,2,0,14,1,0,0.5752934787919708,0.5640540588038875,0.36414665872738533
ICLR2018-SySisz-CW-R1,Reject,"This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN. The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences. By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies). This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework. The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework. They also claim that this analysis can be used to improve the performance of the models. Pros: -- SDR characterisation of the convolutional filters is interesting -- The authors show that filters with different characteristics are responsible for different aspects of image modelling Cons: -- The authors do not actually demonstrate how their analysis can be used to improve VAEs or GANs -- Their proposed SDR analysis does not actually find much difference between the generator and the discriminator of the GAN -- The clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form). Grammatical and spelling mistake are frequent. More background information could be helpful in section 2.2. All figures (but in particular Figure 3) need more informative captions -- The authors talk a lot about disentangling in the introduction, but this does not seem to be followed up in the rest of the text. Furthermore, they are missing a reference to beta-VAE (Higgins et al, 2017) when discussing VAE-based approaches to disentangled factor learning In summary, the paper is not ready for publication in its current form. The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs.",18,353,27.15384615384616,5.46865671641791,179,1,352,0.002840909090909,0.0083565459610027,0.8118,106,34,56,13,9,6,"{'ABS': 0, 'INT': 2, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 5, 'EXP': 1, 'RES': 4, 'TNF': 1, 'ANA': 5, 'FWK': 0, 'OAL': 4, 'BIB': 1, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 1, 'EMP': 2, 'SUB': 5, 'CLA': 2}",0,2,0,1,0,5,1,4,1,5,0,4,1,0,1,0,0,0,2,1,2,5,2,0.6442856698663421,0.6678580341294529,0.4583548669046136
ICLR2018-SySisz-CW-R2,Reject,"This work exploits the causality principle to quantify how the weights of successive layers adapt to each other. Some interesting results are obtained, such as enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures . Generally, the result is interesting and the presentation is easy to follow. However, the proposed approach and the experiments are not convincible enough. For example,  it is hard to obtain the conclusion more independence lead to better performance from the experimental results. Maybe more justifications are needed.",6,90,15.0,5.7444444444444445,60,2,88,0.0227272727272727,0.0326086956521739,0.9295,22,16,16,5,6,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 1, 'EXP': 3, 'RES': 3, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,1,3,3,0,0,0,1,0,0,0,0,0,0,1,0,4,1,0,0.4288552239402649,0.3351992056378622,0.21351560015138465
ICLR2018-SySisz-CW-R3,Reject,"The paper presents an application of a measure of dependence between the input power spectrum and the frequency response of a filter (Spectral Density Ratio from [Shajarisales et al 2015]) to cascades of two filters in successive layers of deep convolutional networks. The authors apply their newly defined measure to DCGANs and plain VAEs with ReLUs, and show that dependency between successive layers may lead to bad performance. The paper proposed a possibly interesting approach, but I found it quite hard to follow, especially Section 4, which I thought was quite unstructured. Also Section 3 could be improved and simplified. It would be also good to add some more related work.  I'm not an expert, but I assume there must be some similar idea in CNNs. From my limited point of view, this seems like a sound, novel and potentially useful application of a interesting idea. If the writing was improved, I think the paper may have even more impact. Smaller details: some spacing issues, some extra punctuation (pg 5 "". . Hence""), a typo (pg. 7 ""training of the VAE did not lead to values as satisfactory AS what we obtained with the GAN"") ",10,193,16.083333333333332,5.022471910112359,123,5,188,0.026595744680851,0.0456852791878172,0.9762,54,24,30,14,6,5,"{'ABS': 0, 'INT': 0, 'RWK': 2, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 2, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 1, 'CLA': 2}",0,0,2,1,0,3,0,1,0,2,0,2,0,0,0,0,1,0,1,0,6,1,2,0.4292224388900487,0.5587464778006245,0.26375917970002455
ICLR2018-SyVOjfbRb-R1,Reject,"Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. I found the paper relatively creative and generally well-founded and well-argued. Nice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product.  Experiments: appreciated the wall clock timings. SGD comparison: ""fixed learning rate. "" Didn't see how the initial (well constant here) step size was tuned? Why not use the more standard 1/t decay? Fig 1: Suspicious CIFAR100 that test objective is so much better than train objective? Legend backwards? Why were so many of the chosen datasets have so few training examples? Paper is mostly very clearly written, though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through.  ",12,153,21.857142857142858,5.468965517241379,120,0,153,0.0,0.0189873417721519,0.9268,41,28,31,16,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 1, 'MET': 5, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 4, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 1, 'CLA': 2}",0,1,0,1,1,5,2,0,0,0,0,4,0,0,0,0,0,0,1,0,8,1,2,0.4297946492424276,0.4488792815591992,0.24303354388802678
ICLR2018-SyVOjfbRb-R2,Reject,"The main idea in the paper is fairly simple:   The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss. The basic form of SGD selects an example uniformly. Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient .    A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015]. With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).    For the current parameter vector theta,  we would like to sample examples so that the probability of sampling larger inner products is larger. The paper uses LSH structures, computed over the set of examples, to quickly sample examples with large inner products with the current parameter vector theta. Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity. So we select examples in the same LSH bucket as theta (for rubstness, we use multiple LSH mappings). strengths:  simple idea that can work well in the context of sampling examples for SGD  weaknesses:     The novelty in the paper is limited. The use of LSH for sampling is a common technique to sample more similar vectors with higher probability. There are theorems,  but they are trivial, straightforward applications of importance sampling. The paper is not well written. The presentation is much more complex that need be. References to classic weighted sampling are The application is limited to certain loss functions for which we can compute LSH structures. This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness. ",17,298,17.529411764705884,5.160958904109589,136,1,297,0.0033670033670033,0.0031446540880503,-0.4803,98,37,44,12,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 10, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0,1,1,1,0,10,0,2,0,0,0,3,0,0,0,1,0,0,1,0,3,0,1,0.4309696971043011,0.4456883593141303,0.24680078790634807
ICLR2018-SyVOjfbRb-R3,Reject,"The main contribution of this work is just a combination of LSH schemes and SGD updates. Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation. The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound. Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity). Strengths:  a sound approach; a simple and straightforward idea that is shown to work well in evaluations. Weaknesses:  1. The phrase of computational chicken-and-egg loop in the title and also in the main body is misleading and not accurate. The so-called chicken-and-egg"" issue concerns the causality dilemma: two causally related things, which comes the first. In the paper, the authors concerned more accurate gradients and faster convergence; their causality is very clear (the first leads to the second), and there is no causality dilemma. Even from a computational perspective, SDG schemes aim for computational efficiency and stochastic makes the convergence slow down are not a causality dilemma. The reason behind is that the latter is the cost of the first one, just the old saying that there is no such thing as a free lunch. Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by twisted and unnatural logics. 2. The depth is so limited. Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided. The theorems (Theorems 1~3) are trivial, with loose relations with LSH. t  3. The LSH schemes are not correctly referred to. Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS. It is not in depth to assume any known LSH scheme in Alg. 2. Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity). 4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase computational chicken-and-egg loop, the organization and presentation of the whole manuscript are poor. 5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully. ",20,382,13.642857142857142,5.276712328767124,187,1,381,0.0026246719160104,0.0103626943005181,0.01,97,57,62,30,7,4,"{'ABS': 0, 'INT': 6, 'RWK': 1, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 3, 'REC': 0, 'EMP': 13, 'SUB': 2, 'CLA': 1}",0,6,1,1,0,11,0,2,0,1,0,2,0,0,0,0,0,0,3,0,13,2,1,0.5027306364851839,0.4521377480184883,0.2876744809552049
ICLR2018-SyXNErg0W-R1,Reject,"After the rebuttal:  I do not think I had a major misunderstanding of the paper. I was aware that the features mostly refers to the inputs to softmax. In my point 4, I was suggesting that in order to have clustering performance, one might alternatively work on the softmax outputs instead of the inputs. My opinion on this paper remains, and I think the contribution of this paper to machine learning is not very clearer at the current stage. It might be the case that the considered scenarios indeed happen in computer vision related problems, but I am not an expert in that regard.                                                                           This paper proposes a regularization to the softmax layer, which try to make the distribution of feature representation (inputs fed to the softmax layer) more meaningful according to the Euclidean distance. The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean, so the features are encouraged to lie close to a sphere. Overall, the proposed method is a relatively simple tweak to softmax. The authors show that empirically, features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks. My main concern with this paper is the motivation: what are the practical scenarios in which one would want to used proposed method? 1. It is true that features learned with the pure softmax loss may not presents the ideal  similarity under the  Euclidean metric (e.g. the problem depicted in Figure 1),  because they are not trained to do so: their purpose is just to predict the correct label. While the proposed regularization does lead to a nicer Euclidean geometry, there is not sufficient motivation and evidence showing this regularization improves classification accuracy. 2. In table 2, the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage. But this does not matter since you already use the labels in the softmax loss. 3. I can not easily think of scenarios in which, we would like to perform KNN in the feature space (Table 3) after training a softmax layer. In fact, Table 3 shows KNN is almost always worse than softmax in terms of classification accuracy. 4. Running kmeans or agglomerative clustering in the feature space (Table 5) *using the Euclidean metric* is again ill-posed, because the softmax layer is not trained to do this. If one really wants good clustering performance, one shall always try to learn a good metric, or , why do not you perform clustering on the softmax output (a probability vector?)  5.  The experiments on adversarial robustness and face verification seems more interesting to me, but the tasks were not carefully explained for someone not familiar with that literature. Perhaps for these tasks, multi-class classification is not the most correct objective, and maybe the proposed regularization can help, but the motivations are not given.     ",22,483,19.32,5.160087719298246,219,9,474,0.0189873417721519,0.0385964912280701,0.8438,128,43,88,39,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 15, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 3}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 2, 'CLA': 0}",0,1,0,1,0,15,2,1,1,0,0,2,0,3,1,0,0,0,0,0,15,2,0,0.5750815317576301,0.3421474139294564,0.28659290169772544
ICLR2018-SyXNErg0W-R2,Reject,"The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space. The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class. The proposed loss has been compared extensively against a number of closely related approaches in methodology. Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means. Pros:   - The idea of isotropic normalization for enhancing compactness of class is well motivated - The paper is mostly clearly organized and presented. - Numerical study shows some promise of the proposed method. Cons:  -  The novelty of method is mostly incremental given the prior work of (Wen et al., 2016) which has provided a slightly different isotropic variant of softmax loss. - The training procedure of the proposed method remains unclear in this paper.    ",9,162,18.0,5.411392405063292,96,0,162,0.0,0.0171428571428571,-0.6597,55,21,25,7,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 1, 'MET': 5, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,1,2,1,5,1,1,0,0,0,1,0,0,0,1,0,2,1,0,4,0,1,0.5724886397455659,0.5575385904172745,0.3615457628923118
ICLR2018-SyXNErg0W-R3,Reject,"In the centre loss, the centre is learned. Now it's calculated as the average of the last layer's features To enable training with SGD, the authors calculate the centre within a mini batch",3,33,16.5,4.6875,25,0,33,0.0,0.0303030303030303,-0.3182,10,2,7,1,2,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 0}",0,0,0,0,0,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0.1431073349709275,0.1111111111111111,0.057077102185130194
ICLR2018-SyhcXjy0Z-R1,Reject,"The paper is relatively clear to follow, and implement. The main concern is that this looks like a class project rather than a scientific paper. For a class project this could get an A in a ML class! In particular, the authors take an already existing dataset, design a trivial convolutional neural network, and report results on it. There is absolutely nothing of interest to ICLR except for the fact that now we know that a trivial network is capable of obtaining 90% accuracy on this dataset.",5,87,21.75,4.936708860759493,55,0,87,0.0,0.0114942528735632,0.6185,22,10,12,5,4,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 2, 'MET': 2, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 2, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 1, 'SUB': 0, 'CLA': 1}",0,0,0,0,2,2,0,2,0,0,0,3,0,0,2,0,0,0,0,0,1,0,1,0.2861471080767744,0.3333429992443015,0.14523524235095595
ICLR2018-SyhcXjy0Z-R2,Reject,"As one can see by the title, the originality (application of DCNN) and significance (limited to ATM domain) is very limited. If this is still enough for ICLR, the paper could be okay. However, even so one can clearly see that the architecture, the depth, the regularization techniques, and the evaluation are clearly behind the state of the art. Especially for this problem domain, drop-out and data augmentation should be investigated. Only one dataset is used for the evaluation and it seems to be very limited and small. Moreover, it seems that the same subjects (even if it is other pictures) may appear in the training set and test set as they were randomly selected. Looking into the referece (to get the details of the dataset -  from a workshop of the IEEE International Conference on Computer Vision Workshops (ICCVW) 2017) reveals, that it has only 25 subjects and 10 disguises . This makes it even likely that the same subject with the same disguise appears in the training and test set. A very bad manner, which unfortunately is often performed by deep learning researchers with limited pattern recognition background, is that the accuracy on the test set is measured for every timestamp and finally the highest accuracy is reported. As such you perform an optimization of the paramerter #iterations on the test set, making it a validation set and not an independent test set. Minor issues: make sure that the capitalization in the references is correct (ATM should be capital, e.g., by putting {ATM} - and many more things).",11,257,23.363636363636363,4.856573705179283,138,6,251,0.0239043824701195,0.0268199233716475,-0.6451,71,22,40,19,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 6, 'MET': 3, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 1, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 1, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 1, 'SUB': 3, 'CLA': 0}",0,1,2,0,6,3,2,0,0,0,0,1,1,1,1,1,1,2,1,0,1,3,0,0.5722760930497008,0.7781082933516107,0.4462636518754912
ICLR2018-SyhcXjy0Z-R3,Reject,"This paper is an application paper on detecting when a face is disguised, however it is poorly written and do not contribute much in terms of novelty of the approach. The application domain is interesting, however it is simply a classification problem The paper is written clearly (with mistakes in an equation), however, it does not contribute much in terms of novelty or new ideas. To make the paper better, more empirical results are needed. In addition, it would be useful to investigate how this particular problem is different than a binary classification problem using CNNs.",8,96,24.0,5.064516129032258,58,0,96,0.0,0.0104166666666666,-0.128,24,7,21,12,6,4,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 3, 'DAT': 0, 'MET': 2, 'EXP': 0, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 2}",0,1,1,3,0,2,0,1,0,0,0,2,0,0,0,2,0,1,0,0,3,0,2,0.4289856392169753,0.4458146019695025,0.24391538409472885
ICLR2018-Syhr6pxCW-R1,Accept,"Overall I like the paper and the results look nice in a diverse set of datasets and tasks such as edge-to-image, super-resolution, etc. Unlike the generative distribution sampling of GANs, the method provides an interesting compositional scheme, where the low frequencies are regressed and the high frequencies are obtained by copying patches from the training set. In some cases the results are similar to pix-to-pix (also in the numerical evaluation) but the method allows for one-to-many image generation, which is a important contribution. Another positive aspect of the paper is that the synthesis results can be analyzed, providing insights for the generation process. While most of the paper is well written, some parts are difficult to parse. For example, the introduction has some parts that look more like related work (that is mostly a personal preference in writting). Also in Section 3, the paragraph for distance functions do not provide any insight about what is used, but it is included in the next paragraph (I would suggest either merging or not highlighting the paragraphs). Q: The spatial grouping that is happening in the compositional stage, is it solely due to the multi-scale hypercolumns? Would the result be more inconsistent if the hypercolumns had smaller receptive field? Q: For the multiple outputs, the k neighbor is selected at random? ",10,218,27.25,5.277511961722488,131,0,218,0.0,0.0136986301369863,0.9284,61,27,38,9,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 6, 'EXP': 2, 'RES': 4, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 2, 'REC': 0, 'EMP': 7, 'SUB': 0, 'CLA': 1}",0,1,0,0,1,6,2,4,0,1,0,3,0,0,0,0,0,0,2,0,7,0,1,0.5015950996548615,0.3371266468661945,0.25498921153870846
ICLR2018-Syhr6pxCW-R2,Accept,"This paper presents a pixel-matching based approach to synthesizing RGB images from input edge or normal maps. The approach is compared to Isola et al's conditional adversarial networks, and unlike the conditional GAN, is able to produce a diverse set of outputs. Overall, the paper describes a computer visions system based on synthesizing images, and not necessarily a new theoretical framework to compete with GANs. With the current focus of the paper being the proposed system, it is interesting to the computer vision community. However, if one views the paper in a different light, namely showing some ""blind-spots"" of current conditional GAN approaches like lack of diversity, then it can be of much more interest to the broader ICLR community. Pros:  Overall the paper is well-written Makes a strong case that random noise injection inside conditional GANs does not produce enough diversity Shows a number of qualitative and quantitative results Concerns about the paper: 1.) It is not clear how well the proposed approach works with CNN architectures other than PixelNet 2.) Since the paper used ""the pre-trained PixelNet to extract surface normal and edge maps"" for ground-truth generation, it is not clear whether the approach will work as well when the input is a ground-truth semantic segmentation map. 3.) Since the paper describes a computer-vision image synthesis system and not a new theoretical result, I believe reporting the actual run-time of the system will make the paper stronger.  Can PixelNN run in real-time? How does the timing compare to Isola et al's Conditional GAN? Minor comments: 1.) The paper mentions making predictions from ""incomplete"" input several times, but in all experiments, the input is an edge map, normal map, or low-resolution image. When reading the manuscript the first time, I was expecting experiments on images that have regions that are visible and regions that are masked out. However, I am not sure if the confusion is solely mine, or shared with other readers. 2.) Equation 1 contains the norm operator twice, and the first norm has no subscript, while the second one has an l_2 subscript. I would expect the notation style to be consistent within a single equation (i.e., use ||w||_2^2, ||w||^2, or ||w||_{l_2}^2)  3.) Table 1 has two sub-tables: left and right. The sub-tables have the AP column in different places. 4.) ""Dense pixel-level correspondences"" are discussed but not evaluated. ",20,393,26.2,5.220708446866485,204,0,393,0.0,0.0251889168765743,0.1956,118,57,66,18,10,6,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 2, 'DAT': 0, 'MET': 10, 'EXP': 4, 'RES': 2, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 2, 'CMP': 2, 'PNF': 1, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 1}",0,1,2,2,0,10,4,2,1,1,0,3,0,1,0,0,2,2,1,0,8,2,1,0.7170823579579526,0.6712922444245267,0.5033541642474518
ICLR2018-Syhr6pxCW-R3,Accept,"This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets. Pros: - Simple approach based on nearest-neighbors, likely easier to train compared to GANs. - Scales to high-resolution images. Cons: - Requires a potentially costly search procedure to generate images. - Seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task.",5,70,14.0,6.088235294117647,52,2,68,0.0294117647058823,0.0405405405405405,0.6808,28,8,13,2,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 1, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,0,0,1,3,2,1,1,0,0,0,0,0,0,0,0,1,1,0,3,0,0,0.3577294371621352,0.3345772482030192,0.17658699530946
ICLR2018-SyjsLqxR--R1,Reject,"Summary:  This paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations. Experiments use a (previously published) iterative fast-gradient-sign-method and use a Resnet on CIFAR. The authors conclude that in this experimental setting: - AT seems to defend models against shared dx's. - This is visible on universal perturbations, which become less effective as more AT is applied. - AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change. - Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT. Pro: - Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking. - The visualizations of universal perturbations as they change during AT are nice. - The basic observation wrt the behavior of AT is clearly communicated. Con: - The experiments performed are interesting directions, although unfocused and rather limited in scope. For instance, does the same phenomenon happen for different datasets? Different models? - What happens when we use adversarial attacks different from FGSM? Do we get similar results? - The papers lacks a more in-depth theoretical analysis. Is there a principled reason AT+FGSM defends against universal perturbations? Overall: - As is, it seems to me the paper lacks a significant central message (due to limited and unfocused experiments) or significant new theoretical insight into the effect of AT. A number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigations.  Detailed: - ",18,280,17.5,5.87593984962406,157,2,278,0.0071942446043165,0.0101351351351351,-0.9223,75,49,50,13,7,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 2, 'DAT': 2, 'MET': 11, 'EXP': 4, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 4, 'CLA': 0}",0,1,0,2,2,11,4,3,0,1,0,0,0,0,0,0,0,0,0,0,10,4,0,0.5029673029001408,0.2281398686607733,0.2301036586873172
ICLR2018-SyjsLqxR--R2,Reject,"This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples. It also analyses how adversarial training affects detection. The robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem. The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect. It's also unclear from which epoch the adversarial examples were generated from in figure 5. Further the transformations in figure 5 are limited to artificially controlled situations, it would be much more interesting to see how the destruction rate changes under real-world test scenarios. The results on the detector are not that surprising since previous work has shown that detectors can learn to classify adversarial examples and the additional finding that they can detect adversarial examples for an adversarially trained model doesn't seem surprising. There is also no analysis of what happens for adversarial examples for the detector. Also, it's not clear from section 3.1 what inputs are used to generate the adversarial examples. Are they a random sample across the whole dataset?  Finally, the paper spends significant time on describing MaxMin and MinMax and the graphical visualizations but the paper fails to show these graphical profiles for real models.",10,243,24.3,5.644351464435147,130,0,243,0.0,0.0450819672131147,-0.9555,61,36,43,18,7,3,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 4, 'TNF': 1, 'ANA': 1, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 1, 'REC': 0, 'EMP': 6, 'SUB': 2, 'CLA': 0}",0,1,1,2,0,4,0,4,1,1,0,0,0,0,0,0,0,0,1,0,6,2,0,0.5009769818813381,0.3365497970158697,0.24666422170729266
ICLR2018-SyjsLqxR--R3,Reject,"This paper investigates the effect of adversarial training. Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against shared adversarial perturbation, in particular against universal perturbation. In contrast, it is less effective to protect against singular perturbations. Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective. Finally, they show that singular perturbations can be easily detected. I like the message conveyed in this paper. However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are. Moreover, is CIFAR 10 experiments conclusive enough. ",8,114,12.666666666666666,6.081081081081081,69,1,113,0.0088495575221238,0.017391304347826,0.7428,28,16,24,12,5,1,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 0, 'DAT': 3, 'MET': 4, 'EXP': 2, 'RES': 2, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 0}",0,0,0,0,3,4,2,2,0,0,0,1,0,0,0,0,0,0,0,0,3,0,0,0.3581075126484162,0.112355025980797,0.14486423270320958
ICLR2018-SyqShMZRb-R1,Accept,"The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable. The idea presented seems to have merit , however, I found the presentation lacking. Many sentences are poorly written making the paper hard to read, especially when not familiar with the presented methods. The experimental section could be organized better. I didn't like that two types of experiment are now presented in parallel. Finally, the paper stops abruptly without any final discussion and/or conclusion. ",7,87,12.428571428571429,5.729411764705882,69,1,86,0.0116279069767441,0.0224719101123595,0.6798,20,9,22,11,5,3,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 2, 'DAT': 1, 'MET': 1, 'EXP': 2, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 4, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0,0,0,2,1,1,2,0,0,0,0,3,0,0,0,0,0,0,4,0,2,0,1,0.3573791904125227,0.3341399975395863,0.17955210844331165
ICLR2018-SyqShMZRb-R2,Accept,"Let me first note that I am not very familiar with the literature on program generation,  molecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess. This paper proposes to include additional constraints into a VAE which generates discrete sequences,  namely constraints enforcing both semantic and syntactic validity. This is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones. These semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar. The authors evaluate their methods on two tasks, program generation and molecule generation. Their method makes use of additional prior knowledge of semantics, which seems task-specific and limits the generality of their model. They report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al. However, it isn't clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al, whereas Kusner et. al do not make any mention of this. The baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though. Can the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format? Typos: - Page 5: while in sampling procedure -> while in the sampling procedure - Page 6: a deep convolution neural networks -> a deep convolutional neural network - Page 6: KL-divergence that proposed in -> KL-divergence that was proposed in  - Page 6: since in training time -> since at training time - Page 6: can effectively computed -> can effectively be computed - Page 7: reset for training -> rest for training ",16,291,19.4,5.21505376344086,146,2,289,0.0069204152249134,0.0260586319218241,0.7117,102,29,43,11,8,3,"{'ABS': 0, 'INT': 1, 'RWK': 5, 'PDI': 0, 'DAT': 2, 'MET': 7, 'EXP': 1, 'RES': 1, 'TNF': 2, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 4, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 0, 'CLA': 6}",0,1,5,0,2,7,1,1,2,0,0,0,0,1,0,0,0,4,0,0,2,0,6,0.5733110644214154,0.3347124537940165,0.28717226469996676
ICLR2018-SyqShMZRb-R3,Accept,"NOTE:   Would the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset? Fair comparison of the data is a serious concern. I have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and I would like to know the author's response. *Overview*  This paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose ""stochastic lazy attributes"", which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid. Using Bayesian Optimization, search over this space can yield decodings with targeted properties. Many of the ideas presented are novel. The results presented are state-of-the art. As noted in the paper, the generation of syntactically and semantically valid data is still an open problem. This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning. *Remarks on methodology*  By initializing a decoding by ""guessing"" a value, the decoder will focus on high-probability starting regions of the space of possible structures. It is not clear to me immediately how this will affect the output distribution. Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space. Does this sacrifice exploration for exploitation in some quantifiable way? Some exploration of this issue or commentary would be valuable. *Nitpicks*  I found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background. In my opinion, this section could benefit from a little more expansion and conceptual definition. The first 3 sections of the paper are very clearly written, but the remainder has many typos and grammatical errors (often word omission). The draft could use a few more passes before publication. ",20,371,20.61111111111111,5.650568181818182,210,2,369,0.005420054200542,0.0212201591511936,0.9738,117,48,54,15,5,7,"{'ABS': 0, 'INT': 0, 'RWK': 0, 'PDI': 1, 'DAT': 5, 'MET': 9, 'EXP': 0, 'RES': 5, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 10, 'SUB': 2, 'CLA': 2}",0,0,0,1,5,9,0,5,0,0,0,3,0,0,0,2,0,1,1,1,10,2,2,0.3596024021996473,0.7836083138550581,0.28354962089907026
ICLR2018-Syr8Qc1CW-R1,Reject,"This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes. Pros:  + The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel. + A theoretical guarantee of the efficiency of an aspect of the proposed method is given. Cons:  - The results are not very appealing visually.  The results from the proposed method do not seem much better than the baselines. What is the objective for the images in Fig. 4?  For example I'm looking at the bottom right, and that image looks more like a merger of images, than a modification of the image in the top-left but adding the attributes of choice. - Quantitative results are missing. - Some unclarity in the description of the method; see below. Questions/other: - What is meant by implicit models? By do not anchor a specific meaning into the disentanglement? By circumscribed in two image domains? - Why does the method require two images? - In the case of images, what is a dominant vs recessive pattern? - It seems artificial to enforce that the attribute-irrelevant part [should] encode some information of images. - Why are (1, 0) and (1, 1) not useful pairs? - Need to be more specific: use some channels to encode the id information.  ",17,213,17.75,5.2227722772277225,113,2,211,0.0094786729857819,0.0260869565217391,-0.4811,61,18,40,9,7,5,"{'ABS': 0, 'INT': 1, 'RWK': 1, 'PDI': 2, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 3, 'TNF': 4, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 1, 'PNF': 3, 'REC': 0, 'EMP': 8, 'SUB': 2, 'CLA': 0}",0,1,1,2,0,9,1,3,4,0,0,0,0,0,0,1,0,1,3,0,8,2,0,0.5022421344396718,0.5601390719553845,0.3151770228755942
ICLR2018-Syr8Qc1CW-R2,Reject,"Summary: This paper investigated the problem of attribute-conditioned image generation using generative adversarial networks. More specifically, the paper proposed to generate images from attribute and latent code as high-level representation. To learn the mapping from image to high-level representations, an auxiliary encoder was introduced. The model was trained using a combination of reconstruction (auto-encoding) and adversarial loss. To further encourage effective disentangling (against trivial solution), an annihilating operation was proposed together with the proposed training pipeline. Experimental evaluations were conducted on standard face image databases such as Multi-PIE and CelebA.    Novelty and Significance    Multi-attribute image generation is an interesting task but has been explored to some extent. The integration of generative adversarial networks with auto-encoding loss is not really a novel contribution. -- Autoencoding beyond pixels using a learned similarity metric. Larsen et al., In ICML 2016.    Technical Quality     First, it is not clear how was the proposed annihilating operation used in the experiments (there is no explanation in the experimental section). Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution). I would appreciate the authors to elaborate this a bit. Second, the iterative training (section 3.4) is not a novel contribution since it was explored in the literature before (e.g., Inverse Graphics network). The proof developed in the paper provides some theoretical analysis but cannot be considered as a significant contribution. Third, it seems that the proposed multi-attribute generation pipeline works for binary attribute only. However, such assumption limits the generality of the work. Since the title is quite general, I would assume to see the results (1) on datasets with real-valued attributes, mixture attributes or even relative attributes and (2) not specific to face images. -- Learning to generate chairs with convolutional neural networks. Dosovitskiy et al., In CVPR 2015. -- Deep Convolutional Inverse Graphics Network. Kulkarni et al., In NIPS 2015. -- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016. -- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Chen et al., In NIPS 2016.Yan et al., In ECCV 2016. Additionally, considering the generation quality, the CelebA samples in the paper are not the state-of-the-art. I suspect the proposed method only works in a more constrained setting (such as Multi-PIE where the images are all well aligned). Overall, I feel that the submitted version is not ready for publication in the current form.  ",23,394,13.133333333333333,5.949333333333334,202,1,393,0.0025445292620865,0.036231884057971,-0.7598,126,65,63,20,10,6,"{'ABS': 0, 'INT': 2, 'RWK': 5, 'PDI': 4, 'DAT': 2, 'MET': 10, 'EXP': 3, 'RES': 2, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 2, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 1, 'CMP': 5, 'PNF': 0, 'REC': 1, 'EMP': 7, 'SUB': 2, 'CLA': 0}",0,2,5,4,2,10,3,2,0,1,0,1,2,0,0,2,1,5,0,1,7,2,0,0.7173393906550525,0.671018845597324,0.4978685260924044
ICLR2018-Syr8Qc1CW-R3,Reject,"Pros: 1. A new DNA structure GAN is utilized to manipulate/disentangle attributes. 2. Non attribute part (Z) is explicitly modeled in the framework. 3. Based on the experiment results, this proposed method outperformed previous methods (TD-GAN, IcGAN). Cons: 1. It assumes that each individual piece represents an independent factor of variation, which can not hold all the time. The authors also admit that when two factors are dependent, this method might fail. 2. In Lreconstruct, only min difference between A and A1 is considered. How about A and A2 here? It seems that A2 should also be similar with A since only one bit in A2 and A1 is different. 3. Only one attribute can be manipulated each time? Is it possible to change more than one attribute each time in this method?",10,133,9.5,5.111111111111111,83,2,131,0.015267175572519,0.037593984962406,-0.7041,33,13,24,8,4,2,"{'ABS': 0, 'INT': 0, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 9, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 0, 'CLA': 0}",0,0,1,0,0,9,1,1,0,0,0,0,0,0,0,0,0,1,0,0,10,0,0,0.287715822624563,0.2278198391358089,0.12802161542568313
ICLR2018-SyrGJYlRZ-R1,Reject,"This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm. Although the results are promising, I found the presentation of this paper almost inaccessible to me. First, though a minor point, but where does the name *YellowFin* come from? For the presentation, the motivation in introduction is fine, but the following section about momentum operator is hard to follow. There are a lot of undefined notation. For example, what does the *convergence rate* mean (what is the measurement for convergence)? And is the *optimal accelerated rate* the same as *convergence rate* mentioned above? Also, what do you mean by *all directions* in the sentence below eq.2? Then the paper talks about robustness properties of the momentum operator. But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative? In the following paragraph, what is *contraction*? Does it have anything to do with the paper as I didn't see it in the remaining text? Lemma 2 seems to use the spectral radius of the momentum operator as the *robustness*. But how can it describe the robustness? More details are needed to understand this. What it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter. I got confused in this section because eq.9 defines f(x) as a quadratic function. Is this f(x) the original function (non quadratic) or just the local quadratic approximation? If it is the local quadratic approximation, how is it correlated to the original function? It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation? If what I think is correct, I think it would be important to show this. Also, the objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning. Overall, I think this is an interesting paper, but the presentation is too fuzzy to get it evaluated.",26,376,31.33333333333333,4.905027932960894,155,7,369,0.018970189701897,0.0292553191489361,0.9752,104,53,70,12,7,4,"{'ABS': 0, 'INT': 3, 'RWK': 1, 'PDI': 0, 'DAT': 0, 'MET': 18, 'EXP': 2, 'RES': 3, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 3, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 9, 'REC': 0, 'EMP': 13, 'SUB': 2, 'CLA': 0}",0,3,1,0,0,18,2,3,0,1,0,3,0,0,0,0,0,1,9,0,13,2,0,0.5045927409861989,0.4525071615613082,0.28344085947054387
ICLR2018-SyrGJYlRZ-R2,Reject,"The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin). They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications. I found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading: Based on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence. There are two major problems with this approach: -First: Exploring 1-dim functions is indeed a nice way to get some intuition. Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions. -Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration. Concretely, the definition of the generalized condition number $ u$, and relating it to the standard definition of the condition number $kappa$, is very misleading. This is since $kappa  1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case. However, $ u$ might be much larger than 1 even in the 1-dim case. Regarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension). And as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions. Thus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed. The authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me. In contrast to the theoretical part, the experiments seems very encouraging. Showing YF to perform very well on several deep learning tasks without (or with very little) tuning. Again, this seems a bit magical or even too good to be truth. I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition. ",17,357,18.789473684210527,5.046376811594203,175,4,353,0.0113314447592068,0.0192837465564738,0.35,82,48,64,34,5,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 15, 'EXP': 5, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 15, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,15,5,1,0,0,0,0,0,0,0,0,0,0,0,0,15,1,0,0.3609903299026771,0.2309296263100238,0.1641034547776119
ICLR2018-SyrGJYlRZ-R3,Reject,"[Apologies for short review, I got called in late. Marking my review as educated guess since i didn't have time for a detailed review] The paper proposes an algorithm to tune the momentum and learning rate for SGD. While the algorithm does not have a theory for general non-quadratic functions, experimental validation is extensive, making it a worthy contribution in my opinion. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.",6,82,20.5,5.25,57,1,81,0.0123456790123456,0.024390243902439,0.4404,24,10,17,5,6,2,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 1, 'DAT': 0, 'MET': 3, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 2}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 2, 'SUB': 1, 'CLA': 0}",0,1,0,1,0,3,1,1,0,0,0,0,0,2,0,0,0,0,0,0,2,1,0,0.429081140867155,0.2228441796570651,0.192500735855882
ICLR2018-SysEexbRb-R1,Accept,"Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks. The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points. The paper is well organized and well written. Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points. This technique generalizes existing work under full relaxation of assumptions. In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices; In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices. Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work. The results obtained from the analytical forms of the critical points are interesting, but one problem is that how to obtain the proper solution of equation (3)? In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12). It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples.",12,250,27.77777777777778,5.530864197530864,118,2,248,0.0080645161290322,0.0039840637450199,0.4854,78,41,28,5,8,5,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 0, 'DAT': 2, 'MET': 6, 'EXP': 0, 'RES': 2, 'TNF': 0, 'ANA': 4, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 0, 'CLA': 1}",0,1,4,0,2,6,0,2,0,4,1,1,0,0,0,0,1,4,1,0,4,0,1,0.5730869464790985,0.5577729155316544,0.3668035050342564
ICLR2018-SysEexbRb-R2,Accept,"This paper studies the critical points of shallow and deep linear networks. The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima. Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices. I have not checked the proofs in detail but the general strategy seems sound. While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR. The authors also study the analytic form of critical points of a single-hidden layer ReLU network. However, given the form of the necessary and sufficient conditions the usefulness of of these results is less clear. Detailed comments:  - I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear. This paper is mostly about linear networks. While a result has been stated for single-hidden ReLU networks. In my view this particular result is an immediate corollary of the result for linear networks. As I explain further below given the combinatorial form of the result, the usefulness of this particular extension to ReLU network is not very clear. I would suggest rewording title/abstract/intro - Theorem 1 is neat, well done! - Page 4 p_i's in proposition 1 From my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly). - Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5 tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc? - Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5 tWould recommend a better exposition why these theorems are useful. What insights do you gain by knowing these theorems etc. Are less sufficient conditions that is more intuitive or useful(an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one). - Page 5 Theorem 2 tDoes this theorem have any computational implications? Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound? - Page 7 proposition 6 seems like an immediate consequence of Theorem 1 however given the combinatorial nature of the K_{I,J} it is not clear why this theorem is useful. e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm?  ",23,456,20.727272727272727,5.265700483091788,194,3,453,0.0066225165562913,0.0364025695931477,0.9958,132,62,67,22,10,4,"{'ABS': 2, 'INT': 3, 'RWK': 1, 'PDI': 1, 'DAT': 1, 'MET': 12, 'EXP': 2, 'RES': 8, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 1, 'NOV': 0, 'IMP': 0, 'CMP': 1, 'PNF': 1, 'REC': 0, 'EMP': 16, 'SUB': 0, 'CLA': 0}",2,3,1,1,1,12,2,8,0,1,0,1,0,0,1,0,0,1,1,0,16,0,0,0.7175664846953053,0.453773805967089,0.4007095462578983
ICLR2018-SysEexbRb-R3,Accept,"This paper mainly focuses on the square loss function of linear networks. It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks. Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks. As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed. In general, this manuscript is well written. Pros: 1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks. 2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn't require assumptions on parameter dimensions and data matrices. The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community. 3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation. Cons: 1. I'm concerned that the contribution of this manuscript is a little incremental. The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g. Hardt & Ma (2017) and Kawaguchi (2016). 2. Unlike one-hidden-layer linear networks, the characterizations of critical points for deep linear networks and deep ReLU networks seem to be hard to be interpreted. This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions. The behaviors of linear networks and practical (deep and nonlinear) networks are very different. Under such circumstance, the results about one-hidden-layer linear networks are less interesting to the deep learning community. Minors: There are some mixed-up notations: tilde{A_i}  > A_i , and rank(A_2)  > rank(A)_2 in Proposition 3.",16,323,14.681818181818182,5.738562091503268,137,1,322,0.0031055900621118,0.0151975683890577,-0.7655,95,63,41,14,8,7,"{'ABS': 0, 'INT': 1, 'RWK': 3, 'PDI': 0, 'DAT': 7, 'MET': 7, 'EXP': 0, 'RES': 3, 'TNF': 0, 'ANA': 3, 'FWK': 1, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 2, 'CMP': 4, 'PNF': 1, 'REC': 0, 'EMP': 4, 'SUB': 3, 'CLA': 1}",0,1,3,0,7,7,0,3,0,3,1,2,0,0,0,1,2,4,1,0,4,3,1,0.5735135057101103,0.7802565274189674,0.45342979247540965
ICLR2018-Syt0r4bRZ-R1,Reject,"Summary: the paper proposes a tree2tree architecture for NLP tasks. Both the encoder and decoder of this architecture make use of memory cells: the encoder looks like a tree-lstm to encode a tree bottom-up, the decoder generates a tree top-down by predicting the number of children first. The objective function is a linear mixture of the cost of generating the tree structure and the target sentence. The proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees, and outperforms an lstm seq2seq on En-Cn translation. Comment:  - The idea of tree2tree has been around recently but it is difficult to make it work. I thus appreciate the authors' effort. However, I wish the authors would have done it more properly. - The computation of the encoder and decoder is not novel. I was wondering how the encoder differs from tree-lstm. The decoder predicts the number of children first, but the authors don't explain why they do that, nor compare this to existing tree generators. - I don't understand the objective function (eq 4 and 5). Both Ls are not cross-entropy because label and childnum are not probabilities. I also don't see why using Adam is more convenient than using SGD. - I think eq 9 is incorrect, because the decoder is not Markovian. To see this we can look at recurrent neural networks for language modeling: generating the current word is conditioning on the whole history (not only the previous word). - I expect the authors would explain more about how difficult the tasks are (eg. some statistics about the datasets), how to choose values for lambda, what the contribution of the new objective is. About writing: - the paper has so many problems with wording, e.g. articles, plurality. - many terms are incorrect, e.g. ""dependent parsing tree"" (should be ""dependency tree""), ""consistency parsing"" (should be ""constituency parsing"") - In 3.1, Socher et al. do not use lstm - I suggest the authors to do some more literature review on tree generation ",20,323,14.681818181818182,5.1245901639344265,167,2,321,0.0062305295950155,0.0239520958083832,-0.0403,92,41,60,22,9,5,"{'ABS': 0, 'INT': 1, 'RWK': 2, 'PDI': 0, 'DAT': 1, 'MET': 15, 'EXP': 1, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 1, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 10, 'SUB': 2, 'CLA': 2}",0,1,2,0,1,15,1,1,0,1,0,2,0,1,0,1,0,2,0,0,10,2,2,0.6464857391605626,0.5614581466055077,0.40709932206797983
ICLR2018-Syt0r4bRZ-R2,Reject,"This paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tree. The contributions of the work are very limited. Basic attention models, which have been shown to help model structures, are not included (or compared). Method-wise, the encoder is not novel and decoder is rather straightforward. The contributions of the work are in general very limited. Moreover, this manuscript contains many grammatical errors. In general, it is not ready for publication. Pros: - Investigating the ability of distributed representation in encoding input structured is in general interesting. Although there have been much previous work, this paper is along this line. Cons: - The contributions of the work are very limited. For example, attention, which have been widely used and been shown to help capture structures in many tasks, are not included and compared in this paper. - Evaluation is not very convincing. The baseline performance in MT is too low. It is unclear if the proposed model is still helpful when other components are considered (e.g., attention). - For the objective function defined in the paper, it may be hard to balance the structure loss and content loss in different problems, and moreover, the loss function may not be even useful in real tasks (e.g, in MT), which often have their own objectives (as discussed in this paper). Earlier work on tree kernels (in terms of defining tree distances) may be related to this work. - The manuscript is full of grammatical errors, and the following are some of them: encoder only only need to For for tree reconstruction task The Socher et al. (2011b) propose a basic form  experiments and theroy analysis are done ",19,281,15.61111111111111,4.949458483754513,139,4,277,0.0144404332129963,0.0208333333333333,-0.9454,73,35,58,22,9,6,"{'ABS': 0, 'INT': 1, 'RWK': 4, 'PDI': 1, 'DAT': 0, 'MET': 7, 'EXP': 3, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 6, 'BIB': 0, 'EXT': 1}","{'APR': 1, 'NOV': 1, 'IMP': 0, 'CMP': 3, 'PNF': 0, 'REC': 0, 'EMP': 7, 'SUB': 5, 'CLA': 2}",0,1,4,1,0,7,3,1,0,1,0,6,0,1,1,1,0,3,0,0,7,5,2,0.6449992448706731,0.6711405774942444,0.4546629568189317
ICLR2018-Syt0r4bRZ-R3,Reject,"This paper presents a model to encode and decode trees in distributed representations. This is not the first attempt of doing these encoders and decoders. However, there is not a comparative evalution with these methods. In fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see Decoding Distributed Tree Structures and Distributed tree kernels. The paper should present a comparison with such kinds of models. ",5,76,12.666666666666666,5.561643835616438,49,0,76,0.0,0.0389610389610389,0.0,19,8,18,3,2,3,"{'ABS': 0, 'INT': 0, 'RWK': 3, 'PDI': 0, 'DAT': 0, 'MET': 4, 'EXP': 0, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 0, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 2, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 0, 'EMP': 0, 'SUB': 1, 'CLA': 0}",0,0,3,0,0,4,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,1,0,0.1437686950032648,0.3334956034750414,0.07100428930279998
